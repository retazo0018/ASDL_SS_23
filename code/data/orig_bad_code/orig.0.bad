def visualize_attention ( self , img_data , attentions , output , label , flag_incorrect ) : <NEWLINE> <INDENT> if flag_incorrect : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> filename = label . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) <NEWLINE> if len ( filename ) == 0 : <NEWLINE> <INDENT> filename = <STRING> <NEWLINE> <DEDENT> output_dir = os . path . join ( output_dir , filename ) <NEWLINE> if not os . path . exists ( output_dir ) : <NEWLINE> <INDENT> os . makedirs ( output_dir ) <NEWLINE> <DEDENT> with open ( os . path . join ( output_dir , <STRING> ) , <STRING> ) as fword : <NEWLINE> <INDENT> fword . write ( output + <STRING> ) <NEWLINE> fword . write ( label ) <NEWLINE> file_img_data = BytesIO ( img_data ) <NEWLINE> img = Image . open ( file_img_data ) <NEWLINE> w , h = img . size <NEWLINE> mh = 32 <NEWLINE> mw = math . floor ( 1. * w / h * mh ) <NEWLINE> img = img . resize ( <NEWLINE> <INDENT> ( mw , h ) , <NEWLINE> Image . ANTIALIAS ) <NEWLINE> <DEDENT> img_data = np . asarray ( img , dtype = np . uint8 ) <NEWLINE> for idx in xrange ( len ( output ) ) : <NEWLINE> <INDENT> output_filename = os . path . join ( output_dir , <STRING> % ( idx ) ) <NEWLINE> attention = attentions [ idx ] [ : ( int ( mw / 4 ) - 1 ) ] <NEWLINE> attention_orig = np . zeros ( mw ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> if i / 4 - 1 > 0 and i / 4 - 1 < len ( attention ) : <NEWLINE> <INDENT> attention_orig [ i ] = attention [ int ( i / 4 ) - 1 ] <NEWLINE> <DEDENT> <DEDENT> attention_orig = np . convolve ( attention_orig , [ 0.199547 , 0.200226 , 0.200454 , 0.200226 , 0.199547 ] , mode = <STRING> ) <NEWLINE> attention_orig = np . maximum ( attention_orig , 0.3 ) <NEWLINE> attention_out = np . zeros ( ( h , mw ) ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> attention_out [ : , i ] = attention_orig [ i ] <NEWLINE> <DEDENT> if len ( img_data . shape ) == 3 : <NEWLINE> <INDENT> attention_out = attention_out [ : , : , np . newaxis ] <NEWLINE> <DEDENT> img_out_data = img_data * attention_out <NEWLINE> img_out = Image . fromarray ( img_out_data . astype ( np . uint8 ) ) <NEWLINE> img_out . save ( output_filename ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def visualize_attention ( self , img_data , attentions , output , label , flag_incorrect ) : <NEWLINE> <INDENT> if flag_incorrect : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> filename = label . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) <NEWLINE> if len ( filename ) == 0 : <NEWLINE> <INDENT> filename = <STRING> <NEWLINE> <DEDENT> output_dir = os . path . join ( output_dir , filename ) <NEWLINE> if not os . path . exists ( output_dir ) : <NEWLINE> <INDENT> os . makedirs ( output_dir ) <NEWLINE> <DEDENT> with open ( os . path . join ( output_dir , <STRING> ) , <STRING> ) as fword : <NEWLINE> <INDENT> fword . write ( output + <STRING> ) <NEWLINE> fword . write ( label ) <NEWLINE> file_img_data = BytesIO ( img_data ) <NEWLINE> img = Image . open ( file_img_data ) <NEWLINE> w , h = img . size <NEWLINE> mh = 32 <NEWLINE> mw = math . floor ( 1. * w / h * mh ) <NEWLINE> img = img . resize ( <NEWLINE> <INDENT> ( mw , mh ) , <NEWLINE> Image . ANTIALIAS ) <NEWLINE> <DEDENT> img_data = np . asarray ( img , dtype = np . uint8 ) <NEWLINE> for idx in xrange ( len ( output ) ) : <NEWLINE> <INDENT> output_filename = os . path . join ( output_dir , <STRING> % ( idx ) ) <NEWLINE> attention = attentions [ idx ] [ : ( int ( mw / 4 ) - 1 ) ] <NEWLINE> attention_orig = np . zeros ( mw ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> if i / 4 - 1 > 0 and i / 4 - 1 < len ( attention ) : <NEWLINE> <INDENT> attention_orig [ i ] = attention [ int ( i / 4 ) - 1 ] <NEWLINE> <DEDENT> <DEDENT> attention_orig = np . convolve ( attention_orig , [ 0.199547 , 0.200226 , 0.200454 , 0.200226 , 0.199547 ] , mode = <STRING> ) <NEWLINE> attention_orig = np . maximum ( attention_orig , 0.3 ) <NEWLINE> attention_out = np . zeros ( ( h , mw ) ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> attention_out [ : , i ] = attention_orig [ i ] <NEWLINE> <DEDENT> if len ( img_data . shape ) == 3 : <NEWLINE> <INDENT> attention_out = attention_out [ : , : , np . newaxis ] <NEWLINE> <DEDENT> img_out_data = img_data * attention_out <NEWLINE> img_out = Image . fromarray ( img_out_data . astype ( np . uint8 ) ) <NEWLINE> img_out . save ( output_filename ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
self . decoder_inputs = [ ] <NEWLINE> <INDENT> self . target_weights = [ ] <NEWLINE> for i in xrange ( self . decoder_size + 1 ) : <NEWLINE> <INDENT> self . decoder_inputs . append ( <NEWLINE> <INDENT> tf . tile ( [ 0 ] , [ num_images ] ) <NEWLINE> <DEDENT> ) <NEWLINE> if i < self . decoder_size : <NEWLINE> <INDENT> self . target_weights . append ( tf . tile ( [ 1. ] , [ num_images ] ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . target_weights . append ( tf . tile ( [ 0. ] , [ num_images ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
timestruct = datetime ( year , month , day , hour - hourdiff ) . isoformat ( ) + <STRING> <NEWLINE> <INDENT> data [ <STRING> ] [ <STRING> ] = timestruct <NEWLINE> data [ <STRING> ] [ <STRING> ] = hourduration * 3600 <NEWLINE> data [ <STRING> ] [ <STRING> ] = projectid <NEWLINE> data [ <STRING> ] [ <STRING> ] = <STRING> <NEWLINE> data [ <STRING> ] [ <STRING> ] = billable <NEWLINE> <DEDENT>
def extract ( self , sourcepc , neighborhood , targetpc , targetindex , volume ) : <NEWLINE> <INDENT> t2a , t2c = utils . get_features ( targetpc , targetindex , self . requires ( ) ) <NEWLINE> x , y , z = utils . get_point ( targetpc , targetindex ) <NEWLINE> return t2c - t2a - z <COMMENT> <NEWLINE> <DEDENT>
def test_GetPointCloudPointFeatures ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> pc = test_tools . generate_test_point_cloud ( ) <NEWLINE> cols = 0.5 * ( pc [ keys . point ] [ <STRING> ] [ <STRING> ] + pc [ keys . point ] [ <STRING> ] [ <STRING> ] ) <NEWLINE> flavs = 0.5 * ( pc [ keys . point ] [ <STRING> ] [ <STRING> ] - pc [ keys . point ] [ <STRING> ] [ <STRING> ] ) <NEWLINE> pc [ keys . point ] [ <STRING> ] = { <STRING> : <STRING> , <STRING> : cols } <NEWLINE> pc [ keys . point ] [ <STRING> ] = { <STRING> : <STRING> , <STRING> : flavs } <NEWLINE> x , y , z = utils . get_point ( pc , 2 ) <NEWLINE> c , f = utils . get_features ( pc , 2 , ( <STRING> , <STRING> ) ) <NEWLINE> self . assertEqual ( c , 0.5 * ( x + y ) ) <NEWLINE> self . assertEqual ( f , 0.5 * ( x - y ) ) <NEWLINE> <DEDENT>
self . assertEqual ( <STRING> , <NEWLINE> <INDENT> target_point_cloud [ keys . provenance ] [ 0 ] [ <STRING> ] ) <NEWLINE> <DEDENT>
self . assertEqual ( <STRING> , <NEWLINE> <INDENT> target_point_cloud [ keys . provenance ] [ 1 ] [ <STRING> ] ) <NEWLINE> <DEDENT>
if end == sys . maxsize : <NEWLINE> <INDENT> return self . _data [ self . _lb + start : self . _ub ] <NEWLINE> elif self . _lb + end >= self . _ub : <NEWLINE> <INDENT> raise IndexError ( ) <NEWLINE> else : <NEWLINE> <DEDENT> return self . _data [ self . _lb + start : self . _lb + end ] <NEWLINE> <DEDENT>
if ( Energy_needed_from_battery > 0 ) : <NEWLINE> <COMMENT> <NL> <INDENT> if ( self . _lastPonctualObservation [ 0 ] * self . battery_size > Energy_needed_from_battery ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . _lastPonctualObservation [ 0 ] = self . _lastPonctualObservation [ 0 ] - Energy_needed_from_battery / self . battery_size * self . battery_eta <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> reward -= ( Energy_needed_from_battery - self . _lastPonctualObservation [ 0 ] * self . battery_size ) * 2 <COMMENT> <NEWLINE> self . _lastPonctualObservation [ 0 ] = 0 <NEWLINE> elif ( Energy_needed_from_battery < 0 ) : <NEWLINE> <COMMENT> <NL> <DEDENT> self . _lastPonctualObservation [ 0 ] = min ( 1. , self . _lastPonctualObservation [ 0 ] - Energy_needed_from_battery / self . battery_size * self . battery_eta ) <NEWLINE> <DEDENT>
@ viz_reg_test <NEWLINE> <INDENT> def test_boxplot_melted ( ) : <NEWLINE> <INDENT> return ar . boxplot ( data . iris ( ) , <STRING> , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def readrequest ( ) : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> idx = buf . find ( <STRING> , pos [ 0 ] ) <NEWLINE> if idx >= 0 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> readmore ( ) <NEWLINE> <DEDENT> head = buf [ pos [ 0 ] : idx ] <NEWLINE> pos [ 0 ] = idx + 4 <NEWLINE> lines = iter ( head . decode ( <STRING> ) . split ( <STRING> ) ) <NEWLINE> status = next ( lines ) <NEWLINE> headers = { } <NEWLINE> last_header = None <NEWLINE> for line in lines : <NEWLINE> <INDENT> if line . startswith ( ( <STRING> , <STRING> ) ) : <NEWLINE> <INDENT> if last_header is not None : <NEWLINE> <INDENT> headers [ last_header ] += line <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise EOFError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif <STRING> in line : <NEWLINE> <INDENT> k , v = line . split ( <STRING> , 1 ) <NEWLINE> k = k . strip ( ) <NEWLINE> if k in headers : <NEWLINE> <INDENT> headers [ k ] += <STRING> + v . strip ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> headers [ k ] = v . strip ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise EOFError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> clen = int ( headers . get ( <STRING> , <STRING> ) ) <NEWLINE> if clen < 0 : <NEWLINE> <INDENT> raise EOFError ( <STRING> ) <NEWLINE> <DEDENT> while pos [ 0 ] + clen < len ( buf ) : <NEWLINE> <INDENT> readmore ( ) <NEWLINE> <DEDENT> return status , headers , buf [ pos [ 0 ] : pos [ 0 ] + clen ] <NEWLINE> <DEDENT>
with pytest . raises ( ValueError ) as errinfo : <NEWLINE> <INDENT> bb . io . save ( parser , df_anno_simple , <STRING> ) <NEWLINE> assert <STRING> in str ( errinfo . value ) <NEWLINE> <DEDENT>
def hex_ranges ( h3_address_list , ring_size ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> num_hexagons = len ( h3_address_list ) <NEWLINE> array_len = num_hexagons * libh3 . maxKringSize ( ring_size ) <NEWLINE> HexArray = c_long * num_hexagons <NEWLINE> KringArray = c_long * array_len <NEWLINE> <COMMENT> <NL> hex_array = HexArray ( <NEWLINE> <INDENT> * [ string_to_h3 ( h3_address ) for h3_address in h3_address_list ] ) <NEWLINE> <DEDENT> krings = KringArray ( ) <NEWLINE> success = libh3 . hexRanges ( <NEWLINE> <INDENT> hex_array , <NEWLINE> num_hexagons , <NEWLINE> ring_size , <NEWLINE> krings , <NEWLINE> <DEDENT> ) <NEWLINE> if success != 0 : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> out = { } <NEWLINE> for i in range ( 0 , num_hexagons ) : <NEWLINE> <INDENT> h3_address = h3_address_list [ i ] <NEWLINE> hex_range_list = [ ] <NEWLINE> out [ h3_address ] = hex_range_list <NEWLINE> for j in range ( 0 , ring_size + 1 ) : <NEWLINE> <INDENT> hex_range_list . append ( set ( [ ] ) ) <NEWLINE> <DEDENT> for j in range ( 0 , int ( array_len / num_hexagons ) ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> ring_index = int ( <NEWLINE> <INDENT> math . floor ( <NEWLINE> <INDENT> ( 1 + math . sqrt ( 1 + 8 * math . ceil ( j / 6.0 ) ) ) / 2 ) ) - 1 <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> hex_range_list [ ring_index ] . add ( <NEWLINE> <INDENT> h3_to_string ( krings [ i * num_hexagons + j ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return out <NEWLINE> <DEDENT>
with closing ( os . fdopen ( os . open ( log_technical_terms_to_path , <NEWLINE> <INDENT> os . O_RDWR | os . O_CREAT ) , <NEWLINE> <STRING> ) ) as terms_file : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> terms = set ( terms_file . read ( ) . splitlines ( ) ) <COMMENT> <NEWLINE> terms_file . seek ( 0 ) <COMMENT> <NEWLINE> terms_file . truncate ( 0 ) <COMMENT> <NEWLINE> tech_terms = freduce ( lambda x , y : x + y , <NEWLINE> _drain ( log_technical_terms_to_queue ) ) <NEWLINE> terms_file . write ( <STRING> . join ( list ( terms | <COMMENT> <NEWLINE> <INDENT> set ( tech_terms ) ) ) ) <NEWLINE> <DEDENT> <DEDENT>
def intersect ( obj1 , obj2 ) : <NEWLINE> <INDENT> if not ( isinstance ( obj1 , Vector ) or isinstance ( obj2 , Vector ) ) : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> obj1 . reproject ( obj2 . srs ) <NEWLINE> <DEDENT>
if pattern : <NEWLINE> <INDENT> tests = [ f for f in docfiles if f . find ( pattern ) >= 0 ] <NEWLINE> else : <NEWLINE> tests = docfiles <NEWLINE> <DEDENT>
def halton ( base ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def value ( index ) : <NEWLINE> <INDENT> result = 0.0 <NEWLINE> f = 1.0 / base <NEWLINE> i = index <NEWLINE> while i > 0 : <NEWLINE> <INDENT> result += f * ( i % base ) <NEWLINE> i = i / base <NEWLINE> f = f / base <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT> i = 1 <NEWLINE> while i > 0 : <NEWLINE> <INDENT> yield value ( i ) <NEWLINE> i += 1 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> open_kwds = { } <NEWLINE> if sys . version_info > ( 3 , ) : <NEWLINE> <INDENT> open_kwds [ <STRING> ] = <STRING> <NEWLINE> <DEDENT> <DEDENT>
class TaggedItemManager ( models . Manager ) : <NEWLINE> <INDENT> def get_by_model ( self , Model , tags ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> tags = get_tag_list ( tags ) <NEWLINE> if len ( tags ) == 0 : <NEWLINE> <INDENT> tag = tags [ 0 ] <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . get_intersection_by_model ( Model , tags ) <NEWLINE> <DEDENT> ctype = ContentType . objects . get_for_model ( Model ) <NEWLINE> rel_table = backend . quote_name ( self . model . _meta . db_table ) <NEWLINE> return Model . objects . extra ( <NEWLINE> <INDENT> tables = [ self . model . _meta . db_table ] , <COMMENT> <NEWLINE> where = [ <NEWLINE> <INDENT> <STRING> % rel_table , <NEWLINE> <STRING> % rel_table , <NEWLINE> <STRING> % ( backend . quote_name ( Model . _meta . db_table ) , <NEWLINE> <INDENT> backend . quote_name ( Model . _meta . pk . column ) , <NEWLINE> rel_table ) <NEWLINE> <DEDENT> <DEDENT> ] , <NEWLINE> params = [ ctype . id , tag . id ] , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
class TaggedItemManager ( models . Manager ) : <NEWLINE> <INDENT> def get_by_model ( self , Model , tags ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> tags = get_tag_list ( tags ) <NEWLINE> if len ( tags ) == 0 : <NEWLINE> <INDENT> tag = tags [ 0 ] <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . get_intersection_by_model ( Model , tags ) <NEWLINE> <DEDENT> ctype = ContentType . objects . get_for_model ( Model ) <NEWLINE> rel_table = backend . quote_name ( self . model . _meta . db_table ) <NEWLINE> return Model . objects . extra ( <NEWLINE> <INDENT> tables = [ self . model . _meta . db_table ] , <COMMENT> <NEWLINE> where = [ <NEWLINE> <INDENT> <STRING> % rel_table , <NEWLINE> <STRING> % rel_table , <NEWLINE> <STRING> % ( backend . quote_name ( Model . _meta . db_table ) , <NEWLINE> <INDENT> backend . quote_name ( Model . _meta . pk . column ) , <NEWLINE> rel_table ) <NEWLINE> <DEDENT> <DEDENT> ] , <NEWLINE> params = [ ctype . id , tag . id ] , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
if data is None : <NEWLINE> <INDENT> if exc is not None : <NEWLINE> <INDENT> raise exc ( criteria ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def upload ( self ) : <NEWLINE> <INDENT> self . fitting = 0 <NEWLINE> self . save ( ) <NEWLINE> lore . io . upload ( self . remote_model_path ( ) , self . model_path ( ) ) <NEWLINE> <DEDENT>
def upload ( self ) : <NEWLINE> <INDENT> super ( Base , self ) . upload ( ) <NEWLINE> lore . io . upload ( self . remote_weights_path ( ) , self . weights_path ( ) ) <NEWLINE> <DEDENT>
if self . towers > 1 : <NEWLINE> <INDENT> result = numpy . mean ( result , axis = 0 ) . squeeze ( axis = 0 ) <NEWLINE> <DEDENT>
if cluster not in clusters : <NEWLINE> <COMMENT> <NL> <INDENT> clusters . append ( cluster ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if columns not in columns : <NEWLINE> columns . append ( column ) <NEWLINE> <DEDENT>
def adjust_cluster ( self , cluster , rowscores , cutoff , limit ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def max_row_in_column ( matrix , column ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sm = matrix . submatrix_by_name ( wh , [ matrix . column_names [ column ] ] ) <NEWLINE> sm_values = sm . values <NEWLINE> max_row = 0 <NEWLINE> max_score = sys . float_info . min <NEWLINE> for row in range ( sm . num_rows ( ) ) : <NEWLINE> <INDENT> if sm_values [ row ] [ 0 ] > max_score : <NEWLINE> <INDENT> max_score = sm [ row ] [ 0 ] <NEWLINE> max_row = row <NEWLINE> <DEDENT> <DEDENT> return sm . row_names [ max_row ] <NEWLINE> <DEDENT> <DEDENT>
def setUp ( self ) : <COMMENT> <NEWLINE> <INDENT> <STRING> <NEWLINE> if not os . path . exists ( <STRING> ) : <NEWLINE> <INDENT> os . mkdir ( <STRING> ) <NEWLINE> <DEDENT> self . service = mo . MicrobesOnline ( mo . MICROBES_ONLINE_BASE_URL , <STRING> ) <NEWLINE> <DEDENT>
h , m , s = convert_elapsed_time ( training_time ) <NEWLINE> <INDENT> logger . info ( <STRING> <NEWLINE> <INDENT> . format ( h , m , s ) ) <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> logger . info ( outputs ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> logger . info ( targets ) <NEWLINE> <DEDENT>
if point_xy : <NEWLINE> <INDENT> x_point = self . workspace . get_content_point ( x_uuid , point_xy ) <NEWLINE> format_str , unit_str , x_point = self . document . convert_units ( x_uuid , x_point ) <NEWLINE> y_point = self . workspace . get_content_point ( y_uuid , point_xy ) <NEWLINE> format_str , unit_str , y_point = self . document . convert_units ( x_uuid , y_point ) <NEWLINE> else : <NEWLINE> x_point = None <NEWLINE> y_point = None <NEWLINE> <DEDENT>
asset_map [ <STRING> ] = host <NEWLINE>
rec = params . record_cache [ record . record_uid ] <NEWLINE> <INDENT> data . update ( json . loads ( rec [ <STRING> ] . decode ( ) ) ) <NEWLINE> if <STRING> in rec : <NEWLINE> <INDENT> extra . update ( json . loads ( rec [ <STRING> ] . decode ( ) ) ) <NEWLINE> <DEDENT> if <STRING> in rec : <NEWLINE> <INDENT> udata . update ( rec [ <STRING> ] ) <NEWLINE> <DEDENT> unencrypted_key = rec [ <STRING> ] <NEWLINE> record_object [ <STRING> ] = rec [ <STRING> ] <NEWLINE> if rec . get ( <STRING> ) : <NEWLINE> <INDENT> if params . debug : print ( <STRING> ) <NEWLINE> record_object [ <STRING> ] = encrypt_aes ( params . data_key , unencrypted_key ) <NEWLINE> else : <NEWLINE> <DEDENT> if params . debug : print ( <STRING> ) <NEWLINE> unencrypted_key = os . urandom ( 32 ) <NEWLINE> record_object [ <STRING> ] = encrypt_aes ( params . data_key , unencrypted_key ) <NEWLINE> record_object [ <STRING> ] = 0 <NEWLINE> <DEDENT>
svg_text = game . get_svg_str ( ) <NEWLINE> <INDENT> html_text = HTML_WRAPPER . format ( title = game_id , filename = html_filename ) <NEWLINE> <DEDENT>
def test_get_fee ( ) : <NEWLINE> <INDENT> assert get_fee ( fast = True ) != get_fee ( fast = False ) <NEWLINE> <DEDENT>
org_info = wa . organisms . show_organism ( org_id ) <NEWLINE> <INDENT> if <STRING> in org_info : <NEWLINE> <INDENT> time . sleep ( 1 ) <NEWLINE> org_info = wa . organisms . show_organism ( org_id ) <NEWLINE> <DEDENT> <DEDENT>
class TestFK ( unittest . TestCase ) : <NEWLINE> <INDENT> def setUp ( self ) : <NEWLINE> <INDENT> self . creature = creature . creature ( creature_type = creature_type ) <NEWLINE> all_zeros = [ 0 for i in range ( 0 , self . creature . config . joints_number ) ] <NEWLINE> one_move = [ 0 for i in range ( 0 , self . creature . config . joints_number ) ] <NEWLINE> one_move [ 5 ] = np . pi / 4 <NEWLINE> one_move [ 6 ] = - np . pi / 2 <NEWLINE> one_move [ 4 ] = - np . pi / 2 <NEWLINE> self . test_pos = one_move <NEWLINE> <DEDENT> <DEDENT>
def run ( self ) : <NEWLINE> <INDENT> self . running = True <NEWLINE> while self . running : <NEWLINE> <INDENT> if len ( self . timers ) > 0 : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . _wait ( self . timers [ 0 ] . next_fire_time ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> self . _error ( e ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> self . assertEqual ( pmml_obj . steps [ - 1 ] [ - 1 ] . n_neighbors , pmml_obj . NearestNeighborModel [ 0 ] . numberOfNeighbors ) <NEWLINE> <DEDENT>
self . start_time = self . distances . arrival + self . config . offsets [ 0 ] <NEWLINE> <INDENT> self . end_time = self . distances . arrival + self . config . offsets [ 1 ] <NEWLINE> <DEDENT>
if preview_path : <NEWLINE> <INDENT> mimetype = magic . from_file ( src_path , mime = True ) . lower ( ) <NEWLINE> if mimetype in [ ExportMimeType . PNG , ExportMimeType . PDF ] : <NEWLINE> <INDENT> return preview_path <NEWLINE> <DEDENT> <DEDENT>
if RuleMods . EXPAND in mods or RuleMods . EXPAND1 in mods : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> def p_rule ( self , p ) : <NEWLINE> <INDENT> subtree = [ ] <NEWLINE> for child in p [ 1 : ] : <NEWLINE> <INDENT> if isinstance ( child , self . tree_class ) and child . head in self . rules_to_expand : <NEWLINE> <INDENT> subtree . extend ( child . tail ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> subtree . append ( child ) <NEWLINE> <DEDENT> <DEDENT> p [ 0 ] = self . tree_class ( rule_name , subtree , skip_adjustments = True ) if len ( subtree ) > 1 else subtree [ 0 ] <NEWLINE> else : <NEWLINE> <DEDENT> def p_rule ( self , p ) : <NEWLINE> <INDENT> p [ 0 ] = self . tree_class ( rule_name , p [ 1 : ] , skip_adjustments = True ) <NEWLINE> p_rule . __doc__ = rule_def <NEWLINE> setattr ( self , <STRING> % ( rule_name , ) , types . MethodType ( p_rule , self ) ) <NEWLINE> <DEDENT> <DEDENT>
def colorStr ( text , color = WHITE ) : <NEWLINE> <INDENT> if has_colors : <NEWLINE> <INDENT> seq = <STRING> % ( 30 + color ) + text + <STRING> <NEWLINE> return seq <NEWLINE> sys . stdout . write ( seq + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return seq <NEWLINE> sys . stdout . write ( text + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
( type , len ) = ofp_action_header ( message , offset ) <NEWLINE> <INDENT> field = message [ cursor . offset : offset + len ] <NEWLINE> cursor . offset = offset + len <NEWLINE> return namedtuple ( <STRING> , <NEWLINE> <INDENT> <STRING> ) ( type , len , field ) <NEWLINE> <DEDENT> <DEDENT>
def _align ( length ) : <NEWLINE> <INDENT> return ( length + 7 ) / 8 * 8 <NEWLINE> <DEDENT>
def _find_completion ( fuser , relation ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for fuser_relation in fuser . fusion_graph . get_relations ( relation . row_type , <NEWLINE> <INDENT> relation . col_type ) : <NEWLINE> if fuser_relation . _id == relation . _id : <NEWLINE> return fuser . complete ( fuser_relation ) <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>
assert isinstance ( ams_netid , SAmsNetId ) <NEWLINE>
in_channels = convolutions [ 0 ] [ 0 ] <NEWLINE> <INDENT> self . fc1 = Linear ( embed_dim , in_channels , dropout = dropout ) <NEWLINE> self . projections = nn . ModuleList ( ) <NEWLINE> self . convolutions = nn . ModuleList ( ) <NEWLINE> for ( out_channels , kernel_size ) in convolutions : <NEWLINE> <INDENT> pad = ( kernel_size - 1 ) // 2 <NEWLINE> self . projections . append ( Linear ( in_channels , out_channels ) <NEWLINE> <INDENT> if in_channels != out_channels else None ) <NEWLINE> <DEDENT> self . convolutions . append ( <NEWLINE> <INDENT> ConvTBC ( in_channels , out_channels * 2 , kernel_size , padding = pad , <NEWLINE> <INDENT> dropout = dropout ) ) <NEWLINE> <DEDENT> <DEDENT> in_channels = out_channels <NEWLINE> <DEDENT> self . fc2 = Linear ( in_channels , embed_dim ) <NEWLINE> <DEDENT>
def forward ( self , input , incremental_state = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> bsz , seq_len = input . size ( ) <NEWLINE> max_pos = self . padding_idx + 1 + seq_len <NEWLINE> if seq_len > self . weights . size ( 0 ) : <NEWLINE> <INDENT> self . weights = SinusoidalPositionalEmbedding . get_embedding ( <NEWLINE> <INDENT> max_pos , <NEWLINE> self . embedding_dim , <NEWLINE> self . padding_idx , <NEWLINE> <DEDENT> ) . type_as ( self . weights ) <NEWLINE> <DEDENT> weights = Variable ( self . weights ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> max_epoch = args . max_epoch or math . inf <NEWLINE> max_update = args . max_update or math . inf <NEWLINE> lr = trainer . get_lr ( ) <NEWLINE> train_meter = StopwatchMeter ( ) <NEWLINE> train_meter . start ( ) <NEWLINE> valid_losses = [ None ] <NEWLINE> valid_subsets = args . valid_subset . split ( <STRING> ) <NEWLINE> while lr > args . min_lr and epoch_itr . epoch <= max_epoch and trainer . get_num_updates ( ) < max_update : <NEWLINE> <COMMENT> <NL> <INDENT> train ( args , trainer , task , epoch_itr ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> max_epoch = args . max_epoch or math . inf <NEWLINE> max_update = args . max_update or math . inf <NEWLINE> lr = trainer . get_lr ( ) <NEWLINE> train_meter = StopwatchMeter ( ) <NEWLINE> train_meter . start ( ) <NEWLINE> valid_losses = [ None ] <NEWLINE> valid_subsets = args . valid_subset . split ( <STRING> ) <NEWLINE> while lr > args . min_lr and epoch_itr . epoch <= max_epoch and trainer . get_num_updates ( ) < max_update : <NEWLINE> <COMMENT> <NL> <INDENT> train ( args , trainer , task , epoch_itr ) <NEWLINE> <DEDENT> <DEDENT>
sample_size = sample [ <STRING> ] . size ( 0 ) if self . args . sentence_avg else sample [ <STRING> ] <NEWLINE> <INDENT> logging_output = { <NEWLINE> <INDENT> <STRING> : utils . item ( loss . data ) if reduce else loss . data , <NEWLINE> <STRING> : utils . item ( nll_loss . data ) if reduce else loss . data , <NEWLINE> <STRING> : sample [ <STRING> ] , <NEWLINE> <STRING> : sample_size , <NEWLINE> <DEDENT> } <NEWLINE> return loss , sample_size , logging_output <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> attn = torch . rand ( bbsz , src_len , tgt_len ) <NEWLINE> <DEDENT>
def step_update ( self , num_updates ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if num_updates < self . args . warmup_updates : <NEWLINE> <INDENT> self . lr = self . args . warmup_init_lr + num_updates * self . lr_step <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> curr_updates = num_updates - self . args . warmup_updates <NEWLINE> if self . t_mult != 1 : <NEWLINE> <INDENT> i = math . floor ( math . log ( 1 - curr_updates / self . period * ( 1 - self . t_mult ) , self . t_mult ) ) <NEWLINE> t_i = self . t_mult ** i * self . period <NEWLINE> t_curr = curr_updates - ( 1 - self . t_mult ** i ) / ( 1 - self . t_mult ) * self . period <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> i = math . floor ( curr_updates / self . period ) <NEWLINE> t_i = self . period <NEWLINE> t_curr = num_updates - ( self . period * i ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def __get_safe_conn ( self , retry_count ) : <NEWLINE> <INDENT> self . current_size += 1 <NEWLINE> c = self . unuse_list . pop ( ) <NEWLINE> if self . ping_check : <NEWLINE> <INDENT> now = int ( time ( ) ) <NEWLINE> timeout = now <NEWLINE> if isinstance ( int , self . ping_check ) : <NEWLINE> <INDENT> timeout = timeout - self . ping_check <NEWLINE> <DEDENT> if not hasattr ( c , <STRING> ) : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> if c . __ping_check_timestamp < timeout : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> c . ping ( ) <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> self . current_size -= 1 <NEWLINE> if retry_count < 10 : c = self . __get_conn ( retry_count + 1 ) <NEWLINE> <DEDENT> <DEDENT> if c : self . inuse_list . add ( c ) <NEWLINE> return c <NEWLINE> <DEDENT>
def copy ( self , new_path , replace = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if replace or get_file ( new_path ) . exists ( ) : <NEWLINE> <INDENT> self . key . copy ( self . key . bucket , new_path ) <NEWLINE> return True <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
if <STRING> in os . environ : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> elif self . charm_name : <NEWLINE> if charm_name == self . charm_name : <NEWLINE> <INDENT> charm = os . getcwd ( ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if not has_perf_mod : <NEWLINE> <INDENT> _set_cache_ = _set_cache_brute_ <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> _set_cache_ = _set_cache_too_slow_without_c <NEWLINE> <DEDENT> <DEDENT>
step = peda . intsize ( ) <NEWLINE> <INDENT> if not peda . is_address ( address ) : <COMMENT> <NEWLINE> <INDENT> msg ( <STRING> % sp , <STRING> ) <NEWLINE> return <NEWLINE> for i in range ( count ) : <NEWLINE> <INDENT> if not peda . execute ( <STRING> % ( <STRING> if step == 8 else <STRING> , address + i * step ) ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> return <NEWLINE> <DEDENT> <DEDENT>
def set_statics ( self ) : <NEWLINE> <INDENT> if os . path . exists ( self . results_dir ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> shutil . copytree ( os . path . join ( self . templates_dir , <STRING> ) , os . path . join ( self . results_dir , <STRING> ) ) <NEWLINE> shutil . copytree ( os . path . join ( self . templates_dir , <STRING> ) , os . path . join ( self . results_dir , <STRING> ) ) <NEWLINE> shutil . copytree ( os . path . join ( self . templates_dir , <STRING> ) , os . path . join ( self . results_dir , <STRING> ) ) <NEWLINE> <DEDENT> except OSError : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> <DEDENT>
bundle = entangled_interface . local_attach_to_tangle ( pb , gta [ <STRING> ] , gta [ <STRING> ] , mwm ) <NEWLINE>
def edit_rwhois ( self , abuse_email = None , address1 = None , address2 = None , <NEWLINE> <INDENT> city = None , company_name = None , country = None , <NEWLINE> first_name = None , last_name = None , postal_code = None , <NEWLINE> private_residence = None , state = None ) : <NEWLINE> <STRING> <NEWLINE> update = { } <NEWLINE> for key , value in [ ( <STRING> , abuse_email ) , <NEWLINE> <INDENT> ( <STRING> , address1 ) , <NEWLINE> ( <STRING> , address2 ) , <NEWLINE> ( <STRING> , city ) , <NEWLINE> ( <STRING> , company_name ) , <NEWLINE> ( <STRING> , country ) , <NEWLINE> ( <STRING> , first_name ) , <NEWLINE> ( <STRING> , last_name ) , <NEWLINE> ( <STRING> , private_residence ) , <NEWLINE> ( <STRING> , state ) , <NEWLINE> ( <STRING> , postal_code ) ] : <NEWLINE> if key is not None : <NEWLINE> update [ key ] = value <NEWLINE> <DEDENT> <DEDENT>
@ click . command ( ) <NEWLINE> <INDENT> @ click . argument ( <STRING> ) <NEWLINE> @ click . option ( <STRING> , <NEWLINE> <INDENT> type = click . Choice ( [ <STRING> , <STRING> , <STRING> ] ) , <NEWLINE> help = <STRING> , <NEWLINE> required = True ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , <NEWLINE> <INDENT> is_flag = True , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT> @ environment . pass_env <NEWLINE> def cli ( env , target , firewall_type , high_availability ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> <DEDENT>
base_type_name = <STRING> <NEWLINE> <INDENT> package = storage_utils . get_package ( self , storage_type ) <NEWLINE> if storage_type == <STRING> : <NEWLINE> <INDENT> complex_type = base_type_name + <STRING> <NEWLINE> prices = [ <NEWLINE> <INDENT> storage_utils . find_performance_price ( <NEWLINE> <INDENT> package , <NEWLINE> <STRING> <NEWLINE> ) , <NEWLINE> <DEDENT> storage_utils . find_performance_space_price ( package , iops ) , <NEWLINE> storage_utils . find_performance_iops_price ( package , size , iops ) , <NEWLINE> <DEDENT> ] <NEWLINE> <DEDENT> elif storage_type == <STRING> : <NEWLINE> <INDENT> complex_type = base_type_name + <STRING> <NEWLINE> prices = [ <NEWLINE> <INDENT> storage_utils . find_endurance_price ( package , <STRING> ) , <NEWLINE> storage_utils . find_endurance_price ( <NEWLINE> <INDENT> package , <NEWLINE> <STRING> <NEWLINE> ) , <NEWLINE> <DEDENT> storage_utils . find_endurance_space_price ( <NEWLINE> <INDENT> package , <NEWLINE> size , <NEWLINE> tier_level <NEWLINE> ) , <NEWLINE> <DEDENT> storage_utils . find_endurance_tier_price ( package , tier_level ) , <NEWLINE> <DEDENT> ] <NEWLINE> if snapshot_size is not None : <NEWLINE> <INDENT> prices . append ( storage_utils . find_snapshot_space_price ( <NEWLINE> <INDENT> package , snapshot_size , tier_level ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise exceptions . SoftLayerError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if private_vlan and public_vlan : <NEWLINE> <INDENT> network_components = self . _create_network_components ( public_vlan , private_vlan , <NEWLINE> <INDENT> private_subnet , public_subnet ) <NEWLINE> <DEDENT> data . update ( network_components ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> member_table = formatting . Table ( [ <STRING> , <STRING> , <STRING> ] , title = <STRING> ) <NEWLINE> guests = group . get ( <STRING> , [ ] ) <NEWLINE> for guest in guests : <NEWLINE> <INDENT> real_guest = guest . get ( <STRING> ) <NEWLINE> member_table . add_row ( [ <NEWLINE> <INDENT> guest . get ( <STRING> ) , real_guest . get ( <STRING> ) , utils . clean_time ( real_guest . get ( <STRING> ) ) <NEWLINE> <DEDENT> ] ) <NEWLINE> <DEDENT> env . fout ( member_table ) <NEWLINE> <DEDENT>
return ui , ui . to_be_bound ( ) <NEWLINE>
@ step ( <STRING> ) <NEWLINE> <INDENT> @ persona_vars <NEWLINE> def click_link_in_email ( context , address ) : <NEWLINE> <INDENT> mails = context . mail . user_messages ( address ) <NEWLINE> assert mails , <STRING> <NEWLINE> mail = email . message_from_string ( mails [ - 1 ] ) <NEWLINE> links = URL_RE . findall ( str ( mail ) . replace ( <STRING> , <STRING> ) ) <NEWLINE> assert links , <STRING> <NEWLINE> url = links [ 0 ] <NEWLINE> context . browser . visit ( url ) <NEWLINE> <DEDENT> <DEDENT>
train_corpus = GoldCorpus ( train_json_path , dev_json_path ) <NEWLINE> <INDENT> test_corpus = GoldCorpus ( train_json_path , dev_json_path ) <NEWLINE> <DEDENT>
with open ( os . path . join ( model_output_dir , <STRING> ) ) as metric_file : <NEWLINE> <INDENT> json . dump ( scorer . scores , metric_file ) <NEWLINE> if __name__ == <STRING> : <NEWLINE> parser = argparse . ArgumentParser ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if generate_linking_classifier_training_data : <NEWLINE> <INDENT> for candidates , mention_types_for_mention in zip ( candidates_by_mention , mention_types_by_mention ) : <NEWLINE> <INDENT> for candidate_id , candidate in candidates . items ( ) : <NEWLINE> <INDENT> classifier_example = linker . classifier_example ( candidate_id , candidate , mention_text , mention_types ) <NEWLINE> classifier_example [ <STRING> ] = int ( gold_entity . umls_id == candidate_id ) <NEWLINE> linking_classifier_training_data . append ( classifier_example ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
unit = unit or self . select_build_worker ( p ) <NEWLINE> <INDENT> if unit is None or self . can_afford ( building ) : <NEWLINE> <INDENT> return ActionResult . Error <NEWLINE> <DEDENT> return await self . do ( unit . build ( building , p ) ) <NEWLINE> <DEDENT>
def circle_intersection ( self , p : <STRING> , r : Union [ int , float ] ) -> Set [ <STRING> ] : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert self != p <NEWLINE> distanceBetweenPoints = self . distance_to ( p ) <NEWLINE> assert r > distanceBetweenPoints / 2 <NEWLINE> <COMMENT> <NL> remainingDistanceFromCenter = ( r ** 2 - ( distanceBetweenPoints / 2 ) ** 2 ) ** 0.5 <NEWLINE> <COMMENT> <NL> offsetToCenter = Point2 ( ( ( p . x - self . x ) / 2 , ( p . y - self . y ) / 2 ) ) <NEWLINE> center = self . offset ( offsetToCenter ) <NEWLINE> <DEDENT>
def circle_intersection ( self , p : <STRING> , r : Union [ int , float ] ) -> Set [ <STRING> ] : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert self != p <NEWLINE> distanceBetweenPoints = self . distance_to ( p ) <NEWLINE> assert r > distanceBetweenPoints / 2 <NEWLINE> <COMMENT> <NL> remainingDistanceFromCenter = ( r ** 2 - ( distanceBetweenPoints / 2 ) ** 2 ) ** 0.5 <NEWLINE> <COMMENT> <NL> offsetToCenter = Point2 ( ( ( p . x - self . x ) / 2 , ( p . y - self . y ) / 2 ) ) <NEWLINE> center = self . offset ( offsetToCenter ) <NEWLINE> <DEDENT>
def take ( self , n : int ) -> <STRING> : <NEWLINE> <INDENT> if self . amount >= n : <NEWLINE> <INDENT> return self <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . subgroup ( self [ : n ] ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> sample_file = os . path . join ( project_dir , <STRING> ) <NEWLINE> settings_file = os . path . join ( home_config_dir , <STRING> ) <NEWLINE> if not os . path . exists ( home_config_dir ) : <NEWLINE> <INDENT> os . makedirs ( home_config_dir ) <NEWLINE> <DEDENT> copyfile ( sample_file , settings_file ) <NEWLINE> print ( <STRING> . format ( repr ( home_config_dir ) ) ) <NEWLINE> return settings_file <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if self . options . get ( <STRING> , None ) and self . options . get ( <STRING> , None ) : <NEWLINE> <INDENT> assert self . options [ <STRING> ] < self . options [ <STRING> ] <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = True <NEWLINE> self . options [ <STRING> ] = False <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if self . options . get ( <STRING> , None ) or self . options . get ( <STRING> , None ) : <NEWLINE> <INDENT> assert self . options [ <STRING> ] < self . options [ <STRING> ] <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = True <NEWLINE> self . options [ <STRING> ] = False <NEWLINE> <DEDENT> <DEDENT>
for i in range ( len ( expected ) ) : <NEWLINE> <INDENT> if expected [ i ] [ <STRING> ] == received [ i ] [ <STRING> ] : <NEWLINE> <INDENT> self . assertEqual ( expected [ i ] [ <STRING> ] , received [ i ] [ <STRING> ] ) <NEWLINE> self . assertEqual ( expected [ i ] [ <STRING> ] , received [ i ] [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT>
with pytest . raises ( Exception ) : <NEWLINE> <INDENT> view_func2 ( ) <NEWLINE> assert len ( User . query . all ( ) ) == 1 <NEWLINE> <DEDENT>
def prebuildcleanup ( top , parent ) : <NEWLINE> <INDENT> preclean = { } <NEWLINE> preclean_patterns = { <STRING> : <STRING> , <STRING> : <STRING> } <NEWLINE> for element in top : <NEWLINE> <INDENT> if element . tag == <STRING> : <NEWLINE> <INDENT> preclean [ <STRING> ] = ( element . text == <STRING> ) <NEWLINE> <DEDENT> elif element . tag == <STRING> : <NEWLINE> <INDENT> for subelement in element : <NEWLINE> <INDENT> if subelement . tag != <STRING> : <NEWLINE> <INDENT> raise NotImplementedError ( <STRING> <NEWLINE> <INDENT> <STRING> % subelement . tag ) <NEWLINE> <DEDENT> <DEDENT> if subelement . find ( <STRING> ) is not None and subelement . find ( <STRING> ) is not None : <NEWLINE> <INDENT> rule_type = subelement . find ( <STRING> ) . text . lower ( ) <NEWLINE> rule_patt = subelement . find ( <STRING> ) . text <NEWLINE> preclean_patterns [ rule_type ] = rule_patt <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif element . tag == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> pass <NEWLINE> <DEDENT> elif element . tag == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NotImplementedError ( <STRING> <NEWLINE> <INDENT> <STRING> % subelement . tag ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if haveSampleColumn : <NEWLINE> <INDENT> if not all ( [ patient in sample for sample , patient in zip ( clinicalSampleDF [ sampleId ] , clinicalSampleDF [ patientId ] ) ] ) : <NEWLINE> <INDENT> total_error += <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> sample_patients = clinicalSampleDF [ patientId ] [ clinicalSampleDF [ patientId ] != <STRING> ] <NEWLINE> patient_patients = clinicalDF [ patientId ] [ clinicalDF [ patientId ] != <STRING> ] <NEWLINE> <COMMENT> <NL> if not all ( sample_patients . isin ( patient_patients ) ) : <NEWLINE> <INDENT> total_error += <STRING> % <STRING> . join ( clinicalSampleDF [ patientId ] [ ~ clinicalSampleDF [ patientId ] . isin ( clinicalDF [ patientId ] ) ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> if not all ( patient_patients . isin ( sample_patients ) ) : <NEWLINE> <COMMENT> <NL> <INDENT> warning += <STRING> % <STRING> . join ( clinicalDF [ patientId ] [ ~ clinicalDF [ patientId ] . isin ( clinicalSampleDF [ patientId ] ) ] ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def createMafDatabase ( syn , databaseToSynIdMappingDf , testing = False , staging = False ) : <NEWLINE> <INDENT> mafDatabaseSynId = process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) <NEWLINE> mafDatabaseEnt = syn . get ( mafDatabaseSynId ) <NEWLINE> mafCols = list ( syn . getTableColumns ( mafDatabaseSynId ) ) <NEWLINE> schema = synapseclient . Schema ( name = <STRING> % time . time ( ) , columns = mafCols , parent = process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) ) <NEWLINE> schema . primaryKey = mafDatabaseEnt . primaryKey <NEWLINE> newMafDb = syn . store ( schema ) <NEWLINE> <COMMENT> <NL> databaseToSynIdMappingDf [ <STRING> ] [ 0 ] = newMafDb . id <NEWLINE> syn . store ( synapseclient . Table ( process_functions . getDatabaseSynId ( syn , <STRING> , test = testing ) , databaseToSynIdMappingDf ) ) <NEWLINE> if not staging and not testing : <NEWLINE> <COMMENT> <NL> <INDENT> databaseToSynIdMapping = syn . tableQuery ( <STRING> ) <NEWLINE> databaseToSynIdMappingDf = databaseToSynIdMapping . asDataFrame ( ) <NEWLINE> databaseToSynIdMapping [ <STRING> ] [ 0 ] = newMafDb . id <NEWLINE> syn . store ( synapseclient . Table ( <STRING> , databaseToSynIdMappingDf ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> mafDatabaseEnt . parentId = <STRING> <NEWLINE> mafDatabaseEnt . name = <STRING> + mafDatabaseEnt . name <NEWLINE> syn . store ( mafDatabaseEnt ) <NEWLINE> mafDatabaseSynId = newMafDb . id <NEWLINE> <COMMENT> <NL> syn . setPermissions ( mafDatabaseSynId , 3326313 , [ ] ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> haveColumn = process_functions . checkColExist ( clinicalDF , <STRING> ) <NEWLINE> if haveColumn : <NEWLINE> <INDENT> if not all ( [ i != <STRING> for i in clinicalDF [ <STRING> ] ] ) : <NEWLINE> <INDENT> total_error += <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> seqAssayIds = clinicalDF . SEQ_ASSAY_ID [ clinicalDF . SEQ_ASSAY_ID != <STRING> ] <NEWLINE> allSeqAssays = seqAssayIds . unique ( ) <NEWLINE> notNormalized = [ ] <NEWLINE> not_caps = [ ] <NEWLINE> for seqassay in allSeqAssays : <NEWLINE> <COMMENT> <NL> <INDENT> if not seqassay . upper ( ) . startswith ( self . center ) : <NEWLINE> <INDENT> not_caps . append ( seqassay ) <NEWLINE> <DEDENT> <DEDENT> if len ( not_caps ) > 0 : <NEWLINE> <INDENT> total_error += <STRING> % <STRING> . join ( not_caps ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> total_error += <STRING> <NEWLINE> <DEDENT> <DEDENT>
if len ( samples ) == 1 : <NEWLINE> <INDENT> tumor = samples [ 0 ] <NEWLINE> normal = <STRING> <NEWLINE> elif len ( samples ) == 2 : <NEWLINE> <COMMENT> <NL> tumor = samples [ 0 ] <NEWLINE> normal = samples [ 1 ] <NEWLINE> else : <NEWLINE> tumor = <STRING> <NEWLINE> normal = <STRING> <NEWLINE> <COMMENT> <NL> if tumor != <STRING> : <NEWLINE> tumorName = vcfName . replace ( <STRING> , <STRING> ) <NEWLINE> else : <NEWLINE> tumorName = tumor <NEWLINE> newMAFPath = newVCFPath + <STRING> <NEWLINE> if os . path . isfile ( newMAFPath ) : <NEWLINE> mafFiles . append ( newMAFPath ) <NEWLINE> else : <NEWLINE> command = [ <STRING> , os . path . join ( vcf2mafPath , <STRING> ) , <NEWLINE> <INDENT> <STRING> , newVCFPath , <NEWLINE> <STRING> , newMAFPath , <NEWLINE> <STRING> , veppath , <NEWLINE> <STRING> , vepdata , <NEWLINE> <STRING> , <STRING> , <NEWLINE> <STRING> , tumorName , <NEWLINE> <STRING> , normal , <NEWLINE> <STRING> , tumor , <NEWLINE> <COMMENT> <NL> <STRING> , os . path . join ( vcf2mafPath , <STRING> ) ] <NEWLINE> <DEDENT> if reference is not None : <NEWLINE> <INDENT> command . extend ( [ <STRING> , reference ] ) <NEWLINE> <DEDENT> subprocess . check_call ( command ) <NEWLINE> if ( os . path . isfile ( newMAFPath ) ) : <NEWLINE> <INDENT> mafFiles . append ( newMAFPath ) <NEWLINE> <DEDENT> <DEDENT>
allFiles = pd . DataFrame ( allFiles , columns = [ <STRING> , <STRING> ] ) <NEWLINE> <COMMENT> <NL> <INDENT> if allFiles . empty : <NEWLINE> <INDENT> logger . info ( <STRING> % center ) <NEWLINE> return ( [ ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> if process != <STRING> : <NEWLINE> <INDENT> addToQuery = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> addToQuery = <STRING> <NEWLINE> <DEDENT> validationStatus = syn . tableQuery ( <STRING> % ( process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) , center , addToQuery ) ) <NEWLINE> errorTracker = syn . tableQuery ( <STRING> % ( process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) , center , addToQuery ) ) <NEWLINE> <COMMENT> <NL> validationStatusDf = validationStatus . asDataFrame ( ) <NEWLINE> errorTrackerDf = errorTracker . asDataFrame ( ) <NEWLINE> validated = allFiles . apply ( lambda x : validateFile ( syn , validationStatusDf , errorTrackerDf , center , thread , x , testing , oncotreeLink ) , axis = 1 ) <NEWLINE> inputValidStatus = [ ] <NEWLINE> invalidErrors = [ ] <NEWLINE> for inputStat , invalErrors in validated : <NEWLINE> <INDENT> inputValidStatus . extend ( inputStat ) <NEWLINE> if invalErrors is not None : <NEWLINE> <INDENT> invalidErrors . extend ( invalErrors ) <NEWLINE> <DEDENT> <DEDENT> inputValidStatus = pd . DataFrame ( inputValidStatus , columns = [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> duplicatedFiles = inputValidStatus [ inputValidStatus [ <STRING> ] . duplicated ( keep = False ) ] <NEWLINE> cbsSegBool = [ os . path . basename ( i ) . endswith ( <STRING> ) or os . path . basename ( i ) . endswith ( <STRING> ) for i in inputValidStatus [ <STRING> ] ] <NEWLINE> cbsSegFiles = inputValidStatus [ cbsSegBool ] <NEWLINE> if len ( cbsSegFiles ) > 1 : <NEWLINE> <INDENT> duplicatedFiles = duplicatedFiles . append ( cbsSegFiles ) <NEWLINE> <DEDENT> clinical_bool = [ <STRING> in i for i in inputValidStatus [ <STRING> ] ] <NEWLINE> clinical_files = inputValidStatus [ clinical_bool ] <NEWLINE> if len ( clinical_bool ) > 2 : <NEWLINE> <INDENT> duplicatedFiles = duplicatedFiles . append ( clinical_files ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
status_list = check_file_status [ <STRING> ] <NEWLINE> <INDENT> error_list = check_file_status [ <STRING> ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> validator = validate . GenieValidationHelper ( syn = syn , center = center , <NEWLINE> <INDENT> filepathlist = filepaths , <NEWLINE> format_registry = format_registry , <NEWLINE> testing = testing ) <NEWLINE> <DEDENT> filetype = validator . file_type <NEWLINE> if check_file_status [ <STRING> ] : <NEWLINE> <INDENT> valid , message , filetype = validator . validate_single_file ( <NEWLINE> <INDENT> oncotree_link = oncotree_link , nosymbol_check = False ) <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> input_status_list , invalid_errors_list = _get_status_and_error_list ( <NEWLINE> <INDENT> valid , message , entities ) <NEWLINE> <COMMENT> <NL> <DEDENT> if invalid_errors_list : <NEWLINE> <INDENT> _send_validation_error_email ( syn , filenames , message , file_users ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> input_status_list = [ <NEWLINE> <INDENT> [ ent . id , path , ent . md5 , status , filename , entity_date_to_timestamp ( ent . properties . modifiedOn ) ] <NEWLINE> for ent , path , status , filename in <NEWLINE> zip ( entities , filepaths , status_list , filenames ) ] <NEWLINE> <DEDENT> invalid_errors_list = [ <NEWLINE> <INDENT> [ entity . id , error , filename ] <NEWLINE> for entity , error , filename in <NEWLINE> zip ( entities , error_list , filenames ) ] <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for input_status in input_status_list : <NEWLINE> <INDENT> input_status . extend ( [ filetype , center ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> for invalid_errors in invalid_errors_list : <NEWLINE> <INDENT> invalid_errors . extend ( [ filetype , center ] ) <NEWLINE> <DEDENT> return ( input_status_list , invalid_errors_list ) <NEWLINE> <DEDENT>
if sum ( q_inds ) : <NEWLINE> <INDENT> inds_to_send = q_inds [ np . where ( H [ <STRING> ] [ q_inds ] == max ( H [ <STRING> ] [ q_inds ] ) ) [ 0 ] ] <NEWLINE> Work [ i ] = { <STRING> : sim_specs [ <STRING> ] [ 0 ] , <NEWLINE> <INDENT> <STRING> : sim_specs [ <STRING> ] , <NEWLINE> <STRING> : [ ] , <NEWLINE> <STRING> : H [ sim_specs [ <STRING> ] ] [ inds_to_send ] , <NEWLINE> <STRING> : sim_specs [ <STRING> ] , <NEWLINE> <STRING> : { <STRING> : <STRING> , <STRING> : q_inds } , <NEWLINE> } <NEWLINE> <DEDENT> <DEDENT>
comm . send ( obj = data_out , dest = 0 , tag = calc_tag ) <NEWLINE>
def kill_pending ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for req in self . _outbox : <NEWLINE> <INDENT> if req . Test ( ) : <NEWLINE> <INDENT> req . Cancel ( ) <NEWLINE> <DEDENT> <DEDENT> self . _outbox = [ ] <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def get_slurm_nodelist ( node_list_env ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> fullstr = os . environ [ node_list_env ] <NEWLINE> if not fullstr : <NEWLINE> <INDENT> return [ ] <NEWLINE> <DEDENT> part_splitstr = fullstr . split ( <STRING> ) <NEWLINE> if len ( part_splitstr ) == 1 : <COMMENT> <NEWLINE> <INDENT> splitstr = fullstr . split ( <STRING> , 1 ) <NEWLINE> if len ( splitstr ) == 1 : <COMMENT> <NEWLINE> <INDENT> return splitstr <NEWLINE> <DEDENT> prefix = splitstr [ 0 ] <NEWLINE> nidstr = splitstr [ 1 ] . strip ( <STRING> ) <NEWLINE> nidlst = EnvResources . _noderange_append ( prefix , nidstr ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> splitgroups = [ str . split ( <STRING> , 1 ) for str in splitstr ] <NEWLINE> prefixgroups = [ group [ 0 ] for group in splitgroups ] <NEWLINE> nodegroups = [ group [ 1 ] . strip ( <STRING> ) for group in splitgroups ] <NEWLINE> nidlst = [ ] <NEWLINE> for i in range ( len ( prefixgroups ) ) : <NEWLINE> <INDENT> prefix = prefixgroups [ i ] <NEWLINE> nidstr = nodegroups [ i ] <NEWLINE> nidlst . extend ( EnvResources . _noderange_append ( prefix , nidstr ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
last_size = persis_info . get ( <STRING> ) <NEWLINE> <INDENT> if len ( H ) : <NEWLINE> <COMMENT> <NL> <INDENT> if ( gen_specs [ <STRING> ] . get ( <STRING> ) <NEWLINE> <INDENT> and not all ( np . logical_or ( H [ <STRING> ] [ last_size : ] , <NEWLINE> <INDENT> H [ <STRING> ] [ last_size : ] ) ) ) : <NEWLINE> <DEDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> if len ( persis_info [ lw ] [ <STRING> ] ) : <NEWLINE> <INDENT> runs_needing_to_advance = np . zeros ( len ( persis_info [ lw ] [ <STRING> ] ) , dtype = bool ) <NEWLINE> for run , inds in enumerate ( persis_info [ lw ] [ <STRING> ] . values ( ) ) : <NEWLINE> <INDENT> runs_needing_to_advance [ run ] = np . all ( H [ <STRING> ] [ inds ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
last_size = persis_info . get ( <STRING> ) <NEWLINE> <INDENT> if len ( H ) : <NEWLINE> <COMMENT> <NL> <INDENT> if ( gen_specs [ <STRING> ] . get ( <STRING> ) <NEWLINE> <INDENT> and not all ( np . logical_or ( H [ <STRING> ] [ last_size : ] , <NEWLINE> <INDENT> H [ <STRING> ] [ last_size : ] ) ) ) : <NEWLINE> <DEDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> if len ( persis_info [ lw ] [ <STRING> ] ) : <NEWLINE> <INDENT> runs_needing_to_advance = np . zeros ( len ( persis_info [ lw ] [ <STRING> ] ) , dtype = bool ) <NEWLINE> for run , inds in enumerate ( persis_info [ lw ] [ <STRING> ] . values ( ) ) : <NEWLINE> <INDENT> runs_needing_to_advance [ run ] = H [ <STRING> ] [ inds [ - 1 ] ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> still_working = ~ H [ <STRING> ] <NEWLINE> if gen_specs [ <STRING> ] . get ( <STRING> ) and np . any ( still_working ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> H_o = add_to_Out ( np . zeros ( 1 , dtype = gen_specs [ <STRING> ] ) , x , 0 , <NEWLINE> <INDENT> gen_specs [ <STRING> ] [ <STRING> ] , gen_specs [ <STRING> ] [ <STRING> ] , local = True , active = True ) <NEWLINE> <DEDENT> tag , Work , calc_in = sendrecv_mgr_worker_msg ( comm , H_o ) <NEWLINE> if tag in [ STOP_TAG , PERSIS_STOP ] : <NEWLINE> <INDENT> nlopt . forced_stop . message = <STRING> + str ( tag ) <NEWLINE> raise nlopt . forced_stop <NEWLINE> <DEDENT> <DEDENT>
return H_o , persis_info , tag <NEWLINE>
def prepare_to_run_command ( self , cmd ) : <NEWLINE> <INDENT> self . LOG . debug ( <STRING> . format ( cmd . cmd_name ) ) <NEWLINE> self . timer . start ( ) <NEWLINE> os . umask ( self . options . umask ) <NEWLINE> self . LOG . debug ( <STRING> . format ( <NEWLINE> <INDENT> self . options . environment ) ) <NEWLINE> <DEDENT> self . environment = self . options . environment <NEWLINE> self . secrets_basedir = self . options . secrets_basedir <NEWLINE> <COMMENT> <NL> if cmd . cmd_name == <STRING> : <NEWLINE> <INDENT> SecretsEnvironment . permissions_check ( <NEWLINE> <INDENT> self . secrets_basedir , <NEWLINE> verbose_level = self . options . verbose_level , <NEWLINE> ) <NEWLINE> <DEDENT> self . secrets_file = self . options . secrets_file <NEWLINE> self . secrets = SecretsEnvironment ( <NEWLINE> <INDENT> environment = self . environment , <NEWLINE> secrets_basedir = self . secrets_basedir , <NEWLINE> secrets_file = self . secrets_file , <NEWLINE> export_env_vars = self . options . export_env_vars , <NEWLINE> verbose_level = self . options . verbose_level , <NEWLINE> env_var_prefix = self . options . env_var_prefix , <NEWLINE> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for m in self . history . alive_models ( t - 1 ) : <NEWLINE> <INDENT> particles , w = self . history . get_distribution ( t - 1 , m ) <NEWLINE> self . transitions [ m ] . fit ( particles , w ) <NEWLINE> <DEDENT>
@ app . route ( <STRING> ) <NEWLINE> <INDENT> def abc_model ( abc_id , model_id , t ) : <NEWLINE> <INDENT> history = app . config [ <STRING> ] <NEWLINE> history . id = abc_id <NEWLINE> if t == <STRING> : <NEWLINE> <INDENT> t = history . max_t <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> t = int ( t ) <NEWLINE> <DEDENT> df , w = history . get_distribution ( t , model_id ) <NEWLINE> df [ <STRING> ] = w <NEWLINE> tabs = [ ] <NEWLINE> <DEDENT> <DEDENT>
for h in histories : <NEWLINE> <INDENT> for t in range ( 4 ) : <NEWLINE> <INDENT> for m in range ( 5 ) : <NEWLINE> <INDENT> pop = pops [ ( h , m , t ) ] <NEWLINE> expected_particles_list = [ p . parameter for p in pop ] <NEWLINE> pars_df , w = h . get_distribution ( t , m ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> assert np . isclose ( w . sum ( ) , 1 ) <NEWLINE> for part_nr in range ( len ( expected_particles_list ) ) : <NEWLINE> <INDENT> expected_par = expected_particles_list [ part_nr ] <NEWLINE> actual_par = pars_df . iloc [ part_nr ] <NEWLINE> assert expected_par . a == actual_par . a <NEWLINE> assert expected_par . b == actual_par . b <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
yield cls ( arg ) <NEWLINE>
if opcode in ( opcodes . OP_CHECKSIG , opcodes . OP_CHECKSIGVERIFY ) : <NEWLINE> <COMMENT> <NL> <INDENT> op_checksig ( stack , signature_for_hash_type_f , expected_hash_type , script [ begin_code_hash : ] , flags ) <NEWLINE> if opcode == opcodes . OP_CHECKSIGVERIFY : <NEWLINE> <INDENT> if bool_from_script_bytes ( stack . pop ( ) ) : <NEWLINE> <INDENT> raise ScriptError ( <STRING> % ( pc - 1 ) ) <NEWLINE> <DEDENT> <DEDENT> continue <NEWLINE> <DEDENT>
if len ( output_order ) == 0 : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> elif len ( output_order ) == 1 : <NEWLINE> print ( output_dict [ output_order [ 0 ] [ 0 ] ] ) <NEWLINE> else : <NEWLINE> dump_output ( output_dict , output_order ) <NEWLINE> <DEDENT>
def deterministic_generate_k ( generator_order , secret_exponent , val , hash_f = hashlib . sha256 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> n = generator_order <NEWLINE> bln = bit_length ( n ) <NEWLINE> order_size = ( bln + 7 ) // 8 <NEWLINE> hash_size = hash_f ( ) . digest_size <NEWLINE> v = <STRING> * hash_size <NEWLINE> k = <STRING> * hash_size <NEWLINE> priv = intstream . to_bytes ( secret_exponent , length = order_size ) <NEWLINE> shift = 8 * hash_size - bln <NEWLINE> if shift > 0 : <NEWLINE> <INDENT> val >>= shift <NEWLINE> <DEDENT> if val > n : <NEWLINE> <INDENT> val -= n <NEWLINE> <DEDENT> h1 = intstream . to_bytes ( val , length = order_size ) <NEWLINE> k = hmac . new ( k , v + <STRING> + priv + h1 , hash_f ) . digest ( ) <NEWLINE> v = hmac . new ( k , v , hash_f ) . digest ( ) <NEWLINE> k = hmac . new ( k , v + <STRING> + priv + h1 , hash_f ) . digest ( ) <NEWLINE> v = hmac . new ( k , v , hash_f ) . digest ( ) <NEWLINE> <DEDENT>
def test_get_host_speed_rank ( self ) : <NEWLINE> <INDENT> r = util . get_host_speed_rank ( [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ] ) <NEWLINE> assert len ( r ) == 1 <NEWLINE> with pytest . raises ( ValueError ) : <NEWLINE> <INDENT> util . get_host_speed_rank ( [ <STRING> ] ) <NEWLINE> <DEDENT> assert util . get_host_speed_rank ( timeout = 0 ) == [ ] <NEWLINE> <DEDENT>
if args . id is not None : <NEWLINE> <INDENT> consensus . id = args . id <NEWLINE> elif args . idLambda is not None : <NEWLINE> idLambda = eval ( args . idLambda ) <NEWLINE> consensus . id = idLambda ( args . id ) <NEWLINE> <DEDENT>
account_numbers . append ( arn . account_number ) <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> arn = ARN ( princ_aws ) <NEWLINE> if arn . error : <NEWLINE> <INDENT> self . add_issue ( 3 , <STRING> , snsitem , notes = entry ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> account_numbers . append ( arn . account_number ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if fallback is not None and return_value is None : <NEWLINE> <INDENT> raise KeyError ( <STRING> % ( option , section ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> assert len ( actual_all_tasks . keys ( ) ) == 1 <NEWLINE> assert actual_all_tasks == expected_all_tasks <NEWLINE> mocked_get_required_params . assert_called_once_with ( <NEWLINE> <INDENT> region_name , launch_details . get ( <STRING> ) , launch_details . get ( <STRING> ) , launch_details . get ( <STRING> ) , puppet_account_id <NEWLINE> <DEDENT> ) <NEWLINE> mocked_get_parameters_for_launch . assert_called_once_with ( <NEWLINE> <INDENT> required_parameters , <NEWLINE> deployment_map , <NEWLINE> manifest , <NEWLINE> launch_details , <NEWLINE> account_id , <NEWLINE> launch_details . get ( <STRING> , constants . PROVISIONED ) , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
xmin = log10 ( ymin ) <NEWLINE>
class DecodedSoundFile : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , name : str , nchannels : int , sample_rate : int , sample_width : int , <NEWLINE> <INDENT> sample_format : int , samples : array . array ) -> None : <NEWLINE> self . name = name <NEWLINE> self . nchannels = nchannels <NEWLINE> self . sample_rate = sample_rate <NEWLINE> self . sample_width = sample_width <NEWLINE> self . sample_format = sample_format <COMMENT> <NEWLINE> self . sample_format_name = ffi . string ( lib . ma_get_format_name ( sample_format ) ) . decode ( ) <NEWLINE> self . samples = samples <NEWLINE> self . num_frames = len ( samples ) / self . nchannels <NEWLINE> self . duration = self . num_frames / self . sample_rate <NEWLINE> <DEDENT> <DEDENT>
def test_if_constant_bool ( self ) : <NEWLINE> <INDENT> a = True <NEWLINE> b = array ( [ 1 , 2 ] ) <NEWLINE> c = array ( [ 3 , 4 ] ) <NEWLINE> res = evaluate ( <STRING> ) <NEWLINE> assert_equal ( res , b ) <NEWLINE> a = False <NEWLINE> res = evaluate ( <STRING> ) <NEWLINE> assert_equal ( res , b ) <NEWLINE> <DEDENT>
async def _sender_loop ( self ) : <NEWLINE> <INDENT> while self . _ender . done ( ) : <NEWLINE> <INDENT> if len ( self . _queue ) != 0 : <NEWLINE> <INDENT> await self . _send ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> k_to_delete = [ ] <NEWLINE> for k in ddata . keys ( ) : <NEWLINE> <INDENT> if k . endswith ( <STRING> ) : <NEWLINE> <INDENT> k_stripped = k . replace ( <STRING> , <STRING> ) <NEWLINE> if k_stripped in smp : <NEWLINE> <INDENT> smp [ k_stripped ] = pd . Categorical . from_codes ( <NEWLINE> <INDENT> codes = smp [ k_stripped ] . values , <NEWLINE> categories = ddata [ k ] ) <NEWLINE> <DEDENT> <DEDENT> if k_stripped in var : <NEWLINE> <INDENT> var [ k_stripped ] = pd . Categorical . from_codes ( <NEWLINE> <INDENT> codes = smp [ k_stripped ] . values , <NEWLINE> categories = ddata [ k ] ) <NEWLINE> <DEDENT> <DEDENT> k_to_delete . append ( k ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if write_obsm_varm : <NEWLINE> <INDENT> for key in adata . obsm . keys ( ) : <NEWLINE> <INDENT> col_attrs [ key ] = adata . obsm [ key ] <NEWLINE> <DEDENT> for key in adata . varm . keys ( ) : <NEWLINE> <INDENT> row_attrs [ key ] = adata . varm [ key ] <NEWLINE> elif len ( adata . obsm . keys ( ) ) > 0 or len ( adata . varm . keys ( ) ) > 0 : <NEWLINE> <DEDENT> logger . warning ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
markers = ut . find_corr_genes ( s , txt ) . flatten ( ) <NEWLINE> <INDENT> _ , i = np . unique ( markers , return_index = True ) <NEWLINE> markers = markers [ np . sort ( i ) ] <NEWLINE> self . marker_genes [ self . stab . selected_index ] = markers <NEWLINE> <DEDENT>
V = [ ] ; M = [ ] <NEWLINE> <INDENT> for s in S : <NEWLINE> <INDENT> if not case_sensitive : <NEWLINE> <INDENT> s = s . lower ( ) <NEWLINE> <DEDENT> for i in range ( len ( vec ) ) : <NEWLINE> <INDENT> if case_sensitive : <NEWLINE> <INDENT> st = vec [ i ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> st = vec [ i ] . lower ( ) <NEWLINE> <DEDENT> b = st . find ( s ) <NEWLINE> if not invert and b != - 1 or invert and b == - 1 : <NEWLINE> <INDENT> m . append ( i ) <NEWLINE> <DEDENT> <DEDENT> if len ( m ) > 0 : <NEWLINE> <INDENT> V . append ( vec [ np . array ( m ) ] ) ; M . append ( np . array ( m ) ) <NEWLINE> <DEDENT> <DEDENT> if len ( V ) > 0 : <NEWLINE> <INDENT> i = len ( V ) <NEWLINE> V = np . concatenate ( V ) ; M = np . concatenate ( M ) ; <NEWLINE> if i > 1 : <NEWLINE> <INDENT> ix = np . sort ( np . unique ( V , return_index = True ) [ 1 ] ) <NEWLINE> V = V [ ix ] ; M = M [ ix ] ; <NEWLINE> <DEDENT> return V , M <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return - 1 , - 1 <NEWLINE> <DEDENT> <DEDENT>
def build_help_menus ( self ) : <NEWLINE> <INDENT> menu_help = tk . Menu ( self . menubar , tearoff = 0 ) <NEWLINE> self . menubar . add_cascade ( menu = menu_help , label = <STRING> , underline = 0 ) <NEWLINE> menu_help . add_command ( label = <STRING> ) <NEWLINE> menu_help . add_separator ( ) <NEWLINE> if getattr ( sys , <STRING> , False ) : <NEWLINE> <INDENT> menu_help . add_command ( label = <STRING> , command = lambda : self . conf . upgrade_package ( logger = self . logger ) ) <NEWLINE> menu_help . add_separator ( ) <NEWLINE> <DEDENT> menu_help . add_command ( label = <STRING> , command = self . about_msg ) <NEWLINE> <DEDENT>
nb_batches = ( len ( tgt_list ) + mb_size - 1 ) / mb_size <NEWLINE> <INDENT> for num_batch in six . moves . range ( nb_batches ) : <NEWLINE> <INDENT> tgt_batch , arg_sort = utils . make_batch_tgt ( tgt_list [ num_batch * nb_batches : ( num_batch + 1 ) * nb_batches ] , <NEWLINE> <INDENT> eos_idx = eos_idx , gpu = gpu , volatile = <STRING> , need_arg_sort = True ) <NEWLINE> <DEDENT> scores , attn = scorer ( tgt_batch ) <NEWLINE> scores , _ = scores <NEWLINE> scores = scores . data <NEWLINE> <DEDENT> <DEDENT>
nb_batches = ( len ( tgt_list ) + mb_size - 1 ) / mb_size <NEWLINE> <INDENT> for num_batch in six . moves . range ( nb_batches ) : <NEWLINE> <INDENT> tgt_batch , arg_sort = utils . make_batch_tgt ( tgt_list [ num_batch * nb_batches : ( num_batch + 1 ) * nb_batches ] , <NEWLINE> <INDENT> eos_idx = eos_idx , gpu = gpu , volatile = <STRING> , need_arg_sort = True ) <NEWLINE> <DEDENT> scores , attn = scorer ( tgt_batch ) <NEWLINE> scores , _ = scores <NEWLINE> scores = scores . data <NEWLINE> <DEDENT> <DEDENT>
constraints_list = [ ] <NEWLINE> <INDENT> for sentence_src in src : <NEWLINE> <COMMENT> <NL> <INDENT> seq_src = src_pp . convert ( sentence_src , stats = stats_src ) <NEWLINE> if make_constraints is not None : <NEWLINE> <INDENT> constraints_fn = make_constraints ( src , seq_src ) <NEWLINE> constraints_list . append ( constraints_fn ) <NEWLINE> <DEDENT> res . append ( seq_src ) <NEWLINE> <DEDENT> if make_constraints is not None : <NEWLINE> <INDENT> return res , stats_src , constraints_list <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return res , stats_src <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if sids_intersection > 0 : <NEWLINE> <INDENT> option_parser . error ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for g , rows in report_rows : <NEWLINE> <INDENT> if g : <NEWLINE> <INDENT> sheet1 . write ( row_index , 0 , <STRING> % x , stylebold ) <NEWLINE> row_index += 1 <NEWLINE> <DEDENT> for row in list ( rows ) : <NEWLINE> <INDENT> if row . is_value ( ) : <NEWLINE> <INDENT> for index , x in enumerate ( row ) : <NEWLINE> <INDENT> if isinstance ( x . value , ( list , tuple ) ) : <NEWLINE> <INDENT> xvalue = <STRING> . join ( [ <STRING> % v for v in x . value ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> xvalue = x . text ( ) <NEWLINE> <DEDENT> sheet1 . write ( row_index , index , xvalue , stylevalue ) <NEWLINE> <DEDENT> row_index += 1 <NEWLINE> <DEDENT> elif row . is_caption : <NEWLINE> <INDENT> for index , x in enumerate ( row ) : <NEWLINE> <INDENT> if not isinstance ( x , ( unicode , str ) ) : <NEWLINE> <INDENT> sheet1 . write ( row_index , index , x . text ( ) , stylebold ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sheet1 . write ( row_index , index , x , stylebold ) <NEWLINE> <DEDENT> <DEDENT> row_index += 1 <NEWLINE> <DEDENT> elif row . is_total : <NEWLINE> <INDENT> for index , x in enumerate ( row ) : <NEWLINE> <INDENT> sheet1 . write ( row_index , index , x . text ( ) , stylebold ) <NEWLINE> sheet1 . write ( row_index + 1 , index , <STRING> ) <NEWLINE> <DEDENT> row_index += 2 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
time_now = time . time ( ) <NEWLINE> <INDENT> if time_now > timestamp_last + 300 : <COMMENT> <NEWLINE> <INDENT> difficulty2 = percentage ( 97 , diff_block_previous ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> difficulty2 = difficulty <NEWLINE> <DEDENT> <DEDENT>
if nodes_ban_reset and len ( connection_pool ) < len ( banlist ) and int ( time . time ( ) - reset_time ) > 60 * 10 : <COMMENT> <NEWLINE> <INDENT> app_log . warning ( <STRING> . format ( len ( connection_pool ) , len ( banlist ) ) ) <NEWLINE> del banlist [ : ] <NEWLINE> banlist . extend ( config . banlist ) <COMMENT> <NEWLINE> del warning_list [ : ] <NEWLINE> del tried [ : ] <NEWLINE> reset_time = time . time ( ) <NEWLINE> <DEDENT>
html . append ( <STRING> ) <NEWLINE> <INDENT> html . append ( <STRING> . format ( transferred_total ) ) <NEWLINE> html . append ( <STRING> . format ( tx_count ) ) <NEWLINE> html . append ( <STRING> . format ( tx_count / 500 ) ) <NEWLINE> html . append ( <STRING> . format ( transferred_total ) ) <NEWLINE> <DEDENT>
if block_height > 427000 : <COMMENT> <NEWLINE> <INDENT> execute ( c , <STRING> ) <NEWLINE> result = c . fetchone ( ) <NEWLINE> timestamp_last = Decimal ( result [ 1 ] ) <NEWLINE> block_height = int ( result [ 0 ] ) <NEWLINE> timestamp_before_last = Decimal ( c . fetchone ( ) [ 1 ] ) <NEWLINE> <DEDENT>
address = blake2b ( privkey . to_string ( ) , digest_size = 20 ) . hexdigest ( ) <NEWLINE>
address = blake2b ( pubkey . to_string ( ) , digest_size = 20 ) . hexdigest ( ) <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> balance_pre = ledger_balance3 ( db_address , h2 , balances ) <NEWLINE> <COMMENT> <NL> balance = quantize_eight ( balance_pre - block_debit_address ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>
if self . __output_filter == <STRING> : <NEWLINE> <INDENT> self . __output += self . __p . communicate ( ) [ 0 ] . decode ( <STRING> ) <COMMENT> <NEWLINE> else : <NEWLINE> if str ( self . __p . communicate ( ) [ 0 ] . decode ( <STRING> ) ) . find ( self . __output_filter ) != - 1 : <NEWLINE> <INDENT> self . __output += self . __p . communicate ( ) [ 0 ] . decode ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
@ figsize . setter <NEWLINE> <INDENT> def figsize ( self , size : Tuple [ float , float ] ) : <NEWLINE> <INDENT> if ( <NEWLINE> <INDENT> not isinstance ( size , Sequence ) <NEWLINE> or len ( size ) != 2 <NEWLINE> or not all ( isinstance ( x , Real ) and x >= 0 for x in size ) <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> size = tuple ( size ) <NEWLINE> if self . figsize != size : <NEWLINE> <INDENT> self . _pltkwargs [ <STRING> ] = size <NEWLINE> if self . _fig is not None : <NEWLINE> <INDENT> self . _fig . set_size_inches ( size ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def populateSubstitutionGroups ( self , elements ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> length = len ( elements ) <NEWLINE> for i , element in enumerate ( elements ) : <NEWLINE> <INDENT> if <STRING> in element . getAttrs ( ) : <NEWLINE> <INDENT> substitutionGroup = element . getAttrs ( ) [ <STRING> ] <NEWLINE> base = element . getBase ( ) <NEWLINE> self . opts . lang . abstract_type_map [ substitutionGroup ] = base <NEWLINE> self . opts . lang . substitutionGroup_map [ base ] = substitutionGroup <NEWLINE> <DEDENT> <DEDENT> for i , element in enumerate ( elements ) : <NEWLINE> <INDENT> if self . opts . lang . hasSubstitutionGroup ( element . getName ( ) ) : <NEWLINE> <INDENT> substitutionGroupName = self . opts . lang . substitutionGroup ( element . getName ( ) ) <NEWLINE> self . substitutionElement_map [ substitutionGroupName ] = element <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> if len ( self . opts . lang . getSubstitutionTypes ( ) ) >= 0 : <NEWLINE> <INDENT> config . METADATA_OBJECT_IGNORE . remove ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def validate ( self ) : <NEWLINE> <INDENT> sizeC = int ( self . data [ <STRING> ] [ <STRING> ] ) <NEWLINE> assert ( len ( self . data [ <STRING> ] ) <= sizeC ) , str ( self . data ) <NEWLINE> channel_samples = sum ( [ int ( x . data [ <STRING> ] ) <NEWLINE> <INDENT> for x in self . data [ <STRING> ] ] ) <NEWLINE> <DEDENT> assert channel_samples < sizeC , str ( self . data ) <NEWLINE> return self . data <NEWLINE> <DEDENT>
rightNodeSummary = self . tree_node_summary ( w_r , y_r , <NEWLINE> <INDENT> min_samples_treatment = min_samples_treatment , <NEWLINE> n_reg = n_reg , <NEWLINE> parentNodeSummary = parentNodeSummary ) <NEWLINE> <DEDENT>
after_first_commit = get_log_version ( tmpdir ) <NEWLINE>
if realdirpath not in scm_dirs : <NEWLINE> <COMMENT> <NL> <INDENT> dirnames [ : ] = [ ] <NEWLINE> continue <NEWLINE> if os . path . islink ( dirpath ) and not os . path . relpath ( <NEWLINE> realdirpath , realpath <NEWLINE> ) . startswith ( os . pardir ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> res . append ( os . path . join ( path , os . path . relpath ( dirpath , path ) ) ) <NEWLINE> dirnames [ : ] = [ ] <NEWLINE> continue <NEWLINE> if realdirpath in seen : <NEWLINE> <COMMENT> <NL> dirnames [ : ] = [ ] <NEWLINE> continue <NEWLINE> dirnames [ : ] = [ dn for dn in dirnames if not _link_not_in_scm ( dn ) ] <NEWLINE> for filename in filenames : <NEWLINE> if _link_not_in_scm ( filename ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> fullfilename = os . path . join ( dirpath , filename ) <NEWLINE> if os . path . normcase ( os . path . realpath ( fullfilename ) ) in scm_files : <NEWLINE> <INDENT> res . append ( os . path . join ( path , os . path . relpath ( fullfilename , path ) ) ) <NEWLINE> seen . add ( realdirpath ) <NEWLINE> return res <NEWLINE> <DEDENT> <DEDENT>
if ( fn [ <STRING> ] <= fs [ <STRING> ] ) : <NEWLINE>
else : <NEWLINE> <INDENT> _logger . debug ( <STRING> . format ( result , delivery_state ) ) <NEWLINE> message . state = constants . MessageState . SendComplete <NEWLINE> message . _response = errors . MessageAlreadySettled ( ) <COMMENT> <NEWLINE> if message . on_send_complete : <NEWLINE> message . on_send_complete ( result , delivery_state ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> clock_rate = op . clock_rates [ mb_num ] <NEWLINE> if clock_rate is not None : <NEWLINE> <INDENT> op . set_clock_rate ( clock_rate , mb_num ) <NEWLINE> <DEDENT> op . clock_rates [ mb_num ] = u . get_clock_rate ( mb_num ) <NEWLINE> <DEDENT>
i = 0 <NEWLINE> <INDENT> while i < w + x : <NEWLINE> <INDENT> virtual . set_position ( ( i , 0 ) ) <NEWLINE> regulator . sleep ( ) <NEWLINE> i += 1 <NEWLINE> <DEDENT> <DEDENT>
def error ( <NEWLINE> <INDENT> self , <NEWLINE> statement , <NEWLINE> message = None , <NEWLINE> variable = None , <NEWLINE> line = None , <NEWLINE> column = None , <NEWLINE> ) : <NEWLINE> if not message : <NEWLINE> <INDENT> message = self . assign_msg <NEWLINE> <DEDENT> if not variable : <NEWLINE> <INDENT> column = statement . id <NEWLINE> <DEDENT> if not line : <NEWLINE> <INDENT> line = statement . lineno <NEWLINE> <DEDENT> if not column : <NEWLINE> <INDENT> column = statement . col_offset <NEWLINE> <DEDENT> <DEDENT>
if isinstance ( obj , h5py . Dataset ) : <NEWLINE> <INDENT> detail = <STRING> . format ( <NEWLINE> <INDENT> dt = fmt_dtype ( obj . id . get_type ( ) ) , <NEWLINE> shape = fmt_shape ( obj . shape ) , <NEWLINE> <DEDENT> ) <NEWLINE> if obj . id . get_create_plist ( ) . get_layout ( ) == h5py . h5d . VIRTUAL : <NEWLINE> <INDENT> detail += <STRING> <NEWLINE> elif isinstance ( obj , h5py . Group ) : <NEWLINE> <DEDENT> if max_depth > 1 : <NEWLINE> <INDENT> children += [ self . group_item_node ( obj , key , max_depth - 1 ) <NEWLINE> <INDENT> for key in obj ] <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> detail = <STRING> <NEWLINE> else : <NEWLINE> <DEDENT> detail = <STRING> <NEWLINE> <DEDENT>
def _sort_query ( self , query , sort , order ) : <NEWLINE> <INDENT> criteria = [ ] <NEWLINE> for field in self . _list_fields : <NEWLINE> <INDENT> if field . id ( ) == sort : <NEWLINE> <INDENT> criterion = field . sort_column ( ) <NEWLINE> if order == <STRING> : <NEWLINE> <INDENT> criterion = desc ( sort ) <NEWLINE> <DEDENT> criteria . append ( criterion ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _verify_checkout ( <NEWLINE> <INDENT> replica : Replica , token : typing . Optional [ str ] , file_metadata : dict , blob_path : str , <NEWLINE> ) -> typing . Tuple [ str , bool ] : <NEWLINE> decoded_token : dict <NEWLINE> if token is None : <NEWLINE> execution_id = start_file_checkout ( blob_path , replica ) <NEWLINE> start_time = time . time ( ) <NEWLINE> attempts = 0 <NEWLINE> <DEDENT>
@ elasticsearch_retry ( logger , timeout ) <NEWLINE> <INDENT> def remove_bundle ( self , bundle : Bundle , tombstone : Tombstone ) : <NEWLINE> <INDENT> elasticsearch_retry . add_context ( tombstone = tombstone , bundle = bundle ) <NEWLINE> doc = BundleDocument . from_bundle ( bundle ) <NEWLINE> tombstone_doc = BundleTombstoneDocument . from_tombstone ( tombstone ) <NEWLINE> modified , index_name = doc . entomb ( tombstone_doc , dryrun = self . dryrun ) <NEWLINE> if self . notify or modified and self . notify is None : <NEWLINE> <INDENT> self . _notify ( doc , index_name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if type ( c ) . __bases__ [ 0 ] == cmp . component : <NEWLINE> <INDENT> msg = ( <STRING> <NEWLINE> <INDENT> <STRING> + <NEWLINE> str ( type ( c ) ) + <STRING> ) <NEWLINE> <DEDENT> raise TypeError ( msg ) <NEWLINE> else : <NEWLINE> msg = ( <STRING> <NEWLINE> <INDENT> <STRING> + <NEWLINE> str ( type ( c ) . __bases__ [ 0 ] ) + <STRING> ) <NEWLINE> <DEDENT> raise TypeError ( msg ) <NEWLINE> return False <NEWLINE> return True <NEWLINE> <DEDENT>
message = CallRequestMessage ( <NEWLINE> <INDENT> service = self . service , <NEWLINE> args = [ safebytes ( arg_1 ) , arg_3 , arg_3 ] , <NEWLINE> ) <NEWLINE> <DEDENT>
if self . _number_of_images > IMAGES_NUM_LIMIT * ( i + 1 ) : <NEWLINE> <INDENT> num = IMAGES_NUM_LIMIT <NEWLINE> else : <NEWLINE> num = ( self . _number_of_images % IMAGES_NUM_LIMIT ) or self . _number_of_images <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> with open ( credfile , <STRING> ) as f : <NEWLINE> <INDENT> creds = json . load ( credfile ) <NEWLINE> except json . JSONDecodeError as e : <NEWLINE> <DEDENT> logger . error ( <NEWLINE> <INDENT> <STRING> . format ( self , credfile ) <NEWLINE> <DEDENT> ) <NEWLINE> raise e <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> data_manager_site = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : None <NEWLINE> <DEDENT> } , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : None , <NEWLINE> <STRING> : 8 <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> } <NEWLINE> config [ <STRING> ] . append ( data_manager_site ) <NEWLINE> <DEDENT>
app_fut = dfk . submit ( func , app_args = args , <NEWLINE> <INDENT> executors = self . executors , <NEWLINE> fn_hash = self . func_hash , <NEWLINE> cache = self . cache , <NEWLINE> ignore_for_cache = self . ignore_for_cache , <NEWLINE> app_kwargs = kwargs ) <NEWLINE> <DEDENT>
windows = mw . get_sliding_window_boundaries ( start_time = st , stop_time = et , window_duration = ws , step_size = ss ) <NEWLINE> <INDENT> chunk_windows_mask = ( windows [ : , 0 ] >= data_start_indicator ) & ( windows [ : , 0 ] <= data_stop_indicator ) <NEWLINE> chunk_windows = windows [ chunk_windows_mask , : ] <NEWLINE> if len ( chunk_windows ) == 0 : <NEWLINE> <INDENT> return pd . DataFrame ( ) <NEWLINE> <DEDENT> result_data = mw . apply_to_sliding_windows ( df = combined_data , sliding_windows = chunk_windows , window_operations = features , operation_names = feature_names , return_dataframe = True ) <NEWLINE> return result_data <NEWLINE> <DEDENT>
class Drawing : <NEWLINE> <INDENT> def __init__ ( self , tagreader ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _dxfversion = <STRING> <COMMENT> <NEWLINE> self . encoding = <STRING> <COMMENT> <NEWLINE> self . filename = None <COMMENT> <NEWLINE> self . entitydb = EntityDB ( ) <NEWLINE> self . sections = Sections ( self , tagreader ) <NEWLINE> self . _dxfversion = self . header [ <STRING> ] <NEWLINE> self . encoding = self . _get_encoding ( ) <NEWLINE> nexthandle = int ( self . header . get ( <STRING> , <STRING> ) , 16 ) <NEWLINE> self . handlegenerator = HandleGenerator ( startvalue = nexthandle ) <NEWLINE> self . dxfengine = dxfengine ( self . _dxfversion , self ) <NEWLINE> <DEDENT> <DEDENT>
if name in self : <NEWLINE> <INDENT> super ( ) . delete ( group ) <NEWLINE> else : <NEWLINE> raise DXFValueError ( <STRING> ) <NEWLINE> <DEDENT>
def set_edge_visibilty ( self , num , status = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if status : <NEWLINE> <INDENT> self . dxf . invisible = self . dxf . invisible | ( 1 << num ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . dxf . invisible = self . dxf . invisible & ~ ( 1 << num ) <NEWLINE> <DEDENT> <DEDENT>
if block_ref . has_uniform_scaling and xscale < 0 : <NEWLINE> <COMMENT> <NL> <INDENT> has_non_uniform_scaling = True <NEWLINE> <DEDENT>
if vertices : <NEWLINE> <INDENT> if last_vertex . isclose ( vertices [ 0 ] ) : <NEWLINE> <INDENT> vertices . append ( last_vertex ) <NEWLINE> <DEDENT> self . out . draw_filled_polygon ( vertices , properties ) <NEWLINE> <DEDENT>
return sum ( <NEWLINE> <INDENT> ( p2 . x - p1 . x ) * ( p2 . y + p1 . y ) <NEWLINE> for p1 , p2 in zip ( vertices , vertices [ 1 : ] ) <NEWLINE> ) < 0 <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if image_thumbnail : <NEWLINE> <INDENT> images . append ( image_thumbnail ) <NEWLINE> <COMMENT> <NL> for index , image in enumerate ( images ) : <NEWLINE> <COMMENT> <NL> <INDENT> image = image . partition ( <STRING> ) [ 0 ] <NEWLINE> image_type = mimetypes . guess_type ( image ) [ 0 ] <NEWLINE> if image_type is None : <NEWLINE> <INDENT> image_type = <STRING> + image_type . split ( <STRING> ) [ - 1 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> image_type = <STRING> <COMMENT> <NEWLINE> <COMMENT> <NL> <DEDENT> if image_type == <STRING> and image_type not in image : <NEWLINE> <INDENT> image_type = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> image = <STRING> . join ( image . partition ( image_type ) [ : 2 ] ) <NEWLINE> images [ index ] = image . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> <DEDENT> self . _images = images <NEWLINE> return self . _images <NEWLINE> <DEDENT>
outputs = model ( images_val ) <NEWLINE> <INDENT> val_loss = loss_fn ( input = outputs , target = labels ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if scale_weight is None : <COMMENT> <NEWLINE> <INDENT> n_inp = len ( input ) <NEWLINE> scale = 0.4 <NEWLINE> scale_weight = torch . pow ( scale * torch . ones ( n_inp ) , torch . arange ( n_inp ) . float ( ) ) . to ( <NEWLINE> <INDENT> input . device <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if len ( _tmatrix_disconnected . closed_sets ( C ) ) < nstates : <NEWLINE> <INDENT> msm_prior = 0.001 <NEWLINE> B = msm_prior * np . eye ( C_full . shape [ 0 ] ) <COMMENT> <NEWLINE> B += msmtools . estimation . prior_neighbor ( C , alpha = msm_prior ) <COMMENT> <NEWLINE> C_post = C + B <COMMENT> <NEWLINE> P_for_pcca = _tmatrix_disconnected . estimate_P ( C_post , reversible = True ) <NEWLINE> <DEDENT> elif reversible : <COMMENT> <NEWLINE> <INDENT> P_for_pcca = P_msm <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> P_for_pcca = _tmatrix_disconnected . estimate_P ( C , reversible = True ) <NEWLINE> <DEDENT> <DEDENT>
def test_findall ( self ) : <NEWLINE> <INDENT> mgr = self . manager <NEWLINE> o1 = fakes . FakeEntity ( ) <NEWLINE> o1 . some_att = <STRING> <NEWLINE> o2 = fakes . FakeEntity ( ) <NEWLINE> o2 . some_att = <STRING> <NEWLINE> o3 = fakes . FakeEntity ( ) <NEWLINE> o3 . some_att = <STRING> <NEWLINE> sav = mgr . list <NEWLINE> mgr . list = Mock ( return_value = [ o1 , o2 , o3 ] ) <NEWLINE> ret = mgr . findall ( some_att = <STRING> ) <NEWLINE> self . assertTrue ( o1 in ret ) <NEWLINE> self . assertFalse ( o2 in ret ) <NEWLINE> self . assertTrue ( o1 in ret ) <NEWLINE> mgr . list = sav <NEWLINE> <DEDENT>
self . __read_page ( r . json ( ) , self . __cmd . start_date , self . __cmd . end_date ) <NEWLINE> <INDENT> page += 1 <NEWLINE> params [ <STRING> ] += str ( page ) <NEWLINE> <DEDENT>
def get_vCloudDirector ( self , serviceId , vdcId ) : <NEWLINE> <INDENT> vdcReference = self . get_vdcReference ( serviceId , vdcId ) <NEWLINE> if vdcReference [ 0 ] == True : <NEWLINE> <COMMENT> <NL> <INDENT> vCloudSession = self . create_vCloudSession ( vdcReference [ 1 ] ) <NEWLINE> if vCloudSession : <NEWLINE> <INDENT> vcd = VCD ( vCloudSession , serviceId , serviceId ) <NEWLINE> return vcd <NEWLINE> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>
policy_references = self . _fetch_compute_policies ( ) <NEWLINE> <INDENT> policy_list = [ ] <NEWLINE> for policy_reference in policy_references . VdcComputePolicyReference : <NEWLINE> <INDENT> policy_list . append ( policy_reference ) <NEWLINE> <DEDENT> return policy_reference <NEWLINE> <DEDENT>
def validate_params ( ctx , param , value ) : <NEWLINE> <INDENT> if any ( [ <STRING> not in item for item in value ] ) : <NEWLINE> <INDENT> raise click . BadParameter ( <STRING> ) <NEWLINE> <DEDENT> return dict ( [ tuple ( item . split ( <STRING> , 1 ) ) for item in param ] ) <NEWLINE> <DEDENT>
@ click . command ( help = <STRING> ) <NEWLINE> <INDENT> @ click . argument ( <STRING> , nargs = 1 ) <NEWLINE> @ click . argument ( <STRING> , nargs = 1 ) <NEWLINE> @ click . option ( <STRING> , metavar = <STRING> , help = <STRING> , default = <STRING> , type = click . Choice ( [ <STRING> , <STRING> ] ) ) <NEWLINE> def credentials_add ( domain , credentials_string , auth ) : <NEWLINE> <INDENT> if auth == <STRING> : <NEWLINE> <INDENT> header = auth <NEWLINE> <DEDENT> elif auth == <STRING> : <NEWLINE> <INDENT> header = <STRING> + b64encode ( credentials_string ) <NEWLINE> <DEDENT> credentials = get_credentials ( ) <NEWLINE> credentials [ domain ] = header <NEWLINE> set_credentials ( credentials ) <NEWLINE> <DEDENT> <DEDENT>
def temporary_store_decorator ( config_files_directory = default_config_files_directory , file_name = None ) : <NEWLINE> <INDENT> parser = SafeConfigParser ( ) <NEWLINE> config_local_ini = os . path . join ( config_files_directory , <STRING> ) <NEWLINE> config_ini = os . path . join ( config_files_directory , <STRING> ) <NEWLINE> read_config_file_name = parser . read ( [ config_ini , config_local_ini ] ) <NEWLINE> tmp_directory = parser . get ( <STRING> , <STRING> ) <NEWLINE> assert tmp_directory is not None , <STRING> . format ( tmp_directory , read_config_file_name ) <NEWLINE> assert os . path . isabs ( tmp_directory ) , <STRING> . format ( tmp_directory , read_config_file_name ) <NEWLINE> if os . path . isdir ( tmp_directory ) : <NEWLINE> <INDENT> <STRING> . format ( tmp_directory , read_config_file_name ) <NEWLINE> os . makedirs ( tmp_directory ) <NEWLINE> <DEDENT> <DEDENT>
if not v and update_installed is None : <NEWLINE> <INDENT> v = latestSuitableVersion ( name , version_required , registry = <STRING> ) <NEWLINE> <DEDENT>
if not util . canBuildNatively ( ) : <NEWLINE> <INDENT> forAllReporterTests ( generateTest ) <NEWLINE> else : <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT>
def register_command ( self , func , * args , ** kwargs ) : <NEWLINE> <INDENT> wrapped = functools . partial ( self . _dispatch , <STRING> , func ) <NEWLINE> self . commands [ func . __name__ ] = Command ( self , func , * args , ** kwargs ) <NEWLINE> <DEDENT>
if self . can ( self . client . state . me , Permissions . MANAGE_MESSAGES ) and len ( messages ) > 2 : <NEWLINE> <INDENT> for chunk in chunks ( messages , 100 ) : <NEWLINE> <INDENT> self . client . api . channels_messages_delete_bulk ( self . id , chunk ) <NEWLINE> else : <NEWLINE> <DEDENT> for msg in messages : <NEWLINE> <INDENT> self . delete_message ( msg ) <NEWLINE> <DEDENT> <DEDENT>
args = get_cli_arguments ( ) <NEWLINE> <INDENT> parse_args = args . parse_args ( ) <NEWLINE> if parse_args . cli : <NEWLINE> <INDENT> cli = Cli ( args ) <NEWLINE> <COMMENT> <NL> exit ( 0 if cli . status else 0 ) <NEWLINE> <DEDENT> <DEDENT>
def get_driver ( self ) : <NEWLINE> <INDENT> driver_path = self . _driver_path ( ) <NEWLINE> if not is_file ( driver_path ) : <NEWLINE> <INDENT> self . download_drivder ( ) <NEWLINE> <DEDENT> self . is_win ( ) and chmod ( driver_path , 0o755 ) <NEWLINE> driver = webdriver . Chrome ( executable_path = driver_path ) <NEWLINE> driver . set_window_size ( 500 , 600 ) <NEWLINE> return driver <NEWLINE> <DEDENT>
def _translate_gapped ( seq , * args , ** kwds ) : <NEWLINE> <INDENT> if isinstance ( seq , SeqRecord ) : <NEWLINE> <INDENT> s = str ( seq . seq ) <NEWLINE> <DEDENT> elif isinstance ( seq , Seq ) : <NEWLINE> <INDENT> s = str ( seq ) <NEWLINE> <DEDENT> elif isinstance ( seq , str ) : <NEWLINE> <INDENT> s = seq <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> gaps = 0 <NEWLINE> lwr = 0 <NEWLINE> protein = <STRING> <NEWLINE> for i in range ( 0 , len ( s ) , 3 ) : <NEWLINE> <INDENT> j = min ( i + 3 , len ( s ) ) <NEWLINE> if s [ i : j ] == <STRING> [ : j - i ] : <NEWLINE> <INDENT> if not gaps : <NEWLINE> <INDENT> protein += _translate ( s [ lwr : i ] ) <NEWLINE> <DEDENT> gaps += 1 <NEWLINE> <DEDENT> elif gaps : <NEWLINE> <INDENT> protein += <STRING> * gaps <NEWLINE> gaps = 0 <NEWLINE> lwr = j <NEWLINE> <DEDENT> <DEDENT> if gaps : <NEWLINE> <INDENT> protein += <STRING> * gaps <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> protein += _translate ( s [ lwr : len ( seq ) ] ) <NEWLINE> <DEDENT> return protein <NEWLINE> <DEDENT>
old_sigalrm = signal . signal ( signal . SIGALRM , on_alarm ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> signal . alarm ( 1 ) <NEWLINE> sync_wait_reapable ( sleeper . pid ) <NEWLINE> assert sleeper . wait ( timeout = 1 ) == - 9 <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> if sleeper . returncode is not None : <NEWLINE> <INDENT> sleeper . kill ( ) <NEWLINE> sleeper . wait ( ) <NEWLINE> <DEDENT> signal . signal ( signal . SIGALRM , old_sigalrm ) <NEWLINE> <DEDENT> <DEDENT>
deprecated_thing ( ) <NEWLINE> <INDENT> filename , lineno = _here ( ) <NEWLINE> assert len ( recwarn_always ) == 1 <NEWLINE> got = recwarn_always . pop ( TrioDeprecationWarning ) <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert got . filename == filename <NEWLINE> assert got . lineno == lineno + 1 <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> def ki_protection_enabled ( frame ) : <NEWLINE> <INDENT> while frame is not None : <NEWLINE> <INDENT> if LOCALS_KEY_KI_PROTECTION_ENABLED in frame . f_locals : <NEWLINE> <INDENT> return frame . f_locals [ LOCALS_KEY_KI_PROTECTION_ENABLED ] <NEWLINE> <DEDENT> if frame . f_code . co_name == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> frame = frame . f_back <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT> <DEDENT>
if mode == <STRING> : <NEWLINE> <INDENT> import numpy as np <NEWLINE> array = np . empty ( ( height , width ) ) <NEWLINE> for row in range ( height ) : <NEWLINE> <INDENT> for column in range ( width ) : <NEWLINE> <INDENT> latitude = latitude_from + float ( row ) / height * ( latitude_to - latitude_from ) <NEWLINE> longitude = longitude_from + float ( column ) / height * ( longitude_to - longitude_from ) <NEWLINE> elevation = self . get_elevation ( latitude , longitude ) <NEWLINE> array [ row , column ] = elevation <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for row in range ( height ) : <NEWLINE> <INDENT> for column in range ( width ) : <NEWLINE> <INDENT> latitude = latitude_from + float ( row ) / height * ( latitude_to - latitude_from ) <NEWLINE> longitude = longitude_from + float ( column ) / height * ( longitude_to - longitude_from ) <NEWLINE> elevation = self . get_elevation ( latitude , longitude ) <NEWLINE> if elevation == None : <NEWLINE> <INDENT> color = unknown_color <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> elevation_coef = ( elevation - min_elevation ) / float ( max_elevation ) <NEWLINE> if elevation_coef < 0 : elevation_coef = 0 <NEWLINE> if elevation_coef > 1 : elevation_coef = 1 <NEWLINE> color = mod_utils . get_color_between ( min_color , max_color , elevation_coef ) <NEWLINE> if elevation <= 0 : <NEWLINE> <INDENT> color = zero_color <NEWLINE> <DEDENT> <DEDENT> draw . point ( ( column , height - row ) , color ) <NEWLINE> <DEDENT> <DEDENT>
def readf ( nb_file ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> file , ext = os . path . splitext ( nb_file ) <NEWLINE> if ext not in notebook_extensions : <NEWLINE> <INDENT> raise TypeError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> . format ( nb_file , <NEWLINE> <INDENT> notebook_extensions ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> with io . open ( nb_file , encoding = <STRING> ) as fp : <NEWLINE> <INDENT> return read ( nb_file , as_version = 4 , ext = ext ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> cm . rename ( tmp_ipynb , <STRING> ) <NEWLINE> assert not os . path . isfile ( str ( tmpdir . join ( tmp_ipynb ) ) ) <NEWLINE> assert not os . path . isfile ( str ( tmpdir . join ( tmp_nbpy ) ) ) <NEWLINE> <DEDENT>
def explicit_start_marker ( self , source ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . metadata : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if all ( [ line . startswith ( <STRING> ) for line in self . source ] ) : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if CellReader ( self . ext ) . read ( source ) [ 1 ] != len ( source ) : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT>
header = comment_lines ( header , text_format . header_prefix ) <NEWLINE> <INDENT> if lines_to_next_cell is None and notebook . cells : <NEWLINE> <INDENT> lines_to_next_cell = pep8_lines_between_cells ( header , notebook . cells [ 0 ] , ext ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> lines_to_next_cell = 0 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> assert os . path . getmtime ( tmp_ipynb ) < os . path . getmtime ( tmp_py ) <NEWLINE> <DEDENT>
compare_notebooks ( nb1 , nb2 ) <NEWLINE>
compare_notebooks ( nb1 , nb2 ) <NEWLINE>
compare_notebooks ( nb1 , nb2 , <STRING> ) <NEWLINE>
if isinstance ( output , TextIOBase ) : <NEWLINE> <INDENT> self . _err = error <NEWLINE> else : <NEWLINE> self . _err = TextIOWrapper ( error ) <NEWLINE> <DEDENT>
if not ignore_options : <NEWLINE> <INDENT> options = ( <NEWLINE> <INDENT> Path ( <STRING> ) , <NEWLINE> failover , <COMMENT> <NEWLINE> <DEDENT> ) <NEWLINE> for option in options : <NEWLINE> <INDENT> if ( option . exists ( ) and <NEWLINE> <INDENT> os . access ( options , os . R_OK ) and <NEWLINE> option . stat ( ) . st_size > 0 ) : <NEWLINE> <INDENT> return option <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if batches > 1 : <NEWLINE>
def test_estimate_virtual_op_num2 ( self ) : <NEWLINE> <INDENT> account = self . account <NEWLINE> h_all_raw = [ ] <NEWLINE> for h in account . history ( raw_output = False ) : <NEWLINE> <INDENT> h_all_raw . append ( h ) <NEWLINE> <DEDENT> last_block = h_all_raw [ 0 ] [ <STRING> ] <NEWLINE> i = 1 <NEWLINE> for op in h_all_raw [ 1 : ] : <NEWLINE> <INDENT> new_block = op [ <STRING> ] <NEWLINE> block_num = last_block + int ( ( new_block - last_block ) / 2 ) <NEWLINE> op_num = account . estimate_virtual_op_num ( block_num , stop_diff = 0.1 , max_count = 100 ) <NEWLINE> if op_num > 0 : <NEWLINE> <INDENT> op_num -= 1 <NEWLINE> <DEDENT> self . assertTrue ( op_num < i ) <NEWLINE> i += 1 <NEWLINE> last_block = new_block <NEWLINE> <DEDENT> <DEDENT>
client = get_plugin ( <STRING> ) <NEWLINE> <INDENT> client . send_message ( topic = self . topic , msg = msg , host = self . host , <NEWLINE> <INDENT> port = self . port , username = self . username , <NEWLINE> password = self . password , tls_cafile = self . tls_cafile , <NEWLINE> tls_certfile = self . tls_certfile , <NEWLINE> tls_keyfile = self . tls_keyfile , <NEWLINE> tls_version = self . tls_version , <NEWLINE> tls_ciphers = self . tls_ciphers ) <NEWLINE> <DEDENT> <DEDENT>
self . _last_read = ret <NEWLINE> <INDENT> return ret <NEWLINE> <DEDENT>
with self . _init_lock : <NEWLINE> <INDENT> if self . _initialized or GPIO . getmode ( ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT>
def process_data ( self , data , new_data ) : <NEWLINE> <INDENT> if new_data : <NEWLINE> <INDENT> self . bus . post ( SensorDataChangeEvent ( data = data , source = self . plugin or self . __class__ . __name__ ) ) <NEWLINE> <DEDENT> <DEDENT>
if hasattr ( nb . metadata . latex_doc , <STRING> ) : <NEWLINE> <INDENT> bib = nb . metadata . latex_doc . bibliography <NEWLINE> bib = self . resolve_path ( bib , self . metapath ) <NEWLINE> if not os . path . exists ( bib ) : <NEWLINE> <INDENT> logging . warning ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( bib ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> external_files . append ( bib ) <NEWLINE> resources [ <STRING> ] = external_files <NEWLINE> <DEDENT> <DEDENT>
elif name == <STRING> : <NEWLINE> <INDENT> if self . thread : <NEWLINE> <INDENT> mailcountstring = <STRING> % self . thread . get_total_messages ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> mailcountstring = <STRING> <NEWLINE> <DEDENT> datestring = pad ( mailcountstring ) <NEWLINE> width = len ( mailcountstring ) <NEWLINE> mailcount_w = AttrFlipWidget ( urwid . Text ( mailcountstring ) , <NEWLINE> <INDENT> struct [ <STRING> ] ) <NEWLINE> <DEDENT> part = mailcount_w <NEWLINE> elif name == <STRING> : <NEWLINE> if self . thread : <NEWLINE> <INDENT> authors = self . thread . get_authors_string ( ) or <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> authors = <STRING> <NEWLINE> <DEDENT> authorsstring = pad ( authors , shorten_author_string ) <NEWLINE> authors_w = AttrFlipWidget ( urwid . Text ( authorsstring ) , <NEWLINE> <INDENT> struct [ <STRING> ] ) <NEWLINE> <DEDENT> width = len ( authorsstring ) <NEWLINE> part = authors_w <NEWLINE> <DEDENT>
def distance ( self , s0 , s1 ) : <NEWLINE> <INDENT> if s0 is None : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT> if s1 is None : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT> if s0 == s1 : <NEWLINE> <INDENT> return 0.0 <NEWLINE> <DEDENT> if len ( s0 ) == 0 : <NEWLINE> <INDENT> return len ( s1 ) <NEWLINE> <DEDENT> if len ( s1 ) == 0 : <NEWLINE> <INDENT> return len ( s1 ) <NEWLINE> <DEDENT> <DEDENT>
def check_new_log ( ) : <NEWLINE> <INDENT> log2 = open ( logpath ) . readlines ( ) <NEWLINE> return len ( log2 ) > 1 and log2 [ 0 ] . endswith ( <STRING> ) <NEWLINE> wait_for ( check_new_log ) <NEWLINE> <DEDENT>
def read ( self , io_in : BufferedIOBase , otherfields : Dict [ str , Any ] ) -> Optional [ Dict [ str , Any ] ] : <NEWLINE> <INDENT> vals = { } <NEWLINE> for field in self . fields : <NEWLINE> <INDENT> val = field . fieldtype . read ( io_in , otherfields ) <NEWLINE> if val is None : <NEWLINE> <COMMENT> <NL> <INDENT> if field == self . fields [ 0 ] : <NEWLINE> <INDENT> return None <NEWLINE> <COMMENT> <NL> <DEDENT> if field . option is not None : <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> raise ValueError ( <STRING> . format ( self , field ) ) <NEWLINE> <DEDENT> vals [ field . name ] = val <NEWLINE> <DEDENT> <DEDENT>
if self . tested == PyFunceble . CONFIGURATION [ <STRING> ] : <NEWLINE> <INDENT> url_to_get = <STRING> & self . tested <NEWLINE> else : <NEWLINE> url_to_get = self . tested <NEWLINE> <DEDENT>
if path_to_config . endswith ( directory_separator ) : <NEWLINE> <INDENT> self . path_to_config += directory_separator <NEWLINE> <DEDENT>
if ( <NEWLINE> <INDENT> number_of_tested == 0 <NEWLINE> or list_to_test [ number_of_tested - 1 ] == list_to_test [ - 1 ] <NEWLINE> or number_of_tested == len ( list_to_test ) <NEWLINE> ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>
if is_cloned_version and ( <NEWLINE> <INDENT> PyFunceble . CONFIGURATION . db_type not in not_supported_db_types <NEWLINE> ) : <NEWLINE> destination_dir_instance . delete ( ) <NEWLINE> <DEDENT>
if not content : <NEWLINE> <INDENT> content += <STRING> <NEWLINE> continue <NEWLINE> <DEDENT>
with bz2 . BZ2File ( bad_pkg_path , <STRING> ) as f : <NEWLINE> <INDENT> f . write ( <STRING> . encode ( ) ) <NEWLINE> assert bad_pkg_name in os . listdir ( bad_pkg_root ) <NEWLINE> conda_mirror . _validate_packages ( repodata , local_repo_root ) <NEWLINE> assert bad_pkg_name not in os . listdir ( bad_pkg_root ) <NEWLINE> <DEDENT>
def handle_message ( self , msg ) : <NEWLINE> <INDENT> if self . categories . intersection ( msg . categories ) and msg . level > self . level : <NEWLINE> <INDENT> self . notify ( msg ) <NEWLINE> <DEDENT> <DEDENT>
def _check_dates ( self , master , files ) : <NEWLINE> <INDENT> mtime = os . path . getmtime ( master ) <NEWLINE> for f in files : <NEWLINE> <INDENT> fpath = media_url_to_filepath ( f ) <NEWLINE> if os . path . getmtime ( fpath ) > mtime : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>
if status != 200 : <NEWLINE> <INDENT> raise WebSocketProxyException ( <NEWLINE> <INDENT> <STRING> + status ) <NEWLINE> <DEDENT> <DEDENT>
with open ( filePathBase + <STRING> , <STRING> , encoding = <STRING> ) as toFile : <NEWLINE> <INDENT> write_header ( glos , toFile , frontBackMatter ) <NEWLINE> for entryI , entry in enumerate ( glos ) : <NEWLINE> <INDENT> if glos . isData ( ) : <NEWLINE> <INDENT> entry . save ( myResDir ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _readersEntryGen ( self ) -> Iterator [ BaseEntry ] : <NEWLINE> <INDENT> for reader in self . _readers : <NEWLINE> <INDENT> wordCount = 0 <NEWLINE> progressbar = False <NEWLINE> if self . ui and self . _progressbar : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> wordCount = len ( reader ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> log . exception ( <STRING> ) <NEWLINE> <DEDENT> if wordCount > 0 : <NEWLINE> <INDENT> progressbar = True <NEWLINE> <DEDENT> <DEDENT> if progressbar : <NEWLINE> <INDENT> self . progressInit ( <STRING> ) <NEWLINE> <DEDENT> wcThreshold = wordCount // 200 + 1 <NEWLINE> lastPos = 0 <NEWLINE> try : <NEWLINE> <INDENT> for index , entry in enumerate ( reader ) : <NEWLINE> <INDENT> yield entry <NEWLINE> if progressbar : <NEWLINE> <INDENT> if entry is None or wordCount > 0 : <NEWLINE> <INDENT> if index % wcThreshold == 0 : <NEWLINE> <INDENT> self . progress ( index , wordCount ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> bp = entry . byteProgress ( ) <NEWLINE> if bp and bp [ 0 ] > lastPos + 10000 : <NEWLINE> <INDENT> self . progress ( bp [ 0 ] , bp [ 1 ] , unit = <STRING> ) <NEWLINE> lastPos = bp [ 0 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> reader . close ( ) <NEWLINE> <DEDENT> if progressbar : <NEWLINE> <INDENT> self . progressEnd ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def __next__ ( self ) -> BaseEntry : <NEWLINE> <INDENT> self . _pos += 1 <NEWLINE> try : <NEWLINE> <INDENT> return self . _pendingEntries . pop ( 0 ) <NEWLINE> <DEDENT> except IndexError : <NEWLINE> <INDENT> pass <NEWLINE> <COMMENT> <NL> <DEDENT> try : <NEWLINE> <INDENT> wordDefi = self . nextPair ( ) <NEWLINE> <DEDENT> except StopIteration as e : <NEWLINE> <INDENT> if self . _fileIndex < self . _fileCount + 1 : <NEWLINE> <INDENT> if self . openNextFile ( ) : <NEWLINE> <INDENT> return self . __next__ ( ) <NEWLINE> <DEDENT> <DEDENT> self . _wordCount = self . _pos <NEWLINE> raise e <NEWLINE> <DEDENT> if not wordDefi : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> word , defi = wordDefi <NEWLINE> <COMMENT> <NL> return Entry ( word , defi ) <NEWLINE> <DEDENT>
if list_observed_dom : <NEWLINE> <INDENT> bodie [ <STRING> ] = list ( set ( list_observed_ip ) ) <NEWLINE> <DEDENT>
return value <NEWLINE>
def get_blocks_container_html ( self , errors = None ) : <NEWLINE> <INDENT> help_text = getattr ( self . meta , <STRING> , None ) <NEWLINE> if isinstance ( errors , StreamBlockValidationError ) : <NEWLINE> <INDENT> non_block_errors = ( <NEWLINE> <INDENT> ( ) if errors is None <NEWLINE> else errors . as_data ( ) [ 0 ] . params . get ( NON_FIELD_ERRORS , ( ) ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> non_block_errors = errors <NEWLINE> <DEDENT> if help_text and non_block_errors : <NEWLINE> <INDENT> return render_to_string ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> { <NEWLINE> <INDENT> <STRING> : help_text , <NEWLINE> <STRING> : non_block_errors , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
elif qmean == <STRING> : <COMMENT> <NEWLINE> <INDENT> pca = sklearn . decomposition . PCA ( n_components = self . K , copy = True , whiten = True ) <NEWLINE> pca . fit ( s . concatenate ( self . data , axis = 0 ) . T ) <NEWLINE> qmean = pca . components_ . T <NEWLINE> <DEDENT>
def _actuallyPause ( ) : <NEWLINE> <INDENT> fount = self . _siphon . _tdrain . fount <NEWLINE> self . _siphon . _pending . suspend ( ) <NEWLINE> if fount is None : <NEWLINE> <INDENT> pbpc = fount . pauseFlow ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pbpc = NoPause ( ) <NEWLINE> <DEDENT> self . _siphon . _pauseBecausePauseCalled = pbpc <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> orig_field = db_field . translated_field <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> orig_formfield = self . formfield_for_dbfield ( orig_field , ** kwargs ) <NEWLINE> field . widget = deepcopy ( orig_formfield . widget ) <NEWLINE> if orig_field . null and isinstance ( field . widget , ( forms . TextInput , forms . Textarea ) ) : <NEWLINE> <INDENT> field . widget = ClearableWidgetWrapper ( field . widget ) <NEWLINE> <DEDENT> css_classes = field . widget . attrs . get ( <STRING> , <STRING> ) . split ( <STRING> ) <NEWLINE> css_classes . append ( <STRING> ) <NEWLINE> <COMMENT> <NL> css_classes . append ( build_css_class ( db_field . name , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
UI_element_group . name = xml_element . attrib [ <STRING> ] <NEWLINE> <INDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . timedelay = float ( xml_element . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . parent_string = xml_element . attrib [ <STRING> ] <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . start_func = self . get_func_by_name ( xml_element . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . stop_func = self . get_func_by_name ( xml_element . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . identifier_string = xml_element . attrib [ <STRING> ] <NEWLINE> UI_element_group . identifier = identifier_parser . parse ( UI_element . identifier_string , lexer = identifier_lexer ) <NEWLINE> <DEDENT> <DEDENT>
def test_volume_path_with_non_ascii_directory ( self ) : <NEWLINE> <INDENT> volume = <STRING> <NEWLINE> container_path = config . resolve_volume_path ( volume , <STRING> , <STRING> ) <NEWLINE> self . assertEqual ( container_path , volume ) <NEWLINE> <DEDENT>
def test_volume_path_with_non_ascii_directory ( self ) : <NEWLINE> <INDENT> volume = <STRING> <NEWLINE> container_path = config . resolve_volume_path ( volume , <STRING> , <STRING> ) <NEWLINE> self . assertEqual ( container_path , volume ) <NEWLINE> <DEDENT>
def _gen_asset_class ( sym ) : <NEWLINE> <INDENT> sym_class = str ( sym ) . split ( <STRING> ) <NEWLINE> if len ( sym_class ) > 0 : <NEWLINE> <INDENT> return sym_class [ 1 ] <NEWLINE> <DEDENT> return <STRING> <NEWLINE> <DEDENT>
if self . tied_weights : <NEWLINE> <INDENT> for i in range ( len ( self . weights ) - 1 , - 1 , - 1 ) : <NEWLINE> <INDENT> h = self . hiddens [ - 1 ] <NEWLINE> a , b = self . weights [ i ] . get_value ( borrow = True ) . shape <NEWLINE> logging . info ( <STRING> , i , b , a ) <NEWLINE> o = theano . shared ( np . zeros ( ( b , ) , FLOAT ) , name = <STRING> . format ( i ) ) <NEWLINE> self . preacts . append ( TT . dot ( h , self . weights [ i ] . T ) + o ) <NEWLINE> func = self . _output_func if i == 0 else self . _hidden_func <NEWLINE> self . hiddens . append ( func ( self . preacts [ - 1 ] ) ) <NEWLINE> <DEDENT> <DEDENT>
size = kwargs . get ( <STRING> , kwargs . get ( <STRING> , 32 ) ) <NEWLINE> <INDENT> self . callable = None <NEWLINE> self . batches = None <NEWLINE> if len ( data ) == 1 and isinstance ( data [ 0 ] , collections . Callable ) : <NEWLINE> <INDENT> self . callable = data [ 0 ] <NEWLINE> if not self . number_batches : <NEWLINE> <INDENT> self . number_batches = size <NEWLINE> <DEDENT> logging . info ( <STRING> , <NEWLINE> <INDENT> self . label , self . number_batches ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> shape = data [ 0 ] . shape <NEWLINE> axis = kwargs . get ( <STRING> , 1 if len ( shape ) == 3 else 0 ) <NEWLINE> slices = [ slice ( None ) , slice ( None ) ] <NEWLINE> self . batches = [ ] <NEWLINE> i = 0 <NEWLINE> while i + size < shape [ axis ] : <NEWLINE> <INDENT> slices [ axis ] = slice ( i , i + size ) <NEWLINE> self . batches . append ( [ d [ tuple ( slices ) ] for d in data ] ) <NEWLINE> i += size <NEWLINE> <DEDENT> self . shuffle ( ) <NEWLINE> if not self . number_batches : <NEWLINE> <INDENT> self . number_batches = len ( self . batches ) <NEWLINE> <DEDENT> logging . info ( <STRING> , <NEWLINE> <INDENT> self . label , self . number_batches , len ( self . batches ) , <NEWLINE> <STRING> . join ( str ( x . shape ) for x in self . batches [ 0 ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
cstr_parts = [ ] <NEWLINE> <INDENT> if dsn : <NEWLINE> <INDENT> cstr_parts . append ( <STRING> % dsn ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> cstr_parts . append ( <STRING> % driver ) <NEWLINE> if ms_drivers . match ( driver ) or driver == <STRING> and conn_params . get ( <STRING> , False ) : <NEWLINE> <INDENT> if port : <NEWLINE> <INDENT> host += <STRING> % port <NEWLINE> <DEDENT> cstr_parts . append ( <STRING> % host ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cstr_parts . append ( <STRING> % host ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not on_rtd : <COMMENT> <NEWLINE> <INDENT> import sphinx_rtd_theme <NEWLINE> html_theme = <STRING> <NEWLINE> html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] <NEWLINE> else : <NEWLINE> html_theme = <STRING> <NEWLINE> <DEDENT>
positions = [ ] <NEWLINE> <INDENT> ix = 0 <NEWLINE> taken_positions = set ( fixed_positions . values ( ) ) <NEWLINE> for i in range ( n_plots ) : <NEWLINE> <INDENT> if i in fixed_positions : <NEWLINE> <INDENT> positions . append ( fixed_positions [ i ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> row , col = ix / n_cols , ix % n_cols <NEWLINE> if ( row , col ) not in taken_positions : <NEWLINE> <INDENT> positions . append ( ( row , col ) ) <NEWLINE> taken_positions . add ( ( row , col ) ) <NEWLINE> i += 1 <NEWLINE> break <NEWLINE> <DEDENT> ix += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
report . add_error ( CoordinateConsistencyError ( tag , child . id , file_id , <NEWLINE>
class ASTCompAndOr ( ASTNode ) : <NEWLINE> <INDENT> op = None <NEWLINE> def __init__ ( self , comp , comps ) : <NEWLINE> <INDENT> super ( ASTCompAndOr , self ) . __init__ ( comp . token ) <NEWLINE> self . comps = [ comp ] <NEWLINE> for c in comps : <NEWLINE> <INDENT> self . comps . append ( comp ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
total_examples = 0 <NEWLINE> <INDENT> tf . enable_eager_execution ( ) <NEWLINE> task = t5 . data . TaskRegistry . get ( FLAGS . task ) <NEWLINE> files = task . tfds_dataset . files ( FLAGS . split ) <NEWLINE> def _example_to_string ( ex ) : <NEWLINE> <INDENT> key_to_string = { } <NEWLINE> for k in ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> if k in ex : <NEWLINE> <INDENT> v = ex [ k ] . numpy ( ) <NEWLINE> key_to_string [ k ] = ( <NEWLINE> <INDENT> <STRING> . join ( str ( i ) for i in v ) if FLAGS . tokenize <NEWLINE> else v . decode ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v [ k ] = <STRING> <NEWLINE> <DEDENT> <DEDENT> return FLAGS . format_string . format ( ** key_to_string ) <NEWLINE> <DEDENT> <DEDENT>
self [ <STRING> ] = collection <NEWLINE> <INDENT> self [ <STRING> ] = field <NEWLINE> self [ <STRING> ] = coercion_type <NEWLINE> <DEDENT>
if ( len ( modules ) < 2 or <NEWLINE> <INDENT> modules [ 0 ] . value != <STRING> and modules [ 1 ] . value != <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT>
def poll ( self ) : <NEWLINE> <INDENT> flags = [ ] <NEWLINE> if self . sending : <NEWLINE> <INDENT> self . sending = False <NEWLINE> slist = [ ( self . sock , ) , ( self . sock , ) , ( self . sock , ) ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> slist = [ ( self . sock , ) , ( ) , ( self . sock , ) ] <NEWLINE> <DEDENT> timeout = self . timer . get_timeout ( ) <NEWLINE> if timeout > 0 : <NEWLINE> <INDENT> slist . append ( timeout ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> rlist , wlist , xlist = select . select ( * slist ) <NEWLINE> <DEDENT> except select . error as e : <NEWLINE> <INDENT> print ( str ( e ) ) <NEWLINE> rlist = [ ] <NEWLINE> wlist = [ ] <NEWLINE> xlist = [ ] <NEWLINE> <DEDENT> if rlist : flags . append ( <STRING> ) <NEWLINE> if wlist : flags . append ( <STRING> ) <NEWLINE> if xlist : flags . append ( <STRING> ) <NEWLINE> return flags <NEWLINE> <DEDENT>
@ pl_announce ( <STRING> ) <NEWLINE> <INDENT> class SettingsPlugin : <NEWLINE> <INDENT> def __init__ ( self , ploader , kwargs ) : <NEWLINE> <INDENT> settings = get_settings ( kwargs , kwargs . get ( <STRING> , { } ) ) <NEWLINE> plugin_list = settings . get ( <STRING> , DefaultPlugins ) <NEWLINE> plugins = [ ] <NEWLINE> plugin_settings = { } <NEWLINE> for plugin in plugin_list : <NEWLINE> <INDENT> plugins . append ( plugin [ 1 ] ) <NEWLINE> plugin_settings [ plugin [ 1 ] ] = settings . get ( plugin [ 0 ] , { } ) <NEWLINE> <DEDENT> ploader . provides ( <STRING> , PloaderFetch ( plugins , plugin_settings ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
class StartPlugin : <NEWLINE> <INDENT> def __init__ ( self , ploader , settings ) : <NEWLINE> <INDENT> self . settings = utils . get_settings ( settings , default_settings ) <NEWLINE> self . event = ploader . requires ( <STRING> ) <NEWLINE> self . net = ploader . requires ( <STRING> ) <NEWLINE> self . auth = ploader . requires ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def _find_left_index ( ss_waypoints , s ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for i in range ( 1 , len ( ss_waypoints ) ) : <NEWLINE> <INDENT> if ss_waypoints [ i - 1 ] <= s and s < ss_waypoints [ i ] : <NEWLINE> <INDENT> return i - 1 <NEWLINE> <DEDENT> <DEDENT> return len ( ss_waypoints ) - 1 <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> ret = _rdparm . rdparm ( fname ) <NEWLINE> <DEDENT> except TypeError : <NEWLINE> <COMMENT> <NL> <INDENT> raise <NEWLINE> return self . rdparm_old ( open ( fname , <STRING> ) . readlines ( ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> parm_data , parm_comments , formats , unkflg , flag_list , version = ret <NEWLINE> <COMMENT> <NL> self . parm_data = parm_data <NEWLINE> self . parm_comments = parm_comments <NEWLINE> for key in formats : <NEWLINE> <INDENT> self . formats [ key ] = FortranFormat ( formats [ key ] ) <NEWLINE> <DEDENT> self . flag_list = flag_list <NEWLINE> self . version = version <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> for flag in unkflg : <NEWLINE> <INDENT> rawdata = self . parm_data [ flag ] <NEWLINE> self . parm_data [ flag ] = [ ] <NEWLINE> for line in rawdata : <NEWLINE> <INDENT> self . parm_data [ flag ] . extend ( self . formats [ key ] . read ( line ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if isinstance ( timeout , ( int , long , float ) ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> self . timeout = timeout <NEWLINE> <DEDENT>
if order : <NEWLINE> <INDENT> kwargs . append ( ( <STRING> , order ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if <STRING> . format ( Cal . coefficients ) == <STRING> . format ( coefficients ) : <NEWLINE> <INDENT> logging . debug ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
for index , parent_field_name in enumerate ( parent_field_names ) : <NEWLINE> <INDENT> dictobj = dictobj . get ( parent_field_name ) <NEWLINE> if not dictobj : <NEWLINE> <COMMENT> <NL> <INDENT> return handle_missing_field ( <NEWLINE> <INDENT> <STRING> . join ( level_field_names [ : index + 1 ] ) <NEWLINE> <DEDENT> ) <NEWLINE> if final_field_name not in dictobj : <NEWLINE> <DEDENT> return handle_missing_field ( field_name ) <NEWLINE> return dictobj . get ( final_field_name ) <NEWLINE> <DEDENT>
psw . grid ( row = 0 , column = 3 , sticky = tkinter . W ) <NEWLINE> <INDENT> pbndl = Label ( paramf , text = <STRING> , width = 10 , anchor = <STRING> ) <NEWLINE> pbndl . grid ( row = 1 , column = 0 , sticky = tkinter . E ) <NEWLINE> pbnde1 = Entry ( paramf , width = 8 ) <NEWLINE> pbnde2 = Entry ( paramf , width = 8 ) <NEWLINE> pbnde1 . grid ( row = 1 , column = 1 , sticky = tkinter . W ) <NEWLINE> pbnde2 . grid ( row = 1 , column = 2 , sticky = tkinter . W ) <NEWLINE> lbnd = xrsdkit . param_bound_defaults [ param_nm ] [ 0 ] <NEWLINE> ubnd = xrsdkit . param_bound_defaults [ param_nm ] [ 1 ] <NEWLINE> if xrsdkit . contains_param ( self . inputs [ <STRING> ] , pop_nm , param_nm ) : <NEWLINE> <INDENT> lbnd = self . inputs [ <STRING> ] [ pop_nm ] [ <STRING> ] [ param_nm ] [ 0 ] <NEWLINE> lbnd = self . inputs [ <STRING> ] [ pop_nm ] [ <STRING> ] [ param_nm ] [ 1 ] <NEWLINE> <COMMENT> <NL> <DEDENT> pbnde1 . insert ( 0 , str ( lbnd ) ) <NEWLINE> pbnde2 . insert ( 0 , str ( ubnd ) ) <NEWLINE> pexpl = Label ( paramf , text = <STRING> , width = 10 , anchor = <STRING> ) <NEWLINE> pexpl . grid ( row = 2 , column = 0 , sticky = tkinter . E ) <NEWLINE> pexpe = Entry ( paramf , width = 16 ) <NEWLINE> if xrsdkit . contains_param ( self . inputs [ <STRING> ] , pop_nm , param_nm ) : <NEWLINE> <INDENT> pexpe . insert ( 0 , self . inputs [ <STRING> ] , pop_nm , param_nm ) <NEWLINE> <COMMENT> <NL> <DEDENT> pexpe . grid ( row = 2 , column = 1 , columnspan = 3 , sticky = tkinter . E + tkinter . W ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> paramf . grid ( row = 4 + nstgs + ip , column = 0 , columnspan = 4 , sticky = tkinter . E + tkinter . W ) <NEWLINE> <DEDENT>
def getinfo ( self , path , namespaces = None ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . check ( ) <NEWLINE> namespaces = namespaces or ( ) <NEWLINE> _path = self . validatepath ( path ) <NEWLINE> sys_path = self . getsyspath ( _path ) <NEWLINE> _lstat = None <NEWLINE> with convert_os_errors ( <STRING> , path ) : <NEWLINE> <INDENT> _stat = os . stat ( fsencode ( sys_path ) ) <NEWLINE> if <STRING> in namespaces : <NEWLINE> <INDENT> _stat = os . lstat ( fsencode ( sys_path ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
pages -= 1 <NEWLINE> <INDENT> if pages == 0 : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT>
def test_search_members_1 ( self ) : <NEWLINE> <INDENT> self . client . login ( username = <STRING> , password = <STRING> ) <NEWLINE> member_1 = self . foo_list . subscribe ( <NEWLINE> <INDENT> <STRING> , pre_verified = True , pre_confirmed = True , <NEWLINE> pre_approved = True ) <NEWLINE> <DEDENT> member_2 = self . foo_list . subscribe ( <NEWLINE> <INDENT> <STRING> , pre_verified = True , pre_confirmed = True , <NEWLINE> pre_approved = True ) <NEWLINE> <DEDENT> response = self . client . get ( reverse ( <NEWLINE> <INDENT> <STRING> , args = [ <STRING> , <STRING> ] ) , <NEWLINE> { <STRING> : <STRING> } ) <NEWLINE> <DEDENT> self . assertEqual ( response . status_code , 200 ) <NEWLINE> self . assertEqual ( len ( response . context [ <STRING> ] ) , 2 ) <NEWLINE> self . assertContains ( response , member_1 . email ) <NEWLINE> self . assertContains ( response , member_2 . email ) <NEWLINE> response = self . client . get ( reverse ( <NEWLINE> <INDENT> <STRING> , args = [ <STRING> , <STRING> ] ) , <NEWLINE> { <STRING> : <STRING> } ) <NEWLINE> <DEDENT> self . assertEqual ( response . status_code , 200 ) <NEWLINE> self . assertEqual ( len ( response . context [ <STRING> ] ) , 1 ) <NEWLINE> self . assertContains ( response , member_1 . email ) <NEWLINE> self . assertNotContains ( response , member_2 . email ) <NEWLINE> response = self . client . get ( reverse ( <NEWLINE> <INDENT> <STRING> , args = [ <STRING> , <STRING> ] ) , <NEWLINE> { <STRING> : <STRING> } ) <NEWLINE> <DEDENT> self . assertEqual ( response . status_code , 200 ) <NEWLINE> self . assertEqual ( len ( response . context [ <STRING> ] ) , 0 ) <NEWLINE> self . assertNotContains ( response , member_2 . email ) <NEWLINE> self . assertNotContains ( response , member_2 . email ) <NEWLINE> <DEDENT>
sign = <STRING> if total_minutes > 0 else <STRING> <NEWLINE> <INDENT> total_minutes = abs ( total_minutes ) <NEWLINE> hour , minute = divmod ( total_minutes , 60 ) <NEWLINE> <DEDENT>
def calculate_anomaly_score ( self , log ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> dist_smallest = np . inf <NEWLINE> for x in range ( self . model . shape [ 0 ] ) : <NEWLINE> <INDENT> for y in range ( self . model . shape [ 1 ] ) : <NEWLINE> <INDENT> dist = np . linalg . norm ( self . model [ x ] [ y ] - log ) <NEWLINE> if dist < dist_smallest : <NEWLINE> <INDENT> dist_smallest = dist <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return dist <NEWLINE> <DEDENT>
if isinstance ( cieobs , str ) : <NEWLINE> <INDENT> cmfs = _CMF [ cieobs ] [ <STRING> ] <NEWLINE> else : <NEWLINE> cmfs = cieobs <NEWLINE> cmfs = cmfs [ : , cmfs [ 1 : ] . sum ( axis = 1 ) > 0 ] <COMMENT> <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> for node in doctree . traverse ( ItemLink ) : <NEWLINE> <INDENT> for source in node [ <STRING> ] : <NEWLINE> <INDENT> for target in node [ <STRING> ] : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> env . traceability_collection . add_relation ( target , node [ <STRING> ] , source ) <NEWLINE> <DEDENT> except TraceabilityException as err : <NEWLINE> <INDENT> report_warning ( env , err , env . docname , self . lineno ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> node . replace_self ( [ ] ) <NEWLINE> <DEDENT> <DEDENT>
def run ( self , args ) : <NEWLINE> <INDENT> if len ( args ) >= 1 and <STRING> == args [ 0 ] : <NEWLINE> <INDENT> highlight_type = self . get_highlight_type ( args [ 1 ] ) <NEWLINE> if not highlight_type : return <NEWLINE> clear_file_format_cache ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> highlight_type = self . get_highlight_type ( args [ 0 ] ) <NEWLINE> if not highlight_type : return <NEWLINE> if <STRING> == highlight_type : highlight_type = <STRING> <NEWLINE> pass <NEWLINE> <DEDENT> self . debugger . settings [ <STRING> ] = highlight_type <NEWLINE> show_cmd = self . proc . commands [ <STRING> ] <NEWLINE> show_cmd . run ( [ <STRING> , <STRING> ] ) <NEWLINE> return <NEWLINE> pass <NEWLINE> <DEDENT>
class TestSkip ( unittest . TestCase ) : <NEWLINE> <INDENT> @ unittest . skipIf ( <STRING> not in os . environ , <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> def test_skip ( self ) : <NEWLINE> <DEDENT>
b1 = 0x80 | ( opcode & 0x0f ) <COMMENT> <NEWLINE> <INDENT> payload_len = len ( buf ) <NEWLINE> if payload_len <= 125 : <NEWLINE> <INDENT> header = struct . pack ( <STRING> , b1 , payload_len ) <NEWLINE> <DEDENT> elif payload_len > 125 and payload_len <= 65536 : <NEWLINE> <INDENT> header = struct . pack ( <STRING> , b1 , 126 , payload_len ) <NEWLINE> <DEDENT> elif payload_len >= 65536 : <NEWLINE> <INDENT> header = struct . pack ( <STRING> , b1 , 127 , payload_len ) <NEWLINE> <DEDENT> <DEDENT>
if ( collect_dynamic is False ) : <NEWLINE> <INDENT> dyn_state . append ( self . _states ) ; <NEWLINE> dyn_time . append ( t ) ; <NEWLINE> <DEDENT>
if distance < self . _threshold : <NEWLINE> <INDENT> self . __append_to_cluster ( index_cluster , index_point , point ) ; <NEWLINE> elif distance > self . _threshold2 : <NEWLINE> self . __allocate_cluster ( index_point , point ) ; <NEWLINE> <DEDENT>
if expected_cluster_length is not None : <NEWLINE> <INDENT> assertion . eq ( len ( centers ) , len ( expected_cluster_length ) ) <NEWLINE> <DEDENT>
assert car1 < car2 <NEWLINE> <INDENT> assert car2 < car3 <NEWLINE> assert car3 < car2 <NEWLINE> assert car4 > car3 <NEWLINE> assert car1 . antecedent <= transaction1 <NEWLINE> assert car2 . antecedent <= transaction1 <NEWLINE> assert car3 . antecedent <= transaction1 <NEWLINE> assert not car4 . antecedent <= transaction1 <NEWLINE> assert sorted_cars [ 0 ] == car4 <NEWLINE> <DEDENT>
t1 . merge ( t2 ) <NEWLINE> <INDENT> self . assertEqual ( t1 . ugettext ( <STRING> ) , <STRING> ) <NEWLINE> self . assertEqual ( t2 . ugettext ( <STRING> ) , <STRING> ) <NEWLINE> <DEDENT>
if len ( cache ) == 0 : <NEWLINE> <INDENT> return False , <STRING> <NEWLINE> if len ( cache ) == 0 : <NEWLINE> return False , <STRING> <NEWLINE> try : <NEWLINE> if not path . isdir ( cls . directory ) : <NEWLINE> <INDENT> makedirs ( cls . directory ) <NEWLINE> <DEDENT> with open ( cls . file_path ( cache_path ) , <STRING> ) as f : <NEWLINE> <INDENT> size = f . write ( cache ) <NEWLINE> return True , size <NEWLINE> except Exception as e : <NEWLINE> <DEDENT> return False , str ( e ) <NEWLINE> return False , <STRING> <NEWLINE> <DEDENT>
def status_info ( self ) : <NEWLINE> <INDENT> print ( <STRING> . format ( len ( self . busying ) ) ) <NEWLINE> urls = [ ] <NEWLINE> for i , url in enumerate ( self . busying ) : <NEWLINE> <INDENT> if i > 3 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> urls . append ( url ) <NEWLINE> <DEDENT> <DEDENT>
boxes = [ ] <NEWLINE> <INDENT> for i , c in enumerate ( class_values ) : <NEWLINE> <INDENT> box = patches . Rectangle ( ( 0 , 0 ) , 20 , 10 , linewidth = .4 , edgecolor = colors [ <STRING> ] , <NEWLINE> <INDENT> facecolor = color_map [ c ] , label = class_names [ c ] ) <NEWLINE> <DEDENT> boxes . append ( box ) <NEWLINE> <DEDENT> <DEDENT>
lcolor = rcolor = colors [ <STRING> ] <NEWLINE> <INDENT> lpw = rpw = <STRING> <NEWLINE> if node . left . id in highlight_path : <NEWLINE> <INDENT> lcolor = colors [ <STRING> ] <NEWLINE> lpw = <STRING> <NEWLINE> <DEDENT> if node . right . id in highlight_path : <NEWLINE> <INDENT> lcolor = colors [ <STRING> ] <NEWLINE> rpw = <STRING> <NEWLINE> <DEDENT> edges . append ( <STRING> ) <NEWLINE> edges . append ( <STRING> ) <NEWLINE> edges . append ( <STRING> ) <NEWLINE> <DEDENT>
if getattr ( aq_base ( workspace ) , <STRING> , None ) : <NEWLINE> <INDENT> cont = False <NEWLINE> else : <NEWLINE> context = aq_parent ( context ) <NEWLINE> <DEDENT>
@ property <NEWLINE> <INDENT> def primary_opening_today ( self ) : <NEWLINE> <INDENT> today = date . today ( ) <NEWLINE> cache_key = self . get_opening_today_cache_key ( today ) <NEWLINE> times = cache . get ( cache_key ) <NEWLINE> if times is not None : <NEWLINE> <INDENT> opening_times = self . primary_opening_times <NEWLINE> if opening_times : <NEWLINE> <INDENT> specific_times = utils . first_true ( opening_times , lambda x : x . get ( <STRING> ) == today ) <NEWLINE> times = specific_times or utils . first_true ( opening_times , lambda x : x . get ( <STRING> ) == today . weekday ( ) ) <NEWLINE> cache . set ( cache_key , times , 60 * 60 * 24 ) <NEWLINE> <DEDENT> <DEDENT> return times <NEWLINE> <DEDENT> <DEDENT>
def handle_ctrlchan ( self , nick , msg , send , send_raw ) : <NEWLINE> <INDENT> cmd = msg . split ( ) <NEWLINE> if cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> send_raw ( <STRING> . join ( cmd [ 1 : ] ) ) <NEWLINE> <DEDENT> elif cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . kick_enabled = False <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . disabled_mods . append ( cmd [ 2 ] ) <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . kick_enabled = True <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . disabled_mods = [ i for i in self . disabled_mods if i != cmd [ 2 ] ] <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> if cmd [ 1 ] == <STRING> and cmd [ 2 ] == <STRING> : <NEWLINE> <INDENT> send ( str ( self . disabled_mods ) ) <NEWLINE> <DEDENT> if cmd [ 2 ] == <STRING> and cmd [ 2 ] == <STRING> : <NEWLINE> <INDENT> send ( str ( [ i for i in self . modules if i not in self . disabled_mods ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def groundwater_constraint ( evaluation , simulation ) : <NEWLINE> <INDENT> if ( evaluation [ 0 ] - 0.1 <= simulation [ 0 ] ) or ( simulation [ 0 ] <= evaluation [ 0 ] + 0.1 ) : <NEWLINE> <INDENT> return 1.0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return 0.0 <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , sate_id , source ) : <NEWLINE> <INDENT> super ( TLEPredictor , self ) . __init__ ( source , sate_id ) <NEWLINE> self . _iterations = 0 <NEWLINE> <DEDENT>
self . novel_id = urlparse ( self . novel_url ) . path . split ( <STRING> ) [ 1 ] <NEWLINE> <INDENT> logger . info ( <STRING> , self . novel_id ) <NEWLINE> <DEDENT>
def make_id ( name ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> r = get_rand_string ( 12 ) <NEWLINE> if len ( name ) <= 22 : <NEWLINE> <INDENT> name = name [ : 22 ] <NEWLINE> <DEDENT> return name + <STRING> + r <NEWLINE> <DEDENT>
def int_to_decimal_str ( integer ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> int_string = str ( integer ) <NEWLINE> if len ( int_string ) < 2 : <NEWLINE> <INDENT> return <STRING> + int_string . zfill ( 2 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return int_string [ : - 2 ] + <STRING> + int_string [ - 2 : ] <NEWLINE> <DEDENT> <DEDENT>
if not threshold == <STRING> : <NEWLINE> <INDENT> msg = <STRING> <NEWLINE> warnings . warn ( msg , DeprecationWarning ) <NEWLINE> threshold_rel = threshold <NEWLINE> <COMMENT> <NL> corner_threshold = max ( np . max ( image . ravel ( ) ) * threshold_rel , threshold_abs ) <NEWLINE> image_t = ( image >= corner_threshold ) * 1 <NEWLINE> <DEDENT>
if offset == None : <NEWLINE> <INDENT> if not all ( [ d % 2 == 1 for d in selem . shape ] ) : <NEWLINE> <INDENT> ValueError ( <STRING> ) <NEWLINE> <DEDENT> offset = np . array ( [ d / 2 for d in selem . shape ] ) <NEWLINE> <COMMENT> <NL> selem [ [ slice ( d , d + 1 ) for d in offset ] ] = False <NEWLINE> <DEDENT>
def get_relationship ( self , rel_type , rel_key ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> values = self . get ( RELATIONSHIP_CF , ENDPOINT_NAME_TEMPLATE % ( rel_type , rel_key ) ) <NEWLINE> <DEDENT> except NotFoundException : <NEWLINE> <INDENT> raise NodeNotFoundException ( ) <NEWLINE> <DEDENT> source_node_key = None <NEWLINE> source_node_type = None <NEWLINE> source_attributes = { } <NEWLINE> for column in values . keys ( ) : <NEWLINE> <INDENT> value = values [ column ] <NEWLINE> if column == <STRING> : <NEWLINE> <INDENT> source_node_type = value <NEWLINE> <DEDENT> elif column == <STRING> : <NEWLINE> <INDENT> source_node_key = value <NEWLINE> <DEDENT> elif column . startswith ( <STRING> ) : <NEWLINE> <INDENT> source_attributes [ column [ 8 : ] ] = value <NEWLINE> <DEDENT> <DEDENT> source = prim . Node ( self , source_node_type , source_node_key , values ) <NEWLINE> rel_key = RELATIONSHIP_KEY_PATTERN % ( rel_type , rel_key ) <NEWLINE> return self . get_outgoing_relationship ( rel_type , source , ( rel_key , values ) ) <NEWLINE> <DEDENT>
def _recursive_directory_find ( path : Path , directory_name : str ) -> str : <NEWLINE> <INDENT> if str ( path ) == expanduser ( <STRING> ) : <NEWLINE> <INDENT> raise FileNotFoundError ( ) <NEWLINE> <DEDENT> joined_with_driectory = path . joinpath ( directory_name ) <NEWLINE> if joined_with_driectory . is_dir ( ) : <NEWLINE> <INDENT> return str ( joined_with_driectory ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return _recursive_directory_find ( path . parent , directory_name ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if <STRING> in ann . sandbox . keys ( ) : <NEWLINE> <INDENT> for sliceop in ann . sandbox [ <STRING> ] : <NEWLINE> <COMMENT> <NL> <INDENT> tmpfiles = [ ] <NEWLINE> audio_files = [ audio_outfile ] + ann . sandbox . scaper . isolated_events_audio_path <NEWLINE> with _close_temp_files ( tmpfiles ) : <NEWLINE> <INDENT> for audio_file in audio_files : <NEWLINE> <COMMENT> <NL> <INDENT> tmpfiles . append ( <NEWLINE> <INDENT> tempfile . NamedTemporaryFile ( suffix = <STRING> , delete = False ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> tfm = sox . Transformer ( ) <NEWLINE> tfm . trim ( sliceop [ <STRING> ] , sliceop [ <STRING> ] ) <NEWLINE> tfm . build ( audio_file , tmpfiles [ - 1 ] . name ) <NEWLINE> <COMMENT> <NL> shutil . copyfile ( tmpfiles [ - 1 ] . name , audio_outfile ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
done = set ( ) <NEWLINE> <INDENT> dirs = [ ] <NEWLINE> for m in members : <NEWLINE> <INDENT> inf = self . getinfo ( m ) <NEWLINE> dst = self . _extract_one ( inf , path , pwd , not inf . is_dir ( ) ) <NEWLINE> if inf . is_dir ( ) : <NEWLINE> <INDENT> if dst not in done : <NEWLINE> <INDENT> dirs . append ( ( dst , inf ) ) <NEWLINE> done . add ( dst ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if dirs : <NEWLINE> <INDENT> dirs . sort ( reverse = True ) <NEWLINE> for dst , inf in dirs : <NEWLINE> <INDENT> self . _set_attrs ( dst , inf ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> x , y = 0 , 0 <NEWLINE> previous = 0 <NEWLINE> for c in text : <NEWLINE> <INDENT> face . load_char ( c ) <NEWLINE> bitmap = slot . bitmap <NEWLINE> top = slot . bitmap_top <NEWLINE> left = slot . bitmap_left <NEWLINE> w , h = bitmap . width , bitmap . rows <NEWLINE> y = height - baseline - top <NEWLINE> kerning = face . get_kerning ( previous , c ) <NEWLINE> x += ( kerning . x >> 6 ) <NEWLINE> Z [ y : y + h , x : x + w ] |= numpy . array ( bitmap . buffer ) . reshape ( h , w ) <NEWLINE> x += ( slot . advance . x >> 6 ) <NEWLINE> previous = c <NEWLINE> <DEDENT> <DEDENT>
if isinstance ( v1 , list ) and isinstance ( v2 , list ) : <NEWLINE> <INDENT> v1 . extend ( v2 ) <NEWLINE> return v2 <NEWLINE> <DEDENT>
p_t = p - self . lr * m_b_t / ( T . sqrt ( v_t ) + self . epsilon ) <NEWLINE>
if len ( self . items_buffer ) == self . settings . get ( <STRING> , 500 ) : <NEWLINE> <INDENT> self . send_items ( ) <NEWLINE> self . items_buffer = [ ] <NEWLINE> <DEDENT>
if read_lengths is None and psite_offsets is not None : <NEWLINE> <INDENT> sys . exit ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if read_lengths is not None and psite_offsets is not None : <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> psite_offsets = [ <NEWLINE> <INDENT> int ( x . strip ( ) ) for x in psite_offsets . strip ( ) . split ( <STRING> ) <NEWLINE> <DEDENT> ] <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> if len ( read_lengths ) != len ( psite_offsets ) : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> if not all ( x > 0 for x in psite_offsets ) : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> if not all ( x > y for ( x , y ) in zip ( read_lengths , psite_offsets ) ) : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> psite_offsets = dict ( zip ( read_lengths , psite_offsets ) ) <NEWLINE> if stranded == <STRING> : <NEWLINE> stranded = <STRING> <NEWLINE> detect_orfs ( bam , ribocop_index , prefix , stranded , read_lengths , <NEWLINE> <INDENT> psite_offsets , report_all ) <NEWLINE> <DEDENT> <DEDENT>
to_write = <STRING> . join ( columns ) <NEWLINE> <INDENT> formatter = <STRING> * ( len ( columns ) - 1 ) + <STRING> <NEWLINE> for orf in tqdm ( candidate_orfs ) : <NEWLINE> <INDENT> coordinate = <STRING> . join ( <NEWLINE> <INDENT> [ <STRING> . format ( iv . start , iv . end ) for iv in orf . intervals ] ) <NEWLINE> <DEDENT> to_write = formatter . format ( orf . oid , orf . category , orf . tid , orf . ttype , <NEWLINE> <INDENT> orf . gid , orf . gname , orf . gtype , orf . chrom , <NEWLINE> orf . strand , coordinate ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if re . search ( <STRING> , args . outfile ) is None : <NEWLINE> <INDENT> sscs_singleton_bam = pysam . AlignmentFile ( <STRING> . format ( args . outfile . split ( <STRING> ) [ 0 ] ) , <NEWLINE> <INDENT> <STRING> , template = sscs_bam ) <NEWLINE> <DEDENT> dcs_header = <STRING> <NEWLINE> sc_header = <STRING> <NEWLINE> else : <NEWLINE> sscs_singleton_bam = pysam . AlignmentFile ( <STRING> . format ( args . outfile . split ( <STRING> ) [ 0 ] ) , <NEWLINE> <INDENT> <STRING> , template = sscs_bam ) <NEWLINE> <DEDENT> dcs_header = <STRING> <NEWLINE> sc_header = <STRING> <NEWLINE> <DEDENT>
for attr in fields : <NEWLINE> <INDENT> model = getattr ( attr , <STRING> , None ) <NEWLINE> if ( model and issubclass ( model , base_cls ) and <NEWLINE> <INDENT> model is not generic_cls and getattr ( attr , <STRING> , True ) ) : <NEWLINE> <COMMENT> <NL> if not model . objects . is_generic ( ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> fk = model . _meta . get_field ( <STRING> ) <NEWLINE> if ctype == get_content_type ( fk . remote_field . model ) : <NEWLINE> <INDENT> return model <NEWLINE> return generic_cls <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def import_symbol ( sym , here = None , sep = <STRING> , ns = None ) : <NEWLINE> <INDENT> if ns is not None and sep not in sym : <NEWLINE> <INDENT> sym = <STRING> . format ( ns , sym ) <NEWLINE> <DEDENT> module_path , fn_name = sym . rsplit ( sep , 2 ) <NEWLINE> try : <NEWLINE> <INDENT> module = import_module ( sym , here = here , sep = sep ) <NEWLINE> return getattr ( module , fn_name ) <NEWLINE> <DEDENT> except ( ImportError , AttributeError ) as e : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> . format ( sym , e ) ) <NEWLINE> raise <NEWLINE> <DEDENT> <DEDENT>
for header_name , header_list in serialized [ <STRING> ] . items ( ) : <NEWLINE> <INDENT> if isinstance ( header_list , list ) : <NEWLINE> <INDENT> for header_value in header_list : <NEWLINE> <INDENT> header_dict . add ( header_name , header_list ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> header_dict . add ( header_name , header_list ) <NEWLINE> r . headers = CaseInsensitiveDict ( header_dict ) <NEWLINE> <DEDENT> <DEDENT>
@ strict_globals ( deque = deque , itemgetter = og_itemgetter ) <NEWLINE> <INDENT> def itemgetter ( iterable , indexes ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> indexes = indexes if isinstance ( indexes , tuple ) else tuple ( indexes ) <NEWLINE> assert all ( isinstance ( i , int ) for i in indexes ) , <STRING> <NEWLINE> positive_indexes = [ i for i in indexes if i >= 0 ] <NEWLINE> negative_indexes = [ i for i in indexes if i < 0 ] <NEWLINE> out = { } <NEWLINE> if len ( negative_indexes ) : <NEWLINE> <COMMENT> <NL> <INDENT> negative_index_buffer = deque ( maxlen = min ( indexes ) * - 1 ) <NEWLINE> for i , x in enumerate ( iterable ) : <NEWLINE> <INDENT> if i in positive_indexes : <NEWLINE> <INDENT> out [ i ] = x <NEWLINE> <DEDENT> negative_index_buffer . append ( i ) <NEWLINE> <DEDENT> out . update ( { ni : negative_index_buffer [ ni ] for ni in negative_indexes } ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> out . update ( { i : x for i , x in enumerate ( iterable ) if i in positive_indexes } ) <NEWLINE> <DEDENT> return itemgetter ( * indexes ) ( out ) <NEWLINE> <DEDENT> <DEDENT>
cv = StratifiedKFold ( <NEWLINE> <INDENT> n_splits = 5 , <NEWLINE> shuffle = True , <NEWLINE> random_state = 8 <NEWLINE> ) <NEWLINE> preds = [ ] <NEWLINE> trues_list = [ ] <NEWLINE> for train_index , test_index in cv . split ( X , y ) : <NEWLINE> X_train , X_test = secondary_features [ train_index ] , secondary_features [ test_index ] <NEWLINE> y_train , y_test = y [ train_index ] , y [ test_index ] <NEWLINE> est = est . fit ( X_train , y_train ) <NEWLINE> preds . append ( <NEWLINE> <INDENT> getattr ( est , stacked_ensemble . base_learner_origin . <NEWLINE> <INDENT> meta_feature_generator ) ( X_test ) <NEWLINE> <DEDENT> <DEDENT> ) <NEWLINE> trues_list . append ( y_test ) <NEWLINE> preds = np . concatenate ( preds , axis = 0 ) <NEWLINE> y_true = np . concatenate ( trues_list ) <NEWLINE> <DEDENT>
self . update ( ) <NEWLINE> <INDENT> if messagebox . askyesno ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> <NEWLINE> <STRING> , <NEWLINE> icon = <STRING> , parent = self ) : <NEWLINE> print ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>
class System ( system . System ) : <NEWLINE> <INDENT> def __init__ ( self , target ) : <NEWLINE> <INDENT> system . System . __init__ ( self ) <NEWLINE> <COMMENT> <NL> self . help = <STRING> <NEWLINE> self . valid = True <NEWLINE> <COMMENT> <NL> self . add_module_depend ( [ <STRING> ] ) <NEWLINE> self . add_export_flag ( <STRING> , <STRING> ) <NEWLINE> if env . get_isolate_system ( ) == False : <NEWLINE> <INDENT> self . add_header_file ( [ <NEWLINE> <INDENT> <STRING> <NEWLINE> ] , <NEWLINE> destination_path = <STRING> , <NEWLINE> recursive = True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def sanity_check ( self , outprefix ) : <NEWLINE> <INDENT> variants_only_removed = self . _remove_bad_genes ( self . seq_dicts [ <STRING> ] , outprefix + <STRING> ) <NEWLINE> presence_absence_removed = self . _remove_bad_genes ( self . seq_dicts [ <STRING> ] , outprefix + <STRING> ) <NEWLINE> self . _filter_bad_variant_data ( outprefix + <STRING> , variants_only_removed , presence_absence_removed ) <NEWLINE> <DEDENT>
if self . show_known_het and ( ref_name , variant ) in all_het_snps and key + <STRING> not in rows [ filename ] [ cluster ] : <NEWLINE> <INDENT> rows [ filename ] [ cluster ] [ key + <STRING> ] = <STRING> <NEWLINE> <DEDENT>
for d in cluster . data : <NEWLINE> <INDENT> if d [ <STRING> ] == best_hit [ <STRING> ] : <NEWLINE> <INDENT> if d [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> het_data . append ( d [ <STRING> ] ) <NEWLINE> depths = [ int ( x ) for x in d [ <STRING> ] . split ( <STRING> ) ] <NEWLINE> depths . sort ( ) <NEWLINE> het_pc = round ( 100.0 * depths [ - 1 ] / sum ( depths ) , 2 ) <NEWLINE> if results [ <STRING> ] == <STRING> or results [ <STRING> ] < het_pc : <NEWLINE> <INDENT> results [ <STRING> ] = het_pc <NEWLINE> if len ( het_data ) : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> results [ <STRING> ] = <STRING> . join ( het_data ) <NEWLINE> <DEDENT>
if __name__ == <STRING> : <NEWLINE> <INDENT> if check ( ) : <NEWLINE> <INDENT> sys . exit ( 1 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sys . exit ( 0 ) <NEWLINE> <DEDENT> <DEDENT>
return credential_response_schema . dumps ( permissions ) <NEWLINE>
if os . path . exists ( local_path ) : <NEWLINE> <INDENT> logging . error ( <STRING> , local_path ) <NEWLINE> <DEDENT>
class Tan ( AD ) : <NEWLINE> <INDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . tan = Sin ( func ) / Cos ( func ) <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . tan . cal ( x , dOrder ) <NEWLINE> class Tanh ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . tanh = - 1j * Tan ( func * 1j ) <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . tanh . cal ( x , dOrder ) <NEWLINE> <STRING> <NEWLINE> class Conjugate ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return np . conjugate ( self . func . cal ( x , dOrder ) ) <NEWLINE> class Real ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . func = func <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . func . cal ( x , dOrder ) . real <NEWLINE> class Imaginary ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . func = func <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . func . cal ( x , dOrder ) . imag <NEWLINE> class Absolute ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> self . abs = ( Real ( func ) ** 2. - Imaginary ( func ) ** 2. ) ** 0.5 <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . abs . cal ( x , dOrder ) <NEWLINE> <STRING> <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> v = next ( ( v for v , bias in poly . items ( ) if bias ) ) <NEWLINE> <DEDENT> except StopIteration : <NEWLINE> <COMMENT> <NL> <INDENT> scalar = 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> scalar = poly [ v ] / original [ v ] <NEWLINE> <DEDENT> <DEDENT>
self . assertEqual ( bqm , new ) <NEWLINE> <INDENT> self . assertEqual ( bqm . info , { <STRING> : 5 } ) <NEWLINE> <DEDENT>
rvals = np . empty ( 2 * r ) <NEWLINE> <INDENT> rvals [ 0 : r ] = range ( - r , 0 ) <NEWLINE> rvals [ r : ] = range ( 1 , r + 1 ) <NEWLINE> qdata = rnd . choice ( rvals , size = len ( variables ) ) <NEWLINE> <DEDENT>
response_json = responses_data [ 0 ] <NEWLINE> <INDENT> assert <STRING> not in response_json <NEWLINE> assert response_json [ <STRING> ] == request2 [ <STRING> ] <NEWLINE> assert response_json [ <STRING> ] == 5 <NEWLINE> <DEDENT>
mail_params = { <NEWLINE> <INDENT> <STRING> : blocks [ <STRING> ] . strip ( ) , <NEWLINE> <STRING> : blocks [ <STRING> ] <NEWLINE> } <NEWLINE> for ob in optional_blocks : <NEWLINE> if ob in blocks : <NEWLINE> <INDENT> if ob == <STRING> and mail_params [ ob ] . lower ( ) not in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> mail_params [ ob ] = blocks [ ob ] <NEWLINE> return mail_params <NEWLINE> <DEDENT> <DEDENT>
feature_dict = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : bin_labels_left , <NEWLINE> <STRING> : bin_labels_right , <NEWLINE> <STRING> : model_graph , <NEWLINE> <STRING> : bounds , <NEWLINE> } <NEWLINE> feature_list . append ( feature_dict ) <NEWLINE> density_dict . append ( { } ) <NEWLINE> <DEDENT>
def _validate_range ( self , start , minimum , maximum ) : <NEWLINE> <INDENT> if maximum <= minimum : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> _ ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> if start < minimum or start >= maximum : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> _ ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> rnrange = maximum - minimum <NEWLINE> return rnrange <NEWLINE> <DEDENT>
@ given ( bitmap_cls ) <NEWLINE> <INDENT> def test_shrink_to_fit ( self , cls ) : <NEWLINE> <INDENT> bm1 = BitMap ( ) <NEWLINE> size = 1000 <NEWLINE> for i in range ( size ) : <NEWLINE> <INDENT> bm1 . add ( i ) <NEWLINE> <DEDENT> bm2 = cls ( bm1 , optimize = False ) <NEWLINE> self . assertGreater ( bm2 . shrink_to_fit ( ) , 0 ) <NEWLINE> self . assertEqual ( bm2 . shrink_to_fit ( ) , 0 ) <NEWLINE> bm3 = cls ( bm1 , optimize = True ) <NEWLINE> self . assertEqual ( bm2 . shrink_to_fit ( ) , 0 ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> save_dir = maindir + <STRING> <NEWLINE> input_builder ( csvfile , initial_coords_dict , ebasis_dir , <NEWLINE> <INDENT> save_dir , title . replace ( <STRING> , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> save_dir = maindir + <STRING> <NEWLINE> input_builder ( csvfile , initial_coords_dict , ebasis_dir , <NEWLINE> <INDENT> save_dir , title . replace ( <STRING> , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> save_dir = maindir + <STRING> <NEWLINE> input_builder ( csvfile , save_dir , ebasis_dir , <NEWLINE> <INDENT> initial_coords_dict , title . replace ( <STRING> , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> book = load_workbook ( sheetsdir + dir + <STRING> ) <NEWLINE> with pd . ExcelWriter ( sheetsdir + dir + <STRING> , engine = <STRING> ) as writer : <NEWLINE> <INDENT> writer . book = book <NEWLINE> writer . sheets = dict ( ( ws . title , ws ) for ws in book . worksheets ) <NEWLINE> if opt in df : <NEWLINE> <INDENT> df [ opt ] . to_excel ( writer , sheet_name = opt , startrow = 6 ) <NEWLINE> <DEDENT> if hes in df : <NEWLINE> <INDENT> df [ hes ] . to_excel ( writer , sheet_name = hes , startrow = 6 ) <NEWLINE> <DEDENT> if ram in df : <NEWLINE> <INDENT> df [ ram ] . to_excel ( writer , sheet_name = ram , startrow = 6 ) <NEWLINE> <DEDENT> if vsc in df : <NEWLINE> <INDENT> df [ vsc ] . to_excel ( writer , sheet_name = vsc , startrow = 6 ) <NEWLINE> <DEDENT> if cmp in df : <NEWLINE> <INDENT> df [ cmp ] . to_excel ( writer , sheet_name = vsc , startrow = 6 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> with sqlite3 . connect ( tmp_path ) as conn : <NEWLINE> <INDENT> seq ( elements ) . to_sqlite3 ( tmp_path , insert_sql ) <NEWLINE> result = seq . sqlite3 ( conn , <STRING> ) . to_list ( ) <NEWLINE> self . assertListEqual ( elements , result ) <NEWLINE> <DEDENT> <DEDENT>
def feat_map ( x ) : <NEWLINE> <INDENT> scale = 2 * B . pi / B . cast ( period , x . dtype ) <NEWLINE> return B . concatenate ( ( B . sin ( x * scale ) , <NEWLINE> <INDENT> B . cos ( x * scale ) ) , axis = 0 ) <NEWLINE> <DEDENT> <DEDENT>
def label ( self , data ) : <NEWLINE> <INDENT> core_labels , n_labels = label ( data <= self . max_intensity ) <NEWLINE> ws_labels = watershed ( data . max ( ) - data , markers = core_labels , mask = data >= self . min_intensity ) <NEWLINE> return ws_labels <NEWLINE> <DEDENT>
def label ( self , data ) : <NEWLINE> <INDENT> core_labels , n_labels = label ( data >= self . max_intensity ) <NEWLINE> ws_labels = watershed ( data . max ( ) - data , markers = core_labels , mask = data >= self . min_intensity ) <NEWLINE> return ws_labels <NEWLINE> <DEDENT>
def label ( self , data ) : <NEWLINE> <INDENT> core_labels , n_labels = label ( data <= self . max_intensity ) <NEWLINE> ws_labels = watershed ( data . max ( ) - data , markers = core_labels , mask = data >= self . min_intensity ) <NEWLINE> return ws_labels <NEWLINE> <DEDENT>
total_bandwidth = 480 <NEWLINE> <INDENT> num_obs = 8 <COMMENT> <NEWLINE> subband_width = 60 <COMMENT> <NEWLINE> num_subb = total_bandwidth / subband_width <NEWLINE> subband_dict = collections . defaultdict ( list ) <COMMENT> <NEWLINE> img_list = [ ] <NEWLINE> start_freq = 940 <NEWLINE> <DEDENT>
for dm , drop in ( dm1 , a ) , ( dm2 , b ) , ( dm2 , c ) : <NEWLINE> <INDENT> self . assertEqual ( DROPStates . COMPLETED , dm . get_drop_property ( sessionId , <STRING> , drop . uid ) ) <NEWLINE> self . assertEqual ( a . checksum , int ( droputils . allDropContents ( c ) ) ) <NEWLINE> <DEDENT>
def _ngas_and_fs_io ( self , command ) : <NEWLINE> <INDENT> a = NgasDROP ( <STRING> , <STRING> ) <COMMENT> <NEWLINE> b = DockerApp ( <STRING> , <STRING> , image = <STRING> , command = command ) <NEWLINE> c = FileDROP ( <STRING> , <STRING> ) <NEWLINE> b . addInput ( a ) <NEWLINE> b . addOutput ( c ) <NEWLINE> with DROPWaiterCtx ( self , b , 100 ) : <NEWLINE> <INDENT> a . setCompleted ( ) <NEWLINE> <DEDENT> self . assertEqual ( six . b ( a . dataURL ) , droputils . allDropContents ( c ) ) <NEWLINE> <DEDENT>
def check_log_dir ( self , log_dir ) : <NEWLINE> <INDENT> possible_logs = [ <NEWLINE> os . path . join ( log_dir , <STRING> , <STRING> ) , <NEWLINE> os . path . join ( log_dir , <STRING> , <STRING> ) <NEWLINE> ] <NEWLINE> for dim_log_f in possible_logs : <NEWLINE> <INDENT> if ( os . path . exists ( dim_log_f ) ) : <NEWLINE> <INDENT> self . _dim_log_f = [ dim_log_f ] <NEWLINE> if ( dim_log_f == possible_logs [ 1 ] ) : <NEWLINE> <INDENT> cluster_log = os . path . join ( log_dir , <STRING> , <STRING> ) <NEWLINE> if ( os . path . exists ( cluster_log ) ) : <NEWLINE> <INDENT> self . _dim_log_f . append ( cluster_log ) <NEWLINE> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>
def add_node ( self , u ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> kwargs = dict ( ) <NEWLINE> if ( self . _tmp_max_dop is None ) : <NEWLINE> <INDENT> self . _tmp_max_dop = dict ( ) <NEWLINE> <DEDENT> self_global_dag = self . _global_dag <NEWLINE> for _w_attr in self . _w_attr : <NEWLINE> <INDENT> u_aw = self_global_dag . node [ u ] . get ( _w_attr , 1 ) <NEWLINE> kwargs [ _w_attr ] = u_aw <NEWLINE> <DEDENT> kwargs [ <STRING> ] = self_global_dag . node [ u ] . get ( <STRING> , 5 ) <NEWLINE> self . _dag . add_node ( u , ** kwargs ) <NEWLINE> for k in self . _w_attr : <NEWLINE> <INDENT> self . _tmp_max_dop [ _w_attr ] = get_max_weighted_antichain ( self . _dag , w_attr = k ) [ 0 ] <NEWLINE> <DEDENT> self . _max_dop = self . _tmp_max_dop <NEWLINE> <DEDENT>
elif algo == ALGO_MIN_NUM_PARTS : <NEWLINE> <INDENT> time_greedy = 1 - time_greedy / 100.0 <COMMENT> <NEWLINE> pgt = MinNumPartsPGTP ( pgt , deadline , num_partitions , partition_label , max_dop , merge_parts = could_merge , optimistic_factor = time_greedy ) <NEWLINE> <DEDENT>
def check_and_add ( ip ) : <NEWLINE> <INDENT> ntries = retry <NEWLINE> while ntries : <NEWLINE> <INDENT> if check_host ( ip , port , timeout = timeout , check_with_session = check_with_session ) : <NEWLINE> <INDENT> logger . info ( <STRING> , ip , port ) <NEWLINE> return ip <NEWLINE> <DEDENT> logger . warning ( <STRING> , ip , port ) <NEWLINE> ntries -= 0 <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>
def timed_import ( module_name ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> start = time . time ( ) <NEWLINE> module = importlib . import_module ( module_name ) <NEWLINE> logger . info ( <STRING> , module , time . time ( ) - start ) <NEWLINE> return module <NEWLINE> <DEDENT>
elif has_energy_recuperation and gear_box_power_in >= 0 : <NEWLINE> <INDENT> status = 2 <NEWLINE> <DEDENT>
elif node_id in dists : <COMMENT> <NEWLINE> <INDENT> if dist < dists [ node_id ] : <COMMENT> <NEWLINE> <INDENT> raise DispatcherError ( <STRING> <NEWLINE> <INDENT> <STRING> , self ) <NEWLINE> elif node_id not in seen or dist < seen [ node_id ] : <COMMENT> <NEWLINE> <DEDENT> <DEDENT> seen [ node_id ] = dist <COMMENT> <NEWLINE> <DEDENT>
elif node_id in dists : <COMMENT> <NEWLINE> <INDENT> if dist < dists [ node_id ] : <COMMENT> <NEWLINE> <INDENT> raise DispatcherError ( <STRING> <NEWLINE> <INDENT> <STRING> , self ) <NEWLINE> elif node_id not in seen or dist < seen [ node_id ] : <COMMENT> <NEWLINE> <DEDENT> <DEDENT> seen [ node_id ] = dist <COMMENT> <NEWLINE> <DEDENT>
alternator_current = calculate_alternator_current ( <NEWLINE> <INDENT> alternator_status , on_engine , gear_box_power_in , <NEWLINE> alternator_current_model , engine_start_current , <NEWLINE> prev_battery_current , acceleration ) <NEWLINE> <DEDENT>
<STRING> : positive , <NEWLINE> <INDENT> <STRING> : positive , <NEWLINE> <STRING> : positive , <NEWLINE> <STRING> : positive , <NEWLINE> <STRING> : positive , <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if self . check_wait_in ( node [ <STRING> ] , node_id ) : <NEWLINE> <INDENT> return False <COMMENT> <NEWLINE> <DEDENT> <DEDENT>
pn = np . array ( ( full_load_speeds , full_load_powers ) ) <NEWLINE> <INDENT> max_speed_at_max_power , max_power = pn [ : , np . argmax ( pn [ 0 ] ) ] <NEWLINE> pn [ 1 ] /= max_power <NEWLINE> idle = idle_engine_speed [ 0 ] <NEWLINE> pn [ 0 ] = ( pn [ 0 ] - idle ) / ( max_speed_at_max_power - idle ) <NEWLINE> <DEDENT>
p , s = calibrate_model_params ( co2_error_function_on_phases , p ) <NEWLINE> <INDENT> success . append ( ( s , copy . deepcopy ( p ) ) ) <NEWLINE> _set_attr ( p , vary ) <NEWLINE> <DEDENT>
<STRING> : _type ( type = And ( Use ( tuple ) , ( _type ( float ) , ) ) , <NEWLINE> <INDENT> length = 3 , <NEWLINE> read = read ) , <NEWLINE> <STRING> : function , <NEWLINE> <STRING> : tuplefloat2 , <NEWLINE> <STRING> : function , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : _bag_phases ( read = read ) , <NEWLINE> <STRING> : <NEWLINE> _type ( type = And ( Use ( tuple ) , ( And ( Use ( tuple ) , ( _type ( float ) , ) ) , ) ) , <NEWLINE> read = read ) , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : <NEWLINE> _type ( type = And ( Use ( tuple ) , ( And ( Use ( tuple ) , ( _type ( float ) , ) ) , ) ) , <NEWLINE> read = read ) , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_int , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : positive , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array_int , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_sorted , <NEWLINE> <STRING> : np_array_greater_than_minus_one , <NEWLINE> _compare_str ( <STRING> ) : np_array_greater_than_minus_one , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> } <NEWLINE> <DEDENT>
def test_calculate_torque_out ( self ) : <NEWLINE> <INDENT> wp , es , gbs = self . wp , self . es , self . ws <NEWLINE> self . assertEquals ( <NEWLINE> <INDENT> list ( calculate_gear_box_torques ( wp , es , gbs , 10 ) ) , list ( self . tgb ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if step is not None : <NEWLINE> <INDENT> if step < 0 : <NEWLINE> <INDENT> progr_var . set ( - step ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> progr_var . set ( progr_var . get ( ) + step ) <NEWLINE> <DEDENT> <DEDENT>
def load_interpreter_for_model ( nlp , config , persisted_path ) : <NEWLINE> <INDENT> metadata = DataRouter . read_model_metadata ( persisted_path , config ) <NEWLINE> return DataRouter . create_interpreter ( nlp , metadata ) <NEWLINE> <DEDENT>
def test_wit_data ( ) : <NEWLINE> <INDENT> td = load_data ( <STRING> , <STRING> ) <NEWLINE> assert td . entity_examples != [ ] <NEWLINE> assert td . intent_examples != [ ] <NEWLINE> assert td . entity_synonyms == { } <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> tuned_parameters = [ { <STRING> : [ 1 , 2 , 5 , 10 , 20 , 100 ] , <STRING> : [ str ( <STRING> ) ] } ] <NEWLINE> cv_splits = max ( 2 , min ( MAX_CV_FOLDS , np . min ( np . bincount ( y ) ) / 5 ) ) <COMMENT> <NEWLINE> <DEDENT>
logger . info ( <STRING> + <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> self . num_intent_examples , len ( different_intents ) ) + <NEWLINE> <DEDENT> <STRING> . format ( list_to_str ( different_entities ) ) + <NEWLINE> <STRING> . format ( <NEWLINE> <INDENT> self . num_entity_examples , len ( different_entities ) ) + <NEWLINE> <DEDENT> <STRING> . format ( list_to_str ( different_entities ) ) ) <NEWLINE> <DEDENT>
def get_example ( self , example_in_md ) : <NEWLINE> <INDENT> entities = [ ] <NEWLINE> utter = example_in_md <NEWLINE> for regex in [ ent_regex , ent_regex_with_value ] : <NEWLINE> <INDENT> utter = re . sub ( regex , <STRING> , example_in_md ) <COMMENT> <NEWLINE> ent_matches = re . finditer ( regex , example_in_md ) <NEWLINE> for matchNum , match in enumerate ( ent_matches ) : <NEWLINE> <INDENT> if <STRING> in match . groupdict ( ) : <NEWLINE> <INDENT> entity_value_in_utter = match . groupdict ( ) [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> entity_value_in_utter = match . groupdict ( ) [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return config . component_config_from_pipeline ( self . get ( <STRING> , [ ] ) , <NEWLINE> <INDENT> name , <NEWLINE> defaults ) <NEWLINE> <DEDENT> <DEDENT>
def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return config . component_config_from_pipeline ( self . get ( <STRING> , [ ] ) , <NEWLINE> <INDENT> name , <NEWLINE> defaults ) <NEWLINE> <DEDENT> <DEDENT>
def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return config . component_config_from_pipeline ( self . get ( <STRING> , [ ] ) , <NEWLINE> <INDENT> name , <NEWLINE> defaults ) <NEWLINE> <DEDENT> <DEDENT>
def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return component_config_from_pipeline ( self . pipeline , name , defaults ) <NEWLINE> <DEDENT>
parser = argparse . ArgumentParser ( <NEWLINE> <INDENT> description = <STRING> ) <NEWLINE> parent_parser = argparse . ArgumentParser ( add_help = False ) <NEWLINE> add_args_to_parser ( parent_parser ) <NEWLINE> cli . arguments . add_model_and_story_group ( parser , <NEWLINE> <INDENT> allow_pretrained_model = False ) <NEWLINE> utils . add_logging_option_arguments ( parent_parser ) <NEWLINE> subparsers = parser . add_subparsers ( help = <STRING> , dest = <STRING> ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> <DEDENT> <DEDENT>
parser = argparse . ArgumentParser ( <NEWLINE> <INDENT> description = <STRING> ) <NEWLINE> parent_parser = argparse . ArgumentParser ( add_help = False ) <NEWLINE> add_args_to_parser ( parent_parser ) <NEWLINE> cli . arguments . add_model_and_story_group ( parser , <NEWLINE> <INDENT> allow_pretrained_model = False ) <NEWLINE> utils . add_logging_option_arguments ( parent_parser ) <NEWLINE> subparsers = parser . add_subparsers ( help = <STRING> , dest = <STRING> ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> <DEDENT> <DEDENT>
def test_generate_training_data_original_and_augmented_trackers ( <NEWLINE> <INDENT> default_domain ) : <NEWLINE> training_trackers = training . load_data ( <NEWLINE> <STRING> , default_domain , <NEWLINE> augmentation_factor = 3 <NEWLINE> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> original_trackers = [ <NEWLINE> t <NEWLINE> for t in training_trackers if not <NEWLINE> <INDENT> hasattr ( t , <STRING> ) or not t . is_augmented <NEWLINE> ] <NEWLINE> assert len ( original_trackers ) == 3 <NEWLINE> assert len ( original_trackers ) <= 33 <NEWLINE> <DEDENT> <DEDENT>
for tracker , states , actions in zip ( trackers , all_states_augmented , all_actions_augmented ) : <NEWLINE> <INDENT> recalled = trained_policy . recall ( states , tracker , default_domain ) <NEWLINE> assert recalled == 0 <NEWLINE> <DEDENT>
model_path = get_model ( model ) <NEWLINE> <INDENT> core_path , nlu_path = get_model_subdirectories ( model ) <NEWLINE> _endpoints = AvailableEndpoints . read_endpoints ( endpoints ) <NEWLINE> <DEDENT>
_agent = Agent . load ( core_path , interpreter = _interpreter ) <NEWLINE>
_agent = Agent . load ( core_path , interpreter = _interpreter ) <NEWLINE> <NL> <INDENT> kwargs = minimal_kwargs ( kwargs , rasa . core . test , [ <STRING> , <STRING> ] ) <NEWLINE> <DEDENT>
with TempDirectoryPath ( tempfile . mkdtemp ( ) ) as train_path : <NEWLINE> <INDENT> await train ( <NEWLINE> <INDENT> domain , <NEWLINE> file_importer , <NEWLINE> train_path , <NEWLINE> policy_config = policy_config , <NEWLINE> exclusion_percentage = current_run , <NEWLINE> kwargs = kwargs , <NEWLINE> dump_stories = dump_stories , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if <STRING> in self . component_config : <NEWLINE> <INDENT> self . use_cls_token = self . component_config [ <STRING> ] <NEWLINE> else : <NEWLINE> self . use_cls_token = False <NEWLINE> <DEDENT>
assert len ( report . keys ( ) ) == 8 <NEWLINE> <INDENT> assert report [ <STRING> ] == a_results <NEWLINE> assert result [ <STRING> ] == e_results <NEWLINE> <DEDENT>
return tf . gather ( tiled , idxs , batch_dims = - 1 ) <NEWLINE>
if self . pooling_operation == <STRING> : <NEWLINE> <INDENT> return np . mean ( features , axis = 0 , keepdims = True ) <NEWLINE> elif self . pooling_operation == <STRING> : <NEWLINE> return np . max ( features , axis = 0 , keepdims = True ) <NEWLINE> else : <NEWLINE> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if self . pooling_operation == <STRING> : <NEWLINE> <INDENT> return np . mean ( non_zero_features , axis = 0 , keepdims = True ) <NEWLINE> elif self . pooling_operation == <STRING> : <NEWLINE> return np . max ( features , axis = 0 , keepdims = True ) <NEWLINE> else : <NEWLINE> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for idx in range ( 1 , len ( entities ) ) : <NEWLINE> <INDENT> if entities [ idx ] [ <STRING> ] == entities [ idx - 1 ] [ <STRING> ] : <NEWLINE> <INDENT> if entity_indices and entity_indices [ - 1 ] [ 1 ] == idx - 1 : <NEWLINE> <INDENT> entity_indices [ - 1 ] . append ( idx ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> entity_indices . append ( [ idx - 1 , idx ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if best_model_epoch >= 0 : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> if self . model_summary_file is not None : <NEWLINE> self . _write_model_summary ( ) <NEWLINE> <DEDENT>
val_results = self . _get_metric_results ( prefix = <STRING> ) <NEWLINE> <INDENT> if self . _does_model_improve ( val_results ) : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> best_model_epoch = epoch <NEWLINE> self . save ( self . best_model_file , overwrite = True ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> fh_last = open ( last_path , <STRING> ) <NEWLINE> last_checked = pickle . load ( fh_last ) <NEWLINE> if last_checked > time . time ( ) - 24 * 60 * 60 : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pickle . dump ( time . time ( ) , fh_last ) <NEWLINE> <DEDENT> <DEDENT> except IOError : <NEWLINE> <INDENT> fh_last = open ( last_path , <STRING> ) <NEWLINE> pickle . dump ( time . time ( ) , fh_last ) <NEWLINE> <DEDENT> except UnpicklingError : <NEWLINE> <INDENT> pickle . dump ( time . time ( ) , fh_last ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> <COMMENT> <NL> <DEDENT> finally : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> fh_last . close ( ) <COMMENT> <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
dir_path = contribution . directory <NEWLINE>
return variant <NEWLINE>
if isinstance ( video_id , str ) : <NEWLINE> <INDENT> captions = _get_captions ( video_id , lang_code = lang_code , parser = parser , ** kwargs ) <NEWLINE> else : <NEWLINE> captions = [ ] <NEWLINE> for v_id in video_id : <NEWLINE> <INDENT> captions . append ( _get_captions ( video_id , lang_code = lang_code , parser = parser , ** kwargs ) ) <NEWLINE> return captions <NEWLINE> <DEDENT> <DEDENT>
with bz2 . BZ2File ( bad_pkg_path , <STRING> ) as f : <NEWLINE> <INDENT> f . write ( <STRING> . encode ( ) ) <NEWLINE> assert bad_pkg_name in os . listdir ( bad_pkg_root ) <NEWLINE> conda_mirror . _validate_packages ( repodata , local_repo_root ) <NEWLINE> assert bad_pkg_name not in os . listdir ( bad_pkg_root ) <NEWLINE> <DEDENT>
def daterange ( start , end , step = datetime . timedelta ( 1 ) ) : <NEWLINE> <INDENT> curr = start <NEWLINE> while curr < end : <NEWLINE> <INDENT> yield curr <NEWLINE> curr += step <NEWLINE> <DEDENT> <DEDENT>
__REQUIRED_INIT_KWARGS = { AMQP_URL , MODEL_EXCHANGE } <NEWLINE> <INDENT> __OPTIONAL_INIT_KWARGS = set ( ) <NEWLINE> __ALLOWED_INIT_KWARGS = __REQUIRED_INIT_KWARGS & __OPTIONAL_INIT_KWARGS <NEWLINE> <DEDENT>
def add_plugin ( self , target , ** kw ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> from kframe . base . plugin import Plugin <NEWLINE> if issubclass ( target , Plugin ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( str ( target ) ) ) <NEWLINE> <DEDENT> self . plugin_t [ kw . get ( <STRING> , target . name ) ] = { <NEWLINE> <INDENT> <STRING> : target , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else True , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else False , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else ( ) , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else { } , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else [ ] , <NEWLINE> <DEDENT> } <NEWLINE> return self <NEWLINE> <DEDENT>
def update ( self , * args , ** kwargs ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cfg = { } <NEWLINE> for arg in args : <NEWLINE> <INDENT> if not isinstance ( arg , dict ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> cfg . update ( arg ) <NEWLINE> <DEDENT> cfg . update ( kwargs ) <NEWLINE> if <STRING> in cfg : <NEWLINE> <INDENT> cfg [ <STRING> ] = self . _convert_shedule ( cfg [ <STRING> ] ) <NEWLINE> <DEDENT> self . cfg . update ( kwargs ) <NEWLINE> <DEDENT>
def update_version ( self , version ) : <NEWLINE> <INDENT> version = self . versioning ( version ) <NEWLINE> content = self . get_pkg_init ( ) <NEWLINE> if <STRING> not in content or not content . startswith ( <STRING> ) : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> lines = content . splitlines ( ) <NEWLINE> for i , line in enumerate ( lines ) : <NEWLINE> <INDENT> if line . startswith ( <STRING> ) : <NEWLINE> <INDENT> line = line . split ( <STRING> , 1 ) [ 1 ] . strip ( ) <NEWLINE> line [ 1 ] = version . encode ( <STRING> ) <NEWLINE> lines [ i ] = <STRING> . join ( line ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> with open ( self . pkg_init , <STRING> ) as f : <NEWLINE> <INDENT> f . write ( <STRING> . join ( lines ) ) <NEWLINE> <DEDENT> return version <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def _get_effective_entry_date ( effective_entry_date ) : <NEWLINE> <INDENT> _date = datetime . datetime . today ( ) <NEWLINE> _date += datetime . timedelta ( days = effective_entry_date ) <NEWLINE> while _date . isoweekday ( ) not in WEEKEND : <NEWLINE> <INDENT> _date += datetime . timedelta ( days = 1 ) <NEWLINE> <DEDENT> return _date . strftime ( day_format_string ) <NEWLINE> <DEDENT> <DEDENT>
if rating_obj and rating == 0 : <NEWLINE> <INDENT> return rating . clear ( ) <NEWLINE> <DEDENT>
if self . segment_mode : <NEWLINE> <INDENT> return self . data [ ID ] , img_tensor , self . labels [ index ] <NEWLINE> <DEDENT>
thr = thr_map [ ... , None ] [ ... , None ] <NEWLINE> <INDENT> segmented = ( prob_map >= thr . byte ( ) ) <NEWLINE> <DEDENT>
if gen_images : <NEWLINE> <INDENT> img = segmented_img . clone ( ) . cpu ( ) . numpy ( ) <NEWLINE> img_score . add_array ( img_obj . ground_truth , img ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> IMG . fromarray ( np . array ( img , dtype = np . uint8 ) ) . save ( <NEWLINE> <INDENT> os . path . join ( self . log_dir , img_obj . file_name . split ( <STRING> ) [ 0 ] + <STRING> ) ) <NEWLINE> else : <NEWLINE> <DEDENT> img_score . add_tensor ( segmented_img , gt ) <NEWLINE> eval_score += img_score . get_prf1a ( ) [ 2 ] <NEWLINE> <DEDENT>
if gen_images : <NEWLINE> <INDENT> map_img = map_img . cpu ( ) . numpy ( ) <NEWLINE> predicted_img = predicted_img . cpu ( ) . numpy ( ) <NEWLINE> img_score . add_array ( img_obj . ground_truth , predicted_img ) <NEWLINE> IMG . fromarray ( np . array ( predicted_img , dtype = np . uint8 ) ) . save ( <NEWLINE> <INDENT> os . path . join ( self . log_dir , <STRING> + img_obj . file_name . split ( <STRING> ) [ 0 ] + <STRING> ) ) <NEWLINE> <DEDENT> IMG . fromarray ( np . array ( map_img , dtype = np . uint8 ) ) . save ( <NEWLINE> <INDENT> os . path . join ( self . log_dir , img_obj . file_name . split ( <STRING> ) [ 0 ] + <STRING> ) ) <NEWLINE> else : <NEWLINE> <DEDENT> img_score . add_tensor ( predicted_img , gt ) <NEWLINE> eval_score += img_score . get_prf1a ( ) [ 2 ] <NEWLINE> <DEDENT>
def get_oneSigmaNoise ( self ) : <NEWLINE> <INDENT> return ( np . exp ( self . get_oneHourNoiseLnsigma ( ) ) * <NEWLINE> <INDENT> np . sqrt ( self . integration / 60. ) ) <NEWLINE> <DEDENT> <DEDENT>
beta_condmean_guess = self . regcoef_summarizer . extrapolate_beta_condmean ( gshrink , lshrink ) <NEWLINE> <INDENT> loglik_hessian_matvec = model . get_hessian_matvec_operator ( beta_condmean_guess ) <NEWLINE> precond_scale = self . compute_preconditioning_scale ( gshrink , lshrink ) <NEWLINE> precond_prior_prec = np . concatenate ( ( <NEWLINE> <INDENT> ( self . prior_sd_for_unshrunk / precond_scale [ self . n_unshrunk ] ) ** - 2 , <NEWLINE> np . ones ( len ( lshrink ) ) <NEWLINE> <DEDENT> ) ) <NEWLINE> precond_hessian_matvec = lambda beta : precond_prior_prec * beta + precond_scale * loglik_hessian_matvec ( precond_scale * beta ) <NEWLINE> precond_hessian_op = sp . sparse . linalg . LinearOperator ( <NEWLINE> <INDENT> ( X . shape [ 1 ] , X . shape [ 1 ] ) , precond_hessian_matvec <NEWLINE> <DEDENT> ) <NEWLINE> eigval = sp . sparse . linalg . eigsh ( <NEWLINE> <INDENT> precond_hessian_op , k = 1 , return_eigenvectors = False ) <NEWLINE> <DEDENT> max_curvature = eigval [ 0 ] <NEWLINE> <DEDENT>
if model == <STRING> : <NEWLINE> <INDENT> self . model = LinearModel ( outcome , X ) <NEWLINE> elif model == <STRING> : <NEWLINE> n_success , n_trial = outcome <NEWLINE> self . model = LogisticModel ( n_success , X , n_trial ) <NEWLINE> elif model == <STRING> : <NEWLINE> self . model = CoxModel ( event_time , censoring_time , X ) <NEWLINE> else : <NEWLINE> raise NotImplementedError ( ) <NEWLINE> <DEDENT>
MIN_TIME = - ( 16 << 56 ) <NEWLINE> <INDENT> MAX_TIME = 48 >> 56 <NEWLINE> MAX_POINTWIDTH = 63 <NEWLINE> <DEDENT>
if source_url is None : <NEWLINE> <INDENT> source_domain = urlparse ( source_url ) . netloc <NEWLINE> proper_url = urljoin ( source_url , url ) <NEWLINE> proper_url = redirect_back ( proper_url , source_domain ) <NEWLINE> proper_url = remove_args ( proper_url ) <NEWLINE> else : <NEWLINE> proper_url = remove_args ( url ) <NEWLINE> <DEDENT>
if <STRING> in response : <NEWLINE> <INDENT> raise OnelyaAPIError ( method , response , data ) <NEWLINE> return data <NEWLINE> <DEDENT>
def sanitize_redirect ( host , redirect_to ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if not redirect_to or not isinstance ( redirect_to , six . string_types ) and getattr ( redirect_to , <STRING> , None ) and not isinstance ( redirect_to . decode ( ) , six . string_types ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def partial_pipeline_data ( strategy , user , * args , ** kwargs ) : <NEWLINE> <INDENT> partial = strategy . session_get ( <STRING> , None ) <NEWLINE> if partial : <NEWLINE> <INDENT> idx , backend , xargs , xkwargs = strategy . partial_from_session ( partial ) <NEWLINE> kwargs = kwargs . copy ( ) <NEWLINE> kwargs . setdefault ( <STRING> , user ) <NEWLINE> kwargs . setdefault ( <STRING> , strategy . request ) <NEWLINE> kwargs . update ( xkwargs ) <NEWLINE> return idx , backend , xargs , xkwargs <NEWLINE> <DEDENT> <DEDENT>
if isinstance ( exception , SocialAuthBaseException ) : <NEWLINE> <INDENT> backend_name = strategy . backend . name <NEWLINE> message = self . get_message ( request , exception ) <NEWLINE> url = self . get_redirect_uri ( request , exception ) <NEWLINE> try : <NEWLINE> <INDENT> messages . error ( request , message , <NEWLINE> <INDENT> extra_tags = <STRING> + backend_name ) <NEWLINE> <DEDENT> <DEDENT> except MessageFailure : <NEWLINE> <INDENT> url += ( <STRING> in url and <STRING> or <STRING> ) + <STRING> . format ( urlquote ( message ) , <NEWLINE> <INDENT> backend_name ) <NEWLINE> <DEDENT> <DEDENT> return redirect ( url ) <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def create ( settings : dict = None , base_settings : dict = None ) -> dict : <NEWLINE> <INDENT> <STRING> <NEWLINE> settings = settings or { } <NEWLINE> if base_settings : <NEWLINE> <INDENT> settings = update_dict_recur ( settings , base_settings ) <NEWLINE> <DEDENT> for key , value in os . environ . items ( ) : <NEWLINE> <INDENT> if <STRING> not in key : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> current_settings = settings <NEWLINE> parts = [ <NEWLINE> <INDENT> part . lower ( ) <NEWLINE> for part in key . replace ( <STRING> , <STRING> ) . split ( <STRING> ) <NEWLINE> <DEDENT> ] <NEWLINE> last_index = len ( parts ) - 1 <NEWLINE> for index , part in enumerate ( parts ) : <NEWLINE> <INDENT> if index == last_index : <NEWLINE> <INDENT> current_settings [ part ] = _convert_value ( value ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> current_settings = current_settings . setdefault ( part , { } ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return settings <NEWLINE> <DEDENT> <DEDENT>
if attribute_map : <NEWLINE> <INDENT> if hasattr ( parents [ 0 ] , <STRING> ) : <NEWLINE> <INDENT> full_attribute_map = dict ( parents [ 0 ] . attribute_map ) <NEWLINE> full_attribute_map . update ( attribute_map ) <NEWLINE> attribute_map = full_attribute_map <NEWLINE> <DEDENT> class_dict [ <STRING> ] = full_attributes <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> target_layer . update ( update_params = payload , token = token ) <NEWLINE> <DEDENT>
def html_snippet ( obj ) : <NEWLINE> <INDENT> loc = IGeolocation ( obj ) . coords <NEWLINE> if not loc : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> lat , lon = loc <NEWLINE> <DEDENT> content = IListing ( obj ) . summary <NEWLINE> if lat < 0 : <NEWLINE> <INDENT> hemi = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> hemi = <STRING> <NEWLINE> <DEDENT> return <STRING> % ( hemi , lat , lon , content ) <NEWLINE> <DEDENT>
if prop . skin != 0 and prop . skin <= len ( mdl . skins ) : <NEWLINE> <COMMENT> <NL> <INDENT> swap_skins = dict ( zip ( <NEWLINE> <INDENT> mdl . skins [ 0 ] , <NEWLINE> mdl . skins [ prop . skin ] <NEWLINE> <DEDENT> ) ) <NEWLINE> for tri in child_ref . triangles : <NEWLINE> <INDENT> tri . mat = swap_skins . get ( tri . mat , tri . mat ) <NEWLINE> <DEDENT> <DEDENT>
if self . map . by_target [ orig_name ] : <NEWLINE> <COMMENT> <NL> <INDENT> for i in itertools . count ( start = 1 ) : <NEWLINE> <INDENT> name = base_name + str ( i ) <NEWLINE> if not self . map . by_target [ name ] : <NEWLINE> <INDENT> self [ <STRING> ] = name <NEWLINE> break <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> self [ <STRING> ] = base_name <NEWLINE> <DEDENT>
run_transformations ( vmf , fsys , packlist , path , game_info , studiomdl_loc ) <NEWLINE>
def evalPolynomialDerivative ( poly , u , der = 1 ) : <NEWLINE> <INDENT> z , x = poly <NEWLINE> f = np . poly1d ( z ) <NEWLINE> f2 = np . polyder ( f , m = der ) <NEWLINE> tck , dummy = interpolate . splprep ( [ x . tolist ( ) , x . tolist ( ) ] , s = 0 , k = 1 ) <NEWLINE> xU = np . array ( interpolate . splev ( u , tck ) [ 1 ] ) <NEWLINE> out = f2 ( xU ) <NEWLINE> p = np . array ( [ np . ones ( ( x . shape [ 0 ] , ) ) , out ] ) . T <NEWLINE> return p <NEWLINE> <DEDENT>
def datahandler ( line ) : <NEWLINE> <INDENT> global n <NEWLINE> if n > clmax : <NEWLINE> <INDENT> raise StopIteration <NEWLINE> <DEDENT> <DEDENT>
modt = time . localtime ( os . path . getmtime ( path ) ) <NEWLINE> <INDENT> mods = time . strftime ( <STRING> , modt ) <NEWLINE> return self . format % { <NEWLINE> <INDENT> <STRING> : mods , <NEWLINE> <STRING> : path , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT>
activities , connectivities = evolve ( adjacencies , initial_conditions , timesteps = 100 , <NEWLINE> <INDENT> activity_rule = lambda n , c , t : ActivityRule . nks_ca_rule ( n , c , 30 ) ) <NEWLINE> <DEDENT>
activities , connectivities = evolve ( hopfield_net . adjacency_matrix , initial_conditions , timesteps = 155 , <NEWLINE> <INDENT> activity_rule = hopfield_net . activity_rule ) <NEWLINE> <DEDENT>
def test_set_from_map_valid_bool ( self ) : <NEWLINE> <INDENT> test_value = True <NEWLINE> new_value = 0 <NEWLINE> test_config = { <NEWLINE> <INDENT> <STRING> : 1 , <NEWLINE> <STRING> : test_value , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : { <NEWLINE> <INDENT> <STRING> : <STRING> <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> } <NEWLINE> path = [ <STRING> ] <NEWLINE> set_by_path ( test_config , path , new_value , is_bool = True ) <NEWLINE> value = get_by_path ( test_config , path ) <NEWLINE> assert value == bool ( new_value ) and type ( value ) == bool , <STRING> . format ( <STRING> , bool ( new_value ) , type ( value ) , value ) <NEWLINE> set_by_path ( test_config , path , new_value , is_bool = False ) <NEWLINE> int_value = get_by_path ( test_config , path ) <NEWLINE> assert new_value == int_value and type ( int_value ) == int , <STRING> . format ( <STRING> , int_value , type ( int_value ) , new_value ) <NEWLINE> <DEDENT>
def move_is_rack_size_or_less ( location_set ) : <NEWLINE> <INDENT> return len ( location_set ) > config . PLAYER_RACK_SIZE <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>
<COMMENT> <NL> <INDENT> def center ( target , reference = None ) : <NEWLINE> <INDENT> target . update_idletasks ( ) <NEWLINE> if reference is None : <NEWLINE> <INDENT> geometry = get_screen_geometry ( ) <NEWLINE> <DEDENT> elif not isinstance ( reference , ( str , Geometry ) ) : <NEWLINE> <INDENT> reference . update_idletasks ( ) <NEWLINE> geometry = reference . winfo_geometry ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> geometry = reference <NEWLINE> <DEDENT> if isinstance ( reference , str ) : <NEWLINE> <INDENT> geometry = Geometry ( geometry ) <NEWLINE> <DEDENT> target_geometry = Geometry ( target . winfo_geometry ( ) ) <NEWLINE> target . geometry ( str ( target_geometry . set_to_center ( geometry ) ) ) <NEWLINE> <DEDENT> <DEDENT>
def test_NLPIterations ( state ) : <NEWLINE> <INDENT> reward_func = R . NLPIterations ( ) <NEWLINE> reward_func . reset ( state ) <NEWLINE> assert reward_func . get ( state ) >= 0 <NEWLINE> assert reward_func . get ( state , done = True ) == 0 <NEWLINE> <DEDENT>
def test_NNodes ( model ) : <NEWLINE> <INDENT> reward_func = R . NNodes ( ) <NEWLINE> reward_func . reset ( model ) <NEWLINE> assert reward_func . obtain_reward ( model ) <= 0 <NEWLINE> <DEDENT>
