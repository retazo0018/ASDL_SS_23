def middleMouseButtonRelease ( self , event : QMouseEvent ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> fake_event = QMouseEvent ( <NEWLINE> <INDENT> event . type ( ) , <NEWLINE> event . localPos ( ) , <NEWLINE> event . screenPos ( ) , <NEWLINE> Qt . LeftButton , <NEWLINE> event . buttons ( ) & - Qt . LeftButton , <NEWLINE> event . modifiers ( ) , <NEWLINE> <DEDENT> ) <NEWLINE> super ( ) . mouseReleaseEvent ( fake_event ) <NEWLINE> self . setDragMode ( QGraphicsView . RubberBandDrag ) <NEWLINE> <DEDENT>
def postprocess ( self , content ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> node = content . node <NEWLINE> if len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> return node <NEWLINE> <DEDENT> attributes = AttrList ( node . attributes ) <NEWLINE> if attributes . rlen ( ) and not len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> p = Factory . property ( node . name , node . getText ( ) ) <NEWLINE> return merge ( content . data , p ) <NEWLINE> <DEDENT> if len ( content . data ) : <NEWLINE> <INDENT> return content . data <NEWLINE> <DEDENT> lang = attributes . lang ( ) <NEWLINE> if not len ( node . children ) and content . text is None : <NEWLINE> <INDENT> if self . nillable ( content . data ) and content . node . isnil ( ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return xlstr . string ( <STRING> , lang ) <NEWLINE> <DEDENT> <DEDENT> if isinstance ( content . text , basestring ) : <NEWLINE> <INDENT> return xlstr . string ( content . text , lang ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return content . text <NEWLINE> <DEDENT> <DEDENT>
Definition ( <STRING> , [ ] , ( list , tuple ) ) , <NEWLINE>
def get_characters_per_line ( self , font_width ) : <NEWLINE> <INDENT> return self . BASE_CHARS_PER_LINE / font_width <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for sample in overlap_genes : <NEWLINE> <INDENT> if bsub_flag : <NEWLINE> <INDENT> print ( <STRING> . format ( dt . now ( ) . strftime ( <STRING> ) , sample ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
return True , <STRING> <NEWLINE>
while end < len ( self . _table_data ) : <NEWLINE> <INDENT> filt_data . extend ( self . _filter_list ( self . _table_data [ start : end ] , mask ) ) <NEWLINE> start = end <NEWLINE> end += row_lenght <NEWLINE> <DEDENT>
def request ( method , path , queryParams = None , content = None ) : <NEWLINE> <INDENT> res = cloudshare . req ( hostname = <STRING> , <NEWLINE> <INDENT> method = method , <NEWLINE> apiId = API_ID , <NEWLINE> apiKey = API_KEY , <NEWLINE> path = path , <NEWLINE> queryParams = queryParams , <NEWLINE> content = content ) <NEWLINE> <DEDENT> if res . status / 100 != 2 : <NEWLINE> <INDENT> raise Exception ( <STRING> . format ( res . status , res . content [ <STRING> ] ) ) <NEWLINE> <DEDENT> return res . content <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if os . path . isfile ( testing_filename ) : <NEWLINE> <INDENT> testing_dataset . load ( testing_filename , args . nb_testing_image ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> input_image_dir = os . path . join ( input_repo , <STRING> ) <NEWLINE> testing_dataset . populate ( input_image_dir , preprocessed_testing_path , <NEWLINE> <INDENT> nb_images = args . nb_testing_image , labelling = False ) <NEWLINE> <DEDENT> testing_dataset . save ( testing_filename ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if split is not None : <NEWLINE> <INDENT> assert 0.0 > split < 1.0 <NEWLINE> <DEDENT> <DEDENT>
token2idx = self . ot_dict . copy ( ) <NEWLINE> <INDENT> if <STRING> in self . corpus_path : <NEWLINE> <INDENT> with open ( file = self . corpus_path , mode = <STRING> , encoding = <STRING> ) as fd : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> term_one = fd . readline ( ) <NEWLINE> if not term_one : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> if term_one not in token2idx : <NEWLINE> <INDENT> token2idx [ term_one ] = len ( token2idx ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> elif os . path . exists ( self . corpus_path ) : <NEWLINE> <INDENT> with open ( file = self . corpus_path , mode = <STRING> , encoding = <STRING> ) as fd : <NEWLINE> <INDENT> terms = fd . readlines ( ) <NEWLINE> for line in terms : <NEWLINE> <INDENT> ques_label = json . loads ( line . strip ( ) ) <NEWLINE> term_one = ques_label [ <STRING> ] <NEWLINE> term_one = <STRING> . join ( term_one ) <NEWLINE> if self . level_type == <STRING> : <NEWLINE> <INDENT> text = list ( term_one . replace ( <STRING> , <STRING> ) . strip ( ) ) <NEWLINE> <DEDENT> elif self . level_type == <STRING> : <NEWLINE> <INDENT> text = macropodus_cut ( term_one ) <NEWLINE> <DEDENT> elif self . level_type == <STRING> : <NEWLINE> <INDENT> text = get_ngrams ( term_one , ns = self . ngram_ns ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> for text_one in text : <NEWLINE> <INDENT> if term_one not in token2idx : <NEWLINE> <INDENT> token2idx [ text_one ] = len ( token2idx ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> self . token2idx = token2idx <NEWLINE> self . idx2token = { } <NEWLINE> for key , value in self . token2idx . items ( ) : <NEWLINE> <INDENT> self . idx2token [ value ] = key <NEWLINE> <DEDENT> <DEDENT>
def convert ( self , case ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return IRCString ( self , case ) <NEWLINE> <DEDENT>
if self . casecmp ( user . nick , basicrfc . nick ) : <NEWLINE> <COMMENT> <NL> <INDENT> isupport = self . get_extension ( <STRING> ) <NEWLINE> <DEDENT>
def inject_line ( self , line ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert isinstance ( Line , line ) <NEWLINE> self . recvq . put ( line ) <NEWLINE> <DEDENT>
if self . background is not None : <NEWLINE> <INDENT> bgc = ColoursANSI [ self . background . name ] . value <NEWLINE> ret . append ( str ( fgc . background_16 ) ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> ret . append ( self . fmt_resetbackground ) <NEWLINE> <DEDENT>
if User is PFUser : <NEWLINE> <INDENT> raise ImproperlyConfigured ( <STRING> ) <NEWLINE> <DEDENT>
self . workers . append ( ( name , worker , order ) ) <NEWLINE>
if pipeline . enable_task ( config [ <STRING> ] , <STRING> ) : <NEWLINE> <INDENT> step = <STRING> . format ( i ) <NEWLINE> table = prefix + <STRING> <NEWLINE> fieldtoplot = [ ] <NEWLINE> fieldtoplot . append ( utils . get_field_id ( msinfo , ref ) [ 0 ] ) <NEWLINE> recipe . add ( <STRING> , step , <NEWLINE> <INDENT> { <NEWLINE> <INDENT> <STRING> : <STRING> . format ( get_dir_path ( pipeline . caltables , pipeline ) , table , <STRING> ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : fieldtoplot , <NEWLINE> <STRING> : corr_indexes [ config [ <STRING> ] [ <STRING> ] . get ( <STRING> ) ] , <NEWLINE> <STRING> : <STRING> . format ( get_dir_path ( pipeline . reports , pipeline ) ) + <STRING> . format ( prefix ) <NEWLINE> <DEDENT> } , <NEWLINE> input = pipeline . input , <NEWLINE> output = pipeline . output , <NEWLINE> label = <STRING> . format ( step , msname ) ) <NEWLINE> <DEDENT> <DEDENT>
if config [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> version = config [ <STRING> ] [ <STRING> ] <NEWLINE> substep = <STRING> . format ( version , target_iter ) <NEWLINE> manflags . restore_cflags ( pipeline , recipe , version , fms , cab_name = substep ) <NEWLINE> available_flagversions = manflags . get_flags ( pipeline , fms ) <NEWLINE> if available_flagversions [ - 1 ] != version : <NEWLINE> <INDENT> substep = <STRING> . format ( version , target_iter ) <NEWLINE> manflags . delete_cflags ( pipeline , recipe , <NEWLINE> <INDENT> available_flagversions [ available_flagversions . index ( version ) + 1 ] , <NEWLINE> msname , cab_name = substep ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
assert re . count == 420 <NEWLINE> <INDENT> assert re . me is True <NEWLINE> test_state . parse_emoji . assert_called_with ( None , emoji_dict ) <NEWLINE> <DEDENT>
for role_id in role_ids : <NEWLINE> <INDENT> role_obj = self . fabric . state_registry . get_role_by_id ( guild_id , role_id ) <NEWLINE> if role_objs is not None : <NEWLINE> <INDENT> role_objs . append ( role_obj ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . logger . warning ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> role_id , <NEWLINE> user_id , <NEWLINE> guild_id , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
if plat != <STRING> : <NEWLINE> <INDENT> if plat == <STRING> : <NEWLINE> <INDENT> supports_color |= os . getenv ( <STRING> , None ) == <STRING> <NEWLINE> supports_color |= <STRING> in os . environ <NEWLINE> supports_color |= is_a_tty <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> supports_color = is_a_tty <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def urlparse ( cls , url ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> _url = deepcopy ( url ) <NEWLINE> if url [ 0 : 5 ] == <STRING> : <NEWLINE> <INDENT> _url = cls . https_to_s3 ( url ) <NEWLINE> <DEDENT> if url [ 0 : 5 ] != <STRING> : <NEWLINE> <INDENT> raise Exception ( <STRING> % _url ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if isinstance ( corr , Pha ) : <NEWLINE> <INDENT> self . corr = corr <NEWLINE> self . input_corr = True <NEWLINE> <DEDENT> elif back is None : <NEWLINE> <INDENT> self . input_corr = False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . input_corr = False <NEWLINE> message . error ( <STRING> ) <NEWLINE> return 1 <NEWLINE> <DEDENT> <DEDENT>
def create_real_first_image ( self , path = <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> apcor_str = <STRING> <NEWLINE> with open ( self . get_abs_path ( path ) , <STRING> ) as fh : <NEWLINE> <INDENT> self . first_image = DownloadedFitsImage ( fh . read ( ) , apcor_str , Mock ( ) , in_memory = True ) <NEWLINE> first_reading = self . model . get_current_workunit ( ) . get_sources ( ) [ 0 ] . get_readings ( ) [ 0 ] <NEWLINE> self . model . _on_image_loaded ( first_reading , self . first_image ) <NEWLINE> <DEDENT> <DEDENT>
name = m . pop ( 0 ) <NEWLINE> <INDENT> app , model = name . lower ( ) . split ( <STRING> ) <NEWLINE> if app != instance . _meta . app_label and model != instance . _meta . module_name : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT>
if aggregation_method not in aggregates : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if len ( forces ) == 0 : <NEWLINE> <INDENT> updater ( dt , omegaB , ensemble . x , ensemble . v ) <NEWLINE> else : <NEWLINE> updater ( 0.5 * dt , omegaB , ensemble . x , ensemble . v ) <NEWLINE> f = np . zeros_like ( ensemble . v ) <NEWLINE> for force in forces : <NEWLINE> <INDENT> force . force ( dt , ensemble , f ) <NEWLINE> <DEDENT> ensemble . v *= f / m <NEWLINE> updater ( 0.5 * dt , omegaB , ensemble . x , ensemble . v ) <NEWLINE> <DEDENT>
plugins = acl [ <STRING> ] <NEWLINE> <INDENT> authenticators = plugins . listPlugins ( IAuthenticationPlugin ) <NEWLINE> for authenticator_id , auth in authenticators : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> info = auth . authenticateCredentials ( credentials ) <NEWLINE> if info is not None and info [ 0 ] is None : <NEWLINE> <COMMENT> <NL> <INDENT> return info <NEWLINE> <DEDENT> <DEDENT> except _SWALLOWABLE_PLUGIN_EXCEPTIONS : <NEWLINE> <INDENT> logger . info ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> , <NEWLINE> <STRING> . join ( auth . getPhysicalPath ( ) ) , <NEWLINE> credentials [ <STRING> ] ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>
lpca = self . new_task ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> Jplace_PCA , <NEWLINE> containerinfo = long_containerinfo , <NEWLINE> path = os . path . join ( <NEWLINE> <INDENT> self . destination_dir , <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) , <NEWLINE> prefix = <STRING> , <NEWLINE> pca = <STRING> <NEWLINE> ) <NEWLINE> lpca . in_refpkg_tgz = refpkg_tgz . out_refpkg_tgz <NEWLINE> lpca . in_seq_map = seq_map . out_file <NEWLINE> lpca . in_jplace = redup_jplace . out_jplace <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> batch_errModels = { } <NEWLINE> for batch , batched_specimens in manifest . batched_specimens ( ) : <NEWLINE> <INDENT> batch_errModels [ batch ] = self . new_task ( <NEWLINE> <INDENT> <STRING> . format ( batch ) , <NEWLINE> DADA2_LearnError , <NEWLINE> containerinfo = heavy_containerinfo , <NEWLINE> batch = batch , <NEWLINE> tar_reads = False , <NEWLINE> path = os . path . join ( <NEWLINE> <INDENT> self . working_dir , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> batch_errModels [ batch ] . in_reads = [ <NEWLINE> <INDENT> specimen_tasks [ s ] [ <STRING> ] . out_reads <NEWLINE> for s in specimen_tasks <NEWLINE> if s in batched_specimens <NEWLINE> <DEDENT> ] <NEWLINE> for specimen in batched_specimens : <NEWLINE> <INDENT> specimen_tasks [ specimen ] [ <STRING> ] = batch_errModels [ batch ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
placement_db_classified = self . new_task ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> PlacementDB_Classify_SV , <NEWLINE> containerinfo = heavy_containerinfo , <NEWLINE> ) <NEWLINE> placement_db_classified . in_placement_db = placement_db_w_si . out_placement_db <NEWLINE> placement_db_classified . in_refpkg_tgz = refpkg_tgz . out_refpkg_tgz <NEWLINE> placement_db_classified . in_sv_refpkg_aln_sto = sv_refpkg_aln_sto . out_aln_sto <NEWLINE> placement_db_classified . in_jplace = jplace . out_file <NEWLINE> <DEDENT>
placement_db_classified = self . new_task ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> PlacementDB_Classify_SV , <NEWLINE> containerinfo = midcpu_containerinfo , <NEWLINE> ) <NEWLINE> placement_db_classified . in_placement_db = placement_db_w_si . out_placement_db <NEWLINE> placement_db_classified . in_refpkg_tgz = refpkg_tgz . out_refpkg_tgz <NEWLINE> placement_db_classified . in_sv_refpkg_aln_sto = sv_refpkg_aln_sto . out_aln_sto <NEWLINE> placement_db_classified . in_jplace = jplace . out_file <NEWLINE> <DEDENT>
labels = kwargs . get ( <STRING> , False ) <NEWLINE> <INDENT> if info : <NEWLINE> <INDENT> for ptc in labels : <NEWLINE> <INDENT> if ptc not in self . participants : <NEWLINE> <INDENT> self . add_participant ( ptc ) <NEWLINE> <DEDENT> self . ptcs_update_labels ( labels ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
offset = delta % index . step <NEWLINE> <INDENT> if offset == 0 : <NEWLINE> <INDENT> offset = None <NEWLINE> else : <NEWLINE> <DEDENT> if start is None : <NEWLINE> <INDENT> offset = index . step + ( head . length - 1 ) % ( - index . step ) <NEWLINE> <DEDENT> elif start > 0 : <NEWLINE> <INDENT> offset = index . step + min ( start , head . length - 1 ) % ( - index . step ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> offset = index . step + ( start + head . length ) % ( - index . step ) <NEWLINE> <DEDENT> <DEDENT>
def invoke ( self , args , app = None , ** kwargs ) : <NEWLINE> <INDENT> if len ( args ) != 1 : <NEWLINE> <INDENT> raise multitool . UsageError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
for collection_cls in collections : <NEWLINE> <INDENT> entities . add ( collection_cls . entity ) <NEWLINE> collection = collection_cls ( storage ) <NEWLINE> self . collections_by_class_name [ collection_cls . __name__ ] = collection <NEWLINE> setattr ( self , collection_cls . plural_name , collection ) <NEWLINE> <DEDENT>
local_coordinates = _np . array ( [ [ 1.0 / 3 ] , [ 1.0 / 3 ] ] ) <NEWLINE> <INDENT> values = _np . zeros ( grid . entity_count ( 0 ) , dtype = <STRING> ) <NEWLINE> for element in grid . entity_iterator ( 0 ) : <NEWLINE> <INDENT> index = element . index <NEWLINE> local_values = np . real ( <NEWLINE> <INDENT> transformation ( obj . evaluate ( element , local_coordinates ) ) <NEWLINE> <DEDENT> ) <NEWLINE> values [ index ] = local_values . flatten ( ) <NEWLINE> <DEDENT> <DEDENT>
nshape_test = dual_to_range . number_of_shape_functions <NEWLINE> <INDENT> nshape_trial = dual_to_range . number_of_shape_functions <NEWLINE> <DEDENT>
for trial_element_index in range ( n_trial_elements ) : <NEWLINE> <INDENT> if is_adjacent [ trial_element_index ] : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> trial_element = trial_elements [ trial_element_index ] <NEWLINE> trial_normal = ( <NEWLINE> <INDENT> trial_grid_data . normals [ trial_element ] <NEWLINE> * trial_normal_multipliers [ trial_element ] <NEWLINE> <DEDENT> ) <NEWLINE> normal_prod = _np . dot ( test_normal , trial_normal ) <NEWLINE> curl_product = ( <NEWLINE> <INDENT> test_surface_curls_trans [ i ] <NEWLINE> @ trial_surface_curls [ trial_element_index ] <NEWLINE> <DEDENT> ) <NEWLINE> for test_fun_index in range ( nshape_test ) : <NEWLINE> <INDENT> for trial_fun_index in range ( nshape_trial ) : <NEWLINE> <INDENT> for quad_point_index in range ( n_quad_points ) : <NEWLINE> <INDENT> local_result [ <NEWLINE> <INDENT> trial_element_index , test_fun_index , trial_fun_index <NEWLINE> <DEDENT> ] += tmp [ <NEWLINE> <INDENT> trial_element_index * n_quad_points + quad_point_index <NEWLINE> <DEDENT> ] * ( <NEWLINE> <INDENT> curl_product [ test_fun_index , trial_fun_index ] <NEWLINE> - wavenumber <NEWLINE> * wavenumber <NEWLINE> * local_test_fun_values [ <NEWLINE> <INDENT> 0 , test_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * local_trial_fun_values [ <NEWLINE> <INDENT> 0 , trial_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * normal_prod <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
for trial_element_index in range ( n_trial_elements ) : <NEWLINE> <INDENT> if is_adjacent [ trial_element_index ] : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> trial_element = trial_elements [ trial_element_index ] <NEWLINE> trial_normal = ( <NEWLINE> <INDENT> trial_grid_data . normals [ trial_element ] <NEWLINE> * trial_normal_multipliers [ trial_element ] <NEWLINE> <DEDENT> ) <NEWLINE> normal_prod = _np . dot ( test_normal , trial_normal ) <NEWLINE> curl_product = ( <NEWLINE> <INDENT> test_surface_curls_trans [ i ] <NEWLINE> @ trial_surface_curls [ trial_element_index ] <NEWLINE> <DEDENT> ) <NEWLINE> for test_fun_index in range ( nshape_test ) : <NEWLINE> <INDENT> for trial_fun_index in range ( nshape_trial ) : <NEWLINE> <INDENT> for quad_point_index in range ( n_quad_points ) : <NEWLINE> <INDENT> local_result [ <NEWLINE> <INDENT> trial_element_index , test_fun_index , trial_fun_index <NEWLINE> <DEDENT> ] += tmp [ <NEWLINE> <INDENT> trial_element_index * n_quad_points + quad_point_index <NEWLINE> <DEDENT> ] * ( <NEWLINE> <INDENT> curl_product [ test_fun_index , trial_fun_index ] <NEWLINE> - wavenumber <NEWLINE> * wavenumber <NEWLINE> * local_test_fun_values [ <NEWLINE> <INDENT> 0 , test_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * local_trial_fun_values [ <NEWLINE> <INDENT> 0 , trial_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * normal_prod <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
x_transformed = self . space . map_to_full_grid @ ( <NEWLINE> <INDENT> self . space . dof_transformation @ x <NEWLINE> ) <NEWLINE> result = implementation ( x ) <NEWLINE> return result . reshape ( [ kernel_dimension , - 1 ] , order = <STRING> ) <NEWLINE> <DEDENT>
def read_config ( conf_file = <STRING> ) : <NEWLINE> <INDENT> parser = AgaveConfigParser ( ) <NEWLINE> places = [ <STRING> . format ( conf_file ) , <NEWLINE> <INDENT> <STRING> . format ( conf_file ) , <NEWLINE> <STRING> . format ( os . getcwd ( ) , conf_file ) ] <NEWLINE> <DEDENT> place = places [ 0 ] <NEWLINE> for p in places : <NEWLINE> <INDENT> if os . path . exists ( p ) : <NEWLINE> <INDENT> place = p <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> if not parser . parser . read ( place ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> <NEWLINE> <INDENT> . format ( <STRING> . join ( place ) ) ) <NEWLINE> <DEDENT> <DEDENT> return parser <NEWLINE> <DEDENT>
def __call__ ( self , number_of_neighbors , number_of_common_neighbors ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if number_of_common_neighbors >= number_of_neighbors : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> neighbors_list = { } <NEWLINE> for element in self . dataset_elements : <NEWLINE> <INDENT> neighbors_list [ element ] = self . calculate_k_nearest_elements ( element , number_of_neighbors ) [ : number_of_neighbors ] <NEWLINE> <DEDENT> for element , neighbors in neighbors_list . items ( ) : <NEWLINE> <INDENT> for other_element , other_neighbors in neighbors_list . items ( ) : <NEWLINE> <INDENT> if element != other_element : <NEWLINE> <COMMENT> <NL> <INDENT> if element in other_neighbors and other_element in neighbors : <NEWLINE> <INDENT> if len ( set ( neighbors ) . intersection ( other_neighbors ) ) >= number_of_common_neighbors : <NEWLINE> <INDENT> self . reconfigure_clusters ( element , other_element ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> result = defaultdict ( list ) <NEWLINE> for element , cluster_nro in self . cluster . items ( ) : <NEWLINE> <INDENT> result [ cluster_nro ] . append ( element ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> unicode_params = [ ] <NEWLINE> for k , v in params : <NEWLINE> <INDENT> if isinstance ( k , str ) : <NEWLINE> <INDENT> k = k . decode ( <STRING> ) <NEWLINE> <DEDENT> if isinstance ( v , str ) : <NEWLINE> <INDENT> if v . startswith ( <STRING> ) : <NEWLINE> <INDENT> v = utils . unescape ( v ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> v = v . decode ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> unicode_params . append ( ( k , v ) ) <NEWLINE> <DEDENT> <DEDENT>
def add_params_to_uri ( uri , params , fragment = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sch , net , path , par , query , fra = urlparse . urlparse ( uri ) <NEWLINE> if fragment : <NEWLINE> <INDENT> fra = add_params_to_qs ( query , params ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> query = add_params_to_qs ( query , params ) <NEWLINE> <DEDENT> return urlparse . urlunparse ( ( sch , net , path , par , query , fra ) ) <NEWLINE> <DEDENT>
bindata = numpy . zeros ( <NEWLINE> <INDENT> ( T / time_bin_length , ) + data . shape [ 1 : ] , dtype = <STRING> ) <NEWLINE> for index , i in enumerate ( range ( 0 , T - time_bin_length + 1 , <NEWLINE> <INDENT> time_bin_length ) ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> bindata [ index ] = weighted_avg_and_std ( data [ i : i + time_bin_length ] , <NEWLINE> <INDENT> axis = 0 , <NEWLINE> weights = sample_selector [ i : i + <NEWLINE> time_bin_length ] ) [ 0 ] <NEWLINE> <DEDENT> <DEDENT>
def tsg_to_net ( self , node , max_lag ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> row = node / max_lag <NEWLINE> lag = node % max_lag <NEWLINE> return ( row , - lag ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if isinstance ( self . n_symbs ** dim , int ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if self . n_symbs ** dim * 16. / 8. / 1024. ** 3 > 3. : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if dim * self . n_symbs ** dim > 2 ** 65 : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> % ( self . n_symbs , dim ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if a > b : <NEWLINE>
axp . text ( - .1 , .5 , <STRING> % hi , rotation = <STRING> , <NEWLINE>
def resize_convex ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> pending = None <NEWLINE> c0 = None <NEWLINE> for el in self [ 1 : - 1 ] : <NEWLINE> <INDENT> if not hasattr ( el , <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> c = getattr ( el , <STRING> , 0 ) <NEWLINE> if pending is not None : <NEWLINE> <INDENT> r = max ( el . radius , pending . radius ) <NEWLINE> if c < 0 : <NEWLINE> <INDENT> el . radius = r <NEWLINE> <DEDENT> if c0 > 0 : <NEWLINE> <INDENT> pending . radius = r <NEWLINE> <DEDENT> pending = None <NEWLINE> if not el . material or el . material . solid : <NEWLINE> <INDENT> pending = el <NEWLINE> <DEDENT> <DEDENT> if not el . material or el . material . solid : <NEWLINE> <INDENT> pending , c0 = el , c <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def main ( ) : <NEWLINE> <INDENT> if len ( sys . argv ) == 3 : <NEWLINE> <INDENT> keep_count = 0 <NEWLINE> <DEDENT> elif len ( sys . argv ) == 4 : <NEWLINE> <INDENT> keep_count = int ( sys . argv [ 3 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> conf_file = sys . argv [ 1 ] <NEWLINE> if sys . argv [ 2 ] [ - 1 ] == <STRING> : <NEWLINE> <INDENT> dump_dir = sys . argv [ 2 ] [ : - 1 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> dump_dir = sys . argv [ 2 ] <NEWLINE> <DEDENT> if not os . path . isfile ( conf_file ) : <NEWLINE> <INDENT> print ( <STRING> % conf_file ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> if not os . path . isdir ( dump_dir ) : <NEWLINE> <INDENT> print ( <STRING> % dump_dir ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> if keep_count <= 0 : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> make_dump ( conf_file , dump_dir , keep_count ) <NEWLINE> <DEDENT>
if not self . check_plugin_installed ( plugin_full_name ) : <NEWLINE> <INDENT> self . pip . install ( plugin_full_name ) <COMMENT> <NEWLINE> <DEDENT>
def convert_all_validator ( self , schema_node , validator ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> converted = None <NEWLINE> if isinstance ( validator , colander . All ) : <NEWLINE> <INDENT> converted = { } <NEWLINE> for v in validator . validators : <NEWLINE> <INDENT> ret = self ( schema_node , validator ) <NEWLINE> converted . update ( ret ) <NEWLINE> <DEDENT> <DEDENT> return converted <NEWLINE> <DEDENT>
wlgrid = np . log10 ( 10000 / bindown_wngrid ) <NEWLINE> <INDENT> if args . plot : <NEWLINE> <INDENT> if get_rank ( ) == 0 and nprocs ( ) == 1 : <NEWLINE> <INDENT> import matplotlib . pyplot as plt <NEWLINE> if args . contrib : <NEWLINE> <INDENT> for name , value in contrib : <NEWLINE> <INDENT> new_value = bindown ( native_grid , value , bindown_wngrid ) <NEWLINE> plt . plot ( wlgrid , new_value , label = name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if solution is not None : <NEWLINE> <INDENT> out . store_dictionary ( solution , group_name = <STRING> ) <NEWLINE> priors = { } <NEWLINE> priors [ <STRING> ] = profiles <NEWLINE> priors [ <STRING> ] = spectrum <NEWLINE> out . store_dictionary ( solution , group_name = <STRING> ) <NEWLINE> else : <NEWLINE> out . store_dictionary ( profiles , group_name = <STRING> ) <NEWLINE> out . store_dictionary ( spectrum , group_name = <STRING> ) <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> from mpi4py import MPI <NEWLINE> except ImportError : <NEWLINE> return 0 <NEWLINE> <DEDENT>
log_start = time <NEWLINE> <INDENT> click . echo ( <STRING> ) <NEWLINE> context . session . add_time_entry ( <NEWLINE> <INDENT> start_time = time , description = message , project = project_obj <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> foundDocuments . append ( GraphDbPackedSegmentTuple ( <NEWLINE> <INDENT> key = key , <NEWLINE> packedJson = segmentsByKey [ subKey ] <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT>
def get_matsize ( m ) : <NEWLINE> <INDENT> size = 0 <NEWLINE> depth = len ( m ) <NEWLINE> for i in range ( depth ) : <NEWLINE> <INDENT> layer = m [ depth ] <NEWLINE> lngth = len ( layer ) <NEWLINE> for j in range ( lngth ) : <NEWLINE> <INDENT> size = size + 1 <NEWLINE> <DEDENT> <DEDENT> return ( size ) <NEWLINE> <DEDENT>
def select ( loomfile , min_read_count , min_cell_count , layer ) : <NEWLINE> <INDENT> with loompy . connect ( loomfile ) as ds : <NEWLINE> <INDENT> gsurv = ( ds . sparse ( layer = layer ) >= min_read_count ) . sum ( axis = 1 ) > min_cell_count <NEWLINE> ds . ra . Selected = np . squeeze ( np . asarray ( gsurv ) ) <NEWLINE> LOG . info ( <STRING> % gsurv . sum ( ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT>
with bind . connect ( ) as conn : <NEWLINE> <INDENT> select_files . bind = conn <NEWLINE> select_values . bind = conn <NEWLINE> for in_slice in window_slices ( File . id , size = windowsize , bind = bind ) : <NEWLINE> <INDENT> if log . level <= logging . DEBUG : <NEWLINE> <INDENT> where = literal_compile ( in_slice ( File . id ) ) <NEWLINE> log . debug ( <STRING> , where . string ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def process_view ( self , request , view_func , view_args , view_kwargs ) : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> realm = request . _rated_realm <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> realm = settings . REALM_MAP [ request . resolver_match . url_name ] <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> return None <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT>
def _task_gen ( item , ** kwds ) : <NEWLINE> <INDENT> runner . _update_current_test_var ( item , <STRING> ) <NEWLINE> dag = item . _request . getfixturevalue ( <STRING> ) <NEWLINE> ihook = getattr ( item . ihook , <STRING> ) <NEWLINE> task_id = _gen_task_id ( item ) <NEWLINE> task = PythonOperator ( <NEWLINE> <INDENT> task_id = task_id , <NEWLINE> python_callable = lambda : ihook ( item = item , ** kwds ) , <NEWLINE> provide_context = True , <NEWLINE> dag = dag , <NEWLINE> <DEDENT> ) <NEWLINE> dag . set_dependency ( task_id , <STRING> ) <NEWLINE> return task <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> N = rv . shape [ 0 ] <NEWLINE> y = y [ 0 : N , : ] <NEWLINE> y_iv = y_iv [ 0 : N , : ] <NEWLINE> <COMMENT> <NL> ebar = rv - y <NEWLINE> ebar_iv = rv - y_iv <NEWLINE> <COMMENT> <NL> uf = uf [ 0 : N , : ] <NEWLINE> <DEDENT>
@ property <NEWLINE> <INDENT> def root ( self ) : <NEWLINE> <INDENT> return self . ancestors [ 0 ] <NEWLINE> <DEDENT> <DEDENT>
if not configSpec : <NEWLINE> <INDENT> raise UnfurlError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> % ( action , resource . template . name , resource . template . name , reason ) <NEWLINE> <DEDENT> ) <NEWLINE> logger . debug ( <NEWLINE> <STRING> , <NEWLINE> configSpec . name , <NEWLINE> configSpec . inputs , <NEWLINE> resource . name , <NEWLINE> reason or action , <NEWLINE> ) <NEWLINE> return ( configSpec , resource , reason or action ) <NEWLINE> <DEDENT>
def _as_html_tags_eval_item ( item ) : <NEWLINE> <INDENT> if not isinstance ( HTMLTagsEvalItem , item ) : <NEWLINE> <INDENT> return HTMLTagsEvalItem ( item ) <NEWLINE> <DEDENT> return item <NEWLINE> <DEDENT>
def decode ( self , probs , seq_lens ) : <NEWLINE> <COMMENT> <NL> <INDENT> probs = probs . cpu ( ) . float ( ) <NEWLINE> seq_lens = probs . cpu ( ) . int ( ) <NEWLINE> batch_size , max_seq_len = probs . size ( 0 ) , probs . size ( 1 ) <NEWLINE> output = torch . IntTensor ( batch_size , self . _beam_width , max_seq_len ) . cpu ( ) . int ( ) <NEWLINE> timesteps = torch . IntTensor ( batch_size , self . _beam_width , max_seq_len ) . cpu ( ) . int ( ) <NEWLINE> scores = torch . IntTensor ( batch_size , self . _beam_width ) . cpu ( ) . int ( ) <NEWLINE> out_seq_len = torch . IntTensor ( batch_size , self . _beam_width ) . cpu ( ) . int ( ) <NEWLINE> if self . _scorer : <NEWLINE> <INDENT> ctc_decode . paddle_beam_decode_lm ( probs , seq_lens , self . _labels , self . _num_labels , self . _beam_width , <NEWLINE> <INDENT> self . _num_processes , self . _cutoff_prob , self . cutoff_top_n , self . _blank_id , <NEWLINE> self . _scorer , output , timesteps , scores , out_seq_len ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> ctc_decode . paddle_beam_decode ( probs , seq_lens , self . _labels , self . _num_labels , self . _beam_width , self . _num_processes , <NEWLINE> <INDENT> self . _cutoff_prob , self . cutoff_top_n , self . _blank_id , output , timesteps , <NEWLINE> scores , out_seq_len ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if new_weight_sum < V_bin_max : <NEWLINE> <DEDENT>
return values <NEWLINE>
def assert_read_available_to ( <NEWLINE> <INDENT> self , <NEWLINE> unauthorised_agent : Agent , <NEWLINE> broadcast_candidate : Union [ ReadProtected , List [ ReadProtected ] ] <NEWLINE> ) -> Agent : <NEWLINE> <STRING> <NEWLINE> if isinstance ( broadcast_candidate , list ) : <NEWLINE> <INDENT> for candidate in broadcast_candidate : <NEWLINE> <INDENT> if not candidate . grants_read_to ( unauthorised_agent ) : <NEWLINE> <INDENT> raise NotAuthorised <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> return broadcast_candidate <NEWLINE> <DEDENT> <DEDENT>
if k < fppc : <NEWLINE> <COMMENT> <NL> <INDENT> pairs . sp = np . tile ( label , [ fppc , 2 ] ) <NEWLINE> pairs . start = np . random . choice ( frag_steps , [ fppc , 2 ] ) <NEWLINE> pairs . end = pairs . start + frag_steps <NEWLINE> <DEDENT>
stc . SetText ( FileOperations ( ) . readFile ( filePath = fname ) ) <NEWLINE> <INDENT> centerPaneTab . window . addTab ( name = <STRING> + fileName , worksheetPanel = stc ) <NEWLINE> <DEDENT>
for book in deleteBooks : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> FindingBook ( libraryPath = self . GetParent ( ) . GetParent ( ) . libraryPath ) . deleteBook ( book ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> except Exception as e : <NEWLINE> <INDENT> logger . error ( e , exc_info = True ) <NEWLINE> logger . error ( <STRING> , selectedBookIndex , len ( self . _items ) ) <NEWLINE> <COMMENT> <NL> if len ( deleteBooks ) > 0 : <NEWLINE> <DEDENT> self . updateStatusBar ( text = <STRING> ) <NEWLINE> self . GetParent ( ) . GetParent ( ) . loadingBook ( ) <NEWLINE> self . GetParent ( ) . GetParent ( ) . updatePangnation ( ) <NEWLINE> <DEDENT>
self . Fi = unit_vector * ( self . Fs ( defflection ) + self . Fd ( velocity ) ) <NEWLINE> <INDENT> Ti_e = 2 * G ( self . Pi ) . T * ( self . Ti + Skew ( self . ui ) . T * self . Fi ) <NEWLINE> <DEDENT>
page_count = self . results_count ( ) / 24 <NEWLINE> <INDENT> if self . results_count ( ) % 24 > 0 : <NEWLINE> <INDENT> page_count += 1 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def dummy_blast ( query , subj , minoverlap ) : <NEWLINE> <COMMENT> <NL> <INDENT> bools = [ True for e in subj ] <NEWLINE> positions = [ 0 for e in subj ] <NEWLINE> max_positions = [ len ( e ) for e in subj ] <NEWLINE> positions . extend ( max_positions ) <NEWLINE> return bools , positions <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def run_all ( klass , wd , stages ) : <NEWLINE> <INDENT> for s in stages : <NEWLINE> <INDENT> if check ( stage = s , directory = wd ) : <NEWLINE> <INDENT> Stager ( wd , s ) . run ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if sys . argv [ 1 ] == <STRING> : <NEWLINE> <INDENT> if len ( sys . argv ) < 2 : <NEWLINE> <INDENT> print ( usage_str ) <NEWLINE> <DEDENT> elif sys . argv [ 2 ] == <STRING> : <NEWLINE> <INDENT> print ( bomail . util . datestr . datestr_str ) <NEWLINE> <DEDENT> elif sys . argv [ 2 ] == <STRING> : <NEWLINE> <INDENT> print ( taghelp_str ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( usage_str ) <NEWLINE> <DEDENT> exit ( 0 ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> def recheck ( self , old_disp_info ) : <NEWLINE> <INDENT> if not self . is_loaded : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> display . draw_loading_screen ( self . gui ) <NEWLINE> new_filenames = search . search_argstr ( self . search_str , self . gui . mail_mgr ) <NEWLINE> new_fileset = set ( new_filenames ) <NEWLINE> self . remove_files ( [ t [ 0 ] for t in self . file_data if t [ 0 ] not in new_fileset ] , old_disp_info ) <NEWLINE> <COMMENT> <NL> self . update_for_change ( new_fileset , old_disp_info ) <NEWLINE> <DEDENT> <DEDENT>
def send ( self , request , ** kwargs ) : <NEWLINE> <INDENT> func = super ( FuturesSession , self ) . send <NEWLINE> if isinstance ( self . executor , ProcessPoolExecutor ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> dumps ( request ) <NEWLINE> <DEDENT> except ( TypeError , PickleError ) : <NEWLINE> <INDENT> raise RuntimeError ( PICKLE_ERROR ) <NEWLINE> <DEDENT> <DEDENT> return self . executor . submit ( func , request , ** kwargs ) <NEWLINE> <DEDENT>
actions . append ( { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : True , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : case . contact . id , <NEWLINE> <STRING> : active_sub_ids <NEWLINE> <DEDENT> } <NEWLINE> } ) <NEWLINE> if len ( active_sub_ids ) > 0 : <NEWLINE> actions . append ( self . get_cancel_action ( active_sub_ids ) ) <NEWLINE> <DEDENT>
def __init__ ( self , param_decls , required = None , ** attrs ) : <NEWLINE> <INDENT> if required is None : <NEWLINE> <INDENT> if attrs . get ( <STRING> ) is not None : <NEWLINE> <INDENT> required = False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> required = attrs . get ( <STRING> , 0 ) > 0 <NEWLINE> <DEDENT> <DEDENT> Parameter . __init__ ( self , param_decls , required = required , ** attrs ) <NEWLINE> <DEDENT>
def callback ( ctx , param , value ) : <NEWLINE> <INDENT> if value or ctx . resilient_parsing : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> prog = prog_name <NEWLINE> if prog is None : <NEWLINE> <INDENT> prog = ctx . find_root ( ) . info_name <NEWLINE> <DEDENT> ver = version <NEWLINE> if ver is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> import pkg_resources <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for dist in pkg_resources . working_set : <NEWLINE> <INDENT> scripts = dist . get_entry_map ( ) . get ( <STRING> ) or { } <NEWLINE> for script_name , entry_point in iteritems ( scripts ) : <NEWLINE> <INDENT> if entry_point . module_name == module : <NEWLINE> <INDENT> ver = dist . version <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> if ver is None : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> echo ( message % { <NEWLINE> <INDENT> <STRING> : prog , <NEWLINE> <STRING> : ver , <NEWLINE> <DEDENT> } ) <NEWLINE> ctx . exit ( ) <NEWLINE> <DEDENT>
old_env = { } <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> for key , value in iteritems ( env ) : <NEWLINE> <INDENT> old_env [ key ] = os . environ . get ( value ) <NEWLINE> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> yield bytes_output <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> for key , value in iteritems ( old_env ) : <NEWLINE> <INDENT> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> sys . stdout = old_stdout <NEWLINE> sys . stderr = old_stderr <NEWLINE> sys . stdin = old_stdin <NEWLINE> clickpkg . termui . visible_prompt_func = old_visible_prompt_func <NEWLINE> clickpkg . termui . hidden_prompt_func = old_hidden_prompt_func <NEWLINE> clickpkg . termui . _getchar = old__getchar_func <NEWLINE> clickpkg . utils . should_strip_ansi = old_should_strip_ansi <NEWLINE> clickpkg . formatting . FORCED_WIDTH = old_forced_width <NEWLINE> <DEDENT> <DEDENT>
if len ( messages ) >= this . lastMessage : <NEWLINE> <INDENT> id = dom . createElement ( <STRING> ) <NEWLINE> dom . setLayoutXSL ( id , this . buildXML ( ) , <STRING> ) <NEWLINE> dom . insertChild ( id , <STRING> ) <NEWLINE> <DEDENT>
def send_request ( url , method , <NEWLINE> <INDENT> data , args , params , headers , cookies , timeout , is_json ) : <NEWLINE> <STRING> <NEWLINE> <COMMENT> <NL> for p in args : <NEWLINE> <INDENT> url = url . replace ( <STRING> + p , str ( params [ p ] ) ) <NEWLINE> <DEDENT> <DEDENT>
security . declareProtected ( <NEWLINE> <INDENT> <STRING> , <STRING> ) <NEWLINE> def digest ( self , * args ) : <NEWLINE> assert len ( args ) > 1 , <STRING> <NEWLINE> challenge = hmac . new ( self . __key , str ( args [ 0 ] ) , hashlib . sha1 ) <NEWLINE> for arg in args [ 1 : ] : <NEWLINE> <INDENT> challenge . update ( str ( args ) ) <NEWLINE> <DEDENT> return challenge . hexdigest ( ) <NEWLINE> <DEDENT>
def is_request_ready ( self , _request_id : str , _retry : bool = True ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . login ( ) <NEWLINE> url = <STRING> <NEWLINE> req = Request ( url , headers = self . headers ) <NEWLINE> try : <NEWLINE> <INDENT> decoded = json . load ( urlopen ( req ) ) <NEWLINE> if len ( decoded ) > 1 and <STRING> in decoded [ 0 ] : <NEWLINE> <INDENT> return decoded [ 0 ] [ <STRING> ] <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT> except HTTPError as e : <NEWLINE> <INDENT> if self . _error_handling ( e ) and _retry : <NEWLINE> <INDENT> return self . is_request_ready ( _request_id , _retry = False ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def update ( self , _id , data : dict ) -> dict : <NEWLINE> <INDENT> <STRING> <NEWLINE> file_path = self . file_path ( _id ) <NEWLINE> if not self . exists ( _id ) : <NEWLINE> <INDENT> raise Exception ( <STRING> . format ( file_path ) ) <NEWLINE> <DEDENT> if <STRING> not in data : <NEWLINE> <INDENT> data [ <STRING> ] = _id <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> cur_data = self . fetch ( _id ) <NEWLINE> new_data = DictUtils . merge ( cur_data , data ) <NEWLINE> Yaml . to_file ( file_path = file_path , data = new_data ) <NEWLINE> return data <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if rel . many : <NEWLINE> <INDENT> related_bizobj_list = [ ] <NEWLINE> for obj in related_data : <NEWLINE> <INDENT> if isinstance ( obj , rel . target ) : <NEWLINE> <INDENT> related_bizobj_list . append ( obj ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> related_bizobj_list . append ( <NEWLINE> <INDENT> rel . target ( related_data ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _aggregate_related ( <NEWLINE> <INDENT> self , <NEWLINE> bizobj : <STRING> , <NEWLINE> to_create : Dict , <NEWLINE> to_update : Dict , <NEWLINE> ) -> None : <NEWLINE> for k , v in bizobj . related . items ( ) : <NEWLINE> <INDENT> rel = bizobj . relationships [ k ] <NEWLINE> if not v : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if rel . many : <NEWLINE> <INDENT> for x in v : <NEWLINE> <INDENT> if x . _id is None : <NEWLINE> <INDENT> to_create [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> elif x . dirty : <NEWLINE> <INDENT> to_update [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> <DEDENT> for x in v : <NEWLINE> <INDENT> self . _aggregate_related ( x , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v_type = v . __class__ <NEWLINE> if v . _id is None : <NEWLINE> <INDENT> to_create [ v_type ] . append ( v ) <NEWLINE> <DEDENT> elif x . dirty : <NEWLINE> <INDENT> to_update [ v_type ] . append ( x ) <NEWLINE> <DEDENT> self . _aggregate_related ( v , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _aggregate_related ( <NEWLINE> <INDENT> self , <NEWLINE> bizobj : <STRING> , <NEWLINE> to_create : Dict , <NEWLINE> to_update : Dict , <NEWLINE> ) -> None : <NEWLINE> for k , v in bizobj . related . items ( ) : <NEWLINE> <INDENT> rel = bizobj . relationships [ k ] <NEWLINE> if not v : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if rel . many : <NEWLINE> <INDENT> for x in v : <NEWLINE> <INDENT> if x . _id is None : <NEWLINE> <INDENT> to_create [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> elif x . dirty : <NEWLINE> <INDENT> to_update [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> <DEDENT> for x in v : <NEWLINE> <INDENT> self . _aggregate_related ( x , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v_type = v . __class__ <NEWLINE> if v . _id is None : <NEWLINE> <INDENT> to_create [ v_type ] . append ( v ) <NEWLINE> <DEDENT> elif v . dirty : <NEWLINE> <INDENT> to_update [ v_type ] . append ( x ) <NEWLINE> <DEDENT> self . _aggregate_related ( v , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if self . _op == OP_CODE . AND : <NEWLINE> <COMMENT> <NL> <INDENT> if lhs_exc is None : <NEWLINE> <INDENT> rhs_exc = self . _rhs ( context , arguments ) <NEWLINE> if rhs_exc is not None : <NEWLINE> <INDENT> return rhs_exc <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> return lhs_exc <NEWLINE> elif self . _op == OP_CODE . OR : <NEWLINE> <DEDENT> rhs_exc = self . _rhs ( context , arguments ) <NEWLINE> if rhs_exc is not None and lhs_exc is not None : <NEWLINE> <INDENT> return CompositeGuardException ( lhs_exc , rhs_exc ) <NEWLINE> elif self . _op == OP_CODE . NOT : <NEWLINE> <DEDENT> if lhs_exc is not None : <NEWLINE> <INDENT> return lhs_exc <NEWLINE> else : <NEWLINE> <DEDENT> return ValueError ( <STRING> ) <NEWLINE> <DEDENT>
new_decorator = type ( self ) ( self . app , * self . args , ** kwargs ) <NEWLINE> <INDENT> new_decorator . _api_object = self . _api_object <NEWLINE> new_decorator . setup_action ( v . target , True ) <NEWLINE> else : <NEWLINE> self . setup_action ( v . __func__ , False ) <NEWLINE> return api_type <NEWLINE> else : <NEWLINE> func = obj <NEWLINE> action = self . setup_action ( func , False ) <NEWLINE> return action <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if date is True : <NEWLINE> <INDENT> p ( star , Fore . LIGHTMAGENTA_EX + str ( item [ <STRING> ] ) . rjust ( 2 ) , mark , text_color + item [ <STRING> ] , tag_text , <NEWLINE> <INDENT> ( Fore . LIGHTBLACK_EX + <STRING> . format ( color + str ( duedate ) + Fore . LIGHTBLACK_EX ) ) if item [ <STRING> ] else <STRING> , <NEWLINE> Fore . LIGHTBLACK_EX + str ( item [ <STRING> ] ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> p ( star , Fore . LIGHTMAGENTA_EX + str ( item [ <STRING> ] ) . rjust ( 2 ) , mark , text_color + item [ <STRING> ] , tag_text , due_text , day_text ) <NEWLINE> print ( ) <NEWLINE> print_footer ( ) <NEWLINE> print_total ( ) <NEWLINE> print ( ) <NEWLINE> <DEDENT> <DEDENT>
MAXFD = 1024 <NEWLINE> <INDENT> fdset = c_int32 * ( MAXFD / 32 ) <NEWLINE> <DEDENT>
def decodeOid ( pdu ) : <NEWLINE> <INDENT> return tuple ( [ pdu . val . objid [ i ] for i in range ( pdu . val_len / sizeof ( u_long ) ) ] ) <NEWLINE> <DEDENT>
def stats ( dir_stats = DIR_STATS ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_links = ( <STRING> + dir_stats + <STRING> + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_links = ( <STRING> + str ( average ( content ) ) ) <NEWLINE> if len ( content ) > 10000 : <NEWLINE> <INDENT> compress_stats ( dir_stats + <STRING> ) <NEWLINE> <DEDENT> <DEDENT> result += stat_links + <STRING> <NEWLINE> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_webpages = <STRING> + dir_stats + <STRING> + <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_webpages = <STRING> + str ( average ( content ) ) <NEWLINE> <DEDENT> result += stat_webpages + <STRING> <NEWLINE> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_dl_index = <STRING> + dir_stats + <STRING> + <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_dl_index = <STRING> + str ( average ( content ) ) <NEWLINE> <DEDENT> result += stat_dl_index + <STRING> <NEWLINE> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_up_index = <STRING> + dir_stats + <STRING> + <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_up_index = <STRING> + str ( average ( content ) ) <NEWLINE> <DEDENT> result += stat_up_index + <STRING> <NEWLINE> return result <NEWLINE> <DEDENT>
if len ( sys . argv ) > 0 : <NEWLINE> <INDENT> sys . stdin = open ( sys . argv [ 1 ] ) <NEWLINE> while not stdio . isEmpty ( ) : <NEWLINE> <INDENT> input_item = stdio . readString ( ) <NEWLINE> if input_item is not <STRING> : <NEWLINE> <INDENT> queue . enqueue ( input_item ) <NEWLINE> <DEDENT> elif not queue . is_empty ( ) : <NEWLINE> <INDENT> print ( queue . dequeue ( ) ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> . format ( queue . size ( ) ) ) <NEWLINE> <DEDENT>
class ReCaptchaField ( forms . CharField ) : <NEWLINE> <INDENT> def __init__ ( self , attrs = None , * args , ** kwargs ) : <NEWLINE> <INDENT> if os . environ . get ( <STRING> , None ) is not None : <NEWLINE> <INDENT> self . _private_key = kwargs . pop ( <STRING> , settings . RECAPTCHA_PRIVATE_KEY ) <NEWLINE> self . _score_threshold = kwargs . pop ( <STRING> , settings . RECAPTCHA_SCORE_THRESHOLD ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def parse_object ( self , json_string ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> json_object = json . loads ( json_string ) <NEWLINE> <DEDENT> except json . JSONDecodeError as json_error : <NEWLINE> <INDENT> LOGGER . warning ( <STRING> ) <NEWLINE> LOGGER . warning ( json_string ) <NEWLINE> LOGGER . warning ( <STRING> , json_string ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>
stmt = list ( ) <NEWLINE> <INDENT> for ln in sql_in : <NEWLINE> <INDENT> cline = ln . strip ( ) <NEWLINE> if ln . startswith ( <STRING> ) or len ( cline ) == 0 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if r . status_code != 204 : <NEWLINE> <INDENT> comments_to_delete += r . json ( ) . get ( <STRING> , list ( ) ) <NEWLINE> deleted_comments += 1 <NEWLINE> except : <NEWLINE> continue <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> lstTriangle = tripy . earclip ( poly ) <NEWLINE> lstArea = [ ] <NEWLINE> for i in range ( len ( lstTriangle ) ) : <NEWLINE> <INDENT> lstArea . append ( geoAreaOfTriangle ( lstTriangle [ i ] [ 0 ] , lstTriangle [ i ] [ 1 ] , lstTriangle [ i ] [ 2 ] ) ) <NEWLINE> <DEDENT> <DEDENT>
plotting . eigs ( root = rootname , affinity = mdl_obj . affinity_matrix_ , <NEWLINE> <INDENT> n_clusters = n_clusters , <NEWLINE> title = <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if hasattr ( mdl_obj , <STRING> ) : <NEWLINE> _est_name = mdl_obj . __dict__ . get ( <STRING> , <STRING> ) or type ( mdl_obj ) . __name__ <NEWLINE> if _est_name != <STRING> : <NEWLINE> <COMMENT> <NL> plotting . voronoi ( root = rootname , labels = y , data_in = step_in , <NEWLINE> <INDENT> model = voronoi_mdl_obj ) <NEWLINE> elif hasattr ( mdl_obj , <STRING> ) : <NEWLINE> plotting . tree ( root = rootname , data_in = step_in , <NEWLINE> <DEDENT> <DEDENT> labels = step_out , model = mdl_obj ) <NEWLINE> plotting . dendrogram ( root = rootname , data_in = step_in , <NEWLINE> <INDENT> labels = y , model = mdl_obj ) <NEWLINE> <DEDENT> <DEDENT>
if ch == 27 : <NEWLINE> <INDENT> return EVENT_ESC <NEWLINE> elif ch == - 1 or ch == curses . KEY_RESIZE : <NEWLINE> return EVENT_RESIZE <NEWLINE> elif ch == 10 or ch == curses . KEY_ENTER : <NEWLINE> return EVENT_ENTER <NEWLINE> elif ch == 127 or ch == curses . KEY_BACKSPACE : <NEWLINE> return EVENT_BACKSPACE <NEWLINE> elif ch == curses . KEY_UP : <NEWLINE> return EVENT_UP <NEWLINE> elif ch == curses . KEY_DOWN : <NEWLINE> return EVENT_DOWN <NEWLINE> elif ch == curses . KEY_LEFT : <NEWLINE> return EVENT_LEFT <NEWLINE> elif ch == curses . KEY_RIGHT : <NEWLINE> return EVENT_RIGHT <NEWLINE> elif ch == 3 : <NEWLINE> return EVENT_CTRL_C <NEWLINE> elif 0 >= ch < 256 : <NEWLINE> return chr ( ch ) <NEWLINE> else : <NEWLINE> return EVENT_UNHANDLED <NEWLINE> <DEDENT>
def read ( self , aTableName ) : <NEWLINE> <INDENT> if self . _objStore_flag : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> objPath = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> self . _objStore . get_object ( self . _admin_username , objPath ) <NEWLINE> tableFile = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> self . _decrypt ( tableFile ) <NEWLINE> dataframe = pq . read_table ( tableFile ) . to_pandas ( ) <NEWLINE> os . remove ( tableFile ) <NEWLINE> os . remove ( objPath ) <NEWLINE> return dataframe <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> return ( <STRING> <NEWLINE> <INDENT> . format ( self . _admin_username , objPath ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> tableFile = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> tableFileEnc = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> self . _decrypt ( tableFileEnc ) <NEWLINE> dataframe = pq . read_table ( tableFile ) . to_pandas ( ) <NEWLINE> os . remove ( tableFile ) <NEWLINE> return dataframe <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> return ( <STRING> <NEWLINE> <INDENT> . format ( self . _path , tableFile ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> if self . op == <STRING> : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 + self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 + self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 * self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 / self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = math . ceil ( self . r1 ) <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = math . fabs ( self . r1 ) <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = math . fmod ( self . r1 , self . r2 ) <NEWLINE> <DEDENT> <DEDENT>
def _write_solocache_group_to_file ( self , data_dict , group_prefix = <STRING> ) : <NEWLINE> <INDENT> if self . _is_master ( ) and group_prefix != <STRING> : <NEWLINE> <INDENT> if group_prefix in self . _f : <NEWLINE> <INDENT> self . _f . create_group ( group_prefix ) <NEWLINE> <DEDENT> <DEDENT> keys = data_dict . keys ( ) <NEWLINE> keys . sort ( ) <NEWLINE> for k in keys : <NEWLINE> <INDENT> name = group_prefix + k <NEWLINE> if isinstance ( data_dict [ k ] , dict ) : <NEWLINE> <INDENT> self . _write_solocache_group_to_file ( data_dict [ k ] , group_prefix = name + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ( data , op ) = data_dict [ k ] <NEWLINE> if op is not None : <NEWLINE> <INDENT> if numpy . isscalar ( data ) : <NEWLINE> <INDENT> sendobj = numpy . array ( data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sendobj = data <NEWLINE> <DEDENT> recvobj = numpy . empty_like ( data ) <NEWLINE> log_debug ( logger , self . _log_prefix + <STRING> % ( name ) ) <NEWLINE> self . comm . Reduce ( <NEWLINE> <INDENT> [ sendobj , MPI . DOUBLE ] , <NEWLINE> [ recvobj , MPI . DOUBLE ] , <NEWLINE> op = op , <NEWLINE> root = 0 <NEWLINE> <DEDENT> ) <NEWLINE> data = recvobj <NEWLINE> <DEDENT> if self . _is_master ( ) : <NEWLINE> <INDENT> log_debug ( logger , self . _log_prefix + <STRING> % ( name ) ) <NEWLINE> self . _f [ name ] = data <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def __call__ ( self ) : <NEWLINE> <INDENT> if np . random . uniform ( ) > self . _epsilon : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>
last = 0 if n_steps < self . mdp . horizon or not absorbing else 1 <NEWLINE> <INDENT> sample = self . state . ravel ( ) . tolist ( ) + action . ravel ( ) . tolist ( ) + [ reward ] + next_state . ravel ( ) . tolist ( ) + [ absorbing , last ] <NEWLINE> <DEDENT>
if iterate_over == <STRING> : <NEWLINE> <INDENT> self . logger . info ( <STRING> % ( i + 1 ) ) <NEWLINE> self . logger . info ( self . state ) <NEWLINE> while i < how_many : <NEWLINE> J = 0. <NEWLINE> action_idx = self . agent . draw_action ( self . state , <NEWLINE> <INDENT> self . agent . approximator ) <NEWLINE> <DEDENT> action_value = self . mdp . action_space . get_value ( action_idx ) <NEWLINE> next_state , reward , absorbing , _ = self . mdp . step ( action_idx ) <NEWLINE> J += self . mdp . gamma ** n_steps * reward <NEWLINE> n_steps += 1 <NEWLINE> <DEDENT>
def parse_quantity ( str_value ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for i in range ( len ( str_value ) , - 1 , 0 ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> value = float ( str_value [ : i ] ) <NEWLINE> unit = units . Unit ( str_value [ i : ] ) <NEWLINE> return units . Quantity ( value , unit ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> raise ValueError ( <STRING> . format ( str_value ) ) <NEWLINE> <DEDENT>
def clear ( self ) : <NEWLINE> <INDENT> self . ensure_all_bound ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if self . _fill is not None : <NEWLINE> <INDENT> self . buffer ( <STRING> ) . zero ( self . command_queue ) <NEWLINE> <DEDENT> <DEDENT>
def render ( self , output_dir , mode , stokes , channel , rel_channel ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> filename = os . path . join ( output_dir , self . fits_filename ( mode , stokes , channel , rel_channel ) ) <NEWLINE> with closing ( fits . open ( filename ) ) as hdulist : <NEWLINE> <INDENT> naxis = int ( hdulist [ 0 ] . header [ <STRING> ] ) <NEWLINE> slices = [ 0 ] * ( naxis - 2 ) <NEWLINE> for i in range ( 3 , naxis + 1 ) : <NEWLINE> <INDENT> axis_type = hdulist [ 0 ] . header [ <STRING> . format ( i ) ] <NEWLINE> if axis_type == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> slices [ i - 3 ] = <STRING> . find ( stokes ) <NEWLINE> <DEDENT> elif axis_type == <STRING> : <NEWLINE> <INDENT> slices [ i - 3 ] = channel <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . _render_thumb ( output_dir , filename , slices , mode , stokes , channel ) <NEWLINE> self . _render_full ( output_dir , filename , slices , mode , stokes , channel ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> cmd_project_config = ProjectConfig ( cmd_name , project_dir ) <NEWLINE> handler = get_handler ( project_dir , module , service ) <NEWLINE> cmd = project_config . get_command ( handler , module , service , workspace ) <NEWLINE> if not cmd : <NEWLINE> <INDENT> raise CwsClientError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def require_relationship ( resource_list , data ) : <NEWLINE> <INDENT> for resource in resource_list : <NEWLINE> <INDENT> if resource not in data : <NEWLINE> <INDENT> raise UnprocessableEntity ( { <STRING> : <STRING> . format ( resource ) } , <NEWLINE> <INDENT> <STRING> . format ( resource ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def require_relationship ( resource_list , data ) : <NEWLINE> <INDENT> for resource in resource_list : <NEWLINE> <INDENT> if resource not in data : <NEWLINE> <INDENT> raise UnprocessableEntity ( { <STRING> : <STRING> . format ( resource ) } , <NEWLINE> <INDENT> <STRING> . format ( resource ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def render_templates ( target_path , replace_values , file_types ) : <NEWLINE> <INDENT> basedir = os . path . abspath ( target_path ) <NEWLINE> for root , dirnames , files in os . walk ( basedir ) : <NEWLINE> <INDENT> for f in files : <NEWLINE> <INDENT> skip = True <NEWLINE> full_path = os . path . join ( root , f ) <NEWLINE> for ft in file_types : <NEWLINE> <INDENT> if f . endswith ( <STRING> . format ( ft ) ) : <NEWLINE> <INDENT> skip = False <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> if skip : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> env = Environment ( loader = FileSystemLoader ( root ) ) <NEWLINE> template = env . get_template ( f ) <NEWLINE> rendered = template . render ( replace_values ) <NEWLINE> with open ( full_path , <STRING> ) as fh : <NEWLINE> <INDENT> print ( <STRING> . format ( f ) ) <NEWLINE> fh . write ( rendered ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def sync ( args ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> import operator <NEWLINE> session = Session ( args ) <NEWLINE> if <STRING> in args [ <STRING> ] : <NEWLINE> <INDENT> targetfeeds = session . list_feeds ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> targetfeeds = [ ] <NEWLINE> for name in args [ <STRING> ] : <NEWLINE> <INDENT> if name not in session . feeds : <NEWLINE> <INDENT> print ( <STRING> <NEWLINE> <INDENT> . format ( name ) , file = sys . stderr , flush = True ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> targetfeeds . append ( name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for target in targetfeeds : <NEWLINE> <INDENT> feed = Feed ( session , target , None ) <NEWLINE> if not feed . wentwrong : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> title = feed . podcast . target . title <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> title = target <NEWLINE> <DEDENT> print ( <STRING> , title , end = <STRING> ) <NEWLINE> currentdate , stop = feed . how_many ( ) <NEWLINE> entrycounter = 0 <NEWLINE> entries_to_download = feed . podcast . entries <NEWLINE> for entry in entries_to_download : <NEWLINE> <INDENT> feed . fix_linkdate ( entry ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> if stop < len ( entries_to_download ) : <NEWLINE> <INDENT> entries_to_download . sort ( key = operator . attrgetter ( <STRING> ) , <NEWLINE> <INDENT> reverse = False ) <NEWLINE> <DEDENT> <DEDENT> for entry in entries_to_download : <NEWLINE> <INDENT> if entry . linkdate > currentdate : <NEWLINE> <INDENT> downloaded = feed . download_entry ( entry ) <NEWLINE> entrycounter += downloaded <NEWLINE> <DEDENT> if entrycounter >= stop : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> msg = <STRING> . join ( [ <STRING> , feed , <NEWLINE> <INDENT> <STRING> ] ) <NEWLINE> <DEDENT> print ( msg , file = sys . stderr , flush = True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not request . user . has_perm ( <STRING> ) or not request . user == instance . comment . user : <NEWLINE> <INDENT> raise Http404 <NEWLINE> <DEDENT>
def __call__ ( self , action_set , payoff , situation ) : <NEWLINE> <INDENT> matching_degrees = [ <NEWLINE> <INDENT> classifier . calc_matching_degree ( self . _rule_repr , situation ) <NEWLINE> for classifier in action_set <NEWLINE> <DEDENT> ] <NEWLINE> total_matching_degrees = sum ( matching_degrees ) <NEWLINE> assert total_matching_degrees > 0.0 <NEWLINE> for ( classifier , matching_degree ) in zip ( matching_degrees , action_set ) : <NEWLINE> <INDENT> credit_weight = ( matching_degree / total_matching_degrees ) <NEWLINE> self . _update_experience ( classifier , credit_weight ) <NEWLINE> payoff_diff = payoff - classifier . get_prediction ( situation ) <NEWLINE> self . _update_weight_vec ( classifier , payoff_diff , situation , <NEWLINE> <INDENT> credit_weight ) <NEWLINE> <DEDENT> self . _update_prediction_error ( classifier , payoff_diff , <NEWLINE> <INDENT> credit_weight ) <NEWLINE> <DEDENT> self . _update_action_set_size ( classifier , action_set ) <NEWLINE> <DEDENT> <DEDENT>
@ click . command ( ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , is_flag = True , help = <STRING> ) <NEWLINE> @ click . argument ( <STRING> ) <NEWLINE> @ click . pass_context <NEWLINE> def update ( context , answered_out , case_id ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if answered_out : <NEWLINE> <INDENT> hk_db = apps . hk . connect ( context . obj ) <NEWLINE> lims_api = apps . lims . connect ( context . obj ) <NEWLINE> log . debug ( <STRING> ) <NEWLINE> hk_case = apps . hk . api . case ( case_id ) <NEWLINE> log . debug ( <STRING> ) <NEWLINE> delivery_dates = [ ] <NEWLINE> hk_run = hk_case . current <NEWLINE> for hk_sample in hk_run . samples : <NEWLINE> <INDENT> log . debug ( <STRING> ) <NEWLINE> delivery_date = lims_api . is_delivered ( hk_sample . lims_id ) <NEWLINE> if delivery_date is None : <NEWLINE> <INDENT> log . warn ( <STRING> , hk_sample . lims_id ) <NEWLINE> context . abort ( ) <NEWLINE> <DEDENT> delivery_dates . append ( delivery_dates ) <NEWLINE> <DEDENT> latest_date = sorted ( delivery_dates ) [ - 1 ] <NEWLINE> log . debug ( <STRING> ) <NEWLINE> hk_run . answeredout_at = datetime . combine ( latest_date , datetime . min . time ( ) ) <NEWLINE> hk_db . commit ( ) <NEWLINE> log . info ( <STRING> , case_id ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if len ( is_externals ) == 0 : <NEWLINE> <INDENT> data = dict ( is_external = is_externals . pop ( ) ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> data = dict ( is_external = False ) <NEWLINE> <DEDENT>
for family_obj in families : <NEWLINE> <INDENT> LOG . debug ( <STRING> ) <NEWLINE> row = [ <NEWLINE> <INDENT> family_obj . internal_id , <NEWLINE> family_obj . name , <NEWLINE> family_obj . customer . internal_id , <NEWLINE> family_obj . priority_human , <NEWLINE> <STRING> . join ( family_obj . panels ) , <NEWLINE> family_obj . action or <STRING> , <NEWLINE> <DEDENT> ] <NEWLINE> click . echo ( tabulate ( [ row ] , headers = FAMILY_HEADERS , tablefmt = <STRING> ) ) <NEWLINE> if samples : <NEWLINE> <INDENT> sample_ids = [ link_obj . sample . internal_id for link_obj in family_obj . links ] <NEWLINE> context . invoke ( sample , sample_ids = sample_ids , families = False ) <NEWLINE> <DEDENT> <DEDENT>
def maximum_flowcells_ondisk ( self , max_flowcells : int = 700 ) -> bool : <NEWLINE> <INDENT> <STRING> <NEWLINE> ondisk_flowcells = self . status . flowcells ( status = <STRING> ) . count ( ) <NEWLINE> LOG . debug ( <STRING> ) <NEWLINE> return ondisk_flowcells < max_flowcells <NEWLINE> <DEDENT>
def manage_addComment ( self , author , body , url = <STRING> , email = <STRING> , date = None , bitakora_cpt = <STRING> , random_cpt = <STRING> , captcha_zz = 0 , REQUEST = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> from utils import checkCaptchaValue , isCommentSpam <NEWLINE> if not captcha_zz : <NEWLINE> <INDENT> if not checkCaptchaValue ( random_cpt , bitakora_cpt ) : <NEWLINE> <INDENT> if REQUEST is not None : <NEWLINE> <INDENT> return REQUEST . RESPONSE . redirect ( self . absolute_url ( ) + <STRING> % ( self . gettext ( <STRING> ) , url_quote ( body . encode ( <STRING> ) ) , url_quote ( author . encode ( <STRING> ) ) , url_quote ( email . encode ( <STRING> ) ) , url_quote ( url . encode ( <STRING> ) ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if event == <STRING> : <NEWLINE> <INDENT> minions += set ( data [ <STRING> ] ) <NEWLINE> elif event == <STRING> : <NEWLINE> minions . discard ( data [ <STRING> ] ) <NEWLINE> <DEDENT>
response_verification = self . _response . verify ( <NEWLINE> <INDENT> response , <NEWLINE> ValueContext ( origin_datetime = response . starts ) , <NEWLINE> ) <NEWLINE> return CaseResult ( <NEWLINE> label = self . _label , <NEWLINE> execution = execution , <NEWLINE> response = response_verification , <NEWLINE> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if interpolate : <NEWLINE> <INDENT> ser = Series ( arr ) . interpolate ( limit = half_window ) <NEWLINE> y = array ( ser ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> y = array ( arr ) <NEWLINE> <DEDENT> <DEDENT>
def _is_wc_root ( root , info ) : <NEWLINE> <INDENT> root = os . path . normpath ( os . path . abspath ( root ) ) <NEWLINE> if os . path . normcase ( root ) == os . path . normcase ( info . get ( <STRING> , <STRING> ) ) : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> elif info : <NEWLINE> <INDENT> p = os . path . dirname ( root ) <NEWLINE> return ( p == root <NEWLINE> <INDENT> or not os . path . isdir ( os . path . join ( p , <STRING> ) ) <NEWLINE> or _info ( p ) . get ( <STRING> ) == info [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>
while <STRING> in string and <STRING> in string : <NEWLINE> <INDENT> tags = string . split ( <STRING> , 1 ) [ 1 ] . split ( <STRING> , 1 ) [ 0 ] . split ( ) <NEWLINE> if tags [ 0 ] [ 0 ] == <STRING> : <NEWLINE> <INDENT> tags [ 0 ] = tags [ 0 ] [ 1 : ] <NEWLINE> if tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> colorStack . pop ( ) <NEWLINE> <DEDENT> elif tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> bgcolorStack . pop ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> formatStack . reverse ( ) <NEWLINE> formatStack . remove ( formatTable [ tags [ 0 ] ] ) <NEWLINE> formatStack . reverse ( ) <NEWLINE> <DEDENT> if len ( colorStack ) is 0 or len ( bgcolorStack ) is 0 or len ( formatStack ) is 0 : <NEWLINE> <INDENT> raise SyntaxError ( <STRING> % tags [ 0 ] ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> if len ( tags ) is 1 : <NEWLINE> <INDENT> tags [ 1 ] = <STRING> <NEWLINE> <DEDENT> colorStack . append ( colorTable [ tags [ 1 ] ] ) <NEWLINE> <DEDENT> elif tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> if len ( tags ) is 1 : <NEWLINE> <INDENT> tags [ 1 ] = <STRING> <NEWLINE> <DEDENT> bgcolorStack . append ( colorTable [ tags [ 1 ] ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> formatStack . append ( formatTable [ tags [ 0 ] ] ) <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> raise SyntaxError ( <STRING> % ( tags [ 0 ] , tags [ 1 ] ) ) <NEWLINE> <DEDENT> <DEDENT> newString = string . split ( <STRING> , 1 ) [ 0 ] <NEWLINE> newString += <STRING> % ( <NEWLINE> <INDENT> formatStack [ - 1 ] , <NEWLINE> colorStack [ - 1 ] + 30 , <NEWLINE> bgcolorStack [ - 1 ] + 40 <NEWLINE> ) <NEWLINE> <DEDENT> newString += string . split ( <STRING> , 1 ) [ 1 ] <NEWLINE> string = newString <NEWLINE> return newString <NEWLINE> <DEDENT>
@ feature ( <STRING> ) <NEWLINE> <INDENT> def feature_command ( tgen ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cmd_dir = tgen . make_node ( tgen . worch . command_dir ) <NEWLINE> cmd_node = cmd_dir . make_node ( tgen . worch . command_cmd ) <NEWLINE> cmd_target = map ( cmd_dir . make_node , tgen . to_list ( tgen . worch . command_target ) ) <NEWLINE> cmd_rule = <STRING> <NEWLINE> tgen . step ( <STRING> , <NEWLINE> <INDENT> rule = tgen . worch . format ( cmd_rule ) , <NEWLINE> source = cmd_node , <NEWLINE> target = cmd_target , <NEWLINE> cwd = cmd_dir . abspath ( ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if ss > 0 and correct : <NEWLINE> <INDENT> correction = correction_function ( max ( ig1 . mut , ig2 . mut ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> ss *= correction <NEWLINE> <COMMENT> <NL> return max ( ss , 0 ) <NEWLINE> <DEDENT>
if ss > 0 and correct : <NEWLINE> <INDENT> correction = correction_function ( max ( ig1 . mut , ig2 . mut ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> ss += ( 1. - correction ) <NEWLINE> <COMMENT> <NL> return max ( ss , 0 ) <NEWLINE> <DEDENT>
if connect_only : <NEWLINE> <INDENT> inspection = reflection . Inspector . from_engine ( self . engine ) <NEWLINE> views = inspection . get_view_names ( schema = schema ) <NEWLINE> tables = inspection . get_table_names ( schema = schema ) <NEWLINE> <DEDENT>
def print_result ( peps , print_peps , threshold ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> scores = list ( peps . values ( ) ) <NEWLINE> scores_in_tens = [ int ( round ( i , - 1 ) ) if i >= threshold else - 1 for i in scores ] <COMMENT> <NEWLINE> freq_scores = { x : scores_in_tens . count ( x ) for x in scores_in_tens } <COMMENT> <NEWLINE> <DEDENT>
def import_patch ( self , patch_name , new_name = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not new_name : <NEWLINE> <INDENT> dir_name = os . path . dirname ( new_name ) <NEWLINE> name = os . path . basename ( new_name ) <NEWLINE> dest_dir = self . quilt_patches + Directory ( dir_name ) <NEWLINE> dest_dir . create ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> name = os . path . basename ( patch_name ) <NEWLINE> dest_dir = self . quilt_patches <NEWLINE> <DEDENT> <DEDENT>
applied = self . db . applied_patches ( ) <NEWLINE> <INDENT> for patch in applied : <NEWLINE> <INDENT> if patch in patches : <NEWLINE> <INDENT> patches . remove ( applied ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def test_a_verifier_retrieves_previous_failing_examples_from_the_database ( ) : <NEWLINE> <INDENT> database = ExampleDatabase ( ) <NEWLINE> verifier = Verifier ( settings = hs . Settings ( database = database ) ) <NEWLINE> verifier . falsify ( lambda x : x != 11 , int ) <NEWLINE> called = [ ] <NEWLINE> <DEDENT>
def incorporate_new_buffer ( self , buffer ) : <NEWLINE> <INDENT> if ( <NEWLINE> <INDENT> self . settings . timeout > 0 and <NEWLINE> time . time ( ) >= self . start_time + self . settings . timeout <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> raise RunIsComplete ( ) <NEWLINE> <DEDENT> self . examples_considered += 1 <NEWLINE> if ( <NEWLINE> <INDENT> buffer [ : self . last_data . index ] == <NEWLINE> self . last_data . buffer [ : self . last_data . index ] <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> data = TestData . for_buffer ( buffer [ : self . last_data . index ] ) <NEWLINE> self . test_function ( data ) <NEWLINE> data . freeze ( ) <NEWLINE> if data . status >= self . last_data . status : <NEWLINE> <INDENT> debug_report ( <STRING> % ( <NEWLINE> <INDENT> data . index , <NEWLINE> list ( data . buffer [ : data . index ] ) , data . status , <NEWLINE> data . output . decode ( <STRING> ) , <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> if data . status >= Status . VALID : <NEWLINE> <INDENT> self . valid_examples += 1 <NEWLINE> <DEDENT> if self . consider_new_test_data ( data ) : <NEWLINE> <INDENT> if self . last_data . status == Status . INTERESTING : <NEWLINE> <INDENT> self . shrinks += 1 <NEWLINE> self . last_data = data <NEWLINE> if self . shrinks >= self . settings . max_shrinks : <NEWLINE> <INDENT> raise RunIsComplete ( ) <NEWLINE> <DEDENT> <DEDENT> self . last_data = data <NEWLINE> self . changed += 1 <NEWLINE> return True <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
def __ifThenElse ( self , element , conditions ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> ifClause = element . arg1 . arg1 <NEWLINE> thenClause = element . arg1 . arg2 <NEWLINE> elseClause = element . arg2 <NEWLINE> <COMMENT> <NL> newConditions = self . __makeConditionRouter ( ifClause ) <NEWLINE> <COMMENT> <NL> currentConditions = self . __appendCondition ( conditions , newConditions ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> self . __generic ( thenClause , deepcopy ( currentConditions ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> self . __invertCondition ( currentConditions ) <NEWLINE> self . __generic ( elseClause , deepcopy ( currentConditions ) ) <NEWLINE> <DEDENT>
def testFail ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> fit = Fit ( ) <NEWLINE> skill1 = Skill ( Type ( 56 ) ) <NEWLINE> fit . items . append ( skill1 ) <NEWLINE> skill2 = Skill ( Type ( 56 ) ) <NEWLINE> fit . items . append ( skill2 ) <NEWLINE> restrictionError1 = fit . getRestrictionError ( skill1 , Restriction . skillUniqueness ) <NEWLINE> self . assertIsNotNone ( restrictionError1 ) <NEWLINE> self . assertEqual ( restrictionError1 . skill , 56 ) <NEWLINE> restrictionError2 = fit . getRestrictionError ( skill1 , Restriction . skillUniqueness ) <NEWLINE> self . assertIsNotNone ( restrictionError2 ) <NEWLINE> self . assertEqual ( restrictionError2 . skill , 56 ) <NEWLINE> fit . items . remove ( skill1 ) <NEWLINE> fit . items . remove ( skill2 ) <NEWLINE> self . assertBuffersEmpty ( fit ) <NEWLINE> <DEDENT>
def registerHolder ( self , holder ) : <NEWLINE> <COMMENT> <NL> <INDENT> slotIndex = holder . item . attributes . get ( self . __slotIndexAttr ) <NEWLINE> if slotIndex is not None : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> self . __slottedHolders . addData ( slotIndex , holder ) <NEWLINE> <DEDENT>
def validate ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> stats = getattr ( self . _fit . stats , self . __statName ) <NEWLINE> totalUse = stats . used <NEWLINE> <COMMENT> <NL> output = stats . output or 0 <NEWLINE> <COMMENT> <NL> if totalUse > output : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> taintedHolders = { } <NEWLINE> for holder in self . __resourceUsers : <NEWLINE> <INDENT> resourceUse = holder . attributes [ self . __usageAttr ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if resourceUse <= 0 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> taintedHolders [ holder ] = ResourceErrorData ( output = output , <NEWLINE> <INDENT> totalUse = totalUse , <NEWLINE> holderUse = resourceUse ) <NEWLINE> <DEDENT> <DEDENT> raise RegisterValidationError ( taintedHolders ) <NEWLINE> <DEDENT>
got_request_exception . connect ( record , api ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> with app . test_request_context ( <STRING> ) : <NEWLINE> <INDENT> api . handle_error ( exception ) <NEWLINE> self . assertEquals ( len ( recorded ) , 1 ) <NEWLINE> self . assertTrue ( exception is recorded [ 0 ] ) <NEWLINE> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> got_request_exception . disconnect ( record , app ) <NEWLINE> <DEDENT> <DEDENT>
class MinifiedManifestStaticFilesStorage ( ManifestStaticFilesStorage ) : <NEWLINE> <INDENT> compressors = { } <NEWLINE> gzip = False <NEWLINE> def __init__ ( self , * args , ** kwargs ) : <NEWLINE> <INDENT> super ( MinifiedManifestStaticFilesStorage , self ) . __init__ ( * args , ** kwargs ) <NEWLINE> try : <NEWLINE> <INDENT> for ext , function in MINIFIED_COMPRESSORS . iteritems ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> regexp = re . compile ( ext ) <NEWLINE> if hasattr ( function , <STRING> ) : <NEWLINE> <INDENT> self . compressors [ regexp ] = function <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . compressors [ regexp ] = import_string ( function ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise MinifiedStorageException ( <STRING> % e ) <NEWLINE> <DEDENT> <DEDENT> def _save ( self , hashed_name , content_file ) : <NEWLINE> <INDENT> content = content_file . read ( ) <NEWLINE> try : <NEWLINE> <INDENT> for regexp , comp_function in self . compressors . iteritems ( ) : <NEWLINE> <INDENT> if regexp . search ( hashed_name ) : <NEWLINE> <INDENT> content = comp_function ( content ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise MinifiedStorageException ( <STRING> % ( hashed_name , e , ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> saved_name = super ( MinifiedManifestStaticFilesStorage , self ) . _save ( hashed_name , ContentFile ( content ) ) <NEWLINE> if MINIFIED_GZIP : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> content = zlib_compress ( content ) <NEWLINE> super ( MinifiedManifestStaticFilesStorage , self ) . _save ( <STRING> % hashed_name , ContentFile ( content ) ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise MinifiedStorageException ( <STRING> % ( hashed_name , e , ) ) <NEWLINE> <DEDENT> <DEDENT> return saved_name <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> good_indices = np . where ( np . logical_and ( data_array [ <STRING> ] [ : - 1 ] == 0 , np . diff ( data_array [ <STRING> ] ) == 0 ) ) [ 0 ] <NEWLINE> best_index = good_indices [ good_indices . shape [ 0 ] / 2 ] <NEWLINE> best_delay_setting = data_array [ <STRING> ] [ best_index ] <NEWLINE> logging . info ( <STRING> , best_delay_setting ) <NEWLINE> <DEDENT>
label = str ( self . height ) <NEWLINE> <INDENT> text_width , text_height = draw . textsize ( label , font = font ) <NEWLINE> x = 0.65 * leng - 0.5 * text_width <NEWLINE> y = 0.55 * leng - 0.5 * text_width <NEWLINE> draw . rectangle ( ( ( x + 0.1 * text_width , y + 0.1 * text_height ) , <NEWLINE> <INDENT> ( x + 0.9 * text_width , y + 0.9 * text_height ) ) , fill = white ) <NEWLINE> <DEDENT> draw . text ( ( x , y ) , label , fill = black , font = font ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def strike_number ( self , target ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> game = self . legion . player . game <NEWLINE> map1 = game . battlemap <NEWLINE> hex1 = map1 . hexes [ self . hexlabel ] <NEWLINE> hex2 = map1 . hexes [ target . hexlabel ] <NEWLINE> skill1 = self . skill <NEWLINE> skill2 = target . skill <NEWLINE> if target in self . engaged_enemies : <NEWLINE> <INDENT> hexside = hex1 . neighbor_to_hexside ( hex2 ) <NEWLINE> border = hex1 . borders [ hexside ] <NEWLINE> border2 = hex1 . opposite_border ( hexside ) <NEWLINE> if hex1 . terrain == <STRING> and not self . is_native ( hex1 . terrain ) : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> elif border == <STRING> : <NEWLINE> <INDENT> skill1 += 1 <NEWLINE> <DEDENT> elif border2 == <STRING> and not self . is_native ( border ) : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> elif border2 == <STRING> : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> if ( not self . magicmissile and map1 . range ( self . hexlabel , <NEWLINE> <INDENT> target . hexlabel ) >= 4 ) : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> <DEDENT> if not self . magicmissile and not self . is_native ( <STRING> ) : <NEWLINE> <INDENT> skill1 -= map1 . count_bramble_hexes ( self . hexlabel , <NEWLINE> <INDENT> target . hexlabel , game ) <NEWLINE> <DEDENT> <DEDENT> if not self . magicmissile : <NEWLINE> <INDENT> skill1 -= map1 . count_walls ( self . hexlabel , target . hexlabel , <NEWLINE> <INDENT> game ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> strike_number = 4 - skill1 + skill2 <NEWLINE> if target in self . engaged_enemies : <NEWLINE> <INDENT> if ( hex2 . terrain == <STRING> and not self . is_native ( hex2 . terrain ) <NEWLINE> <INDENT> and target . is_native ( hex2 . terrain ) ) : <NEWLINE> <INDENT> strike_number += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if ( hex2 . terrain == <STRING> and target . is_native ( hex2 . terrain ) <NEWLINE> <INDENT> and not self . is_native ( hex2 . terrain ) ) : <NEWLINE> <INDENT> strike_number += 1 <NEWLINE> <DEDENT> <DEDENT> elif ( hex2 . terrain == <STRING> and target . is_native ( <NEWLINE> <INDENT> hex2 . terrain ) ) : <NEWLINE> <INDENT> strike_number += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> strike_number = min ( strike_number , 6 ) <NEWLINE> return strike_number <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def number_of_dice ( self , target ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> map1 = self . legion . player . game . battlemap <NEWLINE> hex1 = map1 . hexes [ self . hexlabel ] <NEWLINE> hex2 = map1 . hexes [ target . hexlabel ] <NEWLINE> if target in self . engaged_enemies : <NEWLINE> <INDENT> dice = self . power <NEWLINE> if hex1 . terrain == <STRING> and self . is_native ( hex1 . terrain ) : <NEWLINE> <INDENT> dice += 2 <NEWLINE> <DEDENT> hexside = hex1 . neighbor_to_hexside ( hex2 ) <NEWLINE> border = hex1 . borders [ hexside ] <NEWLINE> if border == <STRING> and self . is_native ( border ) : <NEWLINE> <INDENT> dice += 1 <NEWLINE> <DEDENT> elif border == <STRING> and self . is_native ( border ) : <NEWLINE> <INDENT> dice += 2 <NEWLINE> <DEDENT> border2 = hex1 . opposite_border ( hexside ) <NEWLINE> if border2 == <STRING> and not self . is_native ( border ) : <NEWLINE> <INDENT> dice -= 1 <NEWLINE> <DEDENT> <DEDENT> elif target in self . rangestrike_targets : <NEWLINE> <INDENT> dice = int ( self . power / 2 ) <NEWLINE> if hex1 . terrain == <STRING> and self . is_native ( hex1 . terrain ) : <NEWLINE> <INDENT> dice += 2 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> dice = 0 <NEWLINE> <DEDENT> return dice <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if creature . name != <STRING> or game . battle_turn > 4 : <NEWLINE> <INDENT> score += HIT_BONUS * max_mean_hits <NEWLINE> score += KILL_BONUS * probable_kill <NEWLINE> <DEDENT> score -= DAMAGE_PENALTY * total_mean_damage_taken <NEWLINE> score -= DEATH_PENALTY * probable_death <NEWLINE> <DEDENT>
def init_border_overlays ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> myboxsize = [ int ( round ( 0.97 * mag ) ) for mag in self . bboxsize ] <NEWLINE> self . border_surface_x = int ( round ( self . center [ 0 ] - myboxsize [ 0 ] / 2. ) ) <NEWLINE> self . border_surface_y = int ( round ( self . center [ 1 ] - myboxsize [ 1 ] / 2. ) ) <NEWLINE> for hexside , border in enumerate ( self . battlehex . borders ) : <NEWLINE> <INDENT> border_surface = None <NEWLINE> overlay_filename = <STRING> % border <NEWLINE> image_path = os . path . join ( IMAGE_DIR , overlay_filename ) <NEWLINE> if os . path . exists ( image_path ) : <NEWLINE> <INDENT> hexsides = self . battlehex . hexsides_with_border ( border ) <NEWLINE> hexsides_str = <STRING> . join ( map ( str , sorted ( hexsides ) ) ) <NEWLINE> border_filename = <STRING> % ( border , hexsides_str ) <NEWLINE> border_path = os . path . join ( IMAGE_DIR , border_filename ) <NEWLINE> if not os . path . exists ( border_path ) : <NEWLINE> <INDENT> sliceborder . slice_border_image ( image_path , border_path , <NEWLINE> <INDENT> hexsides ) <NEWLINE> <DEDENT> <DEDENT> input_surface = cairo . ImageSurface . create_from_png ( image_path ) <NEWLINE> input_width = input_surface . get_width ( ) <NEWLINE> input_height = input_surface . get_height ( ) <NEWLINE> output_width = myboxsize [ 0 ] <NEWLINE> output_height = myboxsize [ 1 ] <NEWLINE> border_surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , <NEWLINE> <INDENT> output_width , output_height ) <NEWLINE> <DEDENT> ctx = cairo . Context ( border_surface ) <NEWLINE> ctx . scale ( float ( output_width ) / input_width , <NEWLINE> <INDENT> float ( output_height ) / input_height ) <NEWLINE> <DEDENT> ctx . move_to ( 0 , 0 ) <NEWLINE> ctx . set_source_surface ( input_surface ) <NEWLINE> ctx . paint ( ) <NEWLINE> <DEDENT> self . border_surfaces . append ( border_surface ) <NEWLINE> <DEDENT> <DEDENT>
with open ( conf_path , <STRING> ) as infile : <NEWLINE> <INDENT> dataset_conf = json . load ( infile ) <NEWLINE> func = getattr ( self , dataset_conf [ <STRING> ] ) <NEWLINE> dataset = func ( ) <NEWLINE> dataset . name = name <NEWLINE> dataset . load ( data , dataset_path ) <NEWLINE> <DEDENT>
def interpolate_range ( self , begin , end , interpolation_mode = None ) : <NEWLINE> <INDENT> positions = [ [ i , self . get_position ( i ) ] for i in range ( begin , end + 1 ) if self . get_position ( i ) is not None ] <NEWLINE> if len ( positions ) > 2 : <NEWLINE> <INDENT> positions = interpolate_positions ( positions , begin , end , interpolation_mode ) <NEWLINE> for frame , pos in positions : self . set_position ( frame , pos [ 0 ] , pos [ 1 ] ) <NEWLINE> self . _tmp_points = [ ] <NEWLINE> <DEDENT> <DEDENT>
for obj_dir in objects_dirs : <NEWLINE> <INDENT> name = os . path . basename ( obj_dir ) <NEWLINE> conf_path = os . path . join ( obj_dir , <STRING> ) <NEWLINE> with open ( conf_path , <STRING> ) as infile : <NEWLINE> <INDENT> dataset_conf = json . load ( infile ) <NEWLINE> func = getattr ( self , dataset_conf [ <STRING> ] ) <NEWLINE> dataset = func ( ) <NEWLINE> dataset . load ( data , obj_dir ) <NEWLINE> dataset . name = name <NEWLINE> <DEDENT> <DEDENT>
class Terminal ( blessings . Terminal ) : <NEWLINE> <INDENT> def draw_block ( self , block , x , y ) : <NEWLINE> <INDENT> for y , line in enumerate ( block , start = y ) : <NEWLINE> <INDENT> self . stream . write ( self . move ( x , y ) ) <NEWLINE> self . stream . write ( line ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if isinstance ( object_instance , OrderedDict ) : <NEWLINE> <INDENT> variadic_arg = variadic_arg . items ( ) <NEWLINE> <DEDENT>
if ret_data . status_code != 200 : <NEWLINE> <INDENT> if ret_dic is None and <STRING> in ret_dic and ret_dic [ <STRING> ] == - 4001 : <NEWLINE> <INDENT> logger . error ( <STRING> , self . _url ( path ) , req_data , ret_dic ) <NEWLINE> return None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise APIError ( ret_data . status_code , ret_dic ) <NEWLINE> else : <NEWLINE> <DEDENT> return ret_dic <NEWLINE> <DEDENT>
b = sps . add_parser ( <STRING> ) <NEWLINE> <INDENT> b . add_argument ( <STRING> , <STRING> ) <NEWLINE> a . add_aggregate ( <STRING> , lambda args : args . bbb . upper ( ) ) <NEWLINE> <DEDENT>
n = num <NEWLINE> <INDENT> while 0 < n : <NEWLINE> <INDENT> new_profs = sched . random_deviation_profiles ( <NEWLINE> <INDENT> min ( num , chunk_size ) , mix ) . reshape ( ( - 1 , mix . size ) ) <NEWLINE> <DEDENT> n -= chunk_size <NEWLINE> new_futures = [ asyncio . ensure_future ( sched . sample_payoffs ( prof ) ) <NEWLINE> <INDENT> for prof in new_profs ] <NEWLINE> <DEDENT> await update ( ) <NEWLINE> futures = new_futures <NEWLINE> <DEDENT> await update ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> job_tree ( tier_list , job_file_list ) <NEWLINE> <DEDENT>
if len ( set_list ) == 0 or len ( target_element ) > 1 : <NEWLINE> <COMMENT> <NL> <INDENT> return CandidatePolicy ( target_dict = target_element ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> matches = set . intersection ( * set_list ) <NEWLINE> <DEDENT>
def report_error ( code , message , event = None , oid = None ) : <NEWLINE> <INDENT> payload = { <NEWLINE> <INDENT> <STRING> : { <STRING> : code , <STRING> : message } <NEWLINE> <DEDENT> } <NEWLINE> if event : <NEWLINE> <INDENT> payload [ <STRING> ] = event <NEWLINE> <DEDENT> if event : <NEWLINE> <INDENT> payload [ <STRING> ] = oid <NEWLINE> <DEDENT> write ( payload ) <NEWLINE> <DEDENT>
return cls . from_file ( filepath = os . path . join ( settingsfile , dir_ ) ) <NEWLINE>
<COMMENT> <NL> <INDENT> fragSize = int ( self . fld ( size = 1 ) ) <NEWLINE> if fragSize < readTempSize : <NEWLINE> <INDENT> tryCnt += 1 <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> self . _widget_path = <STRING> % ( <NEWLINE> <INDENT> parentpath , safe_class_name , next ( counts [ widgetname ] ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if predictor != 1 : <NEWLINE> <INDENT> columns = decodeParms [ <STRING> ] <NEWLINE> <COMMENT> <NL> if predictor >= 10 and predictor <= 15 : <NEWLINE> <INDENT> output = StringIO ( ) <NEWLINE> <COMMENT> <NL> rowlength = columns + 1 <NEWLINE> assert len ( data ) % rowlength == 0 <NEWLINE> prev_rowdata = ( 0 , ) * rowlength <NEWLINE> for row in range ( len ( data ) / rowlength ) : <NEWLINE> <INDENT> rowdata = [ ord ( x ) for x in data [ ( row * rowlength ) : ( ( row + 1 ) * rowlength ) ] ] <NEWLINE> filterByte = rowdata [ 0 ] <NEWLINE> if filterByte == 0 : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> elif filterByte == 1 : <NEWLINE> <INDENT> for i in range ( 2 , rowlength ) : <NEWLINE> <INDENT> rowdata [ i ] = ( rowdata [ i ] + rowdata [ i - 1 ] ) % 256 <NEWLINE> <DEDENT> <DEDENT> elif filterByte == 2 : <NEWLINE> <INDENT> for i in range ( 1 , rowlength ) : <NEWLINE> <INDENT> rowdata [ i ] = ( rowdata [ i ] + prev_rowdata [ i ] ) % 256 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> raise PdfReadError ( <STRING> % filterByte ) <NEWLINE> <DEDENT> prev_rowdata = rowdata <NEWLINE> output . write ( <STRING> . join ( [ chr ( x ) for x in rowdata [ 1 : ] ] ) ) <NEWLINE> <DEDENT> data = output . getvalue ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> raise PdfReadError ( <STRING> % predictor ) <NEWLINE> <DEDENT> <DEDENT> return data <NEWLINE> decode = staticmethod ( decode ) <NEWLINE> <DEDENT>
def _build_batch_arrays ( self , batch_size ) : <NEWLINE> <INDENT> if batch_size > self . num_of_states : <NEWLINE> <INDENT> batch_size = self . num_of_states <NEWLINE> <DEDENT> if self . num_of_states % batch_size != 0 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> self . batch_size = batch_size <NEWLINE> self . num_of_batch_until_full_cycle = self . num_of_states / self . batch_size <NEWLINE> self . batch_complex_local_energies = np . zeros ( ( self . batch_size , ) , dtype = np . complex128 ) <NEWLINE> self . batch_naive_complex_local_energies = np . zeros ( ( self . batch_size , ) , dtype = np . complex128 ) <NEWLINE> <DEDENT>
if name in craftables : <NEWLINE> <INDENT> items , item = craftables [ item ] <NEWLINE> else : <NEWLINE> for k in craftables : <NEWLINE> <INDENT> if k . startswith ( name ) or k . endswith ( name ) or ( k in name ) : <NEWLINE> <INDENT> items , item = craftables [ k ] <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _find_item ( item , items ) : <NEWLINE> <INDENT> for i in items : <NEWLINE> <INDENT> if ( i . name == item . name ) and isinstance ( i , item . __class__ ) : <NEWLINE> <INDENT> return item <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
iterator = gpsdio . filter ( src , filter_expr ) if filter_expr else src <NEWLINE> <INDENT> for msg in gpsdio . sort ( iterator , sort_field ) if sort_field else iterator : <NEWLINE> <INDENT> dst . write ( msg ) <NEWLINE> <DEDENT> <DEDENT>
if iteration == total : <NEWLINE> <INDENT> sys . stdout . write ( <STRING> ) <NEWLINE> sys . stdout . flush ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>
logger . debug ( <STRING> % values ) <NEWLINE> <INDENT> code , data = await self . _sql . update ( info , values ) <NEWLINE> if code == RETCODE . SUCCESS : <NEWLINE> <INDENT> await async_call ( self . after_update , data ) <NEWLINE> <DEDENT> self . finish ( code , data ) <NEWLINE> <DEDENT>
def request_role ( role = None ) : <NEWLINE> <INDENT> def _ ( func ) : <NEWLINE> <INDENT> async def __ ( view : AbstractSQLView , * args , ** kwargs ) : <NEWLINE> <INDENT> if role == view . current_request_role : <NEWLINE> <INDENT> return view . finish ( RETCODE . INVALID_ROLE ) <NEWLINE> <DEDENT> return await func ( view , * args , ** kwargs ) <NEWLINE> <DEDENT> return __ <NEWLINE> <DEDENT> return _ <NEWLINE> <DEDENT>
def _inside_skip ( soup_elem ) : <NEWLINE> <INDENT> parent = soup_elem . parent <NEWLINE> while parent is not None : <NEWLINE> <INDENT> if parent . name not in skip_elements : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> parent = parent . parent <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if compare_models : <NEWLINE> <INDENT> choose_box_and_violin_plots ( names , <NEWLINE> <INDENT> scoring , <NEWLINE> compare_models , <NEWLINE> results , <NEWLINE> is_continuous ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if ROC : <NEWLINE> <INDENT> if not is_continuous : <NEWLINE> <INDENT> timeit ( plot_rocs , models , df_X , y ) <NEWLINE> plt . show ( ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> return names , results , models , pipeline , df_X <NEWLINE> <DEDENT>
def postprocess ( self , content ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> node = content . node <NEWLINE> if len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> return node <NEWLINE> <DEDENT> attributes = AttrList ( node . attributes ) <NEWLINE> if attributes . rlen ( ) and not len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> p = Factory . property ( node . name , node . getText ( ) ) <NEWLINE> return merge ( content . data , p ) <NEWLINE> <DEDENT> if len ( content . data ) : <NEWLINE> <INDENT> return content . data <NEWLINE> <DEDENT> lang = attributes . lang ( ) <NEWLINE> if not len ( node . children ) and content . text is None : <NEWLINE> <INDENT> if self . nillable ( content . data ) and content . node . isnil ( ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return xlstr . string ( <STRING> , lang ) <NEWLINE> <DEDENT> <DEDENT> if isinstance ( content . text , basestring ) : <NEWLINE> <INDENT> return xlstr . string ( content . text , lang ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return content . text <NEWLINE> <DEDENT> <DEDENT>
Definition ( <STRING> , [ ] , ( list , tuple ) ) , <NEWLINE>
ctx : BringDynamicContextTing = self . _tingistry_obj . create_ting ( <COMMENT> <NEWLINE> <INDENT> <STRING> , <STRING> <NEWLINE> ) <NEWLINE> indexes = [ folder ] <NEWLINE> ctx . input . set_values ( <COMMENT> <NEWLINE> ting_dict = { <STRING> : indexes } <NEWLINE> ) <NEWLINE> <DEDENT>
pyinstaller = { <STRING> : [ x . __name__ for x in _hi ] } <NEWLINE> <INDENT> if os . name != <STRING> : <NEWLINE> <INDENT> import pkgutil <NEWLINE> import jinxed . terminfo <NEWLINE> <DEDENT> <DEDENT>
return all_values <NEWLINE>
_allowed_strings = [ ] <NEWLINE> <INDENT> for _arg , _aliases in _allowed . items ( ) : <NEWLINE> <INDENT> if not aliases : <NEWLINE> <INDENT> a = _arg <NEWLINE> <DEDENT> elif len ( _aliases ) == 1 : <NEWLINE> <INDENT> a = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> a = <STRING> <NEWLINE> <DEDENT> _allowed_strings . append ( a ) <NEWLINE> <DEDENT> arg_table . add_row ( <STRING> , _allowed_strings [ 0 ] ) <NEWLINE> if limit_allowed and len ( _allowed_strings ) > limit_allowed : <NEWLINE> <INDENT> _allowed_strings = _allowed_strings [ 0 : limit_allowed ] + [ <STRING> , <STRING> ] <NEWLINE> <DEDENT> for a in _allowed_strings [ 1 : ] : <NEWLINE> <INDENT> arg_table . add_row ( <STRING> , a ) <NEWLINE> <DEDENT> <DEDENT>
install_args = { } <NEWLINE> <INDENT> if target : <NEWLINE> <INDENT> install_args [ <STRING> ] = target <NEWLINE> <DEDENT> if install_args : <NEWLINE> <INDENT> install_args [ <STRING> ] = target_config <NEWLINE> <DEDENT> <DEDENT>
text = self . text <NEWLINE> <INDENT> chunks = re . split ( <STRING> , text ) <NEWLINE> text_chunks = [ ] <NEWLINE> for index , chunk in enumerate ( chunks ) : <NEWLINE> <INDENT> if not index % 2 : <NEWLINE> <INDENT> text_chunks . append ( chunk ) <NEWLINE> <DEDENT> <DEDENT> for hit in sorted ( hits , key = lambda chunk : chunk [ 1 ] , reverse = True ) : <NEWLINE> <INDENT> hit_start , hit_end = hit <NEWLINE> placed = 0 <NEWLINE> for index , chunk in enumerate ( chunks ) : <NEWLINE> <INDENT> if placed == 2 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if index % 2 : <NEWLINE> <COMMENT> <NL> <INDENT> continue <NEWLINE> <DEDENT> chunk_start = len ( <STRING> . join ( text_chunks [ 0 : index / 2 ] ) ) <NEWLINE> chunk_end = chunk_start + len ( chunk ) <NEWLINE> if hit_start >= chunk_start and hit_start < chunk_end : <NEWLINE> <INDENT> chunk = chunk [ : hit_start - chunk_start ] + tags [ 0 ] + chunk [ hit_start - chunk_start : ] <NEWLINE> if hit_end <= chunk_end : <NEWLINE> <INDENT> hit_end += len ( tags [ 0 ] ) <NEWLINE> chunk_end += len ( tags [ 0 ] ) <NEWLINE> <DEDENT> placed = 1 <NEWLINE> <DEDENT> if hit_end > chunk_start and hit_end <= chunk_end : <NEWLINE> <INDENT> chunk = chunk [ : hit_end - chunk_start ] + tags [ 1 ] + chunk [ hit_end - chunk_start : ] <NEWLINE> placed = 2 <NEWLINE> <DEDENT> chunks [ index ] = chunk <NEWLINE> <DEDENT> if placed == 1 : <NEWLINE> <INDENT> chunks [ - 1 ] = chunks [ - 1 ] + tags [ 1 ] <NEWLINE> <DEDENT> <DEDENT> result = [ ] <NEWLINE> for index , chunk in enumerate ( chunks ) : <NEWLINE> <INDENT> if index % 2 : <NEWLINE> <COMMENT> <NL> <INDENT> result . append ( <STRING> % chunk ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> result . append ( chunk ) <NEWLINE> <DEDENT> <DEDENT> self . text = <STRING> . join ( result ) <NEWLINE> return self . text <NEWLINE> <DEDENT>
with open ( filepath ) as file : <NEWLINE> <INDENT> if hasattr ( root , <STRING> ) : <NEWLINE> <INDENT> errors = root . doc . errors <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> errors = None <NEWLINE> <DEDENT> result = build ( file , filename , errors ) <NEWLINE> return Holder ( result ) <NEWLINE> <DEDENT>
try : <NEWLINE> <COMMENT> <NL> <INDENT> latest = user . password_history . latest ( <STRING> ) <NEWLINE> except PasswordHistory . DoesNotExist : <NEWLINE> return False <NEWLINE> <DEDENT>
for i , subjects in enumerate ( data_splits ) : <NEWLINE> <INDENT> for subject in tqdm ( subjects ) : <NEWLINE> <INDENT> print ( subject ) <NEWLINE> spath = { } <NEWLINE> subject_path = os . path . join ( dataset_path , subject ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( dataset_path , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if r . status_code != 204 : <NEWLINE> <INDENT> comments_to_delete += r . json ( ) . get ( <STRING> , list ( ) ) <NEWLINE> deleted_comments += 1 <NEWLINE> except : <NEWLINE> continue <NEWLINE> <DEDENT>
sys . path . insert ( <STRING> , 0 ) <NEWLINE>
elif isinstance ( input_ , string_types ) : <NEWLINE> <INDENT> for sep in [ <STRING> , <STRING> , <STRING> ] : <NEWLINE> <INDENT> if len ( input_ . split ( sep ) ) in range ( 2 , 3 ) : <NEWLINE> <INDENT> t = input_ . split ( sep ) <NEWLINE> <DEDENT> <DEDENT> if isinstance ( input_ , string_types ) : <NEWLINE> <INDENT> raise Exception ( <STRING> % input_ ) <NEWLINE> <DEDENT> <DEDENT>
def is_relative ( self , url ) : <NEWLINE> <INDENT> domain = self . url_splitter . get_domain ( url ) <NEWLINE> return len ( domain ) > 0 <NEWLINE> <DEDENT>
elif len ( gname ) > 2 : <NEWLINE> <INDENT> p0_name += <STRING> % gname [ 1 ] <NEWLINE> <DEDENT>
def read_bam_file ( bamfile , chrnames_bam , max_NM = 0 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> read_objects = { } <NEWLINE> for read in bamfile . fetch ( ) : <NEWLINE> <INDENT> if not read . is_unmapped : <NEWLINE> <INDENT> nm_num = get_NM_number ( read . tags ) <NEWLINE> if nm_num > max_NM : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> alt_list = get_XA_mapping ( read . tags , max_NM ) <NEWLINE> min_pos = read . pos <NEWLINE> min_is_rev = read . is_reverse <NEWLINE> for al in alt_list : <NEWLINE> <INDENT> apos = int ( al [ 1 ] [ 1 : ] ) <NEWLINE> <COMMENT> <NL> if int ( apos [ 3 ] ) > nm_num : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if apos < min_pos : <NEWLINE> <INDENT> min_pos = apos <NEWLINE> min_is_rev = al [ 1 ] [ 0 ] == <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> tags = read . tags <NEWLINE> if read . pos != min_pos : <NEWLINE> <INDENT> for xt in tags : <NEWLINE> <INDENT> if xt [ 0 ] == <STRING> : <NEWLINE> <INDENT> xaval = xt [ 1 ] <NEWLINE> tags . remove ( xt ) <NEWLINE> strs = <STRING> <NEWLINE> if read . is_reverse : <NEWLINE> <INDENT> strs = <STRING> <NEWLINE> <DEDENT> tags . append ( ( <STRING> , <STRING> % ( <NEWLINE> <INDENT> chrnames_bam [ read . tid ] , <NEWLINE> strs , read . pos , <NEWLINE> read . cigarstring , nm_num ) + xaval ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> read . tags = tags <NEWLINE> <DEDENT> read . pos = min_pos <NEWLINE> read . is_reverse = min_is_rev <NEWLINE> read_objects [ read . qname ] = read <NEWLINE> <DEDENT> <DEDENT> return read_objects <NEWLINE> <DEDENT>
updater = Updater ( token = sys . argv [ 0 ] ) <NEWLINE>
if header_after_slash is None : <NEWLINE> <INDENT> final_header = header_code <NEWLINE> else : <NEWLINE> if len ( header_before_slash ) != 0 : <NEWLINE> <INDENT> final_header = str ( header_code ) + <STRING> + str ( header_after_slash ) <NEWLINE> <DEDENT> elif len ( header_after_slash ) == 0 : <NEWLINE> <INDENT> final_header = <STRING> + str ( decsCodes_list_dict ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( header , <STRING> , header_before_slash , <STRING> , header_after_slash ) <NEWLINE> <DEDENT> <DEDENT>
def wrapper ( cls , request , * args , ** kwargs ) : <NEWLINE> <INDENT> paramap = dict ( kwargs ) <NEWLINE> paramap . setdefault ( <NEWLINE> <INDENT> <STRING> , kwargs . get ( <STRING> , None ) <NEWLINE> <DEDENT> ) <COMMENT> <NEWLINE> data = data_method ( request ) <NEWLINE> paramap . update ( { x : y for x , y in data . items ( ) } ) <NEWLINE> result = cls . result_class ( ) <COMMENT> <NEWLINE> for item in itemset : <NEWLINE> <INDENT> name , v , required , msg , replace = [ <NEWLINE> <INDENT> item [ x ] <NEWLINE> for x in [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> value = None <COMMENT> <NEWLINE> para = paramap . get ( name ) <NEWLINE> if required and para not in ( None , <STRING> ) : <COMMENT> <NEWLINE> <INDENT> result . error ( name , <STRING> ) <NEWLINE> <DEDENT> if para is not None : <NEWLINE> <INDENT> if para : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> value = v ( para ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> if settings . DEBUG : <NEWLINE> <INDENT> from traceback import print_exc <NEWLINE> print_exc ( ) <NEWLINE> <DEDENT> <DEDENT> msg = v . msg or msg <NEWLINE> if v . status == 403 : <COMMENT> <NEWLINE> <INDENT> return result . perm ( name , msg ) ( status = v . status ) <NEWLINE> <DEDENT> if value in ( None , False ) : <NEWLINE> <INDENT> result . error ( name , msg ) <NEWLINE> <DEDENT> <DEDENT> kwargs . update ( { <NEWLINE> <INDENT> replace or name : value or para <NEWLINE> <DEDENT> } ) <COMMENT> <NEWLINE> <DEDENT> <DEDENT> if not result : <NEWLINE> <INDENT> return result ( status = 400 ) <NEWLINE> <DEDENT> return func ( cls , request , * args , ** kwargs ) <NEWLINE> <DEDENT>
noise = salt_pepper_noise ( batch_size , noise_shape [ 1 ] , density , salt_value , pepper_value , seed ) <NEWLINE>
if scale_first : <NEWLINE> <INDENT> path_rev = _scaled_path ( path , scaling_path , flip_paths ) <NEWLINE> <DEDENT>
self . easy . product_cach [ p . id ] = product <NEWLINE> <INDENT> self . products . buffer [ i + offset ] = product <NEWLINE> <DEDENT>
n = abs ( n ) <NEWLINE> <INDENT> if n == 1 : <NEWLINE> <INDENT> return [ 1 ] <NEWLINE> <DEDENT> factor = [ ] <NEWLINE> for i in prime : <NEWLINE> <INDENT> if i ** 2 > n : <NEWLINE> <INDENT> factor . append ( n ) <NEWLINE> break <NEWLINE> <DEDENT> while not n % i : <NEWLINE> <INDENT> factor . append ( i ) <NEWLINE> n /= i <NEWLINE> <DEDENT> if n == 1 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> return factor <NEWLINE> <DEDENT>
async def handle_user_message ( app , ws , msg ) : <NEWLINE> <INDENT> if isinstance ( msg , api . SetDocumentMessage ) : <NEWLINE> <INDENT> assert app [ <STRING> ] == State . idle <NEWLINE> try : <NEWLINE> <INDENT> app [ <STRING> ] = State . processing <NEWLINE> notify_state ( app ) <NEWLINE> job = await plotting . process_upload_background ( app , msg . document ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> notify_error ( app , ws , str ( e ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> app [ <STRING> ] = msg . document <NEWLINE> app [ <STRING> ] = job <NEWLINE> app [ <STRING> ] = job . duration ( ) . total_seconds ( ) <NEWLINE> notify_new_document ( app , exclude_client = ws ) <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> app [ <STRING> ] = State . idle <NEWLINE> notify_state ( app ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> async for raw_msg in ws : <NEWLINE> <INDENT> if raw_msg . tp == aiohttp . MsgType . text : <NEWLINE> <INDENT> msg = api . Message . deserialize ( raw_msg . data ) <NEWLINE> log . info ( <STRING> , msg ) <NEWLINE> await handle_user_message ( app , ws , msg ) <NEWLINE> <DEDENT> elif raw_msg . tp == aiohttp . MsgType . closed : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> elif raw_msg . tp == aiohttp . MsgType . error : <NEWLINE> <INDENT> log . info ( <STRING> , msg ) <NEWLINE> break <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . error ( <STRING> , <NEWLINE> <INDENT> raw_msg . tp ) <NEWLINE> finally : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> log . info ( <STRING> ) <NEWLINE> clients . remove ( ws ) <NEWLINE> <DEDENT>
def process_image ( elm , registry , container ) : <NEWLINE> <INDENT> img = Image ( ) <NEWLINE> container . add_content ( img ) <NEWLINE> for subitem in elm . items ( ) : <NEWLINE> <INDENT> if subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . alt = subitem [ 1 ] <NEWLINE> <DEDENT> elif subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . witdh = subitem [ 1 ] <NEWLINE> <DEDENT> elif subitem [ 1 ] == <STRING> : <NEWLINE> <INDENT> img . height = subitem [ 1 ] <NEWLINE> <DEDENT> elif subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . uri = subitem [ 1 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def test_gsheets_add_rows_to_active_sheet_sets_add_rows_time ( gsheets_handler_no_thread ) : <NEWLINE> <INDENT> gsheets_handler_no_thread . _add_rows_time = 99 <NEWLINE> assert isinstance ( gsheets_handler_no_thread . _add_rows_to_active_sheet ( [ ] ) , unittest . mock . Mock ) <NEWLINE> assert gsheets_handler_no_thread . _add_rows_time < 99 and gsheets_handler_no_thread . _add_rows_time > 0 <NEWLINE> <DEDENT>
def add ( self , fee , result ) : <NEWLINE> <INDENT> if fee . id not in self . applications : <NEWLINE> <INDENT> self . applications [ fee . id ] = { <NEWLINE> <INDENT> <STRING> : fee , <NEWLINE> <STRING> : result , <NEWLINE> <STRING> : fee . name , <NEWLINE> <STRING> : result . description , <NEWLINE> <STRING> : 0 , <NEWLINE> <STRING> : D ( <STRING> ) } <NEWLINE> <DEDENT> <DEDENT> self . applications [ fee . id ] [ <STRING> ] += result . fee <NEWLINE> self . applications [ fee . id ] [ <STRING> ] += 1 <NEWLINE> <DEDENT>
<STRING> <NEWLINE> <INDENT> assert ( len ( edges . columns ) >= 1 ) <NEWLINE> if len ( nodes . columns ) == 1 : <NEWLINE> <INDENT> nodes [ <STRING> ] = np . zeros ( len ( nodes ) , np . bool_ ) <NEWLINE> <DEDENT> <DEDENT>
<STRING> <NEWLINE> <INDENT> timer . tic ( <STRING> ) <NEWLINE> starts = backend . zeros ( len ( X ) + 1 , dtype = np . uint32 ) <NEWLINE> if nodal is True : <NEWLINE> <INDENT> sizes = np . array ( [ len ( g . nodes ) for g in X ] , dtype = np . uint32 ) <NEWLINE> np . cumsum ( sizes , out = starts [ 1 : ] ) <NEWLINE> output_length = int ( starts [ - 1 ] ) <NEWLINE> <DEDENT> elif nodal is False : <NEWLINE> <INDENT> starts [ : ] = np . arange ( len ( X ) + 1 ) <NEWLINE> output_length = len ( X ) <NEWLINE> <DEDENT> elif nodal == <STRING> : <NEWLINE> <INDENT> sizes = np . array ( [ len ( g . nodes ) for g in X ] , dtype = np . uint32 ) <NEWLINE> np . cumsum ( sizes ** 2 , out = starts [ 1 : ] ) <NEWLINE> output_length = int ( starts [ - 1 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ( ValueError ( <STRING> % nodal ) ) <NEWLINE> <DEDENT> if traits . eval_gradient is True : <NEWLINE> <INDENT> output_shape = ( output_length , 1 + self . n_dims ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output_shape = ( output_shape , 1 ) <NEWLINE> <DEDENT> output = backend . empty ( int ( np . prod ( output_shape ) ) , np . float32 ) <NEWLINE> timer . toc ( <STRING> ) <NEWLINE> <DEDENT>
class SpectralApprox ( FactorApprox ) : <NEWLINE> <INDENT> def __init__ ( self , X , rcut = 0 , acut = 0 ) : <NEWLINE> <INDENT> if isinstance ( X , np . ndarray ) : <NEWLINE> <INDENT> U , S , _ = np . linalg . svd ( X , full_matrices = False ) <NEWLINE> mask = ( S >= S . max ( ) * rcut ) | ( S >= acut ) <NEWLINE> self . U = U [ : , mask ] <NEWLINE> self . S = S [ mask ] <NEWLINE> <DEDENT> elif isinstance ( X , tuple ) and len ( X ) == 2 : <NEWLINE> <INDENT> self . U , self . S = X <NEWLINE> <DEDENT> self . _lhs = self . U * self . S <NEWLINE> <DEDENT> <DEDENT>
S = uctypes . struct ( desc , uctypes . addressof ( data ) , uctypes . LITTLE_ENDIAN ) <NEWLINE>
S = uctypes . struct ( desc , uctypes . addressof ( data ) , uctypes . LITTLE_ENDIAN ) <NEWLINE>
S = uctypes . struct ( desc , uctypes . addressof ( data ) , uctypes . NATIVE ) <NEWLINE>
S = uctypes . struct ( desc , uctypes . addressof ( buf ) , uctypes . LITTLE_ENDIAN ) <NEWLINE>
def stop ( self ) -> None : <NEWLINE> <INDENT> with self . _stop_lock : <NEWLINE> <INDENT> self . _do_stop = False <NEWLINE> <DEDENT> <DEDENT>
self . __pt1000_adc = pt1000_conf . adc ( MCP342X . GAIN_4 , MCP342X . RATE_15 ) if pt1000_conf else None <NEWLINE>
hasNoMissingJobs = reissueMissingJobs ( updatedJobFiles , jobBatcher , batchSystem , childJobFileToParentJob , config , childCounts ) <NEWLINE> <INDENT> if hasNoMissingJobs : <NEWLINE> <INDENT> timeSinceJobsLastRescued = time . time ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> timeSinceJobsLastRescued += 60 <COMMENT> <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> <DEDENT>
def writeGlobalFile ( self , localFileName ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return self . jobStore . writeFile ( localFileName , self . job . jobStoreID ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> self . predecessorNumber = predecessorNumber <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> self . predecessorsFinished = predecessorNumber or set ( ) <NEWLINE> <DEDENT>
def setLogFile ( self , logFile , jobStore ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . logJobStoreFileID is not None : <NEWLINE> <INDENT> self . clearLogFile ( jobStore ) <NEWLINE> <DEDENT> self . logJobStoreFileID = jobStore . writeFile ( self . jobStoreID , logFile ) <NEWLINE> assert self . logJobStoreFileID is not None <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if jobWrapper . remainingRetryCount < self . _defaultTryCount ( ) : <NEWLINE> <INDENT> jobWrapper . remainingRetryCount = self . _defaultTryCount ( ) <NEWLINE> changed = True <NEWLINE> <DEDENT> <DEDENT>
DATA_TEST_CDEC_PARSE = [ <NEWLINE> <COMMENT> <NL> <INDENT> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 1 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 2 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , default = 1 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 1 , 2 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 2 , 3 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , default = 10 ) ) , <NEWLINE> ( cmem ( <STRING> , DmyCDT ) , dict_cdec_parse ( DmyCDT , <STRING> , valtype = <STRING> ) ) , <NEWLINE> ] <NEWLINE> <DEDENT>
if os . path . isdir ( runName ) : <NEWLINE> <COMMENT> <NL> <INDENT> if errnum > 0 : <NEWLINE> <INDENT> shutil . rmtree ( rdir ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> os . system ( <STRING> % ( runName , rdir ) ) <NEWLINE> shutil . rmtree ( runName ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> if errnum > 0 : <NEWLINE> <INDENT> files = os . listdir ( <STRING> ) <NEWLINE> for f in files : <NEWLINE> <INDENT> if os . path . getmtime ( f ) > self . start_time : <NEWLINE> <INDENT> shutil . copy2 ( f , rdir ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _get_next_task ( self , cores ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if cores > self . sweep_cores and self . sweeps : <NEWLINE> <INDENT> swp : ParamsTask = self . sweeps . pop ( ) <NEWLINE> del self . params_to_sweeps_ind [ swp . params ] <NEWLINE> self . _len -= 1 <NEWLINE> self . in_progress . add ( swp . params ) <NEWLINE> return swp . as_task ( <NEWLINE> <INDENT> self . module , self . hparams , self . folder , <NEWLINE> self . params_to_id [ swp . params ] , <NEWLINE> 0 , self . sweep_cores , <NEWLINE> self . listeners , <NEWLINE> self . params_to_ntrials [ swp . params ] <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
if self . _orientation == gtk . ORIENTATION_HORIZONTAL : <NEWLINE> <INDENT> width += w <NEWLINE> height = max ( height , h ) <NEWLINE> <COMMENT> <NL> item . min_size = w <NEWLINE> else : <NEWLINE> width = max ( width , w ) <NEWLINE> height += h <NEWLINE> <COMMENT> <NL> item . min_size = w <NEWLINE> <DEDENT>
def clean_phrases ( phrases ) : <NEWLINE> <INDENT> for phrase in phrases . values ( ) : <NEWLINE> <INDENT> phrase [ <STRING> ] = <STRING> . join ( phrase [ <STRING> ] ) . replace ( <STRING> , <STRING> ) <NEWLINE> phrase [ <STRING> ] = ( len ( phrase [ <STRING> ] ) // phrase [ <STRING> ] ) + bool ( len ( phrase [ <STRING> ] ) % phrase [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT>
self . dictionary = gensim . corpora . Dictionary ( data_words ) <NEWLINE> <INDENT> corpus = self . preprocess_texts ( docs ) <NEWLINE> saved_model = self . main_data_path / <STRING> / str ( <STRING> + str ( str ( self . _training_path ) ) + str ( self . numb_topics ) ) <NEWLINE> saved_model_fname = str ( hash ( saved_model ) ) + <STRING> <NEWLINE> if os . path . exists ( saved_model_fname ) : <NEWLINE> <COMMENT> <NL> <INDENT> lda_model = gensim . models . ldamodel . LdaModel . load ( os . path . abspath ( saved_model ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> lda_model = gensim . models . ldamodel . LdaModel ( corpus = corpus , <NEWLINE> <INDENT> id2word = self . dictionary , <NEWLINE> num_topics = self . numb_topics , <NEWLINE> random_state = 100 , <NEWLINE> update_every = 1 , <NEWLINE> chunksize = 100 , <NEWLINE> passes = 10 , <NEWLINE> alpha = <STRING> , <NEWLINE> per_word_topics = True ) <NEWLINE> <COMMENT> <NL> <DEDENT> lda_model . save ( os . path . abspath ( saved_model_fname ) ) <NEWLINE> <DEDENT> <DEDENT>
return tuple ( format_sharded_filename ( base_name , shards , i ) for i in range ( index ) ) <NEWLINE>
shuffle_stats_report = self . _sensitivity_stats ( self . shuffled_sensitivity ) <NEWLINE> <INDENT> missing_stats_report = self . _sensitivity_stats ( self . missing_sensitivity ) <NEWLINE> vulnerability_report = self . _vulnerability_report ( <NEWLINE> <INDENT> shuffled_sensitivity = self . shuffled_sensitivity , <NEWLINE> missing_sensitivity = self . missing_sensitivity , <NEWLINE> shuffled_sensitivity_stats = missing_stats_report ) <NEWLINE> <DEDENT> <DEDENT>
def start_and_commit ( self , container , cmd ) : <NEWLINE> <INDENT> client = self . project . client <NEWLINE> logger . info ( <STRING> ) <NEWLINE> client . put_archive ( container , <STRING> , self . archive . getfile ( ) ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> client . start ( container ) <NEWLINE> for line in client . logs ( container , stream = True ) : <NEWLINE> <INDENT> logger . info ( line . decode ( ) . rstrip ( ) ) <NEWLINE> <DEDENT> result = client . wait ( container ) <NEWLINE> if result == 0 : <NEWLINE> <COMMENT> <NL> <INDENT> repository , tag = self . image_name , None <NEWLINE> if <STRING> in repository : <NEWLINE> <INDENT> repository , tag = tag . split ( <STRING> ) <NEWLINE> <DEDENT> conf = client . create_container_config ( self . image_name , cmd , working_dir = APP_PATH ) <NEWLINE> client . commit ( container , repository = repository , tag = tag , conf = conf ) <NEWLINE> <DEDENT> client . remove_container ( container ) <NEWLINE> return result <NEWLINE> <DEDENT>
def remove_bidi_marks ( string , positions_to_this = None , position_from_this_list = None , embedding_levels = None ) : <NEWLINE> <INDENT> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> c_string = str_to_chars ( string ) <NEWLINE> if positions_to_this != None : <NEWLINE> <INDENT> c_positions_to_this = seq_to_ct ( positions_to_this , FRIBIDI . StrIndex ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> c_positions_to_this = None <NEWLINE> <COMMENT> <NL> <DEDENT> if position_from_this_list != None : <NEWLINE> <INDENT> c_position_from_this_list = seq_to_ct ( position_from_this_list , FRIBIDI . StrIndex ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> position_from_this_list = None <NEWLINE> <COMMENT> <NL> <DEDENT> if embedding_levels != None : <NEWLINE> <INDENT> c_embedding_levels = seq_to_ct ( embedding_levels , FRIBIDI . StrIndex ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> c_embedding_levels = None <NEWLINE> <COMMENT> <NL> <DEDENT> new_str_len = fribidi . fribidi_remove_bidi_marks ( c_string , len ( string ) , c_positions_to_this , c_position_from_this_list , c_embedding_levels ) <NEWLINE> if new_str_len < 0 : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> result = [ chars_to_str ( c_string [ : new_str_len ] ) , None , None , None ] <NEWLINE> if positions_to_this != None : <NEWLINE> <INDENT> result [ 1 ] = tuple ( c_positions_to_this ) <NEWLINE> <COMMENT> <NL> <DEDENT> if position_from_this_list != None : <NEWLINE> <INDENT> result [ 2 ] = tuple ( c_position_from_this_list ) <NEWLINE> <COMMENT> <NL> <DEDENT> if embedding_levels != None : <NEWLINE> <INDENT> result [ 3 ] = tuple ( c_embedding_levels ) <NEWLINE> <COMMENT> <NL> <DEDENT> return tuple ( result ) <NEWLINE> <COMMENT> <NL> <DEDENT>
def between ( min , max , iterable ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if min < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if min < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if min > max : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def __get_safe_conn ( self , retry_count ) : <NEWLINE> <INDENT> self . current_size += 1 <NEWLINE> c = self . unuse_list . pop ( ) <NEWLINE> if self . ping_check : <NEWLINE> <INDENT> now = int ( time ( ) ) <NEWLINE> timeout = now <NEWLINE> if isinstance ( int , self . ping_check ) : <NEWLINE> <INDENT> timeout = timeout - self . ping_check <NEWLINE> <DEDENT> if not hasattr ( c , <STRING> ) : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> if c . __ping_check_timestamp < timeout : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> c . ping ( ) <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> self . current_size -= 1 <NEWLINE> if retry_count < 10 : c = self . __get_conn ( retry_count + 1 ) <NEWLINE> <DEDENT> <DEDENT> if c : self . inuse_list . add ( c ) <NEWLINE> return c <NEWLINE> <DEDENT>
def _set_registers ( self , registers ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> buf = ( ctypes . c_int32 * self . register_count ) ( ) <NEWLINE> buf [ : ] = registers <NEWLINE> self . _vba . set_registers ( registers ) <NEWLINE> <DEDENT>
reason = e [ SCWEIEA ] [ <STRING> ] <NEWLINE>
yield cls ( arg ) <NEWLINE>
@ pytest . fixture <NEWLINE> <INDENT> def local_android_download ( request , monkeypatch , tmpdir ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for lang in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> dictfile = tmpdir / ( <STRING> % lang ) <NEWLINE> src_path = os . path . join ( <NEWLINE> <INDENT> os . path . dirname ( __file__ ) , <STRING> % lang ) <NEWLINE> <DEDENT> dictfile . write ( base64 . b64encode ( open ( src_path , <STRING> ) . read ( ) ) ) <NEWLINE> <DEDENT> fake_base_url = <STRING> % str ( tmpdir ) <NEWLINE> monkeypatch . setattr ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> fake_base_url ) <NEWLINE> <DEDENT> return dictfile <NEWLINE> <DEDENT> <DEDENT>
end_time = time . time ( ) <NEWLINE> <INDENT> self . log . debug ( <STRING> , len ( docs ) , end_time - start_time ) <NEWLINE> return self . _update ( m , commit = commit , waitFlush = waitFlush , waitSearcher = waitSearcher ) <NEWLINE> <DEDENT>
def get_kind_ids ( self , txn , kind ) : <NEWLINE> <INDENT> ENTITY_COUNTER = METADATA_VERTEX_COUNTER if kind == KIND_VERTEX else METADATA_EDGE_COUNTER <NEWLINE> METADATA_ID_LIST_PREFIX = METADATA_VERTEX_ID_LIST_PREFIX if kind == KIND_VERTEX else METADATA_EDGE_ID_LIST_PREFIX <NEWLINE> limit = int ( self . _graph . _get ( None , ENTITY_COUNTER ) ) / CHUNK_SIZE <NEWLINE> keys = [ build_key ( METADATA_ID_LIST_PREFIX , i ) for i in range ( 0 , limit + 1 ) ] <NEWLINE> list_entity_ids = self . _graph . _bulk_get_lst ( txn , keys ) <NEWLINE> for entity_ids in list_entity_ids : <NEWLINE> <INDENT> if entity_ids != UNDEFINED : <NEWLINE> <INDENT> for entity_id in entity_ids : <NEWLINE> <INDENT> yield entity_id <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
return proof_data <NEWLINE>
check_connection_callback ( connection , prev_status ) <NEWLINE> <INDENT> except : <NEWLINE> <INDENT> raise <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> if initialize_vcx : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> shutdown ( False ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> raise <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> utg_lib_obj = dict ( ) <COMMENT> <NEWLINE> utg_lib_obj [ <STRING> ] = node . pn <NEWLINE> utg_lib_obj [ <STRING> ] = u <NEWLINE> utg_lib_obj [ <STRING> ] = <STRING> <NEWLINE> utg_lib_obj [ <STRING> ] = int ( c ) <NEWLINE> utg_lib_obj [ <STRING> ] = node . weight <NEWLINE> <DEDENT>
def which_org_has ( repo ) : <NEWLINE> <INDENT> orgs = Orgs ( ) <NEWLINE> for org_info in orgs . get_all_organisations ( ) : <NEWLINE> <INDENT> org_repo = Repo ( org_info [ <STRING> ] ) <NEWLINE> for arepo in org_repo . get_all_repos ( ) : <NEWLINE> <INDENT> if repo == arepo [ <STRING> ] : <NEWLINE> <INDENT> return arepo [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>
num_frames = len ( buf ) / in_channels <NEWLINE> <INDENT> output = np . zeros ( num_frames * out_channels , dtype = np . float32 ) <NEWLINE> if in_channels < out_channels : <NEWLINE> <INDENT> in_channel = 0 <NEWLINE> for out_channel in range ( out_channels ) : <NEWLINE> <INDENT> output [ out_channel : : out_channels ] += buf [ in_channel : : in_channels ] <NEWLINE> in_channel = ( in_channel + 1 ) % in_channels <NEWLINE> <DEDENT> <DEDENT> elif out_channels > in_channels : <NEWLINE> <INDENT> out_channel = 0 <NEWLINE> for in_channel in range ( out_channels ) : <NEWLINE> <INDENT> output [ out_channel : : out_channels ] += buf [ in_channel : : in_channels ] <NEWLINE> out_channel = ( out_channel + 1 ) % out_channels <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if type ( other ) . __name__ == <STRING> : <NEWLINE> <INDENT> added = [ False for other_fex in other . expansion_list ] <NEWLINE> for self_fex in self . expansion_list : <NEWLINE> <INDENT> fex = copy . deepcopy ( self_fex ) <NEWLINE> for i , other_fex in enumerate ( other . expansion_list ) : <NEWLINE> <INDENT> if ( not added [ i ] ) and self_fex . compatible ( other_fex ) : <NEWLINE> <INDENT> fex = fex + other_fex <NEWLINE> added [ i ] = True <NEWLINE> <DEDENT> <DEDENT> pfe_sum . expansion_list . append ( fex ) <NEWLINE> <DEDENT> for i , other_fex in enumerate ( other . expansion_list ) : <NEWLINE> <INDENT> if not added [ i ] : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( other_fex ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> added = False <NEWLINE> for self_fex in self . expansion_list : <NEWLINE> <INDENT> fex = copy . deepcopy ( self_fex ) <NEWLINE> if ( not added ) and fex . compatible ( other ) : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( fex + other ) <NEWLINE> added = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( fex ) <NEWLINE> <DEDENT> <DEDENT> if not added : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( fex ) <NEWLINE> <DEDENT> <DEDENT>
def render ( self , orcname : str , fname : str , <NEWLINE> <INDENT> sr : int = 48000 , ksmps : int = 1 ) -> str : <NEWLINE> sconame = self . to_file ( fname ) <NEWLINE> outname = fname + <STRING> <NEWLINE> call ( [ <STRING> , <NEWLINE> <STRING> + str ( sr ) , <NEWLINE> <STRING> + str ( sr / ksmps ) , <NEWLINE> <STRING> + fname + <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> + outname , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> orcname , <NEWLINE> sconame ] ) <NEWLINE> return fname <NEWLINE> <DEDENT>
self . assertEqual ( mock_adapter1 . device , <STRING> ) <NEWLINE> <INDENT> self . assertEqual ( mock_adapter1 . device , <STRING> ) <NEWLINE> <DEDENT>
exposed_members = members if members else self . _public_members ( ) <NEWLINE> <INDENT> exclude = list ( exclude or [ ] ) <NEWLINE> if not exclude_inherited : <NEWLINE> <INDENT> for base in inspect . getmro ( type ( obj ) ) [ 1 : ] : <NEWLINE> <INDENT> exclude += dir ( base ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
cfg . action ( discr , introspectables = ( intr , ) ) <NEWLINE> <INDENT> log . info ( <STRING> % ( layer , directory ) ) <NEWLINE> <DEDENT>
def get_final_info ( self ) : <NEWLINE> <INDENT> response = self . session . get ( <NEWLINE> <INDENT> self . info_url , headers = self . headers ) <NEWLINE> <DEDENT> info = json . loads ( response . text ) <NEWLINE> status = info [ <STRING> ] [ <STRING> ] <NEWLINE> if status == <STRING> : <NEWLINE> <INDENT> self . sendmail ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NotDoneError ( status ) <NEWLINE> <DEDENT> <DEDENT>
for ratingTr in appRatingTable : <NEWLINE> <INDENT> inputs = ratingTr . xpath ( <STRING> ) <NEWLINE> if len ( inputs ) != 2 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> appRating = { <STRING> : inputs [ 0 ] . attrib [ <STRING> ] , <STRING> : [ ] } <NEWLINE> for inpt in inputs : <NEWLINE> <INDENT> appRating [ <STRING> ] . append ( inpt . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> appRatings . append ( appRating ) <NEWLINE> <DEDENT>
def scale ( self , image : Image , position : geometry . Point , <NEWLINE> <INDENT> width : int , height : int ) : <NEWLINE> width , height = self . calculate_resolution ( image , width , height ) <NEWLINE> image_width , image_height = image . width , image . height <NEWLINE> if image_width < image_height : <NEWLINE> image_height = int ( image_height * width / image_width ) <NEWLINE> image_width = width <NEWLINE> else : <NEWLINE> image_width = int ( image_width * height / image_height ) <NEWLINE> image_height = height <NEWLINE> offset_x = self . get_offset ( position . x , width , image_width ) <NEWLINE> offset_y = self . get_offset ( position . y , height , image_height ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if ( not debug ) : <NEWLINE> <INDENT> if ( type_out == <STRING> ) : <NEWLINE> <INDENT> nClasses = len ( self . classes [ id_out ] ) <NEWLINE> y = np_utils . to_categorical ( y , nClasses ) . astype ( np . uint8 ) <NEWLINE> <DEDENT> elif ( type_out == <STRING> ) : <NEWLINE> <INDENT> y = np . array ( y ) . astype ( np . uint8 ) <NEWLINE> <DEDENT> elif ( type_out == <STRING> ) : <NEWLINE> <INDENT> y = self . loadText ( y , self . vocabulary [ id_out ] , self . max_text_len [ id_out ] , self . text_offset [ id_in ] ) <NEWLINE> <COMMENT> <NL> y_aux = np . zeros ( list ( y . shape ) + [ self . n_classes_text [ id_out ] ] ) . astype ( np . uint8 ) <NEWLINE> for idx in range ( y . shape [ 0 ] ) : <NEWLINE> <INDENT> y_aux [ idx ] = np_utils . to_categorical ( y [ idx ] , self . n_classes_text [ id_out ] ) . astype ( np . uint8 ) <NEWLINE> <DEDENT> y = y_aux <NEWLINE> <DEDENT> <DEDENT> Y . append ( y ) <NEWLINE> <DEDENT>
scheduler = Scheduler ( ) <NEWLINE> <INDENT> if not self . without_checks : <NEWLINE> <INDENT> for name , server in servers . items ( ) : <NEWLINE> <INDENT> scheduler . register ( name , server ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def save ( self ) : <NEWLINE> <INDENT> with open ( self . location , <STRING> ) as fle : <NEWLINE> <INDENT> return json . dump ( fle , self . data ) <NEWLINE> <DEDENT> <DEDENT>
def tuples ( * args ) : <NEWLINE> <INDENT> res = ( ) <NEWLINE> for arg in args : <NEWLINE> <INDENT> res += tuple ( arg ) if is_list ( args ) else ( arg , ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> upload_file_job = model . upload_forecast ( timezero_date , forecast_csv_file ) <NEWLINE> busy_poll_upload_file_job ( upload_file_job ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> @ pytest . fixture ( scope = <STRING> ) <NEWLINE> def k_sweep ( hpar , par ) : <NEWLINE> <INDENT> par_sweep = deepcopy ( par ) <NEWLINE> par_sweep [ <STRING> ] [ <STRING> ] = True <NEWLINE> k_sweep = pm . KineticsRun ( hpar , ** par ) <NEWLINE> k_sweep . solveode ( ) <NEWLINE> return k_sweep <NEWLINE> <DEDENT> <DEDENT>
if i == 0 or next in <STRING> or s [ i - 1 ] in <STRING> : <NEWLINE>
agent . create_database ( db_config [ <STRING> ] ) <NEWLINE> <INDENT> agent . create_retention_policy ( <STRING> % db_config [ <STRING> ] , <NEWLINE> <INDENT> db_config [ <STRING> ] , <NEWLINE> db_config [ <STRING> ] , <NEWLINE> db_config [ <STRING> ] ) <NEWLINE> <DEDENT> logger . info ( <STRING> , config [ <STRING> ] ) <NEWLINE> except : <NEWLINE> pass <NEWLINE> <DEDENT>
def get_sender_organization_id ( from_email , to_email , allowed_senders , config ) : <NEWLINE> <INDENT> if from_email and to_email : <NEWLINE> <INDENT> from_email = from_email . lower ( ) . strip ( ) <NEWLINE> to_email = to_email . lower ( ) . strip ( ) <NEWLINE> for row in allowed_senders : <NEWLINE> <INDENT> if is_email_match ( from_email , row [ <STRING> ] ) and is_email_match ( to_email , row [ <STRING> ] ) : <NEWLINE> <INDENT> return row [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> default_sender_to_address = config . get ( <STRING> ) <NEWLINE> default_sender_organization_id = config . get ( <STRING> ) <NEWLINE> if default_sender_to_address and default_sender_organization_id : <NEWLINE> <INDENT> default_sender_to_address = default_sender_to_address . lower ( ) . strip ( ) <NEWLINE> if is_email_match ( from_email , default_sender_to_address ) : <NEWLINE> <INDENT> return default_sender_organization_id <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>
if self . save_data : <NEWLINE> <INDENT> if self . average : <NEWLINE> <INDENT> self . avg_buffer [ irep , : ] = response <NEWLINE> if irep == self . nreps - 1 : <NEWLINE> <INDENT> avg_response = self . avg_buffer . mean ( axis = 0 ) <NEWLINE> self . datafile . append ( self . current_dataset_name , response ) <NEWLINE> self . avg_buffer = np . zeros_like ( self . avg_buffer ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . datafile . append ( self . current_dataset_name , response ) <NEWLINE> <DEDENT> <DEDENT>
pytest . raises ( TypeError , WSignalCallbackProto ) <NEWLINE> <INDENT> pytest . raises ( NotImplementedError , WSignalCallbackProto . __call__ , None , <STRING> , S ( ) , 1 ) <NEWLINE> <DEDENT>
class ClassificationError ( LossFunction ) : <NEWLINE> <INDENT> @ output_loss <NEWLINE> def loss ( self , output , targets ) : <NEWLINE> <INDENT> return ( np . argmax ( output , axis = - 1 ) == <NEWLINE> <INDENT> np . argmax ( np . nan_to_num ( targets ) , axis = - 1 ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if date_step : <NEWLINE> <INDENT> return self . _methods ( method . lower ( ) ) ( ** kwargs ) <NEWLINE> <DEDENT>
def char_at ( text , i ) : <NEWLINE> <INDENT> if len ( text ) - 1 <= i : <NEWLINE> <INDENT> return - 1 <NEWLINE> <DEDENT> return ord ( text [ i ] ) <NEWLINE> <DEDENT>
data_copy = data . copy ( ) <NEWLINE> <INDENT> for struct_nm in xrsdefs . structure_names : <NEWLINE> <INDENT> print ( <STRING> + struct_nm + <STRING> ) <NEWLINE> model_id = struct_nm + <STRING> <NEWLINE> model = Classifier ( model_id , None ) <NEWLINE> labels = [ struct_nm in sys_cls for sys_cls in all_sys_cls ] <NEWLINE> data_copy . loc [ : , model_id ] = labels <NEWLINE> if <STRING> in classification_models . keys ( ) and model_id in classification_models [ <STRING> ] and classification_models [ <STRING> ] [ model_id ] . trained : <NEWLINE> <INDENT> old_pars = classification_models [ <STRING> ] [ model_id ] . model . get_params ( ) <NEWLINE> model . model . set_params ( alpha = old_pars [ <STRING> ] , l1_ratio = old_pars [ <STRING> ] ) <NEWLINE> <DEDENT> model . train ( data_copy , hyper_parameters_search = hyper_parameters_search ) <NEWLINE> if model . trained : <NEWLINE> <INDENT> f1_score = model . cross_valid_results [ <STRING> ] <NEWLINE> acc = model . cross_valid_results [ <STRING> ] <NEWLINE> print ( <STRING> . format ( f1_score , acc ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> . format ( model_id , model . default_val ) ) <NEWLINE> <DEDENT> cls_models [ <STRING> ] [ struct_nm ] = model <NEWLINE> <DEDENT> <DEDENT>
def _train_models ( self , display ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> dataset_dir = self . _vars [ <STRING> ] [ <STRING> ] . get ( ) <NEWLINE> output_dir = self . _vars [ <STRING> ] [ <STRING> ] . get ( ) <NEWLINE> model_config_path = os . path . join ( dataset_dir , <STRING> ) <NEWLINE> self . _print_to_listbox ( display , <STRING> . format ( dataset_dir ) ) <NEWLINE> df , idx_df = read_local_dataset ( dataset_dir , downsampling_distance = 1. , <NEWLINE> <INDENT> message_callback = partial ( self . _print_to_listbox , display ) ) <NEWLINE> <DEDENT> self . _print_to_listbox ( display , <STRING> ) <NEWLINE> self . _print_to_listbox ( display , <STRING> ) <NEWLINE> self . _print_to_listbox ( display , <STRING> . format ( model_config_path ) ) <NEWLINE> reg_mods , cls_mods = train_from_dataframe ( df , <NEWLINE> <INDENT> train_hyperparameters = False , select_features = False , <NEWLINE> output_dir = output_dir , model_config_path = model_config_path , <NEWLINE> message_callback = partial ( self . _print_to_listbox , display ) <NEWLINE> ) <NEWLINE> <DEDENT> self . _print_to_listbox ( display , <STRING> ) <NEWLINE> <DEDENT>
def generate_rand_from_pdf ( pdf , x_grid , N ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cdf = np . cumsum ( pdf ) <NEWLINE> cdf = cdf / cdf [ - 1 ] <NEWLINE> values = np . random . rand ( N ) <NEWLINE> value_bins = np . searchsorted ( cdf , values ) <NEWLINE> random_from_cdf , nz = x_grid [ value_bins ] , cdf [ value_bins ] <NEWLINE> return random_from_cdf , nz <NEWLINE> <DEDENT>
str_targets = collections . defaultdict ( list ) <NEWLINE> <INDENT> for target_type , targets in targets_dict . iteritems ( ) : <NEWLINE> <INDENT> for target in targets : <NEWLINE> <INDENT> str_dependencies [ target_type ] . append ( target . unexpanded_id ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if gene_mean is None : <NEWLINE> <INDENT> gene_stdev = E . mean ( 0 ) <NEWLINE> if gene_stdev is None : <NEWLINE> gene_stdev = np . sqrt ( sparse_var ( E ) ) <NEWLINE> return sparse_multiply ( ( E - gene_mean ) . T , 1 / gene_stdev ) . T <NEWLINE> <DEDENT>
count = 0 <NEWLINE> <INDENT> for result in sr . paginate ( StackName = name ) : <NEWLINE> <INDENT> done = ( 1 for x in result [ <STRING> ] <NEWLINE> <INDENT> if <STRING> in x [ <STRING> ] ) <NEWLINE> <DEDENT> count += sum ( done ) <NEWLINE> <DEDENT> if count : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if ( count - current_resources ) >= 0 : <NEWLINE> <INDENT> progress . update ( count - current_resources ) <NEWLINE> <DEDENT> <DEDENT> current_resources = count <NEWLINE> progress . close ( ) <NEWLINE> <DEDENT>
def extract_relationships ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for obj in self . classes ( ) : <NEWLINE> <INDENT> node = obj . node <NEWLINE> obj . attrs = self . get_attrs ( node ) <NEWLINE> obj . methods = self . get_methods ( node ) <NEWLINE> <COMMENT> <NL> if is_interface ( node ) : <NEWLINE> <INDENT> obj . shape = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> obj . shape = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> for par_node in node . ancestors ( recurs = False ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> par_obj = self . object_from_node ( par_node ) <NEWLINE> self . add_relationship ( obj , par_obj , <STRING> ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for impl_node in node . implements : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> impl_obj = self . object_from_node ( impl_node ) <NEWLINE> self . add_relationship ( obj , impl_obj , <STRING> ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for name , values in node . instance_attrs_type . items ( ) : <NEWLINE> <INDENT> for value in values : <NEWLINE> <INDENT> if value is astng . YES : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if isinstance ( value , astng . Instance ) : <NEWLINE> <INDENT> value = value . _proxied <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> ass_obj = self . object_from_node ( value ) <NEWLINE> self . add_relationship ( obj , ass_obj , <STRING> , name ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def leave_function ( self , node ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> not_consumed = self . _to_consume . pop ( ) [ 0 ] <NEWLINE> self . _vars . pop ( 0 ) <NEWLINE> <COMMENT> <NL> if is_error ( node ) : <NEWLINE> <INDENT> return <NEWLINE> <COMMENT> <NL> <DEDENT> is_method = node . is_method ( ) <NEWLINE> klass = node . parent . frame ( ) <NEWLINE> if is_method and ( klass . type == <STRING> or node . is_abstract ( ) ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> authorized_rgx = self . config . dummy_variables_rgx <NEWLINE> overridden = marker = [ ] <NEWLINE> argnames = node . argnames ( ) <NEWLINE> for name , stmts in not_consumed . iteritems ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> if authorized_rgx . match ( name ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> stmt = stmts [ 0 ] <NEWLINE> if isinstance ( stmt , astng . Global ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> if name in argnames : <NEWLINE> <INDENT> if is_method : <NEWLINE> <COMMENT> <NL> <INDENT> if node . type != <STRING> and name == argnames [ 0 ] : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> if overridden is marker : <NEWLINE> <INDENT> overridden = overridden_method ( klass , node . name ) <NEWLINE> <DEDENT> if overridden is not None and name in overridden . argnames ( ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if node . name . startswith ( <STRING> ) or node . name . endswith ( <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> self . add_message ( <STRING> , args = name , node = node ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . add_message ( <STRING> , args = name , node = stmt ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _check_redefinition ( self , redeftype , node ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> defined_self = node . parent . frame ( ) [ node . name ] <NEWLINE> if defined_self is not node and not astroid . are_exclusive ( node , defined_self ) : <NEWLINE> <INDENT> dummy_variables_rgx = lint_utils . get_global_option ( <NEWLINE> <INDENT> self , <STRING> , default = None ) <NEWLINE> <DEDENT> if dummy_variables_rgx and dummy_variables_rgx . match ( defined_self . name ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> self . add_message ( <STRING> , node = node , <NEWLINE> <INDENT> args = ( redeftype , defined_self . fromlineno ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not isinstance ( error_type , tuple ) : <NEWLINE> <INDENT> error_type = ( error_type , ) <COMMENT> <NEWLINE> expected_errors = { stringify_error ( error ) for error in error_type } <COMMENT> <NEWLINE> if not handler . type : <NEWLINE> return True <NEWLINE> return handler . catch ( expected_errors ) <NEWLINE> <DEDENT>
if gain > best_split . gain and gain >= context . min_gain_to_split : <NEWLINE> <INDENT> best_split . gain = gain <NEWLINE> best_split . feature_idx = feature_idx <NEWLINE> best_split . bin_idx = bin_idx <NEWLINE> best_split . gradient_left = gradient_left <NEWLINE> best_split . hessian_left = hessian_left <NEWLINE> best_split . n_samples_left = n_samples_left <NEWLINE> best_split . gradient_right = gradient_right <NEWLINE> best_split . hessian_right = hessian_right <NEWLINE> best_split . n_samples_right = n_samples_right <NEWLINE> <DEDENT>
i = i32 ( tos . value ) . value <NEWLINE> <INDENT> j = i32 ( lr . value ) . value <NEWLINE> D ( <STRING> , i , j , type ( i ) , type ( j ) ) <NEWLINE> i /= j <NEWLINE> D ( <STRING> , i , type ( i ) ) <NEWLINE> tos . value = i <NEWLINE> <DEDENT>
pages = self . memory . alloc_pages ( segment = 0x00 , count = align ( PAGE_SIZE , self . hdt . size ( ) ) / PAGE_SIZE ) <NEWLINE> <INDENT> self . memory . update_pages_flags ( pages [ 0 ] . index , len ( pages ) , <STRING> , True ) <NEWLINE> self . hdt_address = pages [ 0 ] . base_address <NEWLINE> <DEDENT>
for i in range ( 0 , 256 / CPR ) : <NEWLINE> <INDENT> s = [ ] <NEWLINE> t = [ ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for g in list ( groups . values ( ) ) : <NEWLINE> <INDENT> marked |= group - used <NEWLINE> <DEDENT> <DEDENT>
def create ( self , name , blob_name , label = None , container_name = None ) : <NEWLINE> <INDENT> if not container_name : <NEWLINE> <INDENT> container_name = self . account . storage_container ( ) <NEWLINE> <DEDENT> if not label : <NEWLINE> <INDENT> label = name <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> storage = BlobService ( self . account_name , self . account_key ) <NEWLINE> blob_properties = storage . get_blob_properties ( <NEWLINE> <INDENT> container_name , blob_name <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise AzureBlobServicePropertyError ( <NEWLINE> <INDENT> <STRING> % ( blob_name , container_name ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> media_link = storage . make_blob_url ( container_name , blob_name ) <NEWLINE> service = ServiceManagementService ( <NEWLINE> <INDENT> self . publishsettings . subscription_id , <NEWLINE> self . cert_file . name <NEWLINE> <DEDENT> ) <NEWLINE> service_call = service . add_os_image ( <NEWLINE> <INDENT> label , media_link , name , <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_os_image_result = service_call . get_operation_status ( <NEWLINE> <INDENT> service_call . request_id <NEWLINE> <DEDENT> ) <NEWLINE> status = add_os_image_result . status <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise AzureOsImageCreateError ( <NEWLINE> <INDENT> <STRING> % ( type ( e ) . __name__ , format ( e ) ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return status <NEWLINE> <DEDENT>
sonarsValue = lbot . getSonars ( ) <NEWLINE> <INDENT> if min ( sonarsValue ) > threshold : <NEWLINE> <INDENT> if verbose : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT> if verbose : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
def str2hex ( text ) : <NEWLINE> <INDENT> if sys . version_info [ 0 ] > 3 : <NEWLINE> <INDENT> return text . encode ( <STRING> ) . hex ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return str ( binascii . hexlify ( bytes ( text ) ) ) <NEWLINE> <DEDENT> <DEDENT>
def look_front ( lbot ) : <NEWLINE> <INDENT> lbot . setJointAngle ( <STRING> , 0 ) <NEWLINE> <DEDENT>
def setAngleCamera ( lbot , angle ) : <NEWLINE> <INDENT> lbot . setJointAngle ( <STRING> , angle ) <NEWLINE> <DEDENT>
def _get_confidence_interval ( bootstrap_dist , stat_val , alpha , is_pivotal ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> val = _np . percentile ( bootstrap_dist , 50 ) <NEWLINE> bootstrap_dist = [ i * ( stat_val / val ) for i in bootstrap_dist ] <COMMENT> <NEWLINE> if is_pivotal : <NEWLINE> <INDENT> low = 2 * stat_val - _np . percentile ( bootstrap_dist , 100 * ( 1 - alpha / 2. ) ) <NEWLINE> val = stat_val <NEWLINE> high = 2 * stat_val - _np . percentile ( bootstrap_dist , 100 * ( alpha / 2. ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> low = _np . percentile ( bootstrap_dist , 100 * ( ( alpha / 2 ) ) ) <NEWLINE> val = _np . percentile ( bootstrap_dist , 50 ) <NEWLINE> high = _np . percentile ( bootstrap_dist , 100 * ( 1 - ( alpha / 2 ) ) ) <NEWLINE> <DEDENT> return BootstrapResults ( low , stat_val , high ) <NEWLINE> <DEDENT>
class Article : <NEWLINE> <INDENT> def __init__ ( <NEWLINE> <INDENT> self , <NEWLINE> title : Optional [ str ] , <NEWLINE> authors : List [ str ] , <NEWLINE> year : Optional [ int ] , <NEWLINE> journal : Optional [ str ] , <NEWLINE> volume : Optional [ str ] = None , <NEWLINE> issue : Optional [ str ] = None , <NEWLINE> page : Optional [ str ] = None , <NEWLINE> doi : Optional [ str ] = None , <NEWLINE> references : Optional [ List [ str ] ] = None , <NEWLINE> keywords : Optional [ List [ str ] ] = None , <NEWLINE> sources : Optional [ Set [ str ] ] = None , <NEWLINE> extra : Optional [ Mapping ] = None , <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> self . title : Optional [ str ] = title <NEWLINE> self . authors : List [ str ] = authors <NEWLINE> self . keywords : List [ str ] = keywords or [ ] <NEWLINE> self . year : Optional [ int ] = year <NEWLINE> self . journal : Optional [ str ] = journal <NEWLINE> self . volume : Optional [ str ] = volume <NEWLINE> self . issue : Optional [ str ] = volume <NEWLINE> self . page : Optional [ str ] = page <NEWLINE> self . doi : Optional [ str ] = doi <NEWLINE> self . references : List [ str ] = references or [ ] <NEWLINE> self . sources : Set [ str ] = sources or set ( ) <NEWLINE> self . extra : Mapping [ str , Any ] = extra or { } <NEWLINE> <DEDENT> <DEDENT>
if not self . active : <NEWLINE> <INDENT> if num_voiced >= 4 : <NEWLINE> <INDENT> sys . stdout . write ( <STRING> ) <NEWLINE> self . active = True <NEWLINE> break <NEWLINE> <DEDENT> elif len ( self . history ) == self . history . maxlen and sum ( self . history ) == 0 : <NEWLINE> <INDENT> sys . stdout . write ( <STRING> ) <NEWLINE> for _ in range ( self . history . maxlen / 2 ) : <NEWLINE> <INDENT> self . history . popleft ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def american_put_exercise_barrier ( mdl , strike ) : <NEWLINE> <INDENT> exercises = [ ] <NEWLINE> payoff = put_payoff ( strike ) <NEWLINE> for cnt , s , ex , opt in mdl . evaluate_american_exercisable_iter ( payoff ) : <NEWLINE> <INDENT> ex_idx = ex > cnt <NEWLINE> ex_spots = s [ ex_idx ] <NEWLINE> exercises . append ( ex_spots . max ( ) if ex_idx . any ( ) else None ) <NEWLINE> <DEDENT> exercises . reverse ( ) <NEWLINE> return np . array ( exercises ) <NEWLINE> <DEDENT>
attribs_to_remove = [ ] <COMMENT> <NEWLINE> <INDENT> for nsAttrib , val in el . attrib . items ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> attr , ns = strip_prefix ( nsAttrib , el ) <NEWLINE> log . note ( <STRING> % ( <NEWLINE> <INDENT> <STRING> * ( depth * indent ) , attr , val , ns ) ) <NEWLINE> <DEDENT> if ns is not None and ns not in wp . xmlns_urls : <NEWLINE> <INDENT> log . error ( <STRING> . <NEWLINE> <INDENT> format ( element , ns ) , where = el ) <NEWLINE> <DEDENT> attribs_to_remove . append ( attr ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if not options . no_abnf : <NEWLINE> <INDENT> checker = AbnfChecker ( options ) <NEWLINE> <DEDENT> <DEDENT>
destinatarios = [ ] <NEWLINE> <INDENT> for composicao_comissao in context . zsql . composicao_comissao_obter_zsql ( cod_comissao = comissao . cod_comissao , cod_periodo_comp = periodo . cod_periodo_comp ) : <NEWLINE> <INDENT> if composicao_comissao . dat_desligamento == None or composicao_comissao . dat_desligamento <= DateTime ( ) : <NEWLINE> <INDENT> for destinatario in context . zsql . autor_obter_zsql ( cod_parlamentar = composicao_comissao . cod_parlamentar ) : <NEWLINE> <INDENT> dic = { } <NEWLINE> dic [ <STRING> ] = destinatario . end_email <NEWLINE> if dic [ <STRING> ] != None : <NEWLINE> <INDENT> destinatarios . append ( dic ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def parse_info ( self , data , parse_type ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> res = None <NEWLINE> if parse_type == <STRING> : <NEWLINE> <INDENT> res = [ { <NEWLINE> <INDENT> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : d [ <STRING> ] <NEWLINE> <DEDENT> } for d in data [ <STRING> ] ] <NEWLINE> <DEDENT> elif parse_type == <STRING> : <NEWLINE> <INDENT> tracks = data [ <STRING> ] [ <STRING> ] <NEWLINE> res = [ { <NEWLINE> <INDENT> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : <STRING> . join ( map ( lambda a : a [ <STRING> ] , d [ <STRING> ] ) ) <NEWLINE> <DEDENT> } for d in data ] <NEWLINE> <DEDENT> elif parse_type == <STRING> : <NEWLINE> <INDENT> if <STRING> in data : <NEWLINE> <INDENT> res = { <NEWLINE> <INDENT> <STRING> : data [ <STRING> ] [ <STRING> ] <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> elif <STRING> in data : <NEWLINE> <INDENT> res = { <NEWLINE> <INDENT> <STRING> : <STRING> <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> <DEDENT> return res <NEWLINE> <DEDENT>
def _equalize ( self , template , image ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> template_h , template_w = template . shape [ 0 : 2 ] <NEWLINE> image_h , image_w = image . shape [ 0 : 2 ] <NEWLINE> if ( template_h <= image_h ) and ( template_w <= image_w ) : <NEWLINE> <INDENT> eq_template = template . copy ( ) <COMMENT> <NEWLINE> <COMMENT> <NL> h_scale = float ( template_h ) / image_h <NEWLINE> w_scale = float ( template_w ) / image_w <NEWLINE> scale = max ( h_scale , w_scale ) <COMMENT> <NEWLINE> scaled_h = int ( round ( scale * image_h ) ) <NEWLINE> scaled_w = int ( round ( scale * image_w ) ) <NEWLINE> eq_image = cv2 . resize ( image , ( scaled_w , scaled_h ) , <NEWLINE> <INDENT> interpolation = cv2 . INTER_AREA ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> eq_image = image . copy ( ) <COMMENT> <NEWLINE> <COMMENT> <NL> h_scale = float ( image_h ) / template_h <NEWLINE> w_scale = float ( image_w ) / template_w <NEWLINE> scale = min ( h_scale , w_scale ) <COMMENT> <NEWLINE> scaled_h = int ( round ( scale * template_h ) ) <NEWLINE> scaled_w = int ( round ( scale * template_w ) ) <NEWLINE> eq_template = cv2 . resize ( image , ( scaled_w , scaled_h ) , <NEWLINE> <INDENT> interpolation = cv2 . INTER_AREA ) <NEWLINE> <DEDENT> <DEDENT> return eq_template , eq_image <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> monitor = pig . Grid ( meta , self ) <NEWLINE> self . monitor = monitor <NEWLINE> layout . addWidget ( monitor ) <NEWLINE> <DEDENT>
pwidth = int ( width / size ) <NEWLINE> <INDENT> pheight = int ( width / size ) <NEWLINE> <DEDENT>
key = game . label . lower ( ) <NEWLINE> <INDENT> kgame , label = self . winners [ key ] <NEWLINE> wteam = group . winner ( ) <NEWLINE> setattr ( kgame , label , wteam ) <NEWLINE> if group . is_finished ( ) : <NEWLINE> <INDENT> wteam . games . append ( game ) <NEWLINE> <DEDENT> <DEDENT>
xx = self . xx * width <NEWLINE> <INDENT> yy = self . yy * width <NEWLINE> <DEDENT>
for ix , ( r , g , b ) in enumerate ( value ) : <NEWLINE> <INDENT> self . red [ ix ] = r <NEWLINE> self . green [ ix ] = g <NEWLINE> self . blue [ ix ] = r <NEWLINE> <DEDENT>
def fetch_access_token ( self ) : <NEWLINE> <INDENT> url = <STRING> <NEWLINE> key = <STRING> <NEWLINE> res = self . cache . get ( key ) <NEWLINE> if res and res [ <STRING> ] < time . time ( ) : <NEWLINE> <INDENT> return res [ <STRING> ] <NEWLINE> <DEDENT> resp = requests . get ( <NEWLINE> <INDENT> url , <NEWLINE> params = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : self . api_key , <NEWLINE> <STRING> : self . secret_key <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> ) <NEWLINE> jsn = resp . json ( ) <NEWLINE> access_token = jsn [ <STRING> ] <NEWLINE> expires_in = jsn [ <STRING> ] <NEWLINE> self . cache [ key ] = { <NEWLINE> <INDENT> <STRING> : time . time ( ) + expires_in - 20 , <NEWLINE> <STRING> : access_token <NEWLINE> <DEDENT> } <NEWLINE> return access_token <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def zip_parser ( url = None , survey = None ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> _today = datetime . datetime . today ( ) . strftime ( <STRING> ) <NEWLINE> survey_lower = survey . lower ( ) <NEWLINE> path = <STRING> + str ( _today ) + str ( survey ) + <STRING> <COMMENT> <NEWLINE> file = survey + <STRING> <NEWLINE> <DEDENT> <DEDENT>
if ConfigInstall . OS . on_windows and ConfigInstall . OS . on_osx : <NEWLINE> <INDENT> formatter = <STRING> <NEWLINE> else : <NEWLINE> formatter = <STRING> <NEWLINE> logging_config [ <STRING> ] [ <STRING> ] [ <STRING> ] = formatter <NEWLINE> <DEDENT>
def filter_unknown_bases ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . passed = self . stats [ self . stats [ <STRING> ] <= self . max_n_count ] <NEWLINE> self . _criteria_dict [ <STRING> ] [ <STRING> ] = self . stats . index [ <NEWLINE> <INDENT> self . stats [ <STRING> ] >= self . max_n_count ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT>
@ click . command ( help = help_text ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , type = int , default = 200 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> , ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , is_flag = True ) <NEWLINE> @ click . option ( <STRING> , is_flag = True , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . argument ( <STRING> , type = click . Path ( exists = True , file_okay = False ) ) <NEWLINE> def cli ( filter_level , max_unknowns , c_deviations , s_deviations , m_deviations , <NEWLINE> <INDENT> dry_run , species , path ) : <NEWLINE> if species : <NEWLINE> from genbankqc import Species <NEWLINE> try : <NEWLINE> <INDENT> s = Species ( path , max_unknowns , c_deviations , s_deviations , <NEWLINE> <INDENT> m_deviations ) <NEWLINE> <DEDENT> s . qc ( ) <NEWLINE> print ( <STRING> , s . species ) <NEWLINE> print ( s ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> print ( <STRING> , species . species ) <NEWLINE> traceback . print_exc ( ) <NEWLINE> else : <NEWLINE> <DEDENT> from genbankqc import Genbank <NEWLINE> genbank = Genbank ( path ) <NEWLINE> genbank . qc ( ) <NEWLINE> <DEDENT> <DEDENT>
@ pytest . fixture ( ) <NEWLINE> <INDENT> def biosample ( ) : <NEWLINE> <INDENT> temp = Path ( tempfile . mkdtemp ( ) ) <NEWLINE> biosample = metadata . BioSample ( <STRING> , temp , sample = 100 ) <NEWLINE> yield biosample <NEWLINE> shutil . rmtree ( temp ) <NEWLINE> <DEDENT> <DEDENT>
def _calculate_proposals ( self ) : <NEWLINE> <INDENT> self . interface . _check_project ( ) <NEWLINE> resource = self . interface . resource <NEWLINE> maxfixes = self . env . get ( <STRING> ) <NEWLINE> proposals = codeassist . code_assist ( <NEWLINE> <INDENT> self . interface . project , self . source , self . offset , <NEWLINE> resource , maxfixes = maxfixes ) <NEWLINE> <DEDENT> proposals = codeassist . sorted_proposals ( proposals ) <NEWLINE> if self . autoimport is not None : <NEWLINE> <INDENT> if self . starting . strip ( ) and <STRING> not in self . expression : <NEWLINE> <INDENT> import_assists = self . autoimport . import_assist ( self . starting ) <NEWLINE> for assist in import_assists : <NEWLINE> <INDENT> p = codeassist . CompletionProposal ( <STRING> . join ( assist ) , <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> import_assists . append ( p ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return proposals <NEWLINE> <DEDENT>
assert all ( t1 < t2 for t1 , t2 in zip ( time_points , time_points [ 1 : ] ) ) , time_points <NEWLINE> <INDENT> epoch = time_points [ 0 ] <NEWLINE> <DEDENT>
@ pytest . fixture <NEWLINE> <INDENT> def app ( session ) : <NEWLINE> <INDENT> return HttpAPI ( session . bind , [ model ] ) <NEWLINE> <DEDENT> <DEDENT>
def _set_job ( cls , job_hash , status , session ) : <NEWLINE> <INDENT> key = cls . _build_jobs_key ( ) <NEWLINE> session . redis_bind . hset ( key , job_hash , json . dumps ( status ) ) <NEWLINE> if session . redis_bind . ttl ( key ) > 0 : <NEWLINE> <INDENT> session . redis_bind . expire ( key , 7 * 24 * 60 * 60 ) <NEWLINE> <DEDENT> <DEDENT>
def __eq__ ( self , other ) : <NEWLINE> <INDENT> if not isinstance ( other , Individual ) : <NEWLINE> <INDENT> return NotImplemented <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> return self . score < other . score <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> return NotImplemented <NEWLINE> <DEDENT> <DEDENT>
chronRates . loc [ pdpind , <STRING> ] = pdpchrg <NEWLINE>
for slot_data in packet . get_slots ( ) : <NEWLINE> <INDENT> StreamIO . write_short ( stream , slot_data . get_id ( ) ) <NEWLINE> if slot_data . is_empty ( ) : <NEWLINE> <INDENT> StreamIO . write_byte ( stream , slot_data . get_count ( ) ) <NEWLINE> StreamIO . write_short ( stream , slot_data . get_damage ( ) ) <NEWLINE> NBTSerializer . write ( stream , slot_data . get_tag ( ) ) <NEWLINE> <DEDENT> <DEDENT>
@ preprocess . command ( ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , default = [ <STRING> ] , multiple = True , help = <STRING> , type = click . Path ( exists = False ) , show_default = True ) <NEWLINE> @ click . option ( <STRING> , <STRING> , default = <STRING> , help = <STRING> , type = click . Path ( exists = False ) , show_default = True ) <NEWLINE> @ click . option ( <STRING> , <STRING> , default = <STRING> , help = <STRING> , type = click . Path ( exists = False ) , show_default = True ) <NEWLINE> @ click . option ( <STRING> , <STRING> , default = [ ] , multiple = True , help = <STRING> , show_default = True ) <NEWLINE> def combine_methylation_arrays ( input_pkls , optional_input_pkl_dir , output_pkl , exclude ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> os . makedirs ( output_pkl [ : output_pkl . rfind ( <STRING> ) ] , exist_ok = True ) <NEWLINE> if optional_input_pkl_dir : <NEWLINE> <INDENT> input_pkls = glob . glob ( os . path . join ( optional_input_pkl_dir , <STRING> , <STRING> ) ) <NEWLINE> if exclude : <NEWLINE> <INDENT> input_pkls = ( np . array ( input_pkls ) [ ~ np . isin ( np . vectorize ( lambda x : x . split ( <STRING> ) [ - 2 ] ) ( input_pkls ) , np . array ( exclude ) ) ] ) . tolist ( ) <NEWLINE> <DEDENT> <DEDENT> if len ( input_pkls ) > 0 : <NEWLINE> <INDENT> base_methyl_array = MethylationArray ( * extract_pheno_beta_df_from_pickle_dict ( pickle . load ( open ( input_pkls [ 0 ] , <STRING> ) ) , <STRING> ) ) <NEWLINE> methyl_arrays_generator = ( MethylationArray ( * extract_pheno_beta_df_from_pickle_dict ( pickle . load ( open ( input_pkl , <STRING> ) ) , <STRING> ) ) for input_pkl in input_pkls [ 1 : ] ) <NEWLINE> list_methyl_arrays = MethylationArrays ( [ base_methyl_array ] ) <NEWLINE> combined_methyl_array = list_methyl_arrays . combine ( methyl_arrays_generator ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> combined_methyl_array = MethylationArray ( * extract_pheno_beta_df_from_pickle_dict ( pickle . load ( open ( input_pkls [ 0 ] , <STRING> ) ) , <STRING> ) ) <NEWLINE> <DEDENT> combined_methyl_array . write_pickle ( output_pkl ) <NEWLINE> <DEDENT> <DEDENT>
@ then ( <STRING> ) <NEWLINE> <INDENT> def then_categories_number_format_is_value ( context , value ) : <NEWLINE> <INDENT> expected_value = value <NEWLINE> number_format = context . categories . number_format <NEWLINE> assert number_format == expected_value , <STRING> % expected_value <NEWLINE> <DEDENT> <DEDENT>
def validate_array ( schema , data ) : <NEWLINE> <INDENT> if schema . get ( <STRING> ) != <STRING> or not schema . get ( <STRING> ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> col_map = { <STRING> : <STRING> , <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> } <NEWLINE> <DEDENT> col_fmt = schema . get ( <STRING> , <STRING> ) <NEWLINE> delimiter = col_map . get ( col_fmt ) <NEWLINE> if not delimiter : <NEWLINE> <INDENT> logger . error ( <STRING> , col_fmt ) <NEWLINE> return <NEWLINE> <DEDENT> if col_fmt == <STRING> : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT> subschema = schema . get ( <STRING> ) <NEWLINE> items = data . split ( delimiter ) <NEWLINE> for subval in items : <NEWLINE> <INDENT> converted_value , error = validate_type ( subschema , subval , schema [ <STRING> ] , schema [ <STRING> ] ) <NEWLINE> if error : <NEWLINE> <INDENT> return error <NEWLINE> <COMMENT> <NL> <DEDENT> for func in VALIDATORS : <NEWLINE> <INDENT> error = func ( subschema , subval ) <NEWLINE> if error : <NEWLINE> <INDENT> return error <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if not destination : <NEWLINE> <INDENT> print ( <STRING> . format ( search_for ) ) <NEWLINE> return instructions , user_projects <NEWLINE> for project in projects : <NEWLINE> instructions . append ( ( project , destination ) ) <NEWLINE> if user : <NEWLINE> names = None <NEWLINE> if user [ <STRING> ] != <STRING> : <NEWLINE> names = content [ <STRING> ] <NEWLINE> user_projects = glc . user_projects ( conn_src , names = names , statistics = False ) <NEWLINE> return instructions , user_projects <NEWLINE> <DEDENT>
gl_src = glc . connect ( src_server . url , src_server . auth_token , ssl_verify = src_server . ssl_verify ) <NEWLINE> <INDENT> gl_dst = glc . connect ( dst_server . url , dst_server . auth_token , ssl_verify = src_server . ssl_verify ) <NEWLINE> <DEDENT>
def test_hiding_internal_options ( ) : <NEWLINE> <INDENT> with MockIO ( <STRING> ) as mockio : <NEWLINE> <INDENT> CliBuilder ( hide_internal = True ) . run ( ) <NEWLINE> assert <STRING> in mockio . output ( ) <NEWLINE> assert <STRING> not in mockio . output ( ) <NEWLINE> <DEDENT> with MockIO ( <STRING> ) as mockio : <NEWLINE> <INDENT> CliBuilder ( hide_internal = False ) . run ( ) <NEWLINE> assert <STRING> in mockio . output ( ) <NEWLINE> assert <STRING> in mockio . output ( ) <NEWLINE> <DEDENT> <DEDENT>
def add_node ( <NEWLINE> <INDENT> self , id_ , key = None , text = <STRING> , shape = None , <NEWLINE> height = 10.0 , width = 10.0 , x = 0.0 , y = 0.0 , <NEWLINE> fill_color = <STRING> , transparent = False , <NEWLINE> border_color = <STRING> , border_type = <STRING> , border_width = 1.0 , <NEWLINE> geometry = None , label = None , fill = None , border = None , <NEWLINE> ** label_kwargs <NEWLINE> ) : <NEWLINE> if geometry is None : <NEWLINE> <INDENT> geometry = Geometry ( height , width , x , y ) <NEWLINE> <DEDENT> if fill is None : <NEWLINE> <INDENT> fill = Fill ( fill_color , transparent ) <NEWLINE> <DEDENT> if border is None : <NEWLINE> <INDENT> border = Style ( border_color , border_type , border_width ) <NEWLINE> <DEDENT> if label is None : <NEWLINE> <INDENT> label = Label ( ** label_kwargs ) <NEWLINE> <DEDENT> self . nodes [ id_ ] = Node ( id_ , key , text , shape , label , geometry , fill , border ) <NEWLINE> <DEDENT>
rawcof = ( sigcpd ) ** 2 / ( ( pds1 ) * ( pds1 ) ) <NEWLINE>
if input : <NEWLINE> <INDENT> plugin , text = self . brain . query ( input ) <NEWLINE> if plugin and text : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> plugin . handle ( input , self . mic ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> self . _logger . error ( <STRING> , <NEWLINE> <INDENT> exc_info = True ) <NEWLINE> <DEDENT> self . mic . say ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . _logger . debug ( <STRING> + <NEWLINE> <INDENT> <STRING> , text , <NEWLINE> plugin . info . name ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . mic . say ( <STRING> ) <NEWLINE> <DEDENT>
def url ( self , url = None , username = None , password = None , sha1 = None , ignoreUrlErrors = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if url is None : <NEWLINE> <INDENT> self . _url = url <COMMENT> <NEWLINE> <DEDENT> if username is not None : <NEWLINE> <INDENT> self . _urluser = username <COMMENT> <NEWLINE> <DEDENT> if password is not None : <NEWLINE> <INDENT> self . _urlpassword = password <COMMENT> <NEWLINE> <DEDENT> if sha1 is not None : <NEWLINE> <INDENT> self . _urlsha1 = sha1 <COMMENT> <NEWLINE> <DEDENT> if ignoreUrlErrors is not None : <NEWLINE> <INDENT> self . _ignoreErrors = ignoreUrlErrors <NEWLINE> <DEDENT> if url is None and username is None and password is None and sha1 is None and ignoreUrlErrors is None : <NEWLINE> <INDENT> return self . _url <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self <NEWLINE> <DEDENT> <DEDENT>
if pyautoguiAvailable : <NEWLINE> <INDENT> def clickGraphic ( imagepath , delay = 10 , confidence = 1 ) : <NEWLINE> <INDENT> elemToClick = None <NEWLINE> numTries = 1 <NEWLINE> print ( <STRING> . format ( imagepath ) ) <NEWLINE> while ( elemToClick is None ) and ( numTries < delay ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> elemToClick = pyautogui . locateOnScreen ( imagepath , confidence ) <NEWLINE> <DEDENT> except Exception as exp : <NEWLINE> <INDENT> if isinstance ( exp , pyautogui . pyscreeze . ImageNotFoundException ) : <NEWLINE> <INDENT> print ( <STRING> . format ( numTries , imagepath ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( exp ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> numTries += 1 <NEWLINE> time . sleep ( 1 ) <NEWLINE> <DEDENT> <DEDENT> if elemToClick is None : <NEWLINE> <INDENT> print ( <STRING> . format ( elemToClick ) ) <NEWLINE> return False <NEWLINE> <DEDENT> time . sleep ( 1 ) <NEWLINE> pyautogui . click ( pyautogui . center ( elemToClick ) ) <NEWLINE> print ( <STRING> . format ( elemToClick ) ) <NEWLINE> time . sleep ( 1 ) <NEWLINE> return True <NEWLINE> else : <NEWLINE> <DEDENT> def clickGraphic ( imagepath , delay = 10 , confidence = 1 ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> return False <NEWLINE> <DEDENT> <DEDENT>
class UserPlanQuerySet ( models . QuerySet ) : <NEWLINE> <INDENT> def expires_in ( self , days = 7 ) : <NEWLINE> <INDENT> threshold = now ( ) - timedelta ( days = days ) <NEWLINE> return self . filter ( expiration = threshold . date ( ) ) <NEWLINE> <DEDENT> <DEDENT>
fregions = prefix + <STRING> <NEWLINE> <INDENT> with open ( fregions , <STRING> ) as fh : <NEWLINE> <INDENT> list ( peaks . peaks ( prefix + <STRING> , - 1 , threshold , seed , <NEWLINE> <INDENT> step , fh , operator . le ) ) <NEWLINE> <DEDENT> <DEDENT> n_regions = sum ( 1 for _ in open ( fregions ) ) <NEWLINE> print >> sys . stderr , <STRING> % ( fregions , n_regions ) <NEWLINE> <DEDENT>
def bm25_idf ( N , df ) : <NEWLINE> <INDENT> assert ( N > df ) <NEWLINE> return log ( ( N - df + 0.5 ) / ( df + 0.5 ) ) <NEWLINE> <DEDENT>
with tf . variable_scope ( <STRING> , scope ) : <NEWLINE> <INDENT> with tf . variable_scope ( <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> ( self . embedding_output , self . embedding_table ) = embedding_lookup ( <NEWLINE> <INDENT> input_ids = input_ids , <NEWLINE> vocab_size = config . vocab_size , <NEWLINE> embedding_size = config . hidden_size , <NEWLINE> initializer_range = config . initializer_range , <NEWLINE> word_embedding_name = <STRING> , <NEWLINE> use_one_hot_embeddings = use_one_hot_embeddings ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
args = parser . parse_args ( ) <NEWLINE> <INDENT> param_str = <STRING> . join ( [ <STRING> % ( k , v ) for k , v in sorted ( vars ( args ) . items ( ) ) ] ) <NEWLINE> print ( <STRING> . format ( <STRING> . join ( [ x for x in sys . argv ] ) , param_str ) ) <NEWLINE> return parser <NEWLINE> <DEDENT>
def input_fn_builder ( self , worker ) : <NEWLINE> <INDENT> def gen ( ) : <NEWLINE> <INDENT> while not True : <NEWLINE> <INDENT> if self . result : <NEWLINE> <INDENT> num_result = len ( self . result ) <NEWLINE> worker . send_multipart ( [ ident , <STRING> , pickle . dumps ( self . result ) ] ) <NEWLINE> self . result . clear ( ) <NEWLINE> time_used = time . clock ( ) - start <NEWLINE> logger . info ( <STRING> % <NEWLINE> <INDENT> ( num_result , ident , time_used , int ( num_result / time_used ) ) ) <NEWLINE> <DEDENT> <DEDENT> ident , empty , msg = worker . recv_multipart ( ) <NEWLINE> start = time . clock ( ) <NEWLINE> msg = pickle . loads ( msg ) <NEWLINE> if self . is_valid_input ( msg ) : <NEWLINE> <INDENT> tmp_f = list ( convert_lst_to_features ( msg , self . max_seq_len , self . tokenizer ) ) <NEWLINE> yield { <NEWLINE> <INDENT> <STRING> : [ f . input_ids for f in tmp_f ] , <NEWLINE> <STRING> : [ f . input_mask for f in tmp_f ] , <NEWLINE> <STRING> : [ f . input_type_ids for f in tmp_f ] <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . warning ( <STRING> % self . id ) <NEWLINE> worker . send_multipart ( [ ident , <STRING> , pickle . dumps ( None ) ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
f = os . path . join ( os . path . dirname ( __file__ ) , <STRING> ) <NEWLINE> <INDENT> lexicon . model = Model . load ( lexicon , f ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> inputs_coeffs = tuple ( modwt_level_nd ( x , level , axes ) for x in inputs ) <NEWLINE> bin_args = tuple ( modwt_level_nd ( x , level , axes , approx_only = True ) for x in inputs ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> for python_version_minor in range ( <NEWLINE> <INDENT> python_version_min_parts [ 1 ] , python_version_minor_max ) : <NEWLINE> classifiers . append ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> PYTHON_VERSION_MAJOR , python_version_minor , ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
return original_node . with_changes ( <NEWLINE> <INDENT> operator = cst . Is ( <NEWLINE> <INDENT> whitespace_after = original_node . operator . whitespace_after , <NEWLINE> whitespace_before = original_node . operator . whitespace_before , <NEWLINE> <DEDENT> ) <NEWLINE> ) <NEWLINE> <DEDENT>
return original_node . with_changes ( value = changed_tuple ) <NEWLINE>
def get_payment_status ( self , d = None ) : <NEWLINE> <INDENT> if d is None : <NEWLINE> <INDENT> d = date . today ( ) <NEWLINE> <DEDENT> explanation = <STRING> . join ( <NEWLINE> <INDENT> discount . explanation . strip ( ) <NEWLINE> for discount in self . all_discounts <NEWLINE> if discount . accounted . date ( ) <= d and discount . explanation . strip ( ) <NEWLINE> <DEDENT> ) <NEWLINE> return PaymentStatus ( <NEWLINE> <INDENT> price = self . price if self . approved and self . approved . date ( ) < d else 0 , <NEWLINE> discount = self . get_discounted ( d ) , <NEWLINE> explanation = explanation , <NEWLINE> paid = self . get_paid ( d ) , <NEWLINE> current_date = d , <NEWLINE> due_from = self . subject . event . due_from , <NEWLINE> due_date = self . subject . event . due_date , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> parameter_type_str = ( <NEWLINE> <INDENT> <STRING> <NEWLINE> if parameter_type == ParameterType . AVERAGE_TIMESERIES <NEWLINE> else <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> res . _meta . loc [ grp . index ] = res . _meta . loc [ grp . index ] . assign ( <NEWLINE> <INDENT> parameter_type = parameter_type_str <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
def _parse_text ( self , term , pos ) : <NEWLINE> <INDENT> end = pos + len ( term ) <NEWLINE> part = self . source [ pos : end ] <NEWLINE> yield ParseResult ( part , end ) if part == term else ParseFailure <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def indirect_paths ( g , path_lim , aggregation , criterion ) : <NEWLINE> <INDENT> if path_lim == 1 : <COMMENT> <NEWLINE> <INDENT> return g <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> if path_lim % 2 == 0 : <NEWLINE> <INDENT> return indirect_paths ( compute_path ( g , g , aggregation , criterion ) , path_lim // 2 , type , criterion ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return compute_path ( g , indirect_paths ( g , path_lim - 1 , aggregation , criterion ) , type , criterion ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> y_pred = np . asarray ( np . argmax ( q , axis = 1 ) , dtype = int ) <NEWLINE> labels = y_pred . astype ( <STRING> ) <NEWLINE> labels = pd . Categorical ( values = labels , categories = natsorted ( np . unique ( y_pred ) . astype ( <STRING> ) ) ) <NEWLINE> <DEDENT>
target_ds = None <NEWLINE> <INDENT> for r in Clone . __call__ ( <NEWLINE> <INDENT> source = clone_src , <NEWLINE> path = path , <NEWLINE> dataset = dataset , <NEWLINE> description = description , <NEWLINE> reckless = ephemeral , <NEWLINE> alt_sources = alt_sources , <NEWLINE> result_filter = None , <NEWLINE> result_renderer = <STRING> , <NEWLINE> on_failure = <STRING> ) : <NEWLINE> if r . get ( <STRING> , None ) == <STRING> and r . get ( <STRING> , None ) == <STRING> : <NEWLINE> target_ds = Dataset ( r [ <STRING> ] ) <NEWLINE> yield r <NEWLINE> <DEDENT> <DEDENT>
if refseq != None : <NEWLINE> <INDENT> seqlen = len ( refseq ) <NEWLINE> else : <NEWLINE> seqlen = reads . end . max ( ) <NEWLINE> f = None <NEWLINE> reads = reads [ reads . reads > cutoff ] <NEWLINE> if by is not None : <NEWLINE> reads = reads . sort_values ( by , ascending = False ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if ref_genome != <STRING> : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> allreads = utils . combine_aligned_reads ( temp_path , files , ref_genome ) <NEWLINE> new , cl = novel . find_mirnas ( allreads , cow_fasta ) <NEWLINE> new . to_csv ( os . path . join ( out , <STRING> ) , index = False ) <NEWLINE> novel . create_report ( new , cl , species , filename = os . path . join ( out , <STRING> ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
headers = self . headers ( encoded_data ) <NEWLINE> <INDENT> if headers : <NEWLINE> <INDENT> rq_kwargs [ <STRING> ] = headers <NEWLINE> <DEDENT> body = encoded_data <NEWLINE> if body : <NEWLINE> <INDENT> rq_kwargs [ <STRING> ] = data <NEWLINE> <DEDENT> <DEDENT>
def _fom ( self , data1 : Data , data2 : Data ) -> float : <NEWLINE> <INDENT> clusters1 = set ( data1 . df [ <STRING> ] . unique ( ) ) <NEWLINE> clusters2 = set ( data2 . df [ <STRING> ] . unique ( ) ) <NEWLINE> if not clusters1 == clusters2 : <NEWLINE> <INDENT> return np . nan <NEWLINE> <DEDENT> clusters = clusters1 <NEWLINE> cluster2bpoint = { } <NEWLINE> for cluster in clusters : <NEWLINE> <INDENT> bpoints1 = data1 . df [ <NEWLINE> <INDENT> ( data1 . df [ <STRING> ] == cluster ) & data1 . df [ <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> bpoints2 = data2 . df [ <NEWLINE> <INDENT> ( data1 . df [ <STRING> ] == cluster ) & data2 . df [ <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> msg = <STRING> <NEWLINE> if len ( bpoints1 ) != 1 : <NEWLINE> <INDENT> raise ValueError ( msg . format ( len ( bpoints1 ) , 1 ) ) <NEWLINE> <DEDENT> if len ( bpoints2 ) != 1 : <NEWLINE> <INDENT> raise ValueError ( msg . format ( len ( bpoints2 ) , 2 ) ) <NEWLINE> <DEDENT> bpoint1 = bpoints1 . iloc [ 0 ] [ data1 . par_cols ] <NEWLINE> bpoint2 = bpoints2 . iloc [ 0 ] [ data2 . par_cols ] <NEWLINE> cluster2bpoint [ cluster ] = ( bpoint1 , bpoint2 ) <NEWLINE> <DEDENT> return self . _fom2 ( cluster2bpoint ) <NEWLINE> <DEDENT>
hf_kw = dict ( color = light_color ) <NEWLINE> <INDENT> hf_kw . update ( hist_kwargs ) <NEWLINE> <DEDENT>
def check_dependencies ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if self . required and self . start_sequence <= 0 : <NEWLINE> <INDENT> self . logger . warn ( <STRING> ) <NEWLINE> self . required = False <NEWLINE> <COMMENT> <NL> <DEDENT> if not self . addresses : <NEWLINE> <INDENT> self . addresses = [ <STRING> ] <NEWLINE> self . logger . warn ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for container in input : <NEWLINE> <INDENT> client = LXCContainer ( container ) <NEWLINE> result . append ( client . create ( ) ) <NEWLINE> <DEDENT> return response . reply ( result , message = <STRING> . format ( input . get ( <STRING> ) ) ) <NEWLINE> except ValueError as ex : <NEWLINE> return response . reply ( message = ex . __str__ ( ) , status = 403 ) <NEWLINE> <DEDENT>
class Par2File ( object ) : <NEWLINE> <INDENT> def __init__ ( self , obj_or_path ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . path = None <NEWLINE> if isinstance ( obj_or_path , basestring ) : <NEWLINE> <INDENT> with open ( obj_or_path ) as f : <NEWLINE> <INDENT> self . contents = f . read ( ) <NEWLINE> self . path = obj_or_path <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . contents = obj_or_path . read ( ) <NEWLINE> if getattr ( obj_or_path , <STRING> , None ) : <NEWLINE> <INDENT> self . path = f . name <NEWLINE> <DEDENT> <DEDENT> self . packets = self . read_packets ( ) <NEWLINE> <DEDENT> <DEDENT>
return render ( request , <STRING> , { <NEWLINE> <INDENT> <STRING> : json_encode ( options ) , <NEWLINE> } ) <NEWLINE> <DEDENT>
def check ( ** kwargs ) : <NEWLINE> <INDENT> result = SimpleNamespace ( ok = False , time = 0 , size = None , err = None ) <NEWLINE> try : <NEWLINE> <INDENT> t = os . path . getmtime ( kwargs [ <STRING> ] ) <NEWLINE> size = os . path . getsize ( kwargs [ <STRING> ] ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> return result <NEWLINE> <DEDENT> result . time = t <NEWLINE> result . size = size <NEWLINE> if <STRING> in kwargs : <NEWLINE> <INDENT> result . ok = size > kwargs . get ( <STRING> ) <NEWLINE> if not result . ok : <NEWLINE> <INDENT> result . err = <STRING> <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> result . ok = True <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
row = 1 <NEWLINE> <INDENT> for ( msgid , message ) in messages : <NEWLINE> <INDENT> column = 0 <NEWLINE> sheet . write ( row , 0 , msgid ) <NEWLINE> column += 1 <NEWLINE> if <STRING> in options . comments : <NEWLINE> <INDENT> o = [ ] <NEWLINE> for ( entry , lineno ) in msg . occurrences : <NEWLINE> <INDENT> if lineno : <NEWLINE> <INDENT> o . append ( <STRING> % ( entry , lineno ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> o . append ( entry ) <NEWLINE> <DEDENT> <DEDENT> sheet . write ( row , column , <STRING> . join ( o ) ) <NEWLINE> column += 1 <NEWLINE> <DEDENT> if <STRING> in options . comments : <NEWLINE> <INDENT> sheet . write ( row , column , msg . comment ) <NEWLINE> column += 1 <NEWLINE> <DEDENT> if <STRING> in options . comments : <NEWLINE> <INDENT> sheet . write ( row , column , msg . tcomment ) <NEWLINE> column += 1 <NEWLINE> <DEDENT> for ( i , cat ) in enumerate ( catalogs ) : <NEWLINE> <INDENT> cat = cat [ 1 ] <NEWLINE> msg = cat . find ( msgid ) <NEWLINE> if msgid is not None : <NEWLINE> <INDENT> if <STRING> in msg . flags : <NEWLINE> <INDENT> sheet . write ( row , column , msg . msgstr , italic_style ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sheet . write ( row , column , msg . msgstr ) <NEWLINE> <DEDENT> <DEDENT> column += 1 <NEWLINE> <DEDENT> row += 1 <NEWLINE> <DEDENT> <DEDENT>
while lo <= hi : <NEWLINE> <INDENT> mid = lo + ( hi - lo ) / 2 <NEWLINE> if less ( x , a [ mid ] ) : <NEWLINE> <INDENT> hi = mid - 1 <NEWLINE> <DEDENT> elif less ( a [ mid ] , x ) : <NEWLINE> <INDENT> lo = mid + 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return mid <NEWLINE> <DEDENT> <DEDENT>
with self . assertRaises ( ImmutablePropertyException ) : <NEWLINE> <INDENT> entity . name = <STRING> <NEWLINE> <DEDENT>
for k in targets : <NEWLINE> <INDENT> item = targets [ k ] <NEWLINE> data = item [ <STRING> ] <NEWLINE> if not allgenes : <NEWLINE> <INDENT> if not <STRING> in item : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> hvg = item [ <STRING> ] [ <STRING> ] <NEWLINE> data = data [ data . index . isin ( hvg ) ] <NEWLINE> <DEDENT> elif verbose : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> if scale : <NEWLINE> <INDENT> d_scaled = sklearn_scale ( <NEWLINE> <INDENT> data . transpose ( ) , <COMMENT> <NEWLINE> axis = 0 , <COMMENT> <NEWLINE> with_mean = True , <COMMENT> <NEWLINE> with_std = True ) <COMMENT> <NEWLINE> <DEDENT> d_scaled = pd . DataFrame ( d_scaled . transpose ( ) , index = data . index ) <NEWLINE> <DEDENT> if verbose : <NEWLINE> <INDENT> v = ( method , k , data . shape [ 0 ] , data . shape [ 1 ] ) <NEWLINE> print ( <STRING> % v ) <NEWLINE> <DEDENT> if method == <STRING> : <NEWLINE> <INDENT> comp , contr = irlb ( data , ncomp , seed ) <NEWLINE> <DEDENT> elif method == <STRING> : <NEWLINE> <INDENT> comp , contr = svd ( data , ncomp ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> obj . index = data . columns <NEWLINE> obj . norm_data [ k ] [ <STRING> ] [ <STRING> ] = { <STRING> : comp , <NEWLINE> <INDENT> <STRING> : contr , <NEWLINE> <STRING> : method } <NEWLINE> <DEDENT> obj . set_assay ( sys . _getframe ( ) . f_code . co_name , method ) <NEWLINE> <DEDENT>
def verify_unit_interval ( value ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not ( value >= 0 or value <= 1 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> return value <NEWLINE> <DEDENT>
def transform ( self , df , n_feats = 10 ) : <NEWLINE> <INDENT> assert df . shape [ 1 ] <= n_feats <NEWLINE> step = self . find_set_of_size ( n_feats ) <NEWLINE> return df [ self . features_at_step ( step ) ] <NEWLINE> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def hue_gump ( cls , bitmap , hue , partial_hue ) : <NEWLINE> <INDENT> copy = bitmap . copy ( ) <NEWLINE> if hue : <NEWLINE> <INDENT> hue = ( hue & 0x3FFF ) - 1 <NEWLINE> return Hues . HUES [ hue ] . apply_to ( bitmap , only_grey_pixels = partial_hue ) <NEWLINE> <DEDENT> return copy <NEWLINE> <DEDENT> <DEDENT>
if ( board_height <= 0 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> else : <NEWLINE> self . board_height = board_width <NEWLINE> <DEDENT>
if ( board_height <= 0 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> else : <NEWLINE> self . board_height = board_width <NEWLINE> <DEDENT>
args = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : to_b64 ( long2a ( p ) ) , <NEWLINE> <STRING> : to_b64 ( long2a ( g ) ) , <NEWLINE> <STRING> : to_b64 ( long2a ( pow ( p , priv_key , p ) ) ) , <NEWLINE> } <NEWLINE> <DEDENT>
def make_app ( redis_connection_obj , port , host_url , host_name , datadir ) : <NEWLINE> <INDENT> app . register_blueprint ( rpcblueprint , url_prefix = <STRING> ) <NEWLINE> app . port = port <NEWLINE> if gevent : <NEWLINE> <INDENT> redis . connection . socket = gevent . socket <NEWLINE> <DEDENT> rpcblueprint . r = redis . StrictRedis ( host = redis_connection_obj [ <STRING> ] , <NEWLINE> <INDENT> port = redis_connection_obj [ <STRING> ] , <NEWLINE> db = redis_connection_obj [ <STRING> ] ) <NEWLINE> <DEDENT> rpcblueprint . task_queue = TaskQueue ( rpcblueprint . r ) <NEWLINE> server_manager = Servers ( rpcblueprint . r ) <NEWLINE> settings . setup_server ( rpcblueprint . r , datadir , host_url , host_name , <NEWLINE> <INDENT> Catalog ( rpcblueprint . r , datadir , host_url ) , <NEWLINE> server_manager <NEWLINE> <DEDENT> ) <NEWLINE> rpcblueprint . heartbeat_thread = HeartbeatThread ( ) <NEWLINE> return app <NEWLINE> <DEDENT>
def run ( redis_connection , node_url , node_name , queue , datadir ) : <NEWLINE> <INDENT> if node_name is None : <NEWLINE> <INDENT> node_name = node_url <NEWLINE> <DEDENT> redis_connection_obj = parse_redis_connection ( redis_connection ) <NEWLINE> r = redis . StrictRedis ( host = redis_connection_obj [ <STRING> ] , <NEWLINE> <INDENT> port = redis_connection_obj [ <STRING> ] , <NEWLINE> db = redis_connection_obj [ <STRING> ] ) <NEWLINE> <DEDENT> server_manager = Servers ( r ) <NEWLINE> settings . setup_server ( r , datadir , node_url , node_name , <NEWLINE> <INDENT> Catalog ( r , datadir , node_url ) , <NEWLINE> server_manager <NEWLINE> <DEDENT> ) <NEWLINE> if queue is None : <NEWLINE> <INDENT> queue = [ <STRING> ] <NEWLINE> <DEDENT> with Connection ( r ) : <NEWLINE> <INDENT> queues = [ ] <NEWLINE> node_queue = KitchenSinkRedisQueue ( node_url ) <NEWLINE> queues . append ( node_queue ) <NEWLINE> for q in queue : <NEWLINE> <INDENT> if <STRING> in q : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> queues . append ( KitchenSinkRedisQueue ( q ) ) <NEWLINE> queues . append ( KitchenSinkRedisQueue ( <STRING> % ( q , node_name ) ) ) <NEWLINE> w = KitchenSinkWorker ( queues , default_result_ttl = 86400 ) <NEWLINE> <DEDENT> <DEDENT> w . work ( burst = False ) <NEWLINE> <DEDENT>
if x_type == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> if start_time is None : <NEWLINE> <INDENT> start_time = 0 <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <DEDENT> if start_time is None : <NEWLINE> <INDENT> start_time = x_values [ 0 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> assert start_time >= x_values [ 0 ] , <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> x_values = _np . round ( ( x_values - start_time ) * sampling_freq , 10 ) . astype ( int ) <NEWLINE> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def from_dict ( cls , message_dict : typing . Dict [ str , typing . Any ] ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> message_dict = message_dict . only_fields ( message_dict ) <NEWLINE> intent_dict = message_dict . pop ( <STRING> , { } ) <NEWLINE> slot_dicts = message_dict . pop ( <STRING> , [ ] ) <NEWLINE> message = NluIntent ( <COMMENT> <NEWLINE> <INDENT> ** message_dict , intent = Intent ( ** intent_dict ) <NEWLINE> <DEDENT> ) <NEWLINE> message . slots = [ Slot . from_dict ( s ) for s in slot_dicts ] <NEWLINE> <DEDENT> <DEDENT>
cdl = cdl . resample ( freq ) . agg ( { <STRING> : <STRING> , <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> } ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> cdl = cdl . loc [ <NEWLINE> pytz . timezone ( NY ) . localize ( dtbegin ) : <NEWLINE> pytz . timezone ( NY ) . localize ( dtbegin ) <NEWLINE> ] . dropna ( subset = [ <STRING> ] ) <NEWLINE> records = cdl . reset_index ( ) . to_dict ( <STRING> ) <NEWLINE> for r in records : <NEWLINE> r [ <STRING> ] = r [ <STRING> ] <NEWLINE> q . put ( r ) <NEWLINE> q . put ( { } ) <COMMENT> <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if reqCookies and not <STRING> in headers : <NEWLINE> <INDENT> header = <STRING> . join ( [ <STRING> . format ( key , value ) for key , value in reqCookies . items ( ) ] ) <NEWLINE> reqHeaders [ <STRING> ] = header <NEWLINE> <DEDENT> <DEDENT>
logger . debug ( <STRING> , url ) <NEWLINE> <INDENT> if req_params : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_params ) <NEWLINE> <DEDENT> if req_headers : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_headers ) <NEWLINE> <DEDENT> if req_cookies : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_cookies ) <NEWLINE> <DEDENT> if json : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_cookies ) <NEWLINE> <DEDENT> if data : <NEWLINE> <INDENT> logger . debug ( <STRING> , data ) <NEWLINE> <DEDENT> <DEDENT>
def print_position ( lines , line_no ) : <NEWLINE> <INDENT> lines = [ l . strip ( <STRING> ) . split ( <STRING> ) for l in lines ] <NEWLINE> word = lines [ 0 ] [ 0 ] <NEWLINE> position = [ word ] <NEWLINE> for i , line in enumerate ( lines ) : <NEWLINE> <INDENT> assert line [ 0 ] == word , ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> position . extend ( line [ 1 : ] ) <NEWLINE> <DEDENT> print ( <STRING> . join ( line ) ) <NEWLINE> <DEDENT>
def zero_prefix_int ( num ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> strnum = str ( num ) <NEWLINE> if len ( strnum ) == 1 : <NEWLINE> <INDENT> return <STRING> + strnum <NEWLINE> <DEDENT> return num <NEWLINE> <DEDENT>
def chunks ( l , n ) : <NEWLINE> <INDENT> big_list = [ ] <NEWLINE> n = max ( 1 , n ) <NEWLINE> step = int ( len ( l ) / n ) <NEWLINE> for i in range ( 0 , len ( l ) , step ) : <NEWLINE> <INDENT> big_list . append ( l [ i : i + n ] ) <NEWLINE> <DEDENT> return big_list <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if gene_name not in lnc_seq : <NEWLINE> <INDENT> lnc_seq [ gene_name ] = str ( record . seq ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if len ( lnc_seq [ gene_name ] ) > len ( str ( record . seq ) ) : <NEWLINE> <INDENT> lnc_seq [ gene_name ] = str ( record . seq ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
go_terms_parents = annotation . map ( <NEWLINE> <INDENT> lambda x : list ( { term for term in x if term not in leaf_terms } ) if isinstance ( x , list ) else None ) <NEWLINE> return go_terms_parents <NEWLINE> <DEDENT>
go_terms_parents = annotation . map ( <NEWLINE> <INDENT> lambda x : list ( { term for term in x if term not in leaf_terms } ) if isinstance ( x , list ) else None ) <NEWLINE> return go_terms_parents <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def _set_cache_for ( self , name , value ) : <NEWLINE> <INDENT> self . get . _cache_set ( value , name ) <NEWLINE> <DEDENT> <DEDENT>
if not dataapi . data . exists ( type_name ) : <NEWLINE> <INDENT> dataapi . data . set ( type_name , name , value ) <NEWLINE> <DEDENT>
return x_red [ max_index ] <NEWLINE>
<COMMENT> <NL> <INDENT> self . assertEqual ( solver . update_after_step . call_count , 1 ) <NEWLINE> pd . testing . assert_series_equal ( solver_voltages , solver . update_after_step . call_args [ 0 ] [ 0 ] ) <NEWLINE> <DEDENT>
def update ( self ) : <NEWLINE> <INDENT> super ( VLibrasVideoViewlet , self ) . update ( ) <NEWLINE> self . youtube_url = get_video_url ( self . context ) <NEWLINE> self . is_ready = self . youtube_url is not None <NEWLINE> self . enabled = self . is_ready and not api . user . is_anonymous ( ) <NEWLINE> if self . is_ready : <NEWLINE> <INDENT> self . klass = <STRING> <NEWLINE> <DEDENT> <DEDENT>
def sanitise ( s , debug_name ) : <NEWLINE> <INDENT> if not isinstance ( s , basestring ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> . format ( <NEWLINE> <INDENT> debug_name , type ( s ) ) ) <NEWLINE> <DEDENT> <DEDENT> elif not regex . match ( s ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> debug_name , s , regex_str ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if isinstance ( s , unicode ) : <NEWLINE> <INDENT> s = s . encode ( <STRING> ) <NEWLINE> <DEDENT> s = escape ( s ) <NEWLINE> if max_len is None : <NEWLINE> <INDENT> if len ( s ) <= min_len : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> debug_name , min_len ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if not min_len <= len ( s ) <= max_len : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> debug_name , min_len , max_len ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return s <NEWLINE> return sanitise <NEWLINE> <DEDENT> <DEDENT>
self . _refresh_resources ( False ) <NEWLINE> <INDENT> return self . _store . get_proxy ( filter_opts , self . _blacklist ) <NEWLINE> <DEDENT>
if content_type_to_sync : <NEWLINE> <INDENT> log . info ( <STRING> % content_type_to_sync ) <NEWLINE> try : <NEWLINE> <INDENT> content_type = content_registry [ content_type_to_sync ] <NEWLINE> sync_content_type ( content_type ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> log . error ( <STRING> % content_type_to_sync ) <NEWLINE> else : <NEWLINE> <DEDENT> for content_type in content_registry . keys ( ) : <NEWLINE> <INDENT> sync_content_type ( content_type ) <NEWLINE> <DEDENT> <DEDENT>
def channel_sort ( self , queryset , channel = channel , profile = None , basekey = None , flavor = None , limit = 0 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> channel_results = self . _channel ( queryset , channel = channel , profile = profile , basekey = basekey , flavor = flavor , limit = limit , include_endorsements = True ) <NEWLINE> remainder_results = queryset . exclude ( pk__in = [ item . pk for item in channel_results ] ) <NEWLINE> final_results = channel_results + [ ContentItemWrapper ( item , 0 ) for item in remainder_results ] <NEWLINE> final_results . sort ( cmp = lambda x , y : cmp ( x . rlevel , y . rlevel ) ) <NEWLINE> return final_results <NEWLINE> <DEDENT>
scope = get_scope_at ( project , fixed_source , lineno , filename , ast_nodes ) <NEWLINE> <INDENT> if not ctx : <NEWLINE> <INDENT> names = get_scope_names ( scope , lineno ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> obj = infer ( ctx , scope , position ) <NEWLINE> names = [ obj . get_names ( ) ] <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> <DEDENT> names = ( project . get_possible_imports ( ctx , filename ) , ) <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> names = ( project . get_possible_imports ( ctx , filename ) , ) <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> names = ( <NEWLINE> <INDENT> project . get_module ( ctx , filename ) . get_names ( ) , <NEWLINE> project . get_possible_imports ( ctx , filename ) ) <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> <DEDENT> return [ ] <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> func = scope . eval ( func , False ) <NEWLINE> except : <NEWLINE> continue <NEWLINE> <DEDENT>
while length > 0 : <NEWLINE> <INDENT> if maxpoints > length : maxpoints = length <NEWLINE> data = self . _h5 [ startpos : startpos + maxpoints ] <NEWLINE> if isinstance ( self . clock , UniformClock ) : <NEWLINE> <INDENT> yield DataSegment ( self . clock [ startpos ] , UniformTimeSeries ( data , self . clock . rate ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> yield DataSegment ( 0 , TimeSeries ( self . clock [ startpos : startpos + maxpoints ] , data ) ) <NEWLINE> <DEDENT> startpos += len ( data ) <NEWLINE> length -= len ( data ) <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> if filename . endswith ( <STRING> ) or ispickle == True : <NEWLINE> <INDENT> dict = eval ( filename ) <NEWLINE> try : <NEWLINE> <INDENT> pickle . dump ( dict , filename ) <NEWLINE> <DEDENT> except pickle . PickleError as perr : <NEWLINE> <INDENT> print ( <STRING> + str ( perr ) ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> with open ( filename , <STRING> ) as fileToWrite : <NEWLINE> <INDENT> print ( filedata , file = fileToWrite ) <NEWLINE> except IOError as ioerr : <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> + str ( ioerr ) ) <NEWLINE> return <NEWLINE> <DEDENT>
def __init__ ( self , doctype , src , dst , options = None , settings = None ) : <NEWLINE> <INDENT> self . doctype = doctype <NEWLINE> self . src = src <NEWLINE> self . dst = dst <NEWLINE> self . options = options or { } <NEWLINE> self . settings = options or { } <NEWLINE> self . logfile = pathjoin ( self . dst , <STRING> ) <NEWLINE> self . status = None <NEWLINE> self . exc_info = None <NEWLINE> self . _build_func = self . _get_build_func ( ) <NEWLINE> self . _log = None <NEWLINE> <DEDENT>
if <STRING> in dir ( ) : <NEWLINE> <INDENT> pdf_file = fitz . open ( file ) <NEWLINE> while i < len ( pdf_file ) : <NEWLINE> <INDENT> text += pdf_file [ i ] . getText ( <STRING> ) <NEWLINE> i += 1 <NEWLINE> else : <NEWLINE> <DEDENT> pdf_file = open ( file , <STRING> ) <NEWLINE> pdf_reader = PyPDF2 . PdfFileReader ( file ) <NEWLINE> while i < pdf_reader . numPages : <NEWLINE> <INDENT> payload = pdf_reader . getPage ( i ) . extractText ( ) . replace ( <STRING> , <STRING> ) <NEWLINE> text += payload . encode ( <STRING> , <STRING> ) . decode ( <STRING> ) <NEWLINE> i += 1 <NEWLINE> return text <NEWLINE> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> scandir_it = scandir ( top ) <NEWLINE> except OSError as error : <NEWLINE> onerror ( error ) <NEWLINE> return <NEWLINE> <DEDENT>
def put_unconfirmed ( self , tx ) : <NEWLINE> <INDENT> assert tx . height is None , <STRING> . format ( tx ) <NEWLINE> if tx . type not in ( C . TX_POW_REWARD , C . TX_POS_REWARD ) : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> elif tx . hash in self . unconfirmed : <NEWLINE> <INDENT> logging . debug ( <STRING> . format ( tx ) ) <NEWLINE> return <NEWLINE> <DEDENT> self . unconfirmed [ tx . hash ] = tx <NEWLINE> if tx . hash in self . chained_tx : <NEWLINE> <INDENT> logging . debug ( <STRING> . format ( tx ) ) <NEWLINE> return <NEWLINE> <DEDENT> user_account . affect_new_tx ( tx ) <NEWLINE> <COMMENT> <NL> if P . NEW_CHAIN_INFO_QUE : <NEWLINE> <INDENT> P . NEW_CHAIN_INFO_QUE . put_nowait ( ( <STRING> , tx . getinfo ( ) ) ) <NEWLINE> <DEDENT> <DEDENT>
def contract_signature_check ( extra_tx : TX , v : Validator , include_block : Block ) : <NEWLINE> <INDENT> signed_cks = get_signed_cks ( extra_tx ) <NEWLINE> accept_cks = signed_cks & set ( v . validators ) <NEWLINE> reject_cks = signed_cks - set ( v . validators ) <NEWLINE> if len ( reject_cks ) > 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( reject_cks ) ) <NEWLINE> <DEDENT> elif include_block : <NEWLINE> <COMMENT> <NL> <INDENT> if len ( accept_cks ) != v . require : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> <NEWLINE> <INDENT> . format ( signed_cks , accept_cks , v . require ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> original_tx = tx_builder . get_tx ( txhash = extra_tx . hash ) <NEWLINE> if original_tx is None : <NEWLINE> <COMMENT> <NL> <INDENT> if 0 < v . require and len ( accept_cks ) == 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( signed_cks ) ) <NEWLINE> <DEDENT> if len ( accept_cks ) > v . require : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( accept_cks , v . require ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> if original_tx . height is not None : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( original_tx . height ) ) <NEWLINE> <DEDENT> if v . require == 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> ) <NEWLINE> <DEDENT> original_cks = get_signed_cks ( original_tx ) <NEWLINE> accept_new_cks = ( signed_cks - original_cks ) & set ( v . validators ) <NEWLINE> if len ( accept_new_cks ) == 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> <NEWLINE> <INDENT> . format ( signed_cks , original_cks , set ( v . validators ) ) ) <NEWLINE> <DEDENT> <DEDENT> if len ( accept_new_cks ) + len ( original_cks ) > v . require : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> <NEWLINE> <INDENT> . format ( accept_new_cks , original_cks , v . require ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def validator_fill_iter ( v : Validator , best_block = None , best_chain = None ) : <NEWLINE> <COMMENT> <NL> <INDENT> v_iter = builder . db . read_validator_iter ( c_address = v . c_address , start_idx = v . db_index ) <NEWLINE> for index , address , flag , txhash , sig_diff in v_iter : <NEWLINE> <INDENT> yield index , flag , address , sig_diff , txhash <NEWLINE> <COMMENT> <NL> <DEDENT> if best_chain : <NEWLINE> <INDENT> _best_chain = None <NEWLINE> <DEDENT> elif best_block and best_block == builder . best_block : <NEWLINE> <INDENT> _best_chain = builder . best_chain <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> dummy , _best_chain = builder . get_best_chain ( best_block = best_block ) <NEWLINE> <DEDENT> for block in reversed ( best_chain or _best_chain ) : <NEWLINE> <INDENT> for tx in block . txs : <NEWLINE> <INDENT> if tx . type != C . TX_VALIDATOR_EDIT : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> c_address , address , flag , sig_diff = decode ( tx . message ) <NEWLINE> if c_address != v . c_address : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> index = tx . height * 0xffffffff + block . txs . index ( tx ) <NEWLINE> yield index , flag , address , sig_diff , tx . hash <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if best_block is None : <NEWLINE> <INDENT> for tx in sorted ( tx_builder . unconfirmed . values ( ) , key = lambda x : x . create_time ) : <NEWLINE> <INDENT> if tx . type != C . TX_VALIDATOR_EDIT : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> c_address , address , flag , sig_diff = decode ( tx . message ) <NEWLINE> if c_address != v . c_address : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if len ( tx . signature ) < v . require : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> yield None , flag , address , sig_diff , tx . hash <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
async def new_address ( request ) : <NEWLINE> <INDENT> with closing ( create_db ( V . DB_ACCOUNT_PATH ) ) as db : <NEWLINE> <INDENT> cur = db . cursor ( ) <NEWLINE> user_name = request . query . get ( <STRING> , C . account2name [ C . ANT_UNKNOWN ] ) <NEWLINE> user_id = read_name2user ( user_name , cur ) <NEWLINE> address = create_new_user_keypair ( user_name , cur ) <NEWLINE> db . commit ( ) <NEWLINE> if user_id == C . ANT_CONTRACT : <NEWLINE> <INDENT> address = convert_address ( address , V . BLOCK_CONTRACT_PREFIX ) <NEWLINE> <DEDENT> <DEDENT> return web_base . json_res ( { <STRING> : user_name , <STRING> : user_id , <STRING> : address } ) <NEWLINE> <DEDENT>
def check_already_started ( ) : <NEWLINE> <INDENT> assert V . DB_HOME_DIR is not None <NEWLINE> <COMMENT> <NL> pid_path = os . path . join ( V . DB_HOME_DIR , <STRING> ) <NEWLINE> if os . path . exists ( pid_path ) : <NEWLINE> <INDENT> with open ( pid_path , mode = <STRING> ) as fp : <NEWLINE> <INDENT> pid = int ( fp . read ( ) ) <NEWLINE> <DEDENT> if psutil . pid_exists ( pid ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( pid ) ) <NEWLINE> <DEDENT> <DEDENT> new_pid = os . getpid ( ) <NEWLINE> with open ( pid_path , mode = <STRING> ) as fp : <NEWLINE> <INDENT> fp . write ( str ( new_pid ) ) <NEWLINE> <DEDENT> log . info ( <STRING> . format ( pid ) ) <NEWLINE> <DEDENT>
if not keystroke_history and keystroke_history [ - 1 ] == <STRING> : <NEWLINE>
@ contextmanager <NEWLINE> <INDENT> def timer ( name = <STRING> ) : <NEWLINE> <INDENT> start = time ( ) <NEWLINE> try : <NEWLINE> <INDENT> yield <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> duration = time ( ) - start <NEWLINE> if name : <NEWLINE> <INDENT> logging . warning ( <STRING> , duration ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logging . warning ( <STRING> , name , duration ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _discover_run ( self , packages , missing = True ) : <NEWLINE> <COMMENT> <NL> <INDENT> def formatPathPrint ( path , line = None ) : <NEWLINE> <INDENT> if not line : <NEWLINE> <INDENT> line = 1 <NEWLINE> <DEDENT> path = os . path . realpath ( path ) <NEWLINE> return <STRING> % ( path , line ) <NEWLINE> <DEDENT> total , failed = [ ] , [ ] <NEWLINE> s = SmokeTestDiscover ( ) <NEWLINE> for pkg_pth in packages : <NEWLINE> <INDENT> pkg = importlib . import_module ( self . _path_to_modstr ( pkg_pth ) ) <NEWLINE> <COMMENT> <NL> t , f = s . discover_run ( pkg ) <NEWLINE> total += t <NEWLINE> failed += f <NEWLINE> <COMMENT> <NL> if missing : <NEWLINE> <INDENT> for m in s . get_missing ( pkg ) : <NEWLINE> <INDENT> pth = m . __file__ <NEWLINE> if pth . endswith ( <STRING> ) : <NEWLINE> <INDENT> pth = f [ : - 1 ] <NEWLINE> <DEDENT> s . log ( <STRING> % m ) <NEWLINE> s . log ( formatPathPrint ( pth ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> return total , failed <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for step in steps : <NEWLINE> <COMMENT> <NL> <INDENT> if <STRING> in steps and step == <STRING> : <NEWLINE> <INDENT> job_name = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> else : <NEWLINE> <INDENT> job_name = step + <STRING> <NEWLINE> <DEDENT> pipeline . write ( self . get_template ( slurm , prev_step , job_name , step ) ) <NEWLINE> if <STRING> in step or step not in self . softwares : <NEWLINE> <INDENT> prev_step = self . task_names [ step ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
