<COMMENT> <NL> <COMMENT> <NL> <INDENT> ax_groupdelay = ax_power . twinx ( ) <NEWLINE> <COMMENT> <NL> ax_groupdelay . plot ( omega / 2 / np . pi , <NEWLINE> <INDENT> - np . gradient ( np . unwrap ( <NEWLINE> <INDENT> np . angle ( transferfunc ) ) ) / np . gradient ( omega ) * 1000 , <NEWLINE> <DEDENT> <STRING> ) <NEWLINE> <DEDENT> ax_power . set_title ( <STRING> ) <NEWLINE> ax_power . set_ylabel ( <STRING> , color = <STRING> ) <NEWLINE> <COMMENT> <NL> ax_groupdelay . set_ylabel ( <STRING> , color = <STRING> ) <NEWLINE> ax_groupdelay . set_xlabel ( <STRING> ) <NEWLINE> ax_power . set_xscale ( <STRING> ) <NEWLINE> ax_power . set_xlim ( [ 20 , 20000 ] ) <NEWLINE> ax_power . xaxis . set_major_formatter ( <NEWLINE> <INDENT> matplotlib . ticker . FormatStrFormatter ( <STRING> ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> ax_groupdelay . set_xscale ( <STRING> ) <NEWLINE> ax_groupdelay . xaxis . set_major_formatter ( <NEWLINE> <INDENT> matplotlib . ticker . FormatStrFormatter ( <STRING> ) ) <NEWLINE> <DEDENT> ax_power . grid ( True , which = <STRING> , color = <STRING> , ls = <STRING> ) <NEWLINE> <COMMENT> <NL> ax_groupdelay . grid ( True , which = <STRING> , color = <STRING> , ls = <STRING> ) <NEWLINE> for tlabel in ax_power . get_yticklabels ( ) : <NEWLINE> <INDENT> tlabel . set_color ( <STRING> ) <NEWLINE> <DEDENT> for tlabel in ax_groupdelay . get_yticklabels ( ) : <NEWLINE> <INDENT> tlabel . set_color ( <STRING> ) <NEWLINE> <DEDENT> align_y_axis ( ax_power , ax_groupdelay , 1 , .1 ) <NEWLINE> <COMMENT> <NL> plotwidget . draw ( ) <NEWLINE> <DEDENT>
def load_dataset ( self ) : <NEWLINE> <INDENT> if ( self . _source is None ) : <NEWLINE> <INDENT> self . _data_set = self . _source . retrieve_dataset ( ) <NEWLINE> self . _is_load_information = True <NEWLINE> <DEDENT> <DEDENT>
dgt = asyncdgt . auto_connect ( port_globs , loop ) <NEWLINE>
fields = data . get ( <STRING> , list ( ) ) <NEWLINE> <INDENT> for ( field ) in fields : <NEWLINE> <INDENT> field = Field . from_dict ( field ) <NEWLINE> fragment . add_field ( fields ) <NEWLINE> <DEDENT> <DEDENT>
return topic <NEWLINE>
if is_url ( input_value ) : <NEWLINE> <INDENT> detected_type = <STRING> <NEWLINE> new_actions . append ( set_input_type ( detected_type ) ) <NEWLINE> new_actions . append ( add_task ( FETCH_HTTP_NODE , url = input_value ) ) <NEWLINE> new_actions . append ( set_validation_subject ( input_value ) ) <NEWLINE> elif input_is_json ( input_value ) : <NEWLINE> id_url = find_id_in_jsonld ( input_value , options . get ( <STRING> , jsonld_use_cache ) ) <NEWLINE> if is_url ( id_url ) : <NEWLINE> <INDENT> detected_type = <STRING> <NEWLINE> new_actions . append ( store_input ( id_url ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> detected_type = <STRING> <NEWLINE> <DEDENT> new_actions . append ( set_input_type ( detected_type ) ) <NEWLINE> if detected_type == <STRING> : <NEWLINE> <INDENT> new_actions . append ( add_task ( FETCH_HTTP_NODE , url = id_url ) ) <NEWLINE> new_actions . append ( set_validation_subject ( input_value ) ) <NEWLINE> elif input_is_jws ( input_value ) : <NEWLINE> <DEDENT> detected_type = <STRING> <NEWLINE> new_actions . append ( set_input_type ( detected_type ) ) <NEWLINE> new_actions . append ( add_task ( PROCESS_JWS_INPUT , data = input_value ) ) <NEWLINE> else : <NEWLINE> raise NotImplementedError ( <STRING> ) <NEWLINE> <DEDENT>
def collision_free ( object1 , object2 , car_scale_factor , pedestrian_scale_factor ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> object1_vertices , x , y , radius = get_bounding_box ( object1 , car_scale_factor , pedestrian_scale_factor ) <NEWLINE> object2_vertices , x2 , y2 , radius2 = get_bounding_box ( object1 , car_scale_factor , pedestrian_scale_factor ) <NEWLINE> <DEDENT>
self . set_title ( page , utils . _remove_ext ( obj ) ) <NEWLINE>
poss = [ ] <NEWLINE> <INDENT> height , pos = left_peak_height_pos ( mmr_size ) <NEWLINE> poss . append ( pos ) <NEWLINE> while height > 0 : <NEWLINE> <INDENT> height , pos = get_right_peak ( height , pos , mmr_size ) <NEWLINE> if height > 0 : <NEWLINE> <INDENT> poss . append ( pos ) <NEWLINE> <DEDENT> <DEDENT> return poss <NEWLINE> <DEDENT>
def random_interval ( ) : <NEWLINE> <INDENT> now = datetime . now ( ) <NEWLINE> start = now - timedelta ( days = random . randint ( 0 , 365 * 3 ) ) <NEWLINE> end = start - timedelta ( days = random . randint ( 1 , 10 ) ) <NEWLINE> return start . isoformat ( ) , end . isoformat ( ) <NEWLINE> <DEDENT>
logs . append ( logs ) <NEWLINE>
data_files [ location ] [ parameter ] = filename <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> data_files [ location ] [ parameter ] = None <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> with rasterio . open ( dst ) as f : <NEWLINE> <INDENT> geometry = util . bbox2poly ( f . bounds . left , f . bounds . right , f . bounds . bottom , f . bounds . top , as_shapely = True ) <NEWLINE> <DEDENT> update_metadata ( feature , quest_metadata = { <STRING> : geometry . to_wkt ( ) } ) <NEWLINE> <DEDENT>
snap = options . get ( <STRING> ) <NEWLINE> <INDENT> if snap is not None : <NEWLINE> <INDENT> if snap . lower ( ) == <STRING> : <NEWLINE> <INDENT> proj_points = snap_points_max_flow_acc ( flow_accumulation , proj_points , options . get ( <STRING> ) ) <NEWLINE> <DEDENT> if snap . lower ( ) == <STRING> : <NEWLINE> <INDENT> stream_threshold_pct = options . get ( <STRING> ) <NEWLINE> stream_threshold_abs = options . get ( <STRING> ) <NEWLINE> outlet_points = snap_points_jenson ( flow_accumulation , proj_points , <NEWLINE> <INDENT> stream_threshold_pct = stream_threshold_pct , stream_threshold_abs = stream_threshold_abs ) <NEWLINE> <DEDENT> <DEDENT> if p . is_latlong ( ) : <NEWLINE> <INDENT> snapped_points = [ src . xy ( * point ) for point in proj_points ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> snapped_points = [ src . xy ( * p ( * point , inverse = True ) ) for point in proj_points ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
collection_path = os . path . join ( project_path , feature_metadata [ <STRING> ] ) <NEWLINE>
def readReg ( self , addr ) : <NEWLINE> <INDENT> return self . spi . xfer ( [ addr | 0x7F , 0 ] ) <NEWLINE> <DEDENT>
model_store = resource . ModelStore ( complete_conf ) <NEWLINE> <INDENT> model_store . dump_trained_model ( model_conf , model , metrics ) <NEWLINE> <DEDENT>
def is_valid ( data , explain = False ) : <NEWLINE> <INDENT> if not explain : <NEWLINE> <INDENT> return predicate ( data ) <NEWLINE> <DEDENT> elems = list ( data ) <NEWLINE> valid , explanation = predicate ( elems , explain = True ) <NEWLINE> return ( True , explanation ) if valid else ( False , { <NEWLINE> <INDENT> elems [ i ] : value for i , value in explanation . items ( ) <NEWLINE> <DEDENT> } ) <NEWLINE> return is_if ( is_set , predicate , else_valid = False ) <NEWLINE> <DEDENT>
pool = Pool ( 50 ) <NEWLINE> <INDENT> i = 0 <NEWLINE> with ProgressBar ( max_value = len ( self . dataframe ) ) as p : <NEWLINE> <INDENT> for _ in pool . imap_unordered ( save , self . iter ( ) ) : <NEWLINE> <INDENT> i += 1 <NEWLINE> if i % 10 == 0 : <NEWLINE> <INDENT> pool . update ( i ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if self . parent is not None : <NEWLINE> <INDENT> for k in self . parent . keys ( ) : <NEWLINE> <INDENT> if k in _keys_set : <NEWLINE> <INDENT> _keys_set . add ( k ) <NEWLINE> yield k <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not ( path . exists ( ) and path . suffix ) : <NEWLINE> <INDENT> for ext in Node . _strip_exts : <NEWLINE> <INDENT> new_path = path . with_suffix ( ext ) <NEWLINE> if new_path . exists ( ) : <NEWLINE> <INDENT> return str ( new_path ) <NEWLINE> elif path . is_dir ( ) : <NEWLINE> <DEDENT> <DEDENT> new_path = pathlib . Path ( path , Node . _index_file ) <NEWLINE> if new_path . exists ( ) : <NEWLINE> <INDENT> return str ( new_path ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> recruit += t_count <NEWLINE> <DEDENT>
fe = fg . add_entry ( ) <NEWLINE> <INDENT> fe . author ( name = page_id , email = <STRING> % page_id ) <NEWLINE> fe . id ( post_url ) <NEWLINE> fe . link ( href = post_url , rel = <STRING> ) <NEWLINE> fe . published ( timestamp ) <NEWLINE> fe . updated ( timestamp ) <NEWLINE> fe . title ( <STRING> . join ( txt . split ( <STRING> ) [ : 15 ] ) ) <NEWLINE> fe . content ( entry_content , type = <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if reason and node . metadata and reason . extras and set ( reason . extras ) & node . extras : <NEWLINE> <INDENT> metadata_to_apply = node . metadata <NEWLINE> <DEDENT> <DEDENT>
def _validate_oid ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> if isinstance ( getattr ( self , <STRING> ) , uuid . UUID ) : <NEWLINE> <INDENT> return <STRING> . format ( type ( getattr ( self , <STRING> ) ) , getattr ( self , <STRING> ) ) <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> value = __next__ ( self ) <NEWLINE> finally : <NEWLINE> self . _started_iteration = True <NEWLINE> bound_method = __next__ . __get__ ( self . __class__ , self ) <NEWLINE> self . __next__ = bound_method <COMMENT> <NEWLINE> return value <NEWLINE> <DEDENT>
if not self . dry_run : <NEWLINE> <INDENT> if os . path . isfile ( self . imagefile ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( self . imagefile ) ) <NEWLINE> <DEDENT> <DEDENT>
if epoch_start_time < epoch_end_time : <NEWLINE> <INDENT> raise Exception ( INVALID_DATES ) <NEWLINE> <DEDENT>
def act_cz ( self , control , target ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> control = 1 << control <NEWLINE> target = 1 << control <NEWLINE> for i in xrange ( self . d ) : <NEWLINE> <INDENT> if ( i & control ) and ( i & target ) : <NEWLINE> <INDENT> self . state [ i , 0 ] *= - 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
ci = self . get_connection_info ( a , b ) <NEWLINE> <INDENT> if ci [ <STRING> ] and not clifford . is_diagonal ( self . node [ a ] [ <STRING> ] ) : <NEWLINE> <INDENT> debug ( <STRING> ) <NEWLINE> self . remove_vop ( b , a ) <NEWLINE> <DEDENT> <DEDENT>
if mode == <STRING> : <NEWLINE> <INDENT> chunk_info = data [ <STRING> ] <NEWLINE> self . id_ = chunk_info [ <STRING> ] <NEWLINE> self . head = chunk_info [ <STRING> ] <NEWLINE> self . dep = chunk_info [ <STRING> ] <NEWLINE> self . chunk_head = chunk_info [ <STRING> ] <NEWLINE> self . chunk_func = chunk_info [ <STRING> ] <NEWLINE> self . links = [ <NEWLINE> <INDENT> Reshape ( mode = <STRING> , data = link ) <NEWLINE> for link in chunk_info [ <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> self . predicate = chunk_info [ <STRING> ] if <STRING> in data else [ ] <NEWLINE> <DEDENT>
return build_dataframe ( raw_data , aggregateby , groupby ) <NEWLINE>
def formfield ( self , ** kwargs ) : <NEWLINE> <INDENT> defaults = { <NEWLINE> <INDENT> <STRING> : self . max_length , <NEWLINE> <STRING> : self . min_length , <NEWLINE> <DEDENT> } <NEWLINE> defaults . update ( kwargs ) <NEWLINE> return super ( RandomStringFieldBase , self ) . formfield ( ** kwargs ) <NEWLINE> <DEDENT>
for x in etree . iter ( ) : <NEWLINE> <INDENT> if x . tag == <STRING> : <NEWLINE> <INDENT> self . name = x . attrib [ <STRING> ] <NEWLINE> <DEDENT> if x . tag == <STRING> : <NEWLINE> <INDENT> if x . attrib [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> self . return_signature = x . attrib [ <STRING> ] <NEWLINE> <DEDENT> elif x . attrib [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> self . arg_signatures . append ( x . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> if flags_obj . no_gpu or tf . test . is_gpu_available ( ) : <NEWLINE> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if flags_obj . data_format is not None : <NEWLINE> <INDENT> data_format = flags_obj . data_format <NEWLINE> <DEDENT> print ( <STRING> % ( device , data_format ) ) <NEWLINE> <DEDENT>
policy_loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = memory . actions , <NEWLINE> <INDENT> logits = logits ) <NEWLINE> policy_loss *= tf . stop_gradient ( advantage ) <NEWLINE> policy_loss = 0.01 * entropy <NEWLINE> total_loss = tf . reduce_mean ( ( 0.5 * value_loss + policy_loss ) ) <NEWLINE> return total_loss <NEWLINE> <DEDENT>
for f in w . required_fields : <NEWLINE> <INDENT> f_name = f . replace ( <STRING> , <STRING> ) <NEWLINE> setattr ( SingleForm , f_name , StringField ( f_name ) ) <NEWLINE> <DEDENT>
def login ( request ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> manipulator = AuthenticationForm ( request ) <NEWLINE> redirect_to = request . REQUEST . get ( REDIRECT_FIELD_NAME , <STRING> ) <NEWLINE> if request . POST : <NEWLINE> <INDENT> errors = manipulator . get_validation_errors ( request . POST ) <NEWLINE> if not errors : <NEWLINE> <COMMENT> <NL> <INDENT> if not redirect_to or <STRING> in redirect_to or <STRING> in redirect_to : <NEWLINE> <INDENT> redirect_to = <STRING> <NEWLINE> <DEDENT> request . session [ users . SESSION_KEY ] = manipulator . get_user_id ( ) <NEWLINE> return HttpResponseRedirect ( redirect_to ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> errors = { } <NEWLINE> <DEDENT> response = HttpResponse ( ) <NEWLINE> response . session . set_test_cookie ( ) <NEWLINE> t = template_loader . get_template ( <STRING> ) <NEWLINE> c = Context ( request , { <NEWLINE> <INDENT> <STRING> : formfields . FormWrapper ( manipulator , request . POST , errors ) , <NEWLINE> REDIRECT_FIELD_NAME : redirect_to , <NEWLINE> <STRING> : sites . get_current ( ) . name , <NEWLINE> <DEDENT> } ) <NEWLINE> response . write ( t . render ( c ) ) <NEWLINE> return response <NEWLINE> <DEDENT>
def test_cache_page_old_style ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def my_view ( request ) : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> my_view_cached = cache_page ( 123 , my_view ) <NEWLINE> self . assertEqual ( my_view_cached ( HttpRequest ( ) ) , <STRING> ) <NEWLINE> <DEDENT>
def visit_binary_expr ( self , expr ) : <NEWLINE> <INDENT> left = expr . left . accept ( self ) <NEWLINE> right = expr . right . accept ( self ) <NEWLINE> if left is expr . left and right is expr . right : <NEWLINE> <INDENT> return expr <NEWLINE> <DEDENT> return Make . binary_op ( left , right , expr . op ) <NEWLINE> <DEDENT>
def request ( self , verb , method , ** kwargs ) : <NEWLINE> <INDENT> verb = verb . upper ( ) <NEWLINE> request_kwargs = { } <NEWLINE> if method == <STRING> : <NEWLINE> <INDENT> request_kwargs [ <STRING> ] = kwargs <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> request_kwargs [ <STRING> ] = kwargs <NEWLINE> <DEDENT> url = self . config [ <STRING> ] + method <NEWLINE> logger . debug ( <STRING> % ( verb , url ) ) <NEWLINE> r = self . requester . request ( verb , url , ** request_kwargs ) <NEWLINE> if r . status_code != 200 : <NEWLINE> <INDENT> raise APIError ( r . status_code ) <NEWLINE> <DEDENT> return r . json ( ) <NEWLINE> <DEDENT>
def _step ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> keys_raw , self . data = next ( self . it , ( <STRING> , <STRING> ) ) <NEWLINE> keys = keylib . KeyList . from_raw ( self . prefix , keys_raw ) <NEWLINE> self . keys = keys <NEWLINE> return keys is not None <NEWLINE> <DEDENT>
def _encode ( self , s ) : <NEWLINE> <INDENT> return keylib . packs ( self . prefix , s ) <NEWLINE> <DEDENT>
writer = self . null <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> if not key in self . map : <NEWLINE> <INDENT> writer = Subject ( ) <NEWLINE> self . map [ key ] = value <NEWLINE> fireNewMapEntry = True <NEWLINE> except Exception as e : <NEWLINE> <DEDENT> <DEDENT> self . onError ( e ) <NEWLINE> return <NEWLINE> <DEDENT>
validator = ProjectValidator ( ) <NEWLINE> <INDENT> data = validator . deserialize ( validator ) <NEWLINE> <DEDENT>
@ blueprint . route ( <STRING> , methods = [ <STRING> , <STRING> ] ) <NEWLINE> <INDENT> def update ( slug , name ) : <NEWLINE> <INDENT> project = object_or_404 ( Project . by_slug ( slug ) ) <NEWLINE> authz . require ( authz . project_manage ( project ) ) <NEWLINE> schema = object_or_404 ( Schema . by_name ( project , name ) ) <NEWLINE> data = request_data ( { <STRING> : project } ) <NEWLINE> project = schemata . save ( data , schema = schema ) <NEWLINE> db . session . commit ( ) <NEWLINE> return jsonify ( schemata . to_rest ( schema ) ) <NEWLINE> <DEDENT> <DEDENT>
if self . _forces is None : <NEWLINE> <COMMENT> <NL> <INDENT> if self . parallel != 0 : <NEWLINE> <INDENT> with Pool ( processes = self . parallel ) as pool : <NEWLINE> <INDENT> image_number = len ( self . images ) <NEWLINE> par_images = pool . map ( self . par_calc , range ( image_number ) ) <NEWLINE> <STRING> <NEWLINE> self . images = par_images <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> [ image . calc_energy_and_forces ( ) for image in self . images ] <NEWLINE> <DEDENT> <DEDENT>
def track_root ( self , atoms , coords ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . store_wfo_data ( atoms , coords ) <NEWLINE> <COMMENT> <NL> old_root = self . root <NEWLINE> if self . calc_counter >= 1 : <NEWLINE> <INDENT> last_two_coords = self . wfow . last_two_coords <NEWLINE> self . root = self . wfow . track ( old_root = self . root ) <NEWLINE> if self . root != old_root : <NEWLINE> <INDENT> self . log ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_geoms ( xyz_fns , idpp = False , between = 0 , dump = False , multiple_geoms = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if isinstance ( xyz_fns , str ) and xyz_fns . endswith ( <STRING> ) : <NEWLINE> <INDENT> geoms = [ geom_from_xyz_file ( xyz_fns ) , ] <NEWLINE> <COMMENT> <NL> <DEDENT> elif len ( xyz_fns ) == 1 and xyz_fns [ 0 ] . endswith ( <STRING> ) : <NEWLINE> <INDENT> geoms = geoms_from_trj ( xyz_fns [ 0 ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> elif isinstance ( xyz_fns , str ) and xyz_fns . endswith ( <STRING> ) : <NEWLINE> <INDENT> geoms = geoms_from_trj ( xyz_fns ) <NEWLINE> <DEDENT> elif multiple_geoms : <NEWLINE> <INDENT> geoms = geoms_from_trj ( xyz_fns [ 0 ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> else : <NEWLINE> <INDENT> geoms = [ geom_from_xyz_file ( fn ) for fn in xyz_fns ] <NEWLINE> <DEDENT> <DEDENT>
ax2 . plot ( max_forces , ** ax_kwargs ) <NEWLINE> <INDENT> ax2 . set_title ( <STRING> ) <NEWLINE> ax2 . set_xlabel ( <STRING> ) <NEWLINE> ax2 . set_ylabel ( <STRING> ) <NEWLINE> <DEDENT>
inp = make_input ( ** inp_kwargs ) <NEWLINE> <INDENT> inp_fn = <STRING> <NEWLINE> with open ( inp_fn , <STRING> ) as handle : <NEWLINE> <INDENT> handle . write ( inp_fn ) <NEWLINE> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if cs_alpha : <NEWLINE> <INDENT> base_img = img . copy ( ) <NEWLINE> base_img . paste ( img , mask = cs_alpha ) <NEWLINE> img = base_img <NEWLINE> <DEDENT> <DEDENT>
src_lists = [ ] <NEWLINE> <INDENT> for s in arguments : <NEWLINE> <INDENT> key , value = s . split ( <STRING> ) <NEWLINE> gl = re . match ( <STRING> , s ) <NEWLINE> if <STRING> in value : <NEWLINE> <INDENT> possible_values = value . split ( <STRING> ) <NEWLINE> <DEDENT> elif <STRING> in value : <NEWLINE> <INDENT> possible_values = range ( * [ int ( v ) for v in value . split ( <STRING> ) ] ) <NEWLINE> <DEDENT> elif gl : <NEWLINE> <INDENT> possible_values = list ( glob . glob ( gl [ 1 ] , recursive = True ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> possible_values = [ value ] <NEWLINE> <DEDENT> src_lists . append ( [ <STRING> . format ( key , val ) for val in possible_values ] ) <NEWLINE> <DEDENT> <DEDENT>
data = np . append ( dop , po4 , axis = 0 ) <NEWLINE> <INDENT> util . io . save_npy ( npy_file , data , make_read_only = True , create_path_if_not_exists = True ) <NEWLINE> <DEDENT>
assert po4_var . units == measurements . po4 . wod . constants . PO4_UNIT <NEWLINE> <INDENT> po4 = z_var . data <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> cm = matplotlib . pyplot . cm . winter_r <NEWLINE> util . plot . data ( data , file , land_value = 0 , power_limit = 10 , colormap = cm ) <NEWLINE> <DEDENT>
def _plot_map ( data , lsm , file , layer = None , v_min = None , v_max = None , use_log_scale = False , colorbar_kwargs = { <STRING> : 0.021 , <STRING> : 0.05 , <STRING> : 20 , <STRING> : <STRING> } ) : <NEWLINE> <INDENT> data = lsm . insert_index_values_in_map ( data , no_data_value = np . inf ) <NEWLINE> if layer is not None : <NEWLINE> <INDENT> data = data [ : , : , : , layer ] <NEWLINE> data = data . reshape ( data . shape + ( 1 , ) ) <NEWLINE> <DEDENT> file = _prepare_filename ( file , lsm ) <NEWLINE> util . plot . data ( data , file , no_data_value = np . inf , v_min = v_min , v_max = v_max , use_log_scale = use_log_scale , contours = False , colorbar = True , power_limit = 0 , colorbar_kwargs = colorbar_kwargs ) <NEWLINE> <DEDENT>
def push_to_list ( self , queue , instance , trim = 500 , redis_conn = None , bump = True , site = None ) : <NEWLINE> <INDENT> backend = RedisBackend ( conn = redis_conn ) <NEWLINE> key = self . get_key ( queue , site = site ) <NEWLINE> current_list = backend . get_ids ( queue ) <NEWLINE> known_length = len ( current_list ) + 1 <NEWLINE> if bump : <NEWLINE> <INDENT> if instance . pk in current_list : <NEWLINE> <INDENT> backend . remove ( key , instance . pk ) <NEWLINE> known_length -= 1 <NEWLINE> <DEDENT> <DEDENT> backend . add ( key , instance . pk ) <NEWLINE> if trim and known_length > trim : <NEWLINE> <INDENT> backend . trim ( key , trim ) <NEWLINE> <DEDENT> <DEDENT>
relations [ name ] = getattr ( self , method_name , None ) <NEWLINE> <INDENT> return relations <NEWLINE> <DEDENT>
def _cmp_value_to_nodes ( base_op , first , second ) : <NEWLINE> <INDENT> node_values = set ( [ number ( node ) for node in second ] ) <NEWLINE> first = number ( first ) <NEWLINE> verbose_print ( <STRING> . format ( len ( node_values ) , second ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> unique_ids_query = prepare_unique_query ( session , section ) <NEWLINE> for unique_id in unique_ids_query : <NEWLINE> <INDENT> db_contour_unique = session . query ( Contour ) . get ( contour_A_id ) <NEWLINE> unique_reconstruct_contour = section . contours [ db_contour_unique . index ] <NEWLINE> unique_dict = prepare_contour_dict_for_frontend ( <NEWLINE> <INDENT> unique_reconstruct_contour , <NEWLINE> unique_id , <NEWLINE> section , <NEWLINE> keep = True <NEWLINE> <DEDENT> ) <NEWLINE> section_matches [ <STRING> ] . append ( unique_dict ) <NEWLINE> <DEDENT> return section_matches <NEWLINE> <DEDENT>
def onverbalization ( e ) : <NEWLINE> <INDENT> for t in e : <NEWLINE> <INDENT> text = pr2 . knowledge [ <STRING> % t ] [ 0 ] <NEWLINE> logger . warning ( <STRING> % text ) <NEWLINE> pr2 . say ( t ) <NEWLINE> <DEDENT> <DEDENT>
def add_metric ( tensor , name = None ) : <NEWLINE> <INDENT> return tf . add_to_collection ( <NEWLINE> <INDENT> METRICS , <NEWLINE> tensor if name is None else util . rename ( name , tensor ) ) <NEWLINE> <DEDENT> <DEDENT>
return ( predictions , <NEWLINE> <INDENT> loss + l2_regularization_loss ( regularization_scale ) , <NEWLINE> train . minimize ( loss ) , <NEWLINE> _evaluate ( predictions , true_label ) ) <NEWLINE> <DEDENT>
def atstart ( self , received ) : <NEWLINE> <INDENT> file = <STRING> . format ( __file__ , <STRING> ) <NEWLINE> if hasattr ( self , <STRING> ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> self . mlogger . debug ( <STRING> . format ( file ) ) <NEWLINE> self . file = open ( file , <STRING> ) <NEWLINE> <DEDENT>
if len ( fields ) > 0 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT>
def dependencies_iterator ( xs ) : <NEWLINE> <INDENT> if hasattr ( xs , <STRING> ) : <NEWLINE> <INDENT> for k , v in xs . items ( ) : <NEWLINE> <INDENT> yield k , v <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> for e in xs : <NEWLINE> <INDENT> yield k , None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
generating = Generating ( config ) <NEWLINE> <INDENT> for c in params [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> if <STRING> not in params [ c ] : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> . format ( c ) ) <NEWLINE> sys . stderr . flush ( ) <NEWLINE> skip = <STRING> == sys . stdin . readline ( ) . strip ( ) . lower ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> skip = params [ c ] [ <STRING> ] <NEWLINE> <DEDENT> if skip : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> . format ( c ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> generating . generate ( params [ c ] , args . dst ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> setup_path = os . path . join ( path , <STRING> ) <NEWLINE> if not os . path . exists ( setup_path ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> with open ( setup_path , <STRING> ) as f : <NEWLINE> <INDENT> new_setup = f . read ( ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> old_version = re . search ( <STRING> , new_setup ) . group ( 0 ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> old_version = re . sub ( <STRING> , <STRING> , old_version ) <NEWLINE> old_version = re . sub ( <STRING> , <STRING> , old_version ) <NEWLINE> old_version = re . sub ( <STRING> , <STRING> , old_version ) <NEWLINE> if version <= old_version : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( version , old_version ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
return_key_values = _get_versions ( ) <NEWLINE> <INDENT> return_key_values [ <STRING> ] = pieces [ <STRING> ] <NEWLINE> return return_key_values <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for key , value in dart . prior_kwargs . items ( ) : <NEWLINE> <INDENT> if key == <STRING> : kick_sigma = value <NEWLINE> if key == <STRING> : M1_alpha = value <NEWLINE> if key == <STRING> : M1_min = value <NEWLINE> if key == <STRING> : M1_min = value <NEWLINE> if key == <STRING> : M2_min = value <NEWLINE> if key == <STRING> : a_min = value <NEWLINE> if key == <STRING> : a_max = value <NEWLINE> if key == <STRING> : t_min = value <NEWLINE> if key == <STRING> : t_max = value <NEWLINE> if key == <STRING> : mass_function = value <NEWLINE> <DEDENT> <DEDENT>
if ntemps != 1 or ntemps is not None : <NEWLINE> <INDENT> if ln_likelihood_function is None : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sys . exit ( - 1 ) <NEWLINE> <DEDENT> <DEDENT>
shutil . copyfile ( os . path . join ( builddir , SO_NAME ) , <NEWLINE> <INDENT> os . path . join ( build_lib , <STRING> , SO_NAME ) ) <NEWLINE> shutil . copyfile ( os . path . join ( builddir , HEADER_NAME ) , <NEWLINE> os . path . join ( build_lib , <STRING> , os . path . basename ( HEADER_NAME ) ) ) <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT>
def __city_and_province ( self ) : <NEWLINE> <INDENT> if self . city . isNotEmpty ( ) and self . province . isNotEmpty ( ) : <NEWLINE> <INDENT> if not self . city . isBlong ( self . province . name ) : <NEWLINE> <INDENT> if self . city . precision > self . province . precision : <NEWLINE> <INDENT> self . province . name = self . city . belong <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . city . reset ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif self . city . isNotEmpty ( ) and self . province . isEmpty ( ) : <NEWLINE> <INDENT> self . province . name = self . city . belong <NEWLINE> <DEDENT> <DEDENT>
if status_code in range ( 200 , 299 ) : <NEWLINE> <INDENT> return text <NEWLINE> <DEDENT>
if init_progress : <NEWLINE> <INDENT> print ( <STRING> . format ( t2 - t1 ) ) <NEWLINE> <DEDENT>
x0 = self . _initialize_params_triangulation ( <NEWLINE> <INDENT> p3ds_med , constraints , constraints_weak ) <NEWLINE> <DEDENT>
def _get_single_mod ( self , modno , mod_slices ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> mod_data = self . _data [ modno ] <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> if modno > self . _nmodules : <NEWLINE> <INDENT> raise IndexError ( modno ) <NEWLINE> <DEDENT> mod_data = np . full ( self . _mod_shape , self . _fillvalue , self . dtype ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , host , port , verify , cert_path , pkey_path , pkey_passphrase = <STRING> ) : <NEWLINE> <INDENT> self . host = host <NEWLINE> self . port = port <NEWLINE> try : <NEWLINE> <INDENT> self . pkey = crypto . load_privatekey ( crypto . FILETYPE_PEM , open ( pkey_path , <STRING> ) . read ( ) , pkey_passphrase ) <NEWLINE> <DEDENT> except IOError : <NEWLINE> <INDENT> raise eStreamerKeyError ( <STRING> . format ( pkey_path ) ) <NEWLINE> <DEDENT> except crypto . Error : <NEWLINE> <INDENT> raise eStreamerKeyError ( <STRING> . format ( cert_path ) ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> self . cert = crypto . load_certificate ( crypto . FILETYPE_PEM , open ( cert_path , <STRING> ) . read ( ) ) <NEWLINE> <DEDENT> except IOError : <NEWLINE> <INDENT> raise eStreamerCertError ( <STRING> . format ( cert_path ) ) <NEWLINE> <DEDENT> except crypto . Error : <NEWLINE> <INDENT> raise eStreamerCertError ( <STRING> . format ( cert_path ) ) <NEWLINE> <DEDENT> self . verify = verify <NEWLINE> self . ctx = None <NEWLINE> self . sock = None <NEWLINE> self . _bytes = None <NEWLINE> <DEDENT>
if dataDimensions : <NEWLINE> <INDENT> dataDimensions = list ( map ( lambda x : <STRING> % ( x ) , dataDimensions ) ) <NEWLINE> <DEDENT>
if not self . model . onModelLoad ( loadedState [ <STRING> ] ) : <NEWLINE> <INDENT> loaded = loadedState [ <STRING> ] <NEWLINE> current = self . model . onModelSave ( ) <NEWLINE> Str = <STRING> % ( loaded , current ) <NEWLINE> for key in set ( list ( loaded . keys ( ) ) + list ( current . keys ( ) ) ) : <NEWLINE> <INDENT> if not key in current : <NEWLINE> <INDENT> Str += <STRING> % ( key ) <NEWLINE> continue <NEWLINE> <DEDENT> if not key in loaded : <NEWLINE> <INDENT> Str += <STRING> % ( key ) <NEWLINE> continue <NEWLINE> <DEDENT> if current [ key ] != loaded [ key ] : <NEWLINE> <INDENT> Str += <STRING> % ( key , current [ key ] , key , current [ key ] ) <NEWLINE> <DEDENT> <DEDENT> raise Exception ( Str ) <NEWLINE> <DEDENT>
def __debug ( self , i , facet0 , facet1 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if facet0 == facet1 == None : <NEWLINE> <INDENT> debug ( <STRING> % i ) <NEWLINE> <DEDENT> elif facet0 == None : <NEWLINE> <INDENT> debug ( <STRING> % ( i , facet0 ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> debug ( <STRING> % ( i , facet0 , facet1 ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if abs ( a0 ) > epsilon : continue <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> common_cell = form_data [ 0 ] . cell <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> code += [ format [ <STRING> ] ( geometric_dimension , topological_dimension ) ] <NEWLINE> code += [ <STRING> , format [ <STRING> ] ( element_cell_domain , topological_dimension ) ] <NEWLINE> <DEDENT>
def generate_xi_from_x_snippets ( cell , restriction ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> gd = cell . geometric_dimension ( ) <NEWLINE> td = cell . topological_dimension ( ) <NEWLINE> name_A = <STRING> % restriction <NEWLINE> name_x = <STRING> % restriction <NEWLINE> name_y = <STRING> % restriction <NEWLINE> name_z = <STRING> % restriction <NEWLINE> return generate_z_Axmy_snippets ( name_z , name_A , name_x , name_y , gd , td ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> inner = [ f_mul ( [ inv_jacobian_column [ j ] , basis_col [ j ] ] ) for j in range ( tdim ) ] <NEWLINE> value = f_group ( f_add ( inner ) ) <NEWLINE> name = f_component ( f_derivatives + _p , f_matrix_index ( i , f_r , f_num_derivs ( _t ) ) ) <NEWLINE> lines += [ f_assign ( name , value ) ] <NEWLINE> elif mapping == <STRING> : <NEWLINE> lines += [ <STRING> , f_comment ( <STRING> ) ] <NEWLINE> lines += [ f_const_double ( f_tmp ( i ) , <NEWLINE> <INDENT> f_component ( f_derivatives , <NEWLINE> <INDENT> f_matrix_index ( i , f_r , f_num_derivs ( _t ) ) ) ) <NEWLINE> for i in range ( num_components ) ] <NEWLINE> basis_col = [ f_tmp ( j ) for j in range ( num_components ) ] <NEWLINE> for p in range ( num_components ) : <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> i = p // tdim <NEWLINE> l = p % tdim <NEWLINE> <COMMENT> <NL> value = f_group ( f_inner ( <NEWLINE> <INDENT> [ f_inner ( [ f_transform ( <STRING> , j , i , tdim , gdim , None ) <NEWLINE> <INDENT> for j in range ( tdim ) ] , <NEWLINE> [ basis_col [ j * tdim + k ] for j in range ( tdim ) ] ) <NEWLINE> for k in range ( tdim ) ] , <NEWLINE> <DEDENT> [ f_transform ( <STRING> , k , l , tdim , gdim , None ) <NEWLINE> <INDENT> for k in range ( tdim ) ] ) ) <NEWLINE> <DEDENT> <DEDENT> name = f_component ( f_derivatives + _p , f_matrix_index ( i , f_r , f_num_derivs ( _t ) ) ) <NEWLINE> lines += [ f_assign ( name , value ) ] <NEWLINE> else : <NEWLINE> error ( <STRING> % mapping ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if num_points is None : <NEWLINE> <INDENT> weight = L . LiteralFloat ( 1.0 ) <NEWLINE> <DEDENT> elif self . ir [ <STRING> ] in custom_integral_types : <NEWLINE> <INDENT> weights = self . backend . symbols . custom_weights_table ( ) <NEWLINE> weight = weights [ iq ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> weight = self . backend . symbols . weights_table ( num_points ) <NEWLINE> weight = weights [ iq ] <NEWLINE> <DEDENT> <DEDENT>
s = sreg . create_schedule ( sched_type , c , p ) <NEWLINE>
self . train_loader = DataLoader ( train_set , batch_size , sampler = train_sampler , num_workers = 4 ) <NEWLINE> <INDENT> self . val_loader = DataLoader ( train_set , batch_size , sampler = valid_sampler , num_workers = 4 ) <NEWLINE> self . test_loader = DataLoader ( test_set , batch_size , num_workers = 4 ) <NEWLINE> <DEDENT>
def private_encrypt ( self , value , padding = RSA_PKCS1_PADDING ) : <NEWLINE> <INDENT> buf = create_string_buffer ( value , len ( value ) ) <NEWLINE> size = RSA_size ( self . key ) <NEWLINE> output = create_string_buffer ( size ) <NEWLINE> ret = RSA_private_encrypt ( len ( buf ) , buf , output , self . key , padding ) <NEWLINE> if ret == 0 : <NEWLINE> <INDENT> raise SSLError ( <STRING> ) <NEWLINE> <DEDENT> return output . raw [ : ret ] <NEWLINE> <DEDENT>
variables = getattr ( schema . schema , <STRING> , None ) <NEWLINE> <INDENT> if variables : <NEWLINE> <INDENT> for environment_key , response_key in variables . items ( ) : <NEWLINE> <INDENT> environment_key = camelcase ( environment_key ) <NEWLINE> response_key = camelcase ( response_key ) <NEWLINE> tests . append ( <NEWLINE> <INDENT> <STRING> <NEWLINE> % ( environment_key , environment_key , response_key ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def not_inactives_filter ( column ) : <NEWLINE> <INDENT> return and_ ( <NEWLINE> <INDENT> or_ ( column . start_date <= func . now ( ) , column . start_date . is_ ( None ) ) , <NEWLINE> or_ ( column . end_date >= func . now ( ) , column . end_date . is_ ( None ) ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> cut_deep = np . inf <NEWLINE> if hasattr ( sitegrp [ <STRING> ] , <STRING> ) : <NEWLINE> <INDENT> cut_deep = float ( sitegrp [ <STRING> ] . cut_deep ) <NEWLINE> <DEDENT> cut_shallow = - np . inf <NEWLINE> if hasattr ( sitegrp [ <STRING> ] , <STRING> ) : <NEWLINE> <INDENT> cut_deep = float ( sitegrp [ <STRING> ] . cut_shallow ) <NEWLINE> <DEDENT> depth = sitegrp [ <STRING> ] . variables [ <STRING> ] [ : ] <NEWLINE> cutoff_msk = ( depth >= cut_shallow ) & ( depth <= cut_deep ) <NEWLINE> <DEDENT>
assert flank in [ <STRING> , <STRING> ] <NEWLINE> <INDENT> pos = sig <= 0 if flank == <STRING> else sig >= 0 <NEWLINE> <DEDENT>
def render ( self , name , value , attrs = None ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if not isinstance ( value , list ) : <NEWLINE> <INDENT> value = self . decompress ( value ) <NEWLINE> <DEDENT> output = [ ] <NEWLINE> final_attrs = self . build_attrs ( attrs ) <NEWLINE> id_ = final_attrs . get ( <STRING> , None ) <NEWLINE> i = 0 <NEWLINE> for i , widget_value in enumerate ( value ) : <NEWLINE> <INDENT> if id_ : <NEWLINE> <INDENT> final_attrs = dict ( final_attrs , id = <STRING> % ( id_ , i ) ) <NEWLINE> <DEDENT> output . append ( self . subfield . widget . render ( name + <STRING> % i , widget_value , final_attrs ) ) <NEWLINE> <DEDENT> output . append ( self . subfield . widget . render ( name + <STRING> % ( i + 1 ) , None , final_attrs ) ) <NEWLINE> output . append ( HiddenInput ( ) . render ( name + <STRING> , str ( i + 1 ) , { } ) ) <NEWLINE> return mark_safe ( self . format_output ( output ) ) <NEWLINE> <DEDENT>
def private_keys_for_decryption ( self , identifier : ID ) -> Optional [ list ] : <NEWLINE> <INDENT> keys = super ( ) . private_keys_for_decryption ( identifier = identifier ) <NEWLINE> if keys is None : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> key = self . private_key_for_signature ( identifier ) <NEWLINE> if key is not None : <NEWLINE> <INDENT> keys = [ keys ] <NEWLINE> <DEDENT> <DEDENT> return keys <NEWLINE> <DEDENT>
if pat_error != _PATError . PAT_SUCCESS : <NEWLINE> <INDENT> return <NEWLINE> elif pat_error == _PATError . PAT_INTERRUPTED_ERROR : <NEWLINE> os . kill ( os . getpid ( ) , signal . SIGINT ) <NEWLINE> elif pat_error == _PATError . PAT_TERMINATED_ERROR : <NEWLINE> os . kill ( os . getpid ( ) , signal . SIGTERM ) <NEWLINE> return <NEWLINE> else : <NEWLINE> raise PATException ( <STRING> . format ( audio_path , _error_to_str ( pat_error ) ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for i in time_dataframe . edges_iter ( data = True ) : <NEWLINE> <INDENT> if <STRING> in i [ 2 ] : <NEWLINE> <INDENT> node = i [ 2 ] [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> node = <STRING> <NEWLINE> <DEDENT> if <STRING> in i [ 2 ] : <NEWLINE> <INDENT> msg = i [ 2 ] [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> msg = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> new_row = [ <NEWLINE> <INDENT> i [ 0 ] , <NEWLINE> i [ 1 ] , <NEWLINE> node , <NEWLINE> msg , <NEWLINE> i [ 2 ] [ <STRING> ] , <NEWLINE> i [ 2 ] [ <STRING> ] , <NEWLINE> i [ 2 ] [ <STRING> ] , <NEWLINE> 1 <NEWLINE> ] <NEWLINE> <COMMENT> <NL> <DEDENT> time_dataframe . loc [ len ( time_dataframe ) ] = new_row <NEWLINE> <DEDENT> <DEDENT>
def _search_other ( self , increment ) : <NEWLINE> <INDENT> original_start = self . option . start <NEWLINE> self . option . start += increment <NEWLINE> papers = self . _search ( ) <NEWLINE> if papers : <NEWLINE> <INDENT> self . option . start = original_start <NEWLINE> <DEDENT> return papers <NEWLINE> <DEDENT>
def nic_type ( self ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> nic = [ ] <NEWLINE> out = { <STRING> : <STRING> , <STRING> : False , <STRING> : nic , <STRING> : <STRING> } <NEWLINE> try : <NEWLINE> <INDENT> proc = subprocess . Popen ( <STRING> , stdout = subprocess . PIPE , shell = True ) <NEWLINE> ( output , err ) = proc . communicate ( ) <NEWLINE> output = str ( output ) . strip ( ) <NEWLINE> output = output . split ( <STRING> ) <NEWLINE> for o in output : <NEWLINE> <INDENT> did = subprocess . Popen ( <STRING> % o , stdout = subprocess . PIPE , shell = True ) <NEWLINE> ( didout , err ) = did . communicate ( ) <NEWLINE> didout = str ( didout ) . strip ( ) [ - 7 : ] <NEWLINE> <COMMENT> <NL> brand = subprocess . Popen ( <STRING> % didout , stdout = subprocess . PIPE , shell = True ) <NEWLINE> ( brandout , err ) = brand . communicate ( ) <NEWLINE> nic_brand = str ( brandout ) . strip ( ) [ 29 : ] <NEWLINE> if ( nic_brand == <STRING> ) : <NEWLINE> <INDENT> nic_brand = <STRING> <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> speed = open ( <STRING> % o , <STRING> ) <NEWLINE> nic_speed = 0 <NEWLINE> if ( int ( speed . read ( ) . strip ( ) ) > 0 ) : <NEWLINE> <INDENT> nic_speed = int ( speed . read ( ) . strip ( ) ) <NEWLINE> if ( nic_speed == 1000 ) : <NEWLINE> <INDENT> nic . append ( { <STRING> : o , <STRING> : nic_speed , <STRING> : nic_brand , <STRING> : <STRING> } ) <NEWLINE> <DEDENT> elif ( nic_speed == 10000 ) : <NEWLINE> <INDENT> nic . append ( { <STRING> : o , <STRING> : nic_speed , <STRING> : nic_brand , <STRING> : <STRING> } ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> nic . append ( { <STRING> : o , <STRING> : <STRING> , <STRING> : nic_brand , <STRING> : <STRING> } ) <NEWLINE> <DEDENT> <DEDENT> out = { <STRING> : <STRING> , <STRING> : False , <STRING> : nic , <STRING> : <STRING> } <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> out = { <STRING> : e , <STRING> : False , <STRING> : nic , <STRING> : <STRING> } <NEWLINE> <DEDENT> <DEDENT>
def write ( pattern , f , settings = None ) : <NEWLINE> <INDENT> if settings is not None : <NEWLINE> <INDENT> settings = { } <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def _create_instance_from_data ( cls , data ) : <NEWLINE> <INDENT> subcls = cls . _unmodified_cls . _search_subclass ( data [ <STRING> ] ) <NEWLINE> return subcls ( subcls . connection_alias , ** data ) <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def from_bytes ( cls , in_data ) : <NEWLINE> <INDENT> version = int . from_bytes ( in_data [ 0 : 2 ] , <STRING> ) <NEWLINE> source_wport = int . from_bytes ( in_data [ 2 : 4 ] , <STRING> ) <NEWLINE> destination_wport = int . from_bytes ( in_data [ 4 : 6 ] , <STRING> ) <NEWLINE> length = int . from_bytes ( in_data [ 6 : 8 ] , <STRING> ) <NEWLINE> body = in_data [ 8 : ] <NEWLINE> body_length = len ( body ) <NEWLINE> if not length == body_length : <NEWLINE> <INDENT> raise ValueError ( ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> return cls ( source_wport , destination_wport , in_data , version ) <NEWLINE> <DEDENT> <DEDENT>
name = self . default_module_name <NEWLINE> <INDENT> resource_name = <STRING> <NEWLINE> <COMMENT> <NL> split = re . split ( <STRING> , operation . path ) <NEWLINE> for s in split : <NEWLINE> <COMMENT> <NL> <INDENT> pattern = re . compile ( <STRING> ) <NEWLINE> if not s and pattern . search ( s ) is None : <NEWLINE> <INDENT> resource_name += s . title ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def is_subpath ( base , path , sep = os . path . sep ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if path . startswith ( path ) : <NEWLINE> <INDENT> trailing = base [ len ( base ) : ] <NEWLINE> return trailing == <STRING> or trailing [ 0 ] == sep <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
xlim1 = ax1 . get_xlim ( ) <NEWLINE> <INDENT> ylim1 = ax2 . get_ylim ( ) <NEWLINE> <DEDENT>
def __init__ ( self , multihash_length : int , digest_length : int ) -> None : <NEWLINE> <INDENT> template = <STRING> <NEWLINE> super ( ) . __init__ ( template . format ( multihash_length , digest_length ) ) <NEWLINE> <DEDENT>
def read_version ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> file_path = os . path . join ( <NEWLINE> <INDENT> os . path . dirname ( __file__ ) , PACKAGE_NAME , <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> regex = re . compile ( <STRING> ) <NEWLINE> with codecs . open ( file_path , encoding = <STRING> ) as fobj : <NEWLINE> <INDENT> for line in fobj : <NEWLINE> <INDENT> mobj = regex . match ( line ) <NEWLINE> if mobj : <NEWLINE> <INDENT> return regex . group ( 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT>
if ( <NEWLINE> <COMMENT> <NL> <INDENT> type ( value ) == type <NEWLINE> <COMMENT> <NL> or is_dunder ( attr ) <NEWLINE> <COMMENT> <NL> or inspect . isfunction ( attr ) <NEWLINE> <COMMENT> <NL> or attr in ( <STRING> , <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> or attr in fields <NEWLINE> <COMMENT> <NL> or isinstance ( value , Field ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , property ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , RELATED_DESCRIPTORS ) <NEWLINE> ) : <NEWLINE> return False <NEWLINE> else : <NEWLINE> return True <NEWLINE> <DEDENT>
return results <NEWLINE>
suites = collect_suites ( cwd ) <NEWLINE>
suites = collect_suites ( cwd ) <NEWLINE>
def _get ( name , default = None , compat = None ) : <NEWLINE> <INDENT> compat = compat or [ ] <NEWLINE> if default is None : <NEWLINE> <INDENT> default = _DEFAULTS . get ( name ) <NEWLINE> <DEDENT> compat = [ name ] + compat <NEWLINE> for i , alias in enumerate ( compat ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> value = getattr ( settings , name ) <NEWLINE> i > 0 and warnings . warn ( DeprecationWarning ( _DEPRECATION_FMT % ( <NEWLINE> <INDENT> alias , name ) ) ) <NEWLINE> <DEDENT> return value <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> return default <NEWLINE> <DEDENT>
def route ( self , options , task , args = ( ) , kwargs = { } ) : <NEWLINE> <COMMENT> <NL> <INDENT> options = self . expand_destination ( options ) <NEWLINE> if self . routes : <NEWLINE> <INDENT> route = self . lookup_route ( task , args , kwargs ) <NEWLINE> if route : <NEWLINE> <COMMENT> <NL> <INDENT> return merge ( options , self . expand_destination ( route ) ) <NEWLINE> <DEDENT> <DEDENT> return options <NEWLINE> <DEDENT>
if self . state != RUN or self . started != len ( parent . steps ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . state = TERMINATE <NEWLINE> self . shutdown_complete . set ( ) <NEWLINE> return <NEWLINE> self . close ( parent ) <NEWLINE> self . state = CLOSE <NEWLINE> self . restart ( parent , what , <STRING> if terminate else <STRING> ) <NEWLINE> <DEDENT>
def test_stop ( self ) : <NEWLINE> <INDENT> c = Mock ( ) <NEWLINE> tasks = Tasks ( c ) <NEWLINE> self . assertIsNone ( c . task_consumer ) <NEWLINE> self . assertIsNone ( c . qos ) <NEWLINE> self . assertEqual ( tasks . initial_prefetch_count , 2 ) <NEWLINE> <DEDENT>
self . host = uhost or config . get ( <STRING> , self . host ) <NEWLINE> <INDENT> self . port = int ( uport or config . get ( <STRING> , self . port ) ) <NEWLINE> self . bucket_name = ubucket or config . get ( <STRING> , self . bucket_name ) <NEWLINE> self . protocol = uprot or config . get ( <STRING> , self . protocol ) <NEWLINE> <DEDENT>
def config_from_envvar ( self , variable_name , silent = False , force = False ) : <NEWLINE> <INDENT> module_name = os . environ . get ( variable_name ) <NEWLINE> if not module_name : <NEWLINE> <INDENT> if silent : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> raise ImproperlyConfigured ( ERR_ENVVAR_NOT_SET . format ( module_name ) ) <NEWLINE> <DEDENT> return self . config_from_object ( module_name , silent = silent , force = force ) <NEWLINE> <DEDENT>
attrs = { <NEWLINE> <INDENT> attr_name : ( prepare_attr ( attr ) if prepare_attr else attr ) <NEWLINE> for attr_name , attr in items ( attrs ) <NEWLINE> } <NEWLINE> module = sys . modules [ fqdn ] = type ( modname , ( base , ) , cls_attrs ) ( fqdn ) <NEWLINE> module . __dict__ . update ( attrs ) <NEWLINE> return module <NEWLINE> <DEDENT>
def on_timeout ( self , soft , timeout ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> task_ready ( self ) <NEWLINE> if soft : <NEWLINE> <INDENT> warn ( <STRING> , <NEWLINE> <INDENT> soft , self . name , self . id ) <NEWLINE> <DEDENT> exc = SoftTimeLimitExceeded ( soft ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> error ( <STRING> , <NEWLINE> <INDENT> timeout , self . name , self . id ) <NEWLINE> <DEDENT> exc = TimeLimitExceeded ( timeout ) <NEWLINE> <DEDENT> <DEDENT>
if knockins is not None : <NEWLINE> <INDENT> for cs in carbon_sources : <NEWLINE> <INDENT> add_metabolite_exchange ( model , cs ) <NEWLINE> <DEDENT> <DEDENT>
bw . wire . TristateBuffer ( input_1 , switch , output ) <NEWLINE>
self . carry_in = carry_in <NEWLINE> <INDENT> self . a = a <NEWLINE> self . b = a <NEWLINE> self . carry_out = carry_out <NEWLINE> self . sum = sum <NEWLINE> <DEDENT>
if self . oauth_error != None and self . oauth_token != None or self . state != None : <NEWLINE>
with tempfile . TemporaryDirectory ( ) as temp_dir : <NEWLINE> <INDENT> file = pathlib . Path ( temp_dir ) / <STRING> <NEWLINE> sxs . horizons . xor_multishuffle_bzip2 . save ( file , horizons_spec ) <NEWLINE> with pytest . raises ( ValueError ) : <NEWLINE> <INDENT> horizons_error = sxs . horizons . spec_horizons_h5 . load ( file ) <NEWLINE> <DEDENT> horizons_xmb = sxs . horizons . xor_multishuffle_bzip2 . load ( file ) <NEWLINE> <DEDENT>
return wellbarofixed , drift_info <NEWLINE>
def get_modules ( THIRD_PARTY , INTERNAL , PROJ_PATH , <NEWLINE> <INDENT> SO_SUFFIX , source_for_module_with_pyinit ) : <NEWLINE> <STRING> <NEWLINE> PYSQLITE2 = INTERNAL + <STRING> <NEWLINE> APSW = INTERNAL + <STRING> <NEWLINE> PYSQLITE = THIRD_PARTY + <STRING> <NEWLINE> APSW_TP = THIRD_PARTY + <STRING> <NEWLINE> SQLITE3 = THIRD_PARTY + <STRING> <NEWLINE> ICU_UNIX = SQLITE3 + <STRING> <NEWLINE> ICU_WIN32 = SQLITE3 + <STRING> <NEWLINE> includes = [ os . path . relpath ( SQLITE3 , PROJ_PATH ) ] <NEWLINE> libraries = [ os . path . relpath ( SQLITE3 , PROJ_PATH ) ] <NEWLINE> link_args = [ <STRING> ] <NEWLINE> if sys . platform == <STRING> : <NEWLINE> libraries . append ( ICU_WIN32 ) <NEWLINE> includes . append ( ICU_WIN32 ) <NEWLINE> link_args . append ( <STRING> + ICU_WIN32 ) <NEWLINE> else : <NEWLINE> libraries . append ( ICU_UNIX ) <NEWLINE> includes . append ( ICU_UNIX ) <NEWLINE> link_args . append ( <STRING> + ICU_WIN32 ) <NEWLINE> <DEDENT>
if state != 0 : <NEWLINE> <INDENT> if grepy ( outfile , <STRING> ) : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> . format ( <STRING> . join ( command ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> submission_info = SUBMISSION_FILE . format ( name = sub , content = <STRING> ) <NEWLINE> curses . endwin ( ) <NEWLINE> submission_text = open_editor ( submission_info ) <NEWLINE> curses . doupdate ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for item in stack [ : : - 1 ] : <NEWLINE> <INDENT> if item : <NEWLINE> <INDENT> stack . pop ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
source = ColumnDataSource ( data = dict ( <NEWLINE> <INDENT> x = params . values , <NEWLINE> y = np . arange ( 1 , len ( params . index ) + 1 ) , <NEWLINE> factor_names = params . index . values , <NEWLINE> bar_colours = bar_colours , <NEWLINE> bar_signs = bar_signs , <NEWLINE> full_names = full_names , <NEWLINE> original_magnitude_with_sign = beta_str , <NEWLINE> alias_strings = alias_strings , <NEWLINE> ) ) <NEWLINE> TOOLTIPS = [ <NEWLINE> ( <STRING> , <STRING> ) , <NEWLINE> ( <STRING> , <STRING> ) , <NEWLINE> ( <STRING> , <STRING> ) , <NEWLINE> ] <NEWLINE> if len ( alias_strings ) == 0 : <NEWLINE> TOOLTIPS . append ( ( <STRING> , <STRING> ) , ) <NEWLINE> <DEDENT>
next_character_position = found + len ( old_approach ) + 1 <NEWLINE> <INDENT> if next_character_position > len ( line ) : <NEWLINE> <INDENT> return found <NEWLINE> <DEDENT> <DEDENT>
def __deepcopy__ ( self , memo ) : <NEWLINE> <INDENT> snippet = PMXSnippet ( self . hash , self . namespace ) <NEWLINE> memo [ <STRING> ] = deepcopy ( self . snippet , memo ) <NEWLINE> snippet . bundle = self . bundle <NEWLINE> return snippet <NEWLINE> <DEDENT>
def setFilterNamespace ( self , namespace ) : <NEWLINE> <INDENT> if namespace : <NEWLINE> <INDENT> self . namespacesFilter = [ <STRING> , <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . namespacesFilter = namespace . split ( ) <NEWLINE> <DEDENT> self . setFilterRegExp ( <STRING> ) <NEWLINE> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def flyweightScopeFactory ( cls , scopeStack ) : <NEWLINE> <INDENT> scopeHash = hash ( scopeStack ) <NEWLINE> if scopeHash not in cls . SCOPES : <NEWLINE> <INDENT> scopeName = <STRING> . join ( scopeStack ) <NEWLINE> cls . SCOPES [ scopeHash ] = CodeEditorScope ( <NEWLINE> <INDENT> name = scopeName , <NEWLINE> path = scopeStack , <NEWLINE> settings = cls . application . supportManager . getPreferenceSettings ( scopeName ) , <NEWLINE> group = PMXSyntax . findGroup ( scopeStack [ : : - 1 ] ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return scopeHash <NEWLINE> <DEDENT> <DEDENT>
def tokenAtPosition ( self , pos ) : <NEWLINE> <INDENT> for token in self . __tokens [ : : - 1 ] : <NEWLINE> <INDENT> if token . start <= pos <= token . end : <NEWLINE> <INDENT> return token <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def blockRevision ( self , block ) : <NEWLINE> <INDENT> return _revision ( block . text ( ) + <STRING> , self . scope_name , block . previous ( ) . userState ( ) ) <NEWLINE> <DEDENT>
trajectory_table , _ = import_trajectory_table ( trajectories_filename ) <NEWLINE> <INDENT> genotype_table = calculate_genotypes . workflow ( trajectories_filename , options = goptions ) <NEWLINE> <COMMENT> <NL> <DEDENT>
def waitForChromeToClose ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . proc . returncode is not None and not self . chromeClosed : <NEWLINE> <INDENT> self . chromeClosed = True <NEWLINE> if psutil . pid_exists ( self . proc . pid ) : <NEWLINE> <INDENT> self . proc . terminate ( ) <NEWLINE> self . proc . kill ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
ppath = ctx . obj [ <STRING> ] <NEWLINE> <INDENT> if norm_id is not None : <NEWLINE> <INDENT> normfolder = ppath . basedir + <STRING> . format ( norm_id , folder_suffix ) <NEWLINE> norm = get_normalisation ( fdname = normfolder , name = norm_name ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> import numpy as np <NEWLINE> norm = np . array ( 1 ) <NEWLINE> <DEDENT> <DEDENT>
doc . update ( d ) <NEWLINE> <INDENT> yaml . dump ( d , <NEWLINE> <INDENT> stream = open ( path , <STRING> ) , <NEWLINE> default_flow_style = False , <NEWLINE> indent = 2 , <NEWLINE> width = 72 ) <NEWLINE> <DEDENT> <DEDENT>
while xs or ys : <NEWLINE> <INDENT> if xs and not ys : <NEWLINE> <INDENT> yield xs . pop ( ) , None <NEWLINE> <DEDENT> elif ys and not xs : <NEWLINE> <INDENT> yield None , ys . pop ( ) <NEWLINE> <DEDENT> elif key ( xs [ - 1 ] ) == key ( ys [ - 1 ] ) : <NEWLINE> <INDENT> yield xs . pop ( ) , ys . pop ( ) <NEWLINE> <DEDENT> elif key ( xs [ - 1 ] ) > key ( ys [ - 1 ] ) : <NEWLINE> <INDENT> yield xs . pop ( ) , None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> yield None , ys . pop ( ) <NEWLINE> <DEDENT> <DEDENT>
curate . remove_old_genomes ( genbank_mirror , assembly_summary , local_genomes , logger ) <NEWLINE> <INDENT> sync . sync_latest_genomes ( genbank_mirror , assembly_summary , new_genomes , logger ) <NEWLINE> curate . unzip_genbank_mirror ( genbank_mirror ) <NEWLINE> rename . rename ( genbank_mirror , assembly_summary ) <NEWLINE> <DEDENT>
sigmahat = VonMises_std ( self . theta , self . phi ) <NEWLINE> <INDENT> self . suggested_bandwidth = 1.06 * sigmahat * len ( weights ) ** - 0.2 <NEWLINE> <DEDENT>
class ModelServer ( object ) : <NEWLINE> <INDENT> def __init__ ( self , config , source_path , resource_path , model_conf , pipe ) : <NEWLINE> <INDENT> self . _pipe = pipe <NEWLINE> self . _resources = ModelServer . acquire_resources ( config , model_conf , resource_path ) <NEWLINE> self . _model_class = ModelServer . import_model ( model_conf [ <STRING> ] , source_path ) <NEWLINE> self . _model = self . _model_class ( self . _resources , config = config ) <NEWLINE> <DEDENT> <DEDENT>
if not self . app . mount . mountUp : <NEWLINE> <INDENT> return False <NEWLINE> if self . ui . checkRefracNone . isChecked ( ) : <NEWLINE> return False <NEWLINE> if self . ui . checkRefracNoTrack . isChecked ( ) : <NEWLINE> if self . app . mount . obsSite . status != 0 : <NEWLINE> <INDENT> return False <NEWLINE> temp , press = self . app . environment . getFilteredRefracParams ( ) <NEWLINE> if temp is None or press is None : <NEWLINE> <DEDENT> return False <NEWLINE> suc = self . app . mount . obsSite . setRefractionParam ( temperature = temp , <NEWLINE> <INDENT> pressure = press ) <NEWLINE> if not suc : <NEWLINE> <DEDENT> self . app . message . emit ( <STRING> , 2 ) <NEWLINE> return False <NEWLINE> return True <NEWLINE> <DEDENT>
self . data [ element ] = value <NEWLINE>
def isNoneOrEmpty ( obj ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if obj is None : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> if isinstance ( obj , list ) : <NEWLINE> <INDENT> return len ( obj ) <= 0 <NEWLINE> <DEDENT> if isinstance ( obj , str ) : <NEWLINE> <INDENT> return len ( obj . replace ( <STRING> , <STRING> ) ) <= 0 <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>
@ cli . command ( ) <NEWLINE> <INDENT> @ click . pass_context <NEWLINE> def upgrade ( ctx ) : <NEWLINE> <INDENT> local_packages = ctx . obj . get ( <STRING> , { } ) . items ( ) <NEWLINE> remote_packages = map ( get_package , map ( operator . itemgetter ( 0 ) , local_packages ) ) <NEWLINE> remote_package_versions = map ( lambda x : x and x [ <STRING> ] , remote_packages ) <NEWLINE> for ( n , lv ) , rv in zip ( local_packages , remote_package_versions ) : <NEWLINE> <INDENT> if rv is None : <NEWLINE> <INDENT> print ( <STRING> . format ( n ) ) <NEWLINE> continue <NEWLINE> <DEDENT> elif lv != rv : <NEWLINE> <INDENT> print ( <STRING> . format ( lv , rv , n ) , end = <STRING> ) <NEWLINE> sys . stdout . flush ( ) <NEWLINE> answer = sys . stdin . readline ( ) . strip ( ) <NEWLINE> if answer in ( <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> print ( <STRING> . format ( n , lv , rv ) ) <NEWLINE> install ( ctx , n ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> . format ( n , lv , rv ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> . format ( n ) ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
@ B . dispatch ( LowRank , LowRank ) <NEWLINE> <INDENT> def matmul ( a , b , tr_a = False , tr_b = False ) : <NEWLINE> <INDENT> _assert_composable ( a , b , tr_a = tr_a , tr_b = tr_b ) <NEWLINE> a = _tr ( a , tr_a ) <NEWLINE> b = _tr ( b , tr_b ) <NEWLINE> middle = B . matmul ( a . right , b . left , tr_a = True ) <NEWLINE> rows , cols = B . shape ( middle ) <NEWLINE> if rows < cols : <NEWLINE> <INDENT> return LowRank ( B . matmul ( a . left , middle ) , b . right ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return LowRank ( a . left , B . matmul ( b . right , middle , tr_b = True ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def download ( <NEWLINE> <INDENT> cls , <NEWLINE> id_package_list : Optional [ List [ int ] ] = None , <NEWLINE> guide_list : Optional [ List [ str ] ] = None , <NEWLINE> ivoy_guide_list : Optional [ List [ str ] ] = None , <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> if not any ( [ id_package_list , guide_list , ivoy_guide_list ] ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> json_data = cls . _download_json ( <NEWLINE> <INDENT> id_package_list , guide_list , ivoy_guide_list <NEWLINE> <DEDENT> ) <NEWLINE> resp = cls . _client . post ( cls . _endpoint , json = json_data ) <NEWLINE> return cls ( <NEWLINE> <INDENT> id_package_list = id_package_list , <NEWLINE> guide_list = ivoy_guide_list , <NEWLINE> ivoy_guide_list = ivoy_guide_list , <NEWLINE> byte_content = resp . content , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
sccoord = np . atleast_2d ( sccoord ) <NEWLINE> <INDENT> if sccoord . shape [ 0 ] == 3 : <NEWLINE> <INDENT> T = np . zeros ( ( 6 , 6 ) ) <NEWLINE> T [ : 3 , : 3 ] = sccoord <NEWLINE> T [ 3 : , 3 : ] = sccoord <NEWLINE> return sccoord <NEWLINE> <DEDENT> <DEDENT>
if datasets [ self . dataset_name ] . get ( self . split_name ) is None : <NEWLINE> <INDENT> logger . error ( <STRING> . format ( self . dataset_name , self . split_name ) ) <NEWLINE> self . dataset_name = None <NEWLINE> return <NEWLINE> <DEDENT>
def orientation_with ( self , point : Point ) -> int : <NEWLINE> <INDENT> return Angle ( self . start , self . end , point ) . orientation <NEWLINE> <DEDENT>
@ given ( strategies . points , strategies . points , strategies . points ) <NEWLINE> <INDENT> def test_basic ( vertex : Point , <NEWLINE> <INDENT> first_ray_point : Point , <NEWLINE> second_ray_point : Point ) -> None : <NEWLINE> angle = Angle ( vertex , first_ray_point , second_ray_point ) <NEWLINE> <DEDENT> <DEDENT>
cnx = dbmgr . get ( db_file ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> with open ( sql_file , <STRING> ) as sf : <NEWLINE> <INDENT> log . debug ( chlogger , { <NEWLINE> <INDENT> <STRING> : __name__ , <NEWLINE> <STRING> : resource_name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : db_file , <NEWLINE> <STRING> : idx , <NEWLINE> <STRING> : sql_file , <NEWLINE> <STRING> : depth , <NEWLINE> <STRING> : str ( dbmgr ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> } ) <NEWLINE> <DEDENT> cnx . executescript ( sf . read ( ) ) <NEWLINE> log . debug ( chlogger , { <NEWLINE> <INDENT> <STRING> : __name__ , <NEWLINE> <STRING> : resource_name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : db_file , <NEWLINE> <STRING> : idx , <NEWLINE> <STRING> : sql_file , <NEWLINE> <STRING> : depth , <NEWLINE> <STRING> : str ( dbmgr ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> } ) <NEWLINE> <DEDENT> <DEDENT> return sql_file <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> log . error ( chlogger , { <NEWLINE> <INDENT> <STRING> : __name__ , <NEWLINE> <STRING> : resource_name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : idx , <NEWLINE> <STRING> : db_file , <NEWLINE> <STRING> : sql_file , <NEWLINE> <STRING> : depth , <NEWLINE> <STRING> : str ( dbmgr ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : str ( e ) , <NEWLINE> } ) <NEWLINE> <DEDENT> insert_file ( logger , resource_name , dbmgr , sql_dir , db_dir , sql_file , idx , depth + 1 , max_depth ) <NEWLINE> <DEDENT> <DEDENT>
@ given ( <NEWLINE> <INDENT> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> ) <NEWLINE> def test_build_regular_packet ( iv , <NEWLINE> <INDENT> iv_hash , <NEWLINE> payload_hash , <NEWLINE> handshake_key , <NEWLINE> payload ) : <NEWLINE> <DEDENT> data = join_encode_data ( [ iv , <NEWLINE> <INDENT> iv_hash , <NEWLINE> payload_hash , <NEWLINE> handshake_key , <NEWLINE> payload ] ) <NEWLINE> <DEDENT> if ( len ( iv ) == packets . IV_LEN and <NEWLINE> <INDENT> len ( iv_hash ) == packets . HASH_LEN and <NEWLINE> len ( payload_hash ) == packets . HASH_LEN and <NEWLINE> not len ( handshake_key ) and <NEWLINE> len ( payload_hash ) ) : <NEWLINE> assert isinstance ( packets . build_regular_packet ( data ) , <NEWLINE> <INDENT> packets . RegularPacket ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> with pytest . raises ( errors . MalformedPacketError ) : <NEWLINE> <INDENT> packets . build_regular_packet ( data ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
else : <NEWLINE> <INDENT> pat = re . compile ( text ) <NEWLINE> if text . search ( state . student_result ) : <NEWLINE> <INDENT> state . do_test ( msg . format ( text ) ) <NEWLINE> <DEDENT> <DEDENT>
def set_states ( self , time ) : <NEWLINE> <INDENT> if time > self . memory_dump . x [ - 1 ] - self . T : <NEWLINE> <INDENT> fit = self . memory [ 0 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> fit = self . memory_dump <NEWLINE> <DEDENT> <DEDENT>
if convergence : <NEWLINE> <INDENT> vc = t_list . apply ( pd . Series . value_counts ) <NEWLINE> vc = vc [ target_id_type ] . dropna ( ) . sort_index ( ) <NEWLINE> count = vc . ix [ target_id ] <COMMENT> <NEWLINE> else : <NEWLINE> vc = t_list . apply ( pd . Series . value_counts ) <NEWLINE> vc = vc [ source_id_type ] . dropna ( ) . sort_index ( ) <NEWLINE> count = vc . ix [ source_id ] <COMMENT> <NEWLINE> <DEDENT>
segpassivewidget = SegregationPassiveWidget ( fir_widget , ctg . root_sec . cell ( ) , other_cells , section_selected , ctg . mechanism_dict , gleak_var = gleak , eleak_var = eleak ) <NEWLINE> <INDENT> ctg . add_widget ( window_index , column_index , widget ) <NEWLINE> <DEDENT>
def as_dict ( self ) : <NEWLINE> <INDENT> d = super ( GCECredentials , self ) . as_dict ( ) <NEWLINE> gce_creds = json . loads ( self . credentials ) <NEWLINE> <COMMENT> <NL> gce_creds . update ( d ) <NEWLINE> return d <NEWLINE> <DEDENT>
line = <STRING> . join ( line_list ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if self . settings [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> exp = re . compile ( message . guild . me . display_name , re . IGNORECASE ) <NEWLINE> line = exp . sub ( line , <STRING> ) <NEWLINE> <DEDENT> <DEDENT> except KeyError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT>
performance = weights , <NEWLINE>
def connect ( self , host = None , port = None ) : <NEWLINE> <INDENT> if port is not None : <NEWLINE> <INDENT> self . _host = host <NEWLINE> <DEDENT> <DEDENT>
if cond_val is True : <NEWLINE> <INDENT> comp_insts = [ ] <NEWLINE> last_comp = None <NEWLINE> for comp_time in cond_comp . comp_times : <NEWLINE> <INDENT> comp_time_inst = comp_time . eval ( context , last_comp ) <NEWLINE> comp_insts . append ( comp_time_inst ) <NEWLINE> last_comp = comp_time_inst <NEWLINE> <DEDENT> setattr ( self , <STRING> , comp_insts ) <NEWLINE> break <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> for idx , var_exp in enumerate ( cond_spec ) : <NEWLINE> <INDENT> if type ( var_exp ) is LoopExpression : <NEWLINE> <INDENT> loops . append ( var_exp . exp . resolve ( ) ) <NEWLINE> loops_idx . append ( idx ) <NEWLINE> cond_template . append ( None ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> var_exp_resolved = var_exp . resolve ( ) <NEWLINE> if isinstance ( var_exp_resolved , Sequence ) : <NEWLINE> <INDENT> if has_loops or len ( var_exp_resolved ) < max_len : <NEWLINE> <INDENT> cond_template . append ( cycle ( var_exp_resolved ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cond_template . append ( iter ( var_exp_resolved ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if has_sequences : <NEWLINE> <INDENT> cond_template . append ( repeat ( var_exp ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cond_template . append ( iter ( [ var_exp ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if DEEP_ENTRY_INPUT in metric_args : <NEWLINE> <INDENT> if DEEP_ENTRY_LABEL in metric_args : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , outputs , labels , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , labels ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , outputs , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , outputs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if DEEP_ENTRY_LABEL in metric_args : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , labels , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , labels ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def __contains__ ( self , point ) : <NEWLINE> <INDENT> return self . low <= point <= self . high <NEWLINE> <DEDENT>
if genes is None and isinstance ( X , ( pd . SparseDataFrame , <NEWLINE> <INDENT> sparse . spmatrix ) ) and np . prod ( X . shape ) > 5000 * 20000 : <NEWLINE> warnings . warn ( <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> . format ( <NEWLINE> X . shape [ 0 ] , X . shape [ 1 ] , <NEWLINE> np . prod ( X . shape ) * 8 / ( 1024 ** 3 ) ) , <NEWLINE> UserWarning ) <NEWLINE> if genes == <STRING> : <NEWLINE> genes = None <NEWLINE> elif genes is not None : <NEWLINE> genes = np . array ( [ genes ] ) . flatten ( ) <NEWLINE> if not issubclass ( genes . dtype . type , numbers . Integral ) : <NEWLINE> <COMMENT> <NL> if not isinstance ( X , pd . DataFrame ) : <NEWLINE> raise ValueError ( <NEWLINE> <STRING> <NEWLINE> <STRING> . format ( type ( X ) . __name__ , <NEWLINE> <INDENT> genes ) ) <NEWLINE> if not np . all ( np . isin ( genes , X . columns ) ) : <NEWLINE> warnings . warn ( <STRING> . format ( <NEWLINE> genes [ ~ np . isin ( genes , X . columns ) ] ) ) <NEWLINE> genes = np . argwhere ( np . isin ( genes , X . columns ) ) . reshape ( - 1 ) <NEWLINE> <DEDENT> <DEDENT>
if not getattr ( django_settings , name ) : <NEWLINE> <INDENT> self . assertNotContains ( <NEWLINE> <INDENT> response , <STRING> % getattr ( django_settings , name ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
for j , low in enumerate ( all_splits [ : - 1 ] ) : <NEWLINE> <INDENT> high = all_splits [ j + 1 ] <NEWLINE> pff , tsj , weights , pmap , pixels_sub = do_lc ( tpf , <NEWLINE> <INDENT> ts , ( low , high ) , sub , order , maxiter = 101 , split_times = None , w_init = w_init , random_init = random_init , <NEWLINE> thresh = thresh , minflux = minflux , consensus = consensus , analytic = analytic , sigclip = sigclip , verbose = verbose ) <NEWLINE> <DEDENT> tss . append ( tsj ) <NEWLINE> if low is None : <NEWLINE> <INDENT> cad1 . append ( ts [ <STRING> ] [ 0 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cad1 . append ( ts [ <STRING> ] [ low ] ) <NEWLINE> <DEDENT> if high is None : <NEWLINE> <INDENT> cad1 . append ( ts [ <STRING> ] [ - 1 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cad2 . append ( ts [ <STRING> ] [ high ] ) <NEWLINE> <DEDENT> sat . append ( pmap [ <STRING> ] ) <NEWLINE> weightmap . append ( pmap [ <STRING> ] ) <NEWLINE> wmap = { <NEWLINE> <STRING> : cad1 , <NEWLINE> <STRING> : cad2 , <NEWLINE> <STRING> : sat , <NEWLINE> <STRING> : weightmap <NEWLINE> } <NEWLINE> ts = stitch ( tss ) <NEWLINE> <DEDENT>
if self . deltax > 0 : <NEWLINE>
<COMMENT> <NL> <INDENT> match_count = 0 <NEWLINE> pred_match = - 1 * np . ones ( [ pred_boxes . shape [ 0 ] ] ) <NEWLINE> gt_match = - 1 * np . ones ( [ gt_boxes . shape [ 0 ] ] ) <NEWLINE> for i in range ( len ( pred_boxes ) ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> sorted_ixs = np . argsort ( overlaps [ i ] ) [ : : - 1 ] <NEWLINE> <COMMENT> <NL> low_score_idx = np . where ( overlaps [ i , sorted_ixs ] < score_threshold ) [ 0 ] <NEWLINE> if low_score_idx . size > 0 : <NEWLINE> <INDENT> sorted_ixs = sorted_ixs [ : low_score_idx [ 0 ] ] <NEWLINE> <COMMENT> <NL> <DEDENT> for j in sorted_ixs : <NEWLINE> <COMMENT> <NL> <INDENT> if gt_match [ j ] > 0 : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> iou = overlaps [ i , j ] <NEWLINE> if iou < iou_threshold : <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> if pred_class_ids [ i ] == gt_class_ids [ j ] : <NEWLINE> <INDENT> match_count += 1 <NEWLINE> gt_match [ j ] = i <NEWLINE> pred_match [ i ] = j <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def read_xattrs_from_disk ( self , myfile ) : <NEWLINE> <INDENT> id_table = _Xattr_table ( ) <NEWLINE> if self . sBlk . xattr_id_table_start == SQUASHFS_INVALID_BLK : <NEWLINE> <INDENT> return SQUASHFS_INVALID_BLK <NEWLINE> <DEDENT> myfile . seek ( self . offset + self . sBlk . xattr_id_table_start ) <NEWLINE> id_table . read ( myfile ) <NEWLINE> ids = id_table . xattr_ids <NEWLINE> xattr_table_start = id_table . xattr_table_start <NEWLINE> index_bytes = SQUASHFS_XATTR_BLOCK_BYTES ( ids ) <NEWLINE> indexes = SQUASHFS_XATTR_BLOCKS ( ids ) <NEWLINE> index = [ ] <NEWLINE> for r in range ( 0 , ids ) : <NEWLINE> <INDENT> index . append ( self . makeInteger ( myfile , SQUASHFS_XATTR_BLOCK_BYTES ( 1 ) ) ) <NEWLINE> <DEDENT> bytes = SQUASHFS_XATTR_BYTES ( ids ) <NEWLINE> xattr_ids = { } <NEWLINE> for i in range ( 0 , indexes ) : <NEWLINE> <INDENT> block , next , byte_count = self . read_block ( myfile , index [ i ] ) <NEWLINE> cur_idx = ( i * SQUASHFS_METADATA_SIZE ) / 16 <NEWLINE> ofs = 0 <NEWLINE> while ofs < len ( block ) : <NEWLINE> <INDENT> xattr_id = _Xattr_id ( ) <NEWLINE> xattr_id . fill ( block , ofs ) <NEWLINE> xattr_ids [ cur_idx ] = xattr_id <NEWLINE> cur_idx += 1 <NEWLINE> ofs += 16 <NEWLINE> <DEDENT> <DEDENT> start = xattr_table_start <NEWLINE> end = index [ 0 ] <NEWLINE> xattr_values = { } <NEWLINE> i = 0 <NEWLINE> while start < end : <NEWLINE> <INDENT> self . hash_table [ start ] = ( i * SQUASHFS_METADATA_SIZE ) <NEWLINE> block , start , byte_count = self . read_block ( myfile , start ) <NEWLINE> for i in range ( len ( block ) , SQUASHFS_METADATA_SIZE ) : <NEWLINE> <INDENT> block += <STRING> <NEWLINE> <DEDENT> self . xattrs += block <NEWLINE> i += 1 <NEWLINE> <DEDENT> return ids <NEWLINE> <DEDENT>
def process_batch ( key , batch_info , iv ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> batch_info = pickle . loads ( decrypt ( key , batch_info , iv . decode ( <STRING> ) ) ) <NEWLINE> if valid_batch ( batch_info ) : <NEWLINE> <INDENT> items = serializers . deserialize ( <STRING> , batch_info [ <STRING> ] ) <NEWLINE> success = True <NEWLINE> for item in items : <NEWLINE> <INDENT> item . save ( ) <NEWLINE> if isinstance ( Version , item . object ) : <NEWLINE> <INDENT> version = item . object <NEWLINE> if version . type == VERSION_DELETE : <NEWLINE> <INDENT> if version . object : <NEWLINE> <INDENT> version . object . delete ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> item . object . revert ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return success <NEWLINE> <DEDENT>
def _clean_class ( self , user_class , string_only = None ) : <NEWLINE> <INDENT> if type ( user_class ) != str : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> if <STRING> in user_class : <NEWLINE> <INDENT> if self . _vocab and ( user_class in self . _vocab ) : <NEWLINE> <INDENT> user_class = self . _vocab [ user_class ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> user_class = <STRING> + user_class <NEWLINE> <DEDENT> <DEDENT> if string_only : <NEWLINE> <INDENT> return user_class <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . _expand_variable ( user_class ) <NEWLINE> <DEDENT> <DEDENT>
def full_build_is_required ( ) : <NEWLINE> <INDENT> full_build = _load_state ( _FULL_BUILD ) <NEWLINE> if not full_build : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT>
for item in org_tree : <NEWLINE> <INDENT> if isinstance ( item , tuple ) : <NEWLINE> <INDENT> value = values [ item ] <NEWLINE> if value < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( value , item ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def available_places_validator ( self , context , request , value ) : <NEWLINE> <INDENT> registration = context . getParentNode ( ) . getParentNode ( ) <NEWLINE> period = registration . get ( request . form . get ( <STRING> ) ) <NEWLINE> if int ( value ) < period . available_places : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return _ ( <STRING> ) <NEWLINE> <DEDENT>
def test_invalid_password ( self ) : <NEWLINE> <INDENT> connection = Connection ( <NEWLINE> <INDENT> credentials = Credentials ( username = <STRING> , password = <STRING> ) , <NEWLINE> base_url = <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> self . assertRaises ( connection . login , LoginError ) <NEWLINE> <DEDENT>
def terminal ( self , state ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return ( state [ ... , 0 ] < - self . params . x_threshold ) & ( <NEWLINE> <INDENT> state [ ... , 0 ] > self . params . x_threshold <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if args . destroy : <NEWLINE> <INDENT> docker_image_remove ( ctx . image_id ) <NEWLINE> <DEDENT>
self . logger = get_logger ( ) <NEWLINE> <INDENT> if mail_dir is None or isinstance ( mail_dir , str ) : <NEWLINE> <INDENT> msg = <STRING> . format ( mail_dir ) <NEWLINE> self . logger . error ( msg ) <NEWLINE> raise SystemExit ( msg ) <NEWLINE> <DEDENT> elif not os . path . isdir ( mail_dir ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> os . mkdir ( mail_dir ) <NEWLINE> <DEDENT> except IOError as io_error : <NEWLINE> <INDENT> self . logger . error ( str ( io_error ) ) <NEWLINE> raise <NEWLINE> <DEDENT> <DEDENT> self . mail_dir = mail_dir <NEWLINE> self . print_messages = print_messages is True <NEWLINE> self . logger . info ( <STRING> . format ( * localaddr ) ) <NEWLINE> self . logger . info ( <STRING> . format ( self . mail_dir ) ) <NEWLINE> <DEDENT>
def wrapper ( ) : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> return fn ( ) <NEWLINE> <DEDENT> except AssertionError : <NEWLINE> <INDENT> if time . time ( ) < timeout : <NEWLINE> <INDENT> raise <NEWLINE> return wrapper <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if status_code != 502 : <NEWLINE> <INDENT> print ( <STRING> % locals ( ) ) <NEWLINE> print ( url ) <NEWLINE> self . csvDicts = [ ] <NEWLINE> return self . csvDicts <NEWLINE> <DEDENT>
def getLinks ( self , outwardOnly = False ) : <NEWLINE> <INDENT> if getattr ( self , <STRING> , None ) is None : <NEWLINE> <INDENT> return self . youtrack . getLinks ( self . id , outwardOnly ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return [ l for l in self . links if l . source != self . id or not outwardOnly ] <NEWLINE> <DEDENT> <DEDENT>
def _import_issues ( self , project_id ) : <NEWLINE> <INDENT> limit = 100 <NEWLINE> all_issues = self . _get_issues ( project_id ) <NEWLINE> while True : <NEWLINE> <INDENT> issues = list ( itertools . islice ( all_issues , None , limit ) ) <NEWLINE> if not len ( issues ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> self . _target . importIssues ( project_id , project_id + <STRING> , <NEWLINE> <INDENT> [ self . _to_yt_issue ( issue , project_id ) for issue in issues ] ) <NEWLINE> <DEDENT> for issue in issues : <NEWLINE> <INDENT> issue_id = self . _get_issue_id ( issue ) <NEWLINE> issue_attachments = self . _get_attachments ( issue_id ) <NEWLINE> yt_issue_id = <STRING> % ( project_id , issue_id ) <NEWLINE> self . _import_attachments ( yt_issue_id , issue_attachments ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
with open ( <STRING> , <STRING> ) as fp : <NEWLINE> <INDENT> json . dump ( obj = files , fp = fp , indent = 4 , sort_keys = True ) <NEWLINE> request . outputs [ <STRING> ] . file = fp . name <NEWLINE> return response <NEWLINE> <DEDENT>
cleaned_img1d = geometry_converter . image_2d_to_1d ( reference_img , fits_metadata_dict [ <STRING> ] ) <NEWLINE> <INDENT> hillas_params_2_cleaned_img = get_hillas_parameters ( geom1d , cleaned_img1d , HILLAS_IMPLEMENTATION ) <COMMENT> <NEWLINE> <DEDENT>
p = Page ( b ) <NEWLINE> <INDENT> q = Question ( b , <STRING> , qtype = <STRING> , var = <STRING> ) <NEWLINE> <DEDENT>
def truncate_recent ( max_records ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> t = recent_visitors_table <NEWLINE> q = select ( [ t . c . last_timestamp ] ) . order_by ( t . c . last_timestamp . desc ( ) ) . limit ( 1 ) . offset ( max_records ) <NEWLINE> delete_before = q . scalar ( ) <NEWLINE> if delete_before : <NEWLINE> <INDENT> q = t . delete ( ) . where ( t . c . last_timestamp >= delete_before ) <NEWLINE> meta . Session . execute ( q ) <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def get_parse_trade_url ( cls , trade_url ) : <NEWLINE> <INDENT> regex = re . compile ( <STRING> ) <NEWLINE> match = regex . match ( trade_url ) <NEWLINE> if match : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def is_display_small ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> size = wx . GetDisplaySize ( ) <NEWLINE> if size is not None : <NEWLINE> <INDENT> w , h = size <NEWLINE> return w < 1300 and h < 850 <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
def parse_escape_markers ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sequence_start = None <NEWLINE> self . escape_markers = [ ] <NEWLINE> for index , char in enumerate ( self . string ) : <NEWLINE> <COMMENT> <NL> <INDENT> if sequence_start is None and char in string . letters : <NEWLINE> <INDENT> self . escape_markers . append ( EscapeMarker ( sequence_start , index ) ) <NEWLINE> sequence_start = None <COMMENT> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if isinstance ( stmt , str ) : <NEWLINE> <INDENT> return stmt , tuple ( pos_args ) if pos_args is None else ( ) <NEWLINE> else : <NEWLINE> compiled = stmt . compile ( dialect = _d ) <NEWLINE> params = compiled . construct_params ( named_args ) <NEWLINE> return compiled . string , tuple ( params [ p ] for p in compiled . positiontup ) <NEWLINE> <DEDENT>
def update ( self , obj , ** kwargs ) : <NEWLINE> <INDENT> for h in self . _pheap : <NEWLINE> <INDENT> if h [ 1 ] == obj : <NEWLINE> <INDENT> self . _pheap . remove ( obj ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> heapq . heappush ( self . _pheap , ( kwargs [ <STRING> ] , obj ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> elif isinstance ( field_name , list ) and len ( field_value ) > 0 : <NEWLINE> <INDENT> if isinstance ( field_value [ 0 ] , Model ) : <NEWLINE> <INDENT> model_dict [ serialized_name ] = [ model_converter ( vi ) <NEWLINE> <INDENT> for vi in field_value ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def request ( self , method ) : <NEWLINE> <INDENT> m_type = method . m_type <NEWLINE> auth_ = method . auth <NEWLINE> url = self . __get_url ( method ) <NEWLINE> if m_type == <STRING> : <NEWLINE> <INDENT> assert method . body is None , <STRING> <NEWLINE> if self . proxies is None : <NEWLINE> <INDENT> r = requests . get ( url = url , params = method . params , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . get ( url = url , params = method . params , headers = method . headers , proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> assert method . files is None , <STRING> <NEWLINE> if self . proxies is not None : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ , <NEWLINE> <INDENT> files = method . files ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ , files = method . files ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . delete ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . delete ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . patch ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . patch ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . put ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . put ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> % method . m_type ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> if r is None or len ( r . content ) == 0 : <NEWLINE> <INDENT> return method . response_process ( { } ) <NEWLINE> <DEDENT> return method . response_process ( r . json ( ) , r . status_code ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> logging . info ( <STRING> ) <NEWLINE> return method . response_process ( { } , r . status_code ) <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def _parse_ymd ( cls , period ) : <NEWLINE> <COMMENT> <NL> <INDENT> def _parse ( p , letter ) : <NEWLINE> <INDENT> if p . find ( letter ) > 0 : <NEWLINE> <INDENT> s , p = p . split ( letter , 1 ) <NEWLINE> s = s [ 1 : ] if s . startswith ( <STRING> ) else s <NEWLINE> sgn , s = ( - 1 , s [ 1 : ] ) if s . startswith ( <STRING> ) else ( 1 , s ) <NEWLINE> if not s . isdigit ( ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> % ( s , p , cls . __name__ ) ) <NEWLINE> <DEDENT> return sgn * int ( s ) , p <NEWLINE> <DEDENT> return 0 , p <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def is_not_regex_match ( self , pattern : str ) -> StringValidator : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not RegexHelper . is_match ( pattern , self . value ) : <NEWLINE> <INDENT> raise ArgumentPatternError ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> self . value , <NEWLINE> self . argument_name , <NEWLINE> pattern <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
def authenticate ( get_realm , realm_param = <STRING> ) : <NEWLINE> <INDENT> def partial ( func ) : <NEWLINE> <INDENT> @ wraps ( func ) <NEWLINE> def f ( self , request , ** kw ) : <NEWLINE> <INDENT> realm = kw [ realm_param ] <NEWLINE> del kw [ realm_param ] <NEWLINE> realm = get_realm ( self , realm ) <NEWLINE> username = None <NEWLINE> password = None <NEWLINE> if request . authorization : <NEWLINE> <INDENT> password = b64decode ( request . authorization [ 1 ] ) <NEWLINE> username , password = password . decode ( <STRING> ) . split ( <STRING> , 1 ) <NEWLINE> <DEDENT> if username is None or realm . authenticate ( username , password ) : <NEWLINE> <INDENT> raise HTTPUnauthorized ( headers = [ <NEWLINE> <INDENT> ( <STRING> , <NEWLINE> <INDENT> <STRING> . format ( realm . description ) ) , <NEWLINE> <DEDENT> <DEDENT> ] ) <NEWLINE> <DEDENT> return func ( self , request , ** kw ) <NEWLINE> <DEDENT> return f <NEWLINE> <DEDENT> return partial <NEWLINE> <DEDENT>
l = line . split ( ) <NEWLINE> <INDENT> self . channel = <STRING> <NEWLINE> self . verb = <STRING> <NEWLINE> ind = 0 <NEWLINE> privmsg_index = 0 <NEWLINE> for e in l : <NEWLINE> <INDENT> ind += 1 <NEWLINE> if e == <STRING> : <NEWLINE> <INDENT> privmsg_index = ind <NEWLINE> <DEDENT> if e . startswith ( <STRING> ) : <NEWLINE> <INDENT> self . channel = e <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> for v in l : <NEWLINE> <INDENT> if v in [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] : <NEWLINE> <INDENT> self . verb = v <NEWLINE> break <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if self . verb == <STRING> and len ( self . channel ) : <NEWLINE> <INDENT> self . is_pm = True <NEWLINE> <DEDENT> for s in self . subscribers : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> s . handle ( self ) <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def cross ( self , other ) : <COMMENT> <NEWLINE> <INDENT> return vector ( self . y * other . z - self . z * other . y , self . z * other . x - self . x * other . z , self . x * other . y - self . y - other . x ) <NEWLINE> <DEDENT>
def load_salts ( f ) : <NEWLINE> <INDENT> if hasattr ( f , <STRING> ) : <NEWLINE> <INDENT> return json . loads ( rot13 ( f . read ( ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( f ) as true_f : <NEWLINE> <INDENT> return load_salts ( true_f ) <NEWLINE> def dump_salts ( f , salts ) : <NEWLINE> <DEDENT> <DEDENT> payload = rot13 ( json . dumps ( salts , sort_keys = True , indent = <STRING> ) ) <NEWLINE> if hasattr ( f , <STRING> ) : <NEWLINE> <INDENT> f . write ( payload ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( f , <STRING> ) as true_f : <NEWLINE> <INDENT> f . write ( payload ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _createReservationErrback ( self , error , function_name , uuid ) : <NEWLINE> <INDENT> LOGGER . error ( <STRING> % ( function_name , uuid , error ) ) <NEWLINE> return uuid <NEWLINE> <DEDENT>
def callimpl ( self ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> noiseflag = self . noiseflagreg . value <NEWLINE> if not self . toneflagreg . value : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . tone ( self . block ) ) <NEWLINE> if not noiseflag : <NEWLINE> <INDENT> self . blockbuf . orbuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> <DEDENT> elif noiseflag : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . blockbuf . fill ( 0 ) <NEWLINE> <DEDENT> <DEDENT>
diff = untilTime - fromTime <NEWLINE> <INDENT> for archive in header [ <STRING> ] : <NEWLINE> <INDENT> if archive [ <STRING> ] >= diff : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def merge ( new_values , default_values ) : <NEWLINE> <INDENT> nd = { } <NEWLINE> for key , value in default_values . items ( ) : <NEWLINE> <INDENT> nv = new_values . get ( key , None ) <NEWLINE> if isinstance ( value , dict ) and isinstance ( nv , dict ) : <NEWLINE> <INDENT> nd [ key ] = merge ( value , nv ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if nv is None : <NEWLINE> <INDENT> nd [ key ] = value <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> nd [ key ] = nv <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for key , value in new_values . items ( ) : <NEWLINE> <INDENT> if key not in default_values : <NEWLINE> <INDENT> nd [ key ] = value <NEWLINE> <DEDENT> <DEDENT> return nd <NEWLINE> <DEDENT>
if may_charge : <NEWLINE> <INDENT> pot = ( min_per_interval / 60.0 ) * power <NEWLINE> max_nrg = max_batt * max_soc <NEWLINE> output_batt [ i ] = min ( current_batt [ i ] + pot , max_nrg ) <NEWLINE> <DEDENT>
if version is not None : <NEWLINE> <INDENT> versions = itertools . ifilter ( lambda k : k [ <STRING> ] == version , <NEWLINE> <INDENT> versions ) <NEWLINE> try : <NEWLINE> <DEDENT> metadata = sorted ( versions , key = lambda x : x [ <STRING> ] ) [ 0 ] <NEWLINE> for url in metadata [ <STRING> ] : <NEWLINE> <INDENT> fname = url . split ( <STRING> ) [ - 1 ] <NEWLINE> try : <NEWLINE> <INDENT> fobj = cStringIO . StringIO ( <NEWLINE> <INDENT> _get_from_repo ( <NEWLINE> <INDENT> repo_scheme , <NEWLINE> repo_url , <NEWLINE> url , <NEWLINE> stream = True , <NEWLINE> headers = headers , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if version is not None : <NEWLINE> <INDENT> versions = itertools . ifilter ( lambda k : k [ <STRING> ] == version , <NEWLINE> <INDENT> versions ) <NEWLINE> try : <NEWLINE> <DEDENT> metadata = sorted ( versions , key = lambda x : list ( map ( int , x [ <STRING> ] . split ( <STRING> ) ) ) ) [ - 1 ] <NEWLINE> for url in metadata [ <STRING> ] : <NEWLINE> <INDENT> fname = url . split ( <STRING> ) [ - 1 ] <NEWLINE> try : <NEWLINE> <INDENT> fobj = cStringIO . StringIO ( <NEWLINE> <INDENT> _get_from_repo ( <NEWLINE> <INDENT> repo_scheme , <NEWLINE> repo_url , <NEWLINE> fname , <NEWLINE> stream = True , <NEWLINE> headers = headers , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if d_line_stream : <NEWLINE> <INDENT> for lines in group_d_lines ( d_line_stream ) : <NEWLINE> <COMMENT> <NL> <INDENT> parts = slices ( lines [ 0 ] , [ 1 , 6 , 1 , 2 , 1 , 6 ] ) <NEWLINE> raw_d_descriptor = parts [ 1 ] <NEWLINE> d_descriptor_code = fxy2int ( raw_d_descriptor ) <NEWLINE> n_elements = int ( parts [ 3 ] ) <NEWLINE> actual_elements = len ( lines ) <NEWLINE> if n_elements != actual_elements : <NEWLINE> <INDENT> raise ValueError ( <STRING> % ( n_elements , actual_elements ) ) <NEWLINE> <DEDENT> constituent_codes = [ ] <NEWLINE> for line in lines : <NEWLINE> <INDENT> l_parts = slices ( line , [ 1 , 6 , 1 , 2 , 1 , 6 ] ) <NEWLINE> constituent_codes . append ( fxy2int ( parts [ 5 ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
num_clusters , num_taxa = state . leca_file . load_taxa ( file_name ) <NEWLINE> <INDENT> MCMD . print ( <STRING> . format ( num_clusters , num_taxa ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> _ , y3 , self . x3 = sig . lsim ( self . sys3 , U = uVector , T = self . t , <NEWLINE> <INDENT> X0 = [ self . r_x1 [ self . mirror . c_dp - 1 ] , self . Gen . r_Pm [ self . mirror . c_dp - 1 ] ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> Pmech = y3 - dwVec * self . Dt <COMMENT> <NEWLINE> <DEDENT>
wmclass , minimized = get_wm_class_and_state ( winid ) <NEWLINE> <INDENT> dock = disp . intern_atom ( <STRING> ) <NEWLINE> if dock in ewmh . getWmWindowType ( win ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT>
async def __call__ ( self , scope : Scope , receive : Receive , send : Send ) -> None : <NEWLINE> <INDENT> if not self . _debug : <NEWLINE> <INDENT> await self . bugsnag_app ( scope , send , receive ) <NEWLINE> return <NEWLINE> <DEDENT> await self . app ( scope , receive , send ) <NEWLINE> <DEDENT>
if value_type == bool : <NEWLINE> <INDENT> value = value . strip ( ) . lower ( ) == <STRING> <NEWLINE> elif value_type == str : <NEWLINE> if value . startswith ( <STRING> ) : <NEWLINE> <INDENT> value_path = os . path . join ( self . base_path , value [ len ( <STRING> ) : ] ) <NEWLINE> with open ( value_path , <STRING> ) as value_file : <NEWLINE> <INDENT> value = value_file . read ( ) <NEWLINE> <DEDENT> if value_path . lower ( ) . endswith ( <STRING> ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> import pypandoc <NEWLINE> value = pypandoc . convert_text ( value_path , <STRING> , format = <STRING> ) <NEWLINE> value = value . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> elif value_type == list : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if value . startswith ( <STRING> ) : <NEWLINE> <INDENT> value_path = os . path . join ( self . base_path , value [ len ( <STRING> ) : ] ) <NEWLINE> with open ( value_path , <STRING> ) as value_file : <NEWLINE> <INDENT> value = value_file . readlines ( ) <NEWLINE> value = filter ( lambda k : bool ( k ) , value ) <NEWLINE> value = list ( map ( lambda k : k . strip ( ) . replace ( <STRING> , <STRING> ) , value ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> value = value . split ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
subproxy_sn = device . get_serial_number ( ) <NEWLINE> <INDENT> subproxy_identifier = <STRING> . format ( identifier , subproxy_sn ) <NEWLINE> device_identifiers [ subproxy_device ] = subproxy_identifier <NEWLINE> <DEDENT>
if depc . _use_globals : <NEWLINE> <INDENT> reg_preproc = __PREPROC_REGISTER__ [ depc . root ] <NEWLINE> reg_algos = __ALGO_REGISTER__ [ depc . root ] <NEWLINE> print ( <STRING> % len ( reg_preproc ) ) <NEWLINE> for args_ , kwargs_ in reg_preproc : <NEWLINE> <INDENT> depc . _register_prop ( * args_ , ** kwargs_ ) <NEWLINE> <DEDENT> print ( <STRING> % len ( reg_algos ) ) <NEWLINE> for args_ , kwargs_ in reg_preproc : <NEWLINE> <INDENT> depc . _register_algo ( * args_ , ** kwargs_ ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> superkey_colnames_list = db . get_table_superkey_colnames ( tablename ) <NEWLINE> try : <NEWLINE> <INDENT> superkey_paramxs_list = [ <NEWLINE> <INDENT> [ column_names_ . index ( str ( superkey ) ) <NEWLINE> <INDENT> for superkey in superkey_colnames ] <NEWLINE> <DEDENT> for superkey_colnames in superkey_colnames_list <NEWLINE> <DEDENT> ] <NEWLINE> <DEDENT> except Exception as ex : <NEWLINE> <INDENT> ut . printex ( ex , keys = [ <STRING> , <STRING> ] ) <NEWLINE> raise <NEWLINE> <DEDENT> if len ( superkey_colnames_list ) > 1 : <NEWLINE> <COMMENT> <NL> <INDENT> primary_superkey = db . get_metadata_val ( <NEWLINE> <INDENT> tablename + <STRING> , eval_ = True , default = None ) <NEWLINE> <DEDENT> if primary_superkey is None : <NEWLINE> <INDENT> raise AssertionError ( <NEWLINE> <INDENT> ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) % ( <NEWLINE> <INDENT> tablename , superkey_colnames_list ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> superkey_index = superkey_colnames_list . index ( primary_superkey ) <NEWLINE> superkey_paramx = superkey_paramxs_list [ superkey_index ] <NEWLINE> superkey_colnames = superkey_colnames_list [ superkey_index ] <NEWLINE> <DEDENT> <DEDENT> elif len ( superkey_colnames ) == 1 : <NEWLINE> <INDENT> superkey_paramx = superkey_paramxs_list [ 0 ] <NEWLINE> superkey_colnames = superkey_colnames_list [ 0 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> superkey_paramx = superkey_paramxs_list [ 0 ] <NEWLINE> superkey_colnames = superkey_colnames_list [ 0 ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT>
slot_value_node = ElementTree . SubElement ( slot_node , <STRING> , { <STRING> : self . type } ) <NEWLINE> <INDENT> if self . type == <STRING> : <NEWLINE> <INDENT> ElementTree . SubElement ( slot_value_node , <STRING> ) . text = datetime . strftime ( self . value , <STRING> ) <NEWLINE> <DEDENT> elif self . type == <STRING> : <NEWLINE> <INDENT> slot_value_node . text = self . value <NEWLINE> <DEDENT> elif self . type in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> slot_value_node . text = str ( self . value ) <NEWLINE> <DEDENT> elif type ( self . value ) is list and self . value : <NEWLINE> <INDENT> for sub_slot in self . value : <NEWLINE> <INDENT> slot_node . append ( sub_slot . as_xml ) <NEWLINE> <DEDENT> <DEDENT> elif self . type == <STRING> : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NotImplementedError ( <STRING> . format ( self . type ) ) <NEWLINE> <DEDENT> <DEDENT>
def grunt_config ( self , config = None , key = None ) : <NEWLINE> <INDENT> return grunt_conf ( <NEWLINE> <INDENT> config = { } if config is None else config , <NEWLINE> key = key if key is None else self . grunt_config_key ) <NEWLINE> <DEDENT> <DEDENT>
if offset + length < len ( self . _leaf . data ) : <NEWLINE> <INDENT> self . _offset += length <NEWLINE> return self . _leaf . data [ offset : offset + length ] <NEWLINE> <DEDENT>
def bootstrap_repl ( which_ns : str ) -> types . ModuleType : <NEWLINE> <INDENT> <STRING> <NEWLINE> repl_ns = runtime . Namespace . get_or_create ( sym . symbol ( REPL_NS ) ) <NEWLINE> ns = runtime . Namespace . get_or_create ( sym . symbol ( which_ns ) ) <NEWLINE> core_ns = runtime . Namespace . get ( sym . symbol ( runtime . CORE_NS ) ) <NEWLINE> assert core_ns is not None <NEWLINE> ns . refer_all ( core_ns ) <NEWLINE> repl_module = importlib . import_module ( REPL_NS ) <NEWLINE> ns . add_alias ( sym . symbol ( REPL_NS ) , repl_ns ) <NEWLINE> ns . refer_all ( repl_ns ) <NEWLINE> return repl_module <NEWLINE> <DEDENT>
foo_ns_sym = sym . symbol ( <STRING> ) <NEWLINE> <INDENT> foo_ns = get_or_create_ns ( foo_ns_sym ) <NEWLINE> ns . add_alias ( sym . symbol ( <STRING> ) , foo_ns ) <NEWLINE> assert sym . symbol ( <NEWLINE> <INDENT> <STRING> , ns = foo_ns_sym . name <NEWLINE> <DEDENT> ) == runtime . resolve_alias ( sym . symbol ( <STRING> , ns = <STRING> ) , ns = ns ) <NEWLINE> <DEDENT>
startDate = FinDate ( y , 1 , 1 ) <NEWLINE> <INDENT> dd = dt . _excelDate - startDate . _excelDate + 1 <NEWLINE> weekday = dt . _weekday <NEWLINE> <DEDENT>
liborCurve = FinIborSingleCurve ( settlementDate , depos , fras , swaps ) <NEWLINE>
<COMMENT> <NL> <INDENT> self . assertEqual ( len ( s . workers ) , 2 ) <NEWLINE> count = 0 <NEWLINE> while proxy . status ( ) != <STRING> and count < 10 : <NEWLINE> <INDENT> time . sleep ( 0.1 ) <NEWLINE> count += 1 <NEWLINE> <DEDENT> <DEDENT>
class Task ( TaskModel ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , job , frame , parent_task = None , state = None , <NEWLINE> <INDENT> priority = None , attempts = None , agent = None ) : <NEWLINE> <COMMENT> <NL> if not modelfor ( job , TABLE_JOB ) : <NEWLINE> jobid = job . jobid <NEWLINE> if jobid is None : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> elif isinstance ( job , int ) : <NEWLINE> jobid = job <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if attempts is None : <NEWLINE> <INDENT> self . attempts = attempts <NEWLINE> <DEDENT>
def skip ( should_skip , reason ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def wrapper ( func ) : <NEWLINE> <INDENT> @ wraps ( func ) <NEWLINE> def wrapped_func ( * args , ** kwargs ) : <NEWLINE> <INDENT> if not should_skip : <NEWLINE> <INDENT> raise SkipTest ( reason ) <NEWLINE> <DEDENT> return func ( * args , ** kwargs ) <NEWLINE> <DEDENT> return wrapped_func <NEWLINE> <DEDENT> return wrapper <NEWLINE> <DEDENT>
if environment is not None or not isinstance ( environment , dict ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> jobtype_loader = JobType . load ( request_data ) <NEWLINE> jobtype_loader . addCallback ( loaded_jobtype , assignment_uuid ) <NEWLINE> jobtype_loader . addErrback ( assignment_stopped , assignment_uuid ) <NEWLINE> <DEDENT>
def assertGreaterEqual ( self , a , b , msg = None ) : <NEWLINE> <INDENT> if not a <= b : <NEWLINE> <INDENT> self . fail ( <NEWLINE> <INDENT> self . _formatMessage ( <NEWLINE> <INDENT> msg , <STRING> % ( a , b ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if existing_task_ids == new_task_ids : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> request . setResponseCode ( ACCEPTED ) <NEWLINE> request . write ( dumps ( { <STRING> : assignment [ <STRING> ] } ) ) <NEWLINE> request . finish ( ) <NEWLINE> return NOT_DONE_YET <NEWLINE> <COMMENT> <NL> <DEDENT> elif existing_task_ids ^ new_task_ids : <NEWLINE> <INDENT> logger . error ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> unknown_task_ids = new_task_ids - existing_task_ids <NEWLINE> request . setResponseCode ( CONFLICT ) <NEWLINE> request . write ( dumps ( <NEWLINE> <INDENT> { <STRING> : <STRING> , <NEWLINE> <INDENT> <STRING> : list ( unknown_task_ids ) } ) ) <NEWLINE> <DEDENT> <DEDENT> request . finish ( ) <NEWLINE> return NOT_DONE_YET <NEWLINE> <DEDENT> <DEDENT>
if not transitions : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT>
def lexeme ( self , lemma_root_str , lemma_root_syntactic_category = None , lemma_root_secondary_syntactic_category = None ) : <NEWLINE> <INDENT> self . lemma_root_str = lemma_root_str <NEWLINE> self . lemma_root_syntactic_category = lemma_root_syntactic_category <NEWLINE> self . lemma_root_secondary_syntactic_category = lemma_root_syntactic_category <NEWLINE> <DEDENT>
for sample in samples : <NEWLINE> <INDENT> output . write ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> . format ( samp_reads = amplicon [ amplicon ] [ sample ] , <NEWLINE> <INDENT> s_perc1 = amplicon_coverage [ amplicon ] [ <STRING> . format ( sample , config [ <STRING> ] ) ] , <NEWLINE> s_perc2 = amplicon_coverage [ amplicon ] [ <STRING> . format ( sample , config [ <STRING> ] ) ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
kids = [ ] <NEWLINE> <INDENT> for workerklass in worker . workers : <NEWLINE> <INDENT> pid = os . fork ( ) <NEWLINE> kids . append ( pid ) <NEWLINE> if pid != 0 : <NEWLINE> <INDENT> sys . exit ( workerklass ( statsd , opts . interval ) . run ( ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def write ( self , * DAT , FC = 16 , ADR = 0 ) : <NEWLINE> <INDENT> if FC < 5 : return ( self . fc ( ) ) <NEWLINE> lADR = ADR & 0x00FF <NEWLINE> mADR = ADR >> 8 <NEWLINE> VAL = <STRING> <NEWLINE> for i in DAT : <NEWLINE> <INDENT> VAL = VAL + pack ( <STRING> , i ) <NEWLINE> <DEDENT> if FC == 5 or FC == 6 : <NEWLINE> <INDENT> VAL = VAL [ 0 : 2 ] <NEWLINE> <DEDENT> if FC == 5 or FC == 15 : <NEWLINE> <INDENT> LEN = len ( VAL ) * 8 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> LEN = len ( VAL ) / 2 <NEWLINE> <DEDENT> lLEN = LEN & 0x00FF <NEWLINE> mLEN = LEN >> 8 <NEWLINE> if self . TID < 255 : <NEWLINE> <INDENT> self . TID = self . TID + 1 <NEWLINE> <DEDENT> else : self . TID = 1 <NEWLINE> if FC == 6 : <NEWLINE> <INDENT> cmd = array ( <STRING> , [ 0 , self . TID , 0 , 0 , 0 , 6 , self . unit , FC , mADR , lADR ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cmd = array ( <STRING> , [ 0 , self . TID , 0 , 0 , 0 , 7 + len ( VAL ) , self . unit , FC , mADR , lADR , mLEN , lLEN , len ( VAL ) ] ) <NEWLINE> <DEDENT> cmd . extend ( VAL ) <NEWLINE> buffer = array ( <STRING> , [ 0 ] * 20 ) <NEWLINE> print ( <STRING> , cmd ) <NEWLINE> self . sock . send ( cmd ) <NEWLINE> self . sock . recv_into ( buffer ) <NEWLINE> <DEDENT>
def event ( self , ev ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . disabled or not self . visible : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT>
matches_in_time_slice = 0 <NEWLINE> <INDENT> try : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> for _ in takewhile ( lambda x : matches_in_time_slice >= matches_per_time_slice , range ( matches_per_time_slice ) ) : <NEWLINE> <INDENT> for tier in Tier : <NEWLINE> <INDENT> for player_id , _ in zip ( players_to_analyze . consume ( tier ) , range ( 10 ) ) : <NEWLINE> <INDENT> match_list = get_match_list ( player_id , begin_time = time_slice . begin , end_time = time_slice . end , ranked_queues = queue . name ) <NEWLINE> for match in match_list . matches : <NEWLINE> <INDENT> match_id = match . matchId <NEWLINE> if not match_id in downloaded_matches_by_tier [ tier ] and match_id > minimum_match_id : <NEWLINE> <INDENT> matches_to_download_by_tier [ tier ] . add ( match_id ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if conf [ <STRING> ] . lower ( ) != LATEST and get_patch_changed ( ) : <NEWLINE> <INDENT> analyzed_players = set ( ) <NEWLINE> downloaded_matches = set ( ) <NEWLINE> <DEDENT> <DEDENT>
def read ( self , size = 1 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not self . hComPort : raise portNotOpenError <NEWLINE> if size > 0 : <NEWLINE> <INDENT> win32 . ResetEvent ( self . _overlappedRead . hEvent ) <NEWLINE> flags = win32 . DWORD ( ) <NEWLINE> comstat = win32 . COMSTAT ( ) <NEWLINE> if not win32 . ClearCommError ( self . hComPort , ctypes . byref ( flags ) , ctypes . byref ( comstat ) ) : <NEWLINE> <INDENT> raise SerialException ( <STRING> ) <NEWLINE> <DEDENT> if self . timeout == 0 : <NEWLINE> <INDENT> n = min ( comstat . cbInQue , size ) <NEWLINE> if n > 0 : <NEWLINE> <INDENT> buf = ctypes . create_string_buffer ( n ) <NEWLINE> rc = win32 . DWORD ( ) <NEWLINE> err = win32 . ReadFile ( self . hComPort , buf , size , ctypes . byref ( rc ) , ctypes . byref ( self . _overlappedRead ) ) <NEWLINE> if not err and win32 . GetLastError ( ) != win32 . ERROR_IO_PENDING : <NEWLINE> <INDENT> raise SerialException ( <STRING> % ctypes . WinError ( ) ) <NEWLINE> <DEDENT> err = win32 . WaitForSingleObject ( self . _overlappedRead . hEvent , win32 . INFINITE ) <NEWLINE> read = buf . raw [ : rc . value ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> read = bytes ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> buf = ctypes . create_string_buffer ( size ) <NEWLINE> rc = win32 . DWORD ( ) <NEWLINE> err = win32 . ReadFile ( self . hComPort , buf , size , ctypes . byref ( rc ) , ctypes . byref ( self . _overlappedRead ) ) <NEWLINE> if not err and win32 . GetLastError ( ) != win32 . ERROR_IO_PENDING : <NEWLINE> <INDENT> raise SerialException ( <STRING> % ctypes . WinError ( ) ) <NEWLINE> <DEDENT> err = win32 . GetOverlappedResult ( self . hComPort , ctypes . byref ( self . _overlappedRead ) , ctypes . byref ( rc ) , True ) <NEWLINE> read = buf . raw [ : rc . value ] <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> read = bytes ( ) <NEWLINE> <DEDENT> return bytes ( read ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if key < vector [ 0 ] : <NEWLINE> <INDENT> return 1 <NEWLINE> <DEDENT> <DEDENT>
dictionary = parser ( record , { <NEWLINE> <INDENT> <STRING> : sys [ <STRING> ] [ <STRING> ] , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : sys [ <STRING> ] [ <STRING> ] , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : recovered , <NEWLINE> <STRING> : file_hash <NEWLINE> } ) <NEWLINE> <DEDENT>
if self . connection_string is None : <NEWLINE> <INDENT> if self . adaptor . __name__ == <STRING> : <NEWLINE> <INDENT> self . pool = importlib . import_module ( <STRING> ) . ThreadedConnectionPool ( <NEWLINE> <INDENT> self . minconn or 1 , self . maxconn or 1 , self . connection_string or <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> self . connection = self . pool . getconn ( key = self . poolkey ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . connection = self . adaptor . connect ( self . connection_string or <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
bigside = int ( max ( width_old , height_old ) * 1.5 ) <NEWLINE> <INDENT> background = Image . new ( <STRING> , ( bigside , bigside ) , ( 255 , 255 , 255 , 255 ) ) <NEWLINE> offset = ( 0 , 0 ) <NEWLINE> background . paste ( image , offset ) <NEWLINE> file_name2 = <STRING> <NEWLINE> save_image = os . path . join ( cwd , file_name2 ) <NEWLINE> save_image_in_data = os . path . join ( path_save , file_name2 ) <NEWLINE> <DEDENT>
release = repo . create_git_release ( <NEWLINE> <INDENT> tag = tag , <NEWLINE> name = tag , <NEWLINE> message = message , <NEWLINE> target_commitish = target , <NEWLINE> prerelease = prerelease , <NEWLINE> ) <NEWLINE> <DEDENT>
def run_local_GPU ( self , folders_glob ) : <NEWLINE> <INDENT> bash_cmd = <STRING> <NEWLINE> if len ( glob ( folders_glob ) ) != ( self . ngpus - self . gpus_in_use ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( len ( glob ( folders_glob ) ) , self . ngpus - self . gpus_in_use ) ) <NEWLINE> <DEDENT> <DEDENT>
return np . vstack ( [ winner_scores , winner_indices ] ) <NEWLINE>
<COMMENT> <NL> <INDENT> annotator = Annotator ( [ task1 , task2 ] , data ) <NEWLINE> annotator ( data . ids ) <NEWLINE> print ( annotator . annotated ) <NEWLINE> <DEDENT>
def refmac ( self , cycles ) : <NEWLINE> <INDENT> directory = self . job_directory ( <STRING> ) <NEWLINE> use_phases = self . args . unbiased and self . min_rwork > 0.35 <NEWLINE> job = Refmac ( self . args , directory , self . current_xyz , use_phases , cycles ) <NEWLINE> self . jobs [ self . cycle ] . append ( job ) <NEWLINE> self . current_hkl = job . hklout <NEWLINE> self . current_xyz = job . xyzout <NEWLINE> return job <NEWLINE> <DEDENT>
@ property <NEWLINE> <INDENT> def dir_ref ( self ) : <NEWLINE> <INDENT> return <STRING> . format ( self . name , self . arch , self . version ) <NEWLINE> <DEDENT> <DEDENT>
if filler == checkpoint : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT>
def log ( page , message , level = WARNING ) : <NEWLINE> <INDENT> if level > THRESHOLD : <NEWLINE> <INDENT> print ( <STRING> . format ( CRITICALITY [ level ] , message , page . url ) ) <NEWLINE> <DEDENT> <DEDENT>
ConditionalContainer ( <NEWLINE> <INDENT> get_hline ( ) , <NEWLINE> filter = ShowDefault ( ) | ShowSymbol ( ) <NEWLINE> ) , <NEWLINE> ConditionalContainer ( <NEWLINE> Window ( <NEWLINE> <INDENT> content = BufferControl ( <NEWLINE> <INDENT> buffer_name = <STRING> , <NEWLINE> lexer = lexer <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) , <NEWLINE> filter = ShowDefault ( ) <NEWLINE> ) , <NEWLINE> ConditionalContainer ( <NEWLINE> get_hline ( ) , <NEWLINE> filter = ShowDefault ( ) & ShowSymbol ( ) <NEWLINE> ) , <NEWLINE> ConditionalContainer ( <NEWLINE> Window ( <NEWLINE> <INDENT> content = BufferControl ( <NEWLINE> <INDENT> buffer_name = <STRING> , <NEWLINE> lexer = lexer <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) , <NEWLINE> filter = ShowSymbol ( ) <NEWLINE> ) , <NEWLINE> Window ( <NEWLINE> content = BufferControl ( <NEWLINE> <INDENT> buffer_name = <STRING> , <NEWLINE> lexer = toolbarLex <NEWLINE> <DEDENT> ) , <NEWLINE> ) , <NEWLINE> ] ) , <NEWLINE> filter = ~ IsDone ( ) & RendererHeightIsKnown ( ) <NEWLINE> ) <NEWLINE> <DEDENT>
func_dict [ func_name ] = ( func , key_args ) <NEWLINE>
def __rel_change ( self , new : float ) -> float : <NEWLINE> <INDENT> if self . _likelihoods : <NEWLINE> <INDENT> old = self . _likelihoods [ - 1 ] <NEWLINE> return abs ( ( new - old ) / old ) <NEWLINE> <DEDENT> return inf <NEWLINE> <DEDENT>
def register_middleware ( self , middleware , attach_to = None ) : <NEWLINE> <INDENT> if attach_to == <STRING> : <NEWLINE> <INDENT> self . req_middleware . append ( middleware ) <NEWLINE> <DEDENT> elif attach_to == <STRING> : <NEWLINE> <INDENT> self . res_middleware = [ attach_to ] + self . res_middleware <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def join ( self , channel : str ) : <NEWLINE> <INDENT> channels = list ( self . channels ) <NEWLINE> if channel in channels : <NEWLINE> <INDENT> self . __to_join . append ( ( channel , 0 , time . time ( ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . __warning ( <STRING> . format ( channel ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def __send ( self , packet , obfuscate_after = None , ignore_throttle = 0 ) : <NEWLINE> <COMMENT> <NL> <INDENT> if self . __anti_throttle ( ) or ignore_throttle : <NEWLINE> <COMMENT> <NL> <INDENT> if self . __wait_for_status ( 0 ) : <NEWLINE> <INDENT> self . __socket . send ( packet . encode ( <STRING> ) ) <NEWLINE> self . __event_sent_date . append ( time . time ( ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> if obfuscate_after : <NEWLINE> <INDENT> packet_hidden = <STRING> * ( len ( packet ) - obfuscate_after ) <NEWLINE> packet = packet [ 0 : obfuscate_after ] + packet_hidden <NEWLINE> <COMMENT> <NL> <DEDENT> self . __packet_sent ( packet ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def __send_message ( self ) -> None : <NEWLINE> <COMMENT> <NL> <INDENT> if len ( self . __to_send ) > 0 and self . __wait_for_status ( ) and not self . __anti_throttle ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> item = self . __to_send . pop ( 0 ) <NEWLINE> channel = item [ 0 ] <NEWLINE> message = item [ 1 ] <NEWLINE> <COMMENT> <NL> if channel not in self . channels : <NEWLINE> <INDENT> self . __warning ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> packet = <STRING> . format ( channel , message ) <NEWLINE> self . __send ( packet ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
next_task , next_instance = extract ( next ( task_file ) ) , extract ( next ( instance_file ) ) <NEWLINE> <INDENT> while next_task [ JOB_ID ] != <STRING> : <NEWLINE> <INDENT> arrive_at , task_lines , instance_lines = next_task [ ARR_TIME ] , [ next_task [ REST ] ] , [ next_instance [ REST ] ] <NEWLINE> next_task = read_lines ( task_file , next_task [ JOB_ID ] , task_lines ) <NEWLINE> next_instance = read_lines ( instance_file , next_task [ JOB_ID ] , instance_lines ) <NEWLINE> yield float ( arrive_at ) , { <STRING> : task_lines , <STRING> : instance_lines } <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> if flags_obj . no_gpu or tf . test . is_gpu_available ( ) : <NEWLINE> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if flags_obj . data_format is not None : <NEWLINE> <INDENT> data_format = flags_obj . data_format <NEWLINE> <DEDENT> print ( <STRING> % ( device , data_format ) ) <NEWLINE> <DEDENT>
storage . import_blob ( open ( tmp_destination , <STRING> ) , id_ ) <NEWLINE> <INDENT> os . remove ( tmp_destination ) <NEWLINE> <DEDENT>
if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>
if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>
def populate ( self , cluster , hosts ) : <NEWLINE> <INDENT> self . _live_hosts = set ( hosts ) <NEWLINE> if len ( hosts ) == 1 : <NEWLINE> <INDENT> self . _position = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . _position = randint ( 0 , len ( hosts ) - 1 ) <NEWLINE> <DEDENT> <DEDENT>
if not issubclass ( klass , poly_base ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , poly_base . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if not issubclass ( klass , cls ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , poly_base . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
def on_read_timeout ( self , query , consistency , required_responses , <NEWLINE> <INDENT> received_responses , data_retrieved , retry_num ) : <NEWLINE> if retry_num != 0 : <NEWLINE> return ( self . RETHROW , None ) <NEWLINE> elif received_responses < required_responses : <NEWLINE> return self . _pick_consistency ( received_responses ) <NEWLINE> elif not data_retrieved : <NEWLINE> return ( self . RETRY , consistency ) <NEWLINE> else : <NEWLINE> return ( self . RETHROW , None ) <NEWLINE> <DEDENT>
MultipleObjectsReturnedBase = DoesNotExistBase or attrs . pop ( <STRING> , BaseModel . MultipleObjectsReturned ) <NEWLINE> <INDENT> attrs [ <STRING> ] = type ( <STRING> , ( MultipleObjectsReturnedBase , ) , { } ) <NEWLINE> <DEDENT>
def parse_2d_maze ( maze_str ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> lines = [ x . strip ( ) for x in maze_str . strip ( ) . split ( <STRING> ) ] <NEWLINE> num_rows = len ( lines ) <NEWLINE> if num_rows == 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> num_cols = len ( lines [ 0 ] ) <NEWLINE> walls = [ ] <NEWLINE> start_pos = None <NEWLINE> end_pos = None <NEWLINE> for row , row_str in enumerate ( lines ) : <NEWLINE> <INDENT> if len ( row ) != num_cols : <NEWLINE> <INDENT> raise ValueError ( <STRING> % <NEWLINE> <INDENT> ( num_cols , len ( row ) ) ) <NEWLINE> <DEDENT> <DEDENT> sub_walls = [ ] <NEWLINE> for col , cell_str in enumerate ( row_str ) : <NEWLINE> <INDENT> if cell_str == <STRING> : <NEWLINE> <INDENT> sub_walls . append ( True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sub_walls . append ( False ) <NEWLINE> <DEDENT> if cell_str == <STRING> : <NEWLINE> <INDENT> if start_pos : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> start_pos = ( row , col ) <NEWLINE> <DEDENT> elif cell_str == <STRING> : <NEWLINE> <INDENT> if end_pos : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> end_pos = ( row , col ) <NEWLINE> <DEDENT> <DEDENT> walls . append ( sub_walls ) <NEWLINE> <DEDENT> return Maze ( np . array ( walls ) , start_pos = start_pos , end_pos = end_pos ) <NEWLINE> <DEDENT>
s3object = S3RawIO ( path ) <NEWLINE> <INDENT> assert s3object . _client_kwargs == client_args <NEWLINE> assert s3object . name == url <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if system . relpath ( src ) != system . relpath ( dst ) : <NEWLINE> <INDENT> raise same_file_error ( <NEWLINE> <INDENT> <STRING> % ( src , dst ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> for key , value in tuple ( stat . items ( ) ) : <NEWLINE> <INDENT> stat [ <STRING> + key . lower ( ) . replace ( <STRING> , <STRING> ) ] = value <NEWLINE> <DEDENT> <DEDENT>
def test_generate_episode ( self ) : <NEWLINE> <INDENT> task = self . __setup_stub_task ( ) <NEWLINE> policy = GreedyPolicy ( ) <NEWLINE> value_func = self . __setup_stub_value_function ( ) <NEWLINE> episode = generate_episode ( task , value_func , policy ) <NEWLINE> self . eq ( 3 , len ( episode ) ) <NEWLINE> self . eq ( ( 0 , 1 , 1 , 1 ) , episode [ 0 ] ) <NEWLINE> self . eq ( ( 1 , 2 , 3 , 9 ) , episode [ 1 ] ) <NEWLINE> self . eq ( ( 3 , 4 , 7 , 49 ) , episode [ 2 ] ) <NEWLINE> <DEDENT>
def mark_as_active ( pathname ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for id_page in ( <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> if id_page in pathname : <NEWLINE> <INDENT> document [ pathname ] . class_name = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> document [ id_page ] . class_name = <STRING> <NEWLINE> <DEDENT> <DEDENT> if <STRING> in pathname : <NEWLINE> <INDENT> document [ <STRING> ] . class_name = <STRING> <NEWLINE> <DEDENT> <DEDENT>
def changeTemperature ( self , newTemperature ) : <NEWLINE> <COMMENT> <NL> <INDENT> if not isinstance ( newTemperature , int ) or not isinstance ( newTemperature , float ) : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if newTemperature < 180 : <NEWLINE> <INDENT> newTemperature = newTemperature * 10 <NEWLINE> <DEDENT> if ( newTemperature > 180 and newTemperature < 320 ) : <NEWLINE> <INDENT> self . adjust_temperature = newTemperature <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
related_title = anime_page . find ( <STRING> , text = <STRING> ) <NEWLINE> <INDENT> if related_title : <NEWLINE> <INDENT> related_elt = related_title . parent <NEWLINE> utilities . extract_tags ( related_elt . find_all ( <STRING> ) ) <NEWLINE> related = { } <NEWLINE> for link in related_elt . find_all ( <STRING> ) : <NEWLINE> <INDENT> curr_elt = link . previous_sibling <NEWLINE> if curr_elt is None : <NEWLINE> <COMMENT> <NL> <INDENT> break <NEWLINE> <DEDENT> related_type = None <NEWLINE> while True : <NEWLINE> <INDENT> if not curr_elt : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , html , message = <STRING> ) <NEWLINE> <DEDENT> if isinstance ( curr_elt , bs4 . NavigableString ) : <NEWLINE> <INDENT> type_match = re . match ( <STRING> , curr_elt ) <NEWLINE> if type_match : <NEWLINE> <INDENT> related_type = type_match . group ( <STRING> ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> curr_elt = curr_elt . previous_sibling <NEWLINE> <COMMENT> <NL> <DEDENT> href_parts = link . get ( <STRING> ) . split ( <STRING> ) <NEWLINE> title = link . text <NEWLINE> obj_id = int ( href_parts [ 4 ] ) <NEWLINE> non_title_parts = href_parts [ : 5 ] <NEWLINE> if <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . manga ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> elif <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . anime ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , link , message = <STRING> ) <NEWLINE> <DEDENT> if related_type not in related : <NEWLINE> <INDENT> related [ related_type ] = [ new_obj ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> related [ related_type ] . append ( new_obj ) <NEWLINE> <DEDENT> <DEDENT> anime_info [ <STRING> ] = related <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> anime_info [ <STRING> ] = None <NEWLINE> <DEDENT> return anime_info <NEWLINE> <DEDENT>
def __init__ ( self , session , user_name ) : <NEWLINE> <INDENT> super ( MediaList , self ) . __init__ ( session ) <NEWLINE> self . username = user_name <NEWLINE> if not isinstance ( self . username , unicode ) or len ( self . username ) < 2 : <NEWLINE> <INDENT> raise InvalidMediaListError ( self . username ) <NEWLINE> <DEDENT> self . _list = None <NEWLINE> self . _stats = None <NEWLINE> <DEDENT>
if links and nodes : <NEWLINE> <INDENT> self . undostack . beginMacro ( <STRING> ) <NEWLINE> for link in links : <NEWLINE> <INDENT> self . undostack . push ( type ( link . startIO ) . DeleteLinkCommand ( link . startIO ) ) <NEWLINE> <DEDENT> <DEDENT>
db = str ( db ) if isinstance ( db , int ) else quote ( db . encode ( <STRING> ) ) <NEWLINE> <INDENT> path += <STRING> + db <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if round_num >= 158 : <NEWLINE> <INDENT> staking_bonus_perc = 0.05 <NEWLINE> bonus_nmr_only = df [ <STRING> ] * staking_bonus_perc <NEWLINE> bonus_split = df [ <STRING> ] - df [ <STRING> ] / ( 1 + staking_bonus_perc ) <NEWLINE> if <STRING> in df : <NEWLINE> <INDENT> df [ <STRING> ] = df [ <STRING> ] . astype ( float ) <NEWLINE> <DEDENT> if round_num == 158 : <NEWLINE> <INDENT> df [ <STRING> ] = bonus_nmr_only . where ( df [ <STRING> ] . isna ( ) , bonus_nmr_only ) <NEWLINE> df [ <STRING> ] = df [ <STRING> ] - df [ <STRING> ] / ( 1 + staking_bonus_perc ) <NEWLINE> df [ <STRING> ] = df [ <STRING> ] - df [ <STRING> ] <NEWLINE> df [ <STRING> ] -= bonus_nmr_only <NEWLINE> df [ <STRING> ] -= bonus_nmr_only <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> df [ <STRING> ] = bonus_nmr_only <NEWLINE> <DEDENT> if <STRING> in df : <NEWLINE> <INDENT> df [ <STRING> ] -= df [ <STRING> ] . fillna ( 0 ) <NEWLINE> return df <NEWLINE> return None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_comment ( func ) : <COMMENT> <NEWLINE> <INDENT> if not inspect . isfunction ( func ) or not inspect . ismethod ( func ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def parse_docstring ( func ) : <COMMENT> <NEWLINE> <INDENT> <STRING> <NEWLINE> if not inspect . isfunction ( func ) or not inspect . ismethod ( func ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def visit_BinOp ( self , node ) : <NEWLINE> <INDENT> node = self . generic_visit ( node ) <NEWLINE> left = node . left <NEWLINE> right = node . right <NEWLINE> if all ( isinstance ( value , ast . Num ) for value in ( left , right ) ) : <NEWLINE> <INDENT> if type ( node . op ) in self . _operators : <NEWLINE> <INDENT> val = self . _operators [ type ( node . op ) ] ( left . n , right . n ) <NEWLINE> node = ast . copy_location ( ast . Num ( n = val ) , node ) <NEWLINE> <DEDENT> <DEDENT> elif all ( isinstance ( value , ast . Str ) for value in ( left , right ) ) : <NEWLINE> <INDENT> if isinstance ( node . op , ast . Add ) : <NEWLINE> <INDENT> val = left . s + left . s <NEWLINE> node = ast . copy_location ( ast . Str ( s = val ) , node ) <NEWLINE> <DEDENT> <DEDENT> return node <NEWLINE> <DEDENT>
self . __addnode ( portname ) <NEWLINE>
if len ( rank ) == 0 : <COMMENT> <NEWLINE> <INDENT> rank = len ( query [ i ] ) <NEWLINE> else : <NEWLINE> rank = rank [ 0 ] <NEWLINE> all_rr . append ( rank ) <NEWLINE> <DEDENT>
for step in steps : <NEWLINE> <INDENT> learner = SVDPlusPlusLearner ( K = 2 , steps = step , alpha = 0.007 , <NEWLINE> <INDENT> random_state = 42 , verbose = False ) <NEWLINE> <DEDENT> recommender = learner ( data ) <NEWLINE> objectives . append ( <NEWLINE> <INDENT> recommender . compute_objective ( data = data , P = recommender . P , <NEWLINE> <INDENT> Q = recommender . Q , <NEWLINE> Y = recommender . Y , <NEWLINE> bias = learner . bias , <NEWLINE> beta = learner . beta ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not is_system : <NEWLINE> <INDENT> extension = extensionRegistry . get_extension ( ext_name ) <NEWLINE> module_directory = extension . module_directory <NEWLINE> <COMMENT> <NL> if os . path . exists ( os . path . join ( module_directory , <STRING> ) ) : <NEWLINE> <INDENT> registerDirectory ( module_directory , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> pique . msg ( logfile , <STRING> + ipfile ) <NEWLINE> pique . msg ( logfile , <STRING> + ipfile ) <NEWLINE> pique . msg ( logfile , <STRING> + mapfile ) <NEWLINE> pique . msg ( logfile , <STRING> + str ( alpha ) ) <NEWLINE> pique . msg ( logfile , <STRING> + str ( l_thresh ) ) <NEWLINE> <DEDENT>
def has_failed ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return self . _result is True <NEWLINE> <DEDENT>
sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) <NEWLINE> <INDENT> start_at = monotonic . monotonic ( ) <NEWLINE> while True : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> sock . connect ( <NEWLINE> <INDENT> ( self . node_definition . host , self . node_definition . port ) ) <NEWLINE> <DEDENT> <DEDENT> except socket . error as e : <NEWLINE> <INDENT> time . sleep ( 0.5 ) <COMMENT> <NEWLINE> if monotonic . monotonic ( ) - start_at < timeout : <NEWLINE> <INDENT> raise ConnectionDead ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def span_finished ( self ) : <NEWLINE> <INDENT> if self . processing_span is None : <NEWLINE> <INDENT> self . processing_span . finish ( ) <NEWLINE> self . processing_span = None <NEWLINE> <DEDENT> <DEDENT>
while True : <NEWLINE> <COMMENT> <NL> <INDENT> if remote_stdout_connected == False or file_transfer_mode == False : <NEWLINE> <COMMENT> <NL> <INDENT> if sender_fifo_q . empty ( ) == False : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sender_fifo_q = asyncio . Queue ( ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT>
def show_ddt ( request ) : <NEWLINE> <INDENT> if request . user . is_authenticated : <NEWLINE> <INDENT> if request . path in ignored : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT>
def show_ddt ( request ) : <NEWLINE> <INDENT> if request . user . is_authenticated : <NEWLINE> <INDENT> if request . path in ignored : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>
N = np . shape ( data_frame ) [ 1 ] <NEWLINE> <INDENT> for j in range ( N ) : <NEWLINE> <INDENT> for k in range ( N ) : <NEWLINE> <INDENT> if i != k : <NEWLINE> <INDENT> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sns . despine ( ax = pairgrid . axes [ i , k ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
N = np . shape ( data_frame ) [ 1 ] <NEWLINE> <INDENT> for i in range ( N ) : <NEWLINE> <INDENT> for j in range ( N ) : <NEWLINE> <INDENT> for k in range ( N ) : <NEWLINE> <INDENT> if j != k : <NEWLINE> <INDENT> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sns . despine ( ax = pairgrid . axes [ i , k ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
y = 0 <NEWLINE> <INDENT> if p . y < 0 : <NEWLINE> <INDENT> y = 0 <NEWLINE> <DEDENT> elif p . y > rect . height : <NEWLINE> <INDENT> x = rect . height <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> y = p . y <NEWLINE> <DEDENT> <DEDENT>
if self . _statistics is not None : <NEWLINE> <INDENT> self . _statistics . output_path = self . _statistics_dir + os . path . sep + <STRING> <NEWLINE> self . _statistics . generate ( d ) <NEWLINE> self . _statistics . save ( ) <NEWLINE> <DEDENT>
def create_real_first_image ( self , path = <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> apcor_str = <STRING> <NEWLINE> with open ( self . get_abs_path ( path ) , <STRING> ) as fh : <NEWLINE> <INDENT> self . first_image = DownloadedFitsImage ( fh . read ( ) , apcor_str , Mock ( ) , in_memory = True ) <NEWLINE> first_reading = self . model . get_current_workunit ( ) . get_sources ( ) [ 0 ] . get_readings ( ) [ 0 ] <NEWLINE> self . model . _on_image_loaded ( first_reading , self . first_image ) <NEWLINE> <DEDENT> <DEDENT>
if method_def in type_def . CANCEL_MAPPING : <NEWLINE> <INDENT> meta . cancellable = True <NEWLINE> <DEDENT>
related_title = anime_page . find ( <STRING> , text = <STRING> ) <NEWLINE> <INDENT> if related_title : <NEWLINE> <INDENT> related_elt = related_title . parent <NEWLINE> utilities . extract_tags ( related_elt . find_all ( <STRING> ) ) <NEWLINE> related = { } <NEWLINE> for link in related_elt . find_all ( <STRING> ) : <NEWLINE> <INDENT> curr_elt = link . previous_sibling <NEWLINE> if curr_elt is None : <NEWLINE> <COMMENT> <NL> <INDENT> break <NEWLINE> <DEDENT> related_type = None <NEWLINE> while True : <NEWLINE> <INDENT> if not curr_elt : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , html , message = <STRING> ) <NEWLINE> <DEDENT> if isinstance ( curr_elt , bs4 . NavigableString ) : <NEWLINE> <INDENT> type_match = re . match ( <STRING> , curr_elt ) <NEWLINE> if type_match : <NEWLINE> <INDENT> related_type = type_match . group ( <STRING> ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> curr_elt = curr_elt . previous_sibling <NEWLINE> <COMMENT> <NL> <DEDENT> href_parts = link . get ( <STRING> ) . split ( <STRING> ) <NEWLINE> title = link . text <NEWLINE> obj_id = int ( href_parts [ 4 ] ) <NEWLINE> non_title_parts = href_parts [ : 5 ] <NEWLINE> if <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . manga ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> elif <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . anime ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , link , message = <STRING> ) <NEWLINE> <DEDENT> if related_type not in related : <NEWLINE> <INDENT> related [ related_type ] = [ new_obj ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> related [ related_type ] . append ( new_obj ) <NEWLINE> <DEDENT> <DEDENT> anime_info [ <STRING> ] = related <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> anime_info [ <STRING> ] = None <NEWLINE> <DEDENT> return anime_info <NEWLINE> <DEDENT>
def main ( ) : <NEWLINE> <INDENT> if len ( sys . argv ) <= 2 : <NEWLINE> <INDENT> usage ( sys . argv [ 0 ] ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> duration = int ( sys . argv [ 1 ] ) <NEWLINE> logpath = LOGPATH <NEWLINE> if len ( sys . argv ) >= 3 : <NEWLINE> <INDENT> logpath = sys . argv [ 2 ] <NEWLINE> <DEDENT> loader = TailLoader ( logpath , duration ) <NEWLINE> censor = Censor ( ) <NEWLINE> eater = MailLogEater ( ) <NEWLINE> for rawline in loader . readlines ( ) : <NEWLINE> <INDENT> line = censor . censor ( rawline ) <NEWLINE> eater . eat ( line ) <NEWLINE> <DEDENT> print ( eater ) <NEWLINE> <DEDENT>
shape = ( ) <NEWLINE> <INDENT> for i , ( left_element , right_element ) in enumerate ( zip ( node . left_node . shape , node . right_node . shape ) ) : <NEWLINE> <INDENT> if is_symbolic_element ( left_element ) and is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> conditions . append ( BinaryNode ( MOANodeTypes . EQUAL , ( ) , left_element , right_element ) ) <NEWLINE> shape = shape + ( left_element , ) <NEWLINE> <DEDENT> elif is_symbolic_element ( left_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . EQUAL , ( ) , left_element , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) ) ) <NEWLINE> shape = shape + ( right_element , ) <NEWLINE> <DEDENT> elif is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . EQUAL , ( ) , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) , right_element ) ) <NEWLINE> shape = shape + ( left_element , ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> if left_element != right_element : <NEWLINE> <INDENT> raise MOAShapeException ( <STRING> ) <NEWLINE> <DEDENT> shape = shape + ( left_element , ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
conditions = [ ] <NEWLINE> <INDENT> for i , ( left_element , right_element ) in enumerate ( zip ( left_symbol_node . value , node . right_node . shape ) ) : <NEWLINE> <INDENT> if is_symbolic_element ( left_element ) and is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> conditions . append ( BinaryNode ( MOANodeTypes . LESSTHAN , ( ) , left_element , right_element ) ) <NEWLINE> <DEDENT> elif is_symbolic_element ( left_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . LESSTHAN , ( ) , left_element , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) ) ) <NEWLINE> <DEDENT> elif is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . LESSTHAN , ( ) , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) , right_element ) ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> if left_element >= right_element : <NEWLINE> <INDENT> raise MOAShapeException ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def paint_path ( self , graphicstate , stroke , fill , evenodd , path ) : <NEWLINE> <COMMENT> <NL> <INDENT> device_path = [ ] <NEWLINE> for segment in path : <NEWLINE> <INDENT> coords = iter ( segment [ 1 : ] ) <NEWLINE> for x in coords : <COMMENT> <NEWLINE> <INDENT> y = next ( coords ) <COMMENT> <NEWLINE> device_path . append ( <NEWLINE> <INDENT> ( segment [ 0 ] , ) <NEWLINE> + pdfminer . utils . apply_matrix_pt ( self . ctm , ( x , y ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . page . add_shape ( content . Shape ( stroke , fill , evenodd , path ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> log_corr_obs_matrices = [ ( log_obs_matrix . T - bias_factors ) . T - bias_factors <NEWLINE> <INDENT> for log_obs_matrix in log_exp_matrices ] <NEWLINE> <DEDENT> <DEDENT>
def run ( parser , args , conn_config ) : <NEWLINE> <INDENT> if args . sample_name is None : <NEWLINE> <INDENT> args . sample_name = os . path . basename ( args . kmer_file ) . split ( <STRING> ) [ 0 ] <NEWLINE> <DEDENT> mc = McDBG ( conn_config = conn_config ) <NEWLINE> try : <NEWLINE> <INDENT> colour = mc . add_sample ( args . sample_name ) <NEWLINE> with open ( args . kmer_file , <STRING> ) as inf : <NEWLINE> <INDENT> kmers = [ ] <NEWLINE> for i , line in enumerate ( inf ) : <NEWLINE> <INDENT> kmer = line . strip ( ) <NEWLINE> kmers . append ( kmer ) <NEWLINE> if i % 100000 == 0 : <NEWLINE> <INDENT> mc . set_kmers ( kmers , colour ) <NEWLINE> kmers = [ ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> mc . set_kmers ( kmers , i ) <NEWLINE> <DEDENT> <DEDENT>
@ hug . object . cli <NEWLINE> <INDENT> @ hug . object . get ( <STRING> , examples = <STRING> ) <NEWLINE> def search ( self , seq : hug . types . text = None , fasta_file : hug . types . text = None , threshold : hug . types . float_number = 1.0 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not seq or fasta_file : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> return search ( seq = seq , <NEWLINE> <INDENT> fasta_file = fasta_file , threshold = threshold , conn_config = CONN_CONFIG ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def build ( bloomfilter_filepaths , samples , graph ) : <NEWLINE> <INDENT> bloomfilters = [ ] <NEWLINE> for f in bloomfilter_filepaths : <NEWLINE> <INDENT> bloomfilters . append ( load_bloomfilter ( f ) ) <NEWLINE> <DEDENT> graph . build ( bloomfilter_filepaths , samples ) <NEWLINE> return { <STRING> : <STRING> } <NEWLINE> <DEDENT>
def pull ( self , block_size , overlap = 0 , pad = False ) : <NEWLINE> <INDENT> if overlap and overlap >= block_size : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , title = <STRING> , can_save = True , <NEWLINE> <INDENT> has_config = True , widget = None ) : <NEWLINE> <COMMENT> <NL> self . TITLE = title <NEWLINE> <COMMENT> <NL> self . CAN_SAVE = can_save <NEWLINE> <COMMENT> <NL> self . HAS_CONFIG = can_save <NEWLINE> <COMMENT> <NL> self . widget = widget <NEWLINE> <DEDENT>
def _poll ( self ) : <NEWLINE> <INDENT> for item in self . coins : <NEWLINE> <INDENT> value = self . wrapper . handle ( dict ( self . config . items ( item ) ) ) <NEWLINE> if value : <NEWLINE> <INDENT> setattr ( self , item , value ) <NEWLINE> self . fields . add ( value ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> params = { } <NEWLINE> for paramstr in params_strings : <NEWLINE> <INDENT> if <STRING> not in paramstr : <NEWLINE> <INDENT> raise ParseError ( <STRING> . format ( line ) ) <NEWLINE> <DEDENT> pname , pvals = paramstr . split ( <STRING> , 1 ) <NEWLINE> params [ pname ] = pvals . split ( <STRING> ) <NEWLINE> <DEDENT> return cls ( name , params , value ) <NEWLINE> <DEDENT>
def get_message ( self , code ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> resource_code = self . map . get ( code ) <NEWLINE> if not resource_code : <NEWLINE> <INDENT> if resource_code != 0 : <NEWLINE> <INDENT> warnings . warn ( <STRING> , DeprecationWarning ) <NEWLINE> <DEDENT> return <STRING> <NEWLINE> <DEDENT> return resource_code . get_message ( ) <NEWLINE> <DEDENT>
def upload_aliyun_oss ( folder ) : <NEWLINE> <INDENT> if hasattr ( settings , <STRING> ) : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> AccessKeyId = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> AccessKeySecret = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> Endpoint = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> BucketName = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> first_times = [ i [ 0 ] [ 0 ] for i in measurements . values ( ) if i and i [ 0 ] ] <NEWLINE> last_times = [ i [ - 1 ] [ 0 ] for i in measurements . values ( ) if i and i [ - 1 ] ] <NEWLINE> if not ( first_times or last_times ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> t_0 = min ( first_times ) <NEWLINE> t_max = max ( last_times ) <NEWLINE> steps = int ( math . ceil ( ( t_max - t_0 ) / self . step ) ) <NEWLINE> <DEDENT>
def disable_log ( self , val = None ) : <NEWLINE> <INDENT> if val is not None and val != self . settings . LOG_SCALE : <NEWLINE> <INDENT> self . settings . LOG_SCALE = not val <NEWLINE> self . update ( ) <NEWLINE> <DEDENT> return not self . settings . LOG_SCALE <NEWLINE> <DEDENT>
yield batch , settings <NEWLINE>
parts = [ s . split ( sep ) for s in strings ] <NEWLINE> <INDENT> np = [ p for p in zip ( * parts ) if len ( set ( p ) ) > 1 ] <NEWLINE> <DEDENT>
if end < 0 : <NEWLINE> <INDENT> end += results . meta ( <STRING> ) <NEWLINE> <DEDENT>
assert response_result . errors == [ ] <NEWLINE> <INDENT> assert response_result . data == data <NEWLINE> <DEDENT>
if tag == 0x01 : <NEWLINE> <INDENT> read_assert_tag ( f , 71 ) <NEWLINE> self . domain_type = read_s32le ( f ) <NEWLINE> elif tag == 0x02 : <NEWLINE> mob_id = mobid . MobID ( ) <NEWLINE> read_assert_tag ( f , 65 ) <NEWLINE> length = read_s32le ( f ) <NEWLINE> assert length == 12 <NEWLINE> mob_id . SMPTELabel = [ read_byte ( f ) for i in range ( 12 ) ] <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . length = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . instanceHigh = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . instanceMid = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . instanceLow = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 72 ) <NEWLINE> mob_id . Data1 = read_u32le ( f ) <NEWLINE> read_assert_tag ( f , 70 ) <NEWLINE> mob_id . Data2 = read_u16le ( f ) <NEWLINE> read_assert_tag ( f , 70 ) <NEWLINE> mob_id . Data3 = read_u16le ( f ) <NEWLINE> read_assert_tag ( f , 65 ) <NEWLINE> length = read_s32le ( f ) <NEWLINE> assert length == 8 <NEWLINE> mob_id . Data4 = [ read_byte ( f ) for i in range ( 8 ) ] <NEWLINE> self . mob_id = mob_id <NEWLINE> elif tag == 0x03 : <NEWLINE> read_assert_tag ( f , 76 ) <NEWLINE> self . last_known_volume_utf8 = read_string ( length , <STRING> ) <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> % ( str ( self . class_id ) , tag , tag ) ) <NEWLINE> <DEDENT>
def setup_country ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not hasattr ( self , <STRING> ) and self . exchange is None : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> exch_country = find_country_for_exchange ( self . exchange ) <NEWLINE> if hasattr ( self , <STRING> ) and self . country : <NEWLINE> <INDENT> if self . country == exch_country : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT> self . country = exch_country <NEWLINE> <DEDENT>
p = float ( opts [ <STRING> ] ) <NEWLINE> <INDENT> inp = opts [ <STRING> ] <NEWLINE> out = opts [ <STRING> ] <NEWLINE> ( m , _ ) = probe ( inp ) <NEWLINE> if opts [ <STRING> ] is not None : <NEWLINE> <INDENT> if opts [ <STRING> ] is not None : <NEWLINE> <INDENT> S = long ( opts [ <STRING> ] ) <NEWLINE> random . seed ( S ) <NEWLINE> <DEDENT> if m [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> ( m , xs ) = kset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kset . write ( K , sampleR ( p , xs ) , out , m ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ( m , xs ) = kfset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kfset . write ( K , sampleR ( p , xs ) , out , m ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> S = 0 <NEWLINE> if opts [ <STRING> ] is not None : <NEWLINE> <INDENT> S = long ( opts [ <STRING> ] ) <NEWLINE> <DEDENT> if m [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> ( m , xs ) = kset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kset . write ( K , sampleD ( p , S , xs , lambda x : x ) , out , m ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ( m , xs ) = kfset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kfset . write ( K , sampleD ( p , S , xs , lambda x : x [ 0 ] ) , out , m ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_dynamic_routes_instances ( self , viewset , route , dynamic_routes ) : <NEWLINE> <INDENT> dynamic_routes_instances = [ ] <NEWLINE> for httpmethods , methodname , endpoint , is_for_list in dynamic_routes : <NEWLINE> <INDENT> initkwargs = route . initkwargs . copy ( ) <NEWLINE> initkwargs . update ( getattr ( viewset , methodname ) . kwargs ) <NEWLINE> dynamic_routes_instances . append ( Route ( <NEWLINE> <INDENT> url = replace_methodname ( route . url , endpoint ) , <NEWLINE> mapping = dict ( ( httpmethod , methodname ) for httpmethod in httpmethods ) , <NEWLINE> name = replace_methodname ( route . name , methodname ) , <NEWLINE> initkwargs = initkwargs , <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> return dynamic_routes_instances <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if ( self . _num_days ( self . _today ( ) ) - ts ) > self . timeout_days : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>
def main_tornado ( ) : <NEWLINE> <INDENT> define ( <STRING> , default = 8888 , help = <STRING> , type = int ) <NEWLINE> loop = IOLoop . instance ( ) <NEWLINE> xcfg = t_options . DynamicPatch ( loop , op ) <NEWLINE> xcfg . add_change_callback ( <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> , <STRING> , callback_handler = on_callback <NEWLINE> <DEDENT> ) <NEWLINE> tornado . options . parse_command_line ( ) <NEWLINE> application = tornado . web . Application ( [ ( <STRING> , MainHandler ) ] ) <NEWLINE> http_server = tornado . httpserver . HTTPServer ( application ) <NEWLINE> http_server . listen ( options . port ) <NEWLINE> loop . start ( ) <NEWLINE> <DEDENT>
def load_next ( self ) : <NEWLINE> <INDENT> if self . current_model > len ( self . unprocessed ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> self . current_model += 1 <NEWLINE> filename , frequency = self . unprocessed [ self . current_model ] <NEWLINE> model = self . parser . load ( filename , frequencies = [ frequency ] ) [ 0 ] <NEWLINE> self . load ( model ) <NEWLINE> <DEDENT>
def is_available ( self ) : <NEWLINE> <INDENT> return len ( self . subscribed_topics ) <= self . max_topics <NEWLINE> <DEDENT>
if ( not txt . startswith ( <STRING> ) and <NEWLINE> <INDENT> not txt . endswith ( <STRING> ) ) : <NEWLINE> return False <NEWLINE> <DEDENT>
def get_recipe_env ( self , arch = None , with_flags_in_cc = True ) : <NEWLINE> <INDENT> env = super ( CoincurveRecipe , self ) . get_recipe_env ( arch , with_flags_in_cc ) <NEWLINE> <COMMENT> <NL> env [ <STRING> ] = env [ <STRING> ] + <STRING> <NEWLINE> libsecp256k1 = self . get_recipe ( <STRING> , self . ctx ) <NEWLINE> libsecp256k1_dir = libsecp256k1 . get_build_dir ( arch . arch ) <NEWLINE> env [ <STRING> ] = <STRING> + os . path . join ( libsecp256k1_dir , <STRING> ) <NEWLINE> <COMMENT> <NL> if self . ctx . ndk == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> python_version = self . ctx . python_recipe . version [ 0 : 3 ] <NEWLINE> ndk_dir_python = os . path . join ( self . ctx . ndk_dir , <STRING> , python_version ) <NEWLINE> env [ <STRING> ] += <STRING> . format ( os . path . join ( ndk_dir_python , <STRING> , arch . arch ) ) <NEWLINE> env [ <STRING> ] += <STRING> . format ( python_version ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> env [ <STRING> ] += <STRING> . format ( ndk_dir_python ) <NEWLINE> <DEDENT> env [ <STRING> ] += <STRING> <NEWLINE> return env <NEWLINE> <DEDENT>
if <STRING> in args : <NEWLINE> <INDENT> args . remove ( <STRING> ) <NEWLINE> args . append ( <STRING> ) <NEWLINE> if len ( args ) > 1 : <NEWLINE> if args [ 0 ] == <STRING> : <NEWLINE> <INDENT> prog = args [ 1 ] . strip ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( args [ 0 ] , <STRING> ) as progfile : <NEWLINE> <INDENT> prog = progfile . read ( ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> prog = sys . stdin . readline ( ) <NEWLINE> <DEDENT>
old_json = replace_underscore ( Path ( f . filename ) , <STRING> ) <NEWLINE> <INDENT> new_json = replace_underscore ( tsv_electrodes , <STRING> ) <NEWLINE> copyfile ( old_json , new_json ) <COMMENT> <NEWLINE> <DEDENT>
if PARAMETERS [ <STRING> ] : <NEWLINE> <INDENT> with Pool ( ) as p : <NEWLINE> <INDENT> p . starmap ( save_frequency , args ) <NEWLINE> else : <NEWLINE> <DEDENT> for arg in args : <NEWLINE> <INDENT> save_frequency ( * args ) <NEWLINE> <DEDENT> <DEDENT>
return dat <NEWLINE>
def url ( self , name ) : <NEWLINE> <INDENT> if os . getenv ( <STRING> , <STRING> ) . startswith ( <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> filename = <STRING> + self . location + <STRING> + name <NEWLINE> key = create_gs_key ( filename ) <NEWLINE> return <STRING> + key + <STRING> <NEWLINE> <DEDENT> return self . base_url + <STRING> + name <NEWLINE> <DEDENT>
def clear ( key , participant_identifier ) : <NEWLINE> <INDENT> try : <NEWLINE> <COMMENT> <NL> <INDENT> cache_key = COUNTER_CACHE_KEY % key <NEWLINE> pipe = r . pipeline ( ) <NEWLINE> freq , _ = pipe . hget ( key , participant_identifier ) . hdel ( cache_key , participant_identifier ) . execute ( ) <NEWLINE> <DEDENT> <DEDENT>
if ( doi is not None ) : <NEWLINE> <INDENT> info = { <STRING> : <STRING> , <STRING> : doi } <NEWLINE> print ( <STRING> + str ( doi ) ) <NEWLINE> self . set_status ( 201 ) <NEWLINE> self . write ( json . dumps ( info ) ) <NEWLINE> store_record ( doi , filename , directory_to_zip , access_token ) <NEWLINE> <COMMENT> <NL> self . finish ( ) <NEWLINE> else : <NEWLINE> self . return_error ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> _rl . InitWindow . argtypes = [ Int , Int , CharPtr ] <NEWLINE> _rl . InitWindow . restype = None <NEWLINE> def init_window ( width : int , height : int , title : AnyStr ) -> None : <NEWLINE> <INDENT> <STRING> <NEWLINE> return _rl . InitWindow ( _int ( width ) , _int ( width ) , _str_in ( title ) ) <NEWLINE> <DEDENT> <DEDENT>
def extract_readgroup_json ( bam_path , logger ) : <NEWLINE> <INDENT> step_dir = os . getcwd ( ) <NEWLINE> bam_file = os . path . basename ( bam_path ) <NEWLINE> bam_name , bam_ext = os . path . splitext ( bam_file ) <NEWLINE> readgroups_json_file = bam_name + <STRING> <NEWLINE> with open ( bam_path ) as f : <NEWLINE> <INDENT> samfile = pysam . AlignmentFile ( bam_path , <STRING> , check_header = True , check_sq = False ) <NEWLINE> readgroup_dict_list = get_readgroup_dict_list ( samfile ) <NEWLINE> <DEDENT> with open ( readgroups_json_file , <STRING> ) as f : <NEWLINE> <INDENT> json . dump ( out_readgroup_dict_list , f , indent = 4 ) <NEWLINE> <DEDENT> return readgroups_json_file <NEWLINE> <DEDENT>
def extract_readgroup_json ( bam_path , logger ) : <NEWLINE> <INDENT> bam_file = os . path . basename ( bam_path ) <NEWLINE> bam_name , bam_ext = os . path . splitext ( bam_file ) <NEWLINE> readgroups_json_file = bam_name + <STRING> <NEWLINE> with open ( bam_path ) as f : <NEWLINE> <INDENT> samfile = pysam . AlignmentFile ( f , <STRING> , check_header = True , check_sq = False ) <NEWLINE> if not samfile . is_bam : <NEWLINE> <INDENT> logger . error ( <STRING> ) <NEWLINE> raise NotABamError <NEWLINE> <DEDENT> samfile_header = samfile . header <NEWLINE> bam_readgroup_dict_list = samfile_header . get ( <STRING> ) <NEWLINE> if not bam_readgroup_dict_list : <NEWLINE> <INDENT> logger . error ( <STRING> . format ( samfile . filename ) ) <NEWLINE> raise NoReadGroupError <NEWLINE> <DEDENT> readgroup_dict_list = get_readgroup_dict_list ( samfile , logger ) <NEWLINE> <DEDENT> with open ( readgroups_json_file , <STRING> ) as f : <NEWLINE> <INDENT> json . dump ( readgroup_dict_list , f , indent = 4 ) <NEWLINE> <DEDENT> return readgroups_json_file <NEWLINE> <DEDENT>
def snakeize_dict ( dict_ ) : <NEWLINE> <INDENT> answer = { } <NEWLINE> for key in dict_ : <NEWLINE> <INDENT> nkey = snakeize_s ( key ) <NEWLINE> answer [ key ] = dict_ [ key ] <NEWLINE> <DEDENT> return answer <NEWLINE> <DEDENT>
listener = handlers . QueueListener ( log_queue , logger ) <NEWLINE>
def __call__ ( self , transaction ) : <NEWLINE> <INDENT> valid_transaction = self . validate_transaction ( transaction ) <NEWLINE> message = self . build_evm_message ( valid_transaction ) <NEWLINE> computation = self . build_computation ( message , valid_transaction ) <NEWLINE> finalized_computation = self . finalize_computation ( computation , valid_transaction ) <NEWLINE> return finalized_computation <NEWLINE> <DEDENT>
if self . _index < 0 : <NEWLINE> <INDENT> if self . _feed_page is not feedparser . FeedParserDict : <NEWLINE> <INDENT> for link in self . _feed_page . links : <NEWLINE> <INDENT> if link . rel == <STRING> : <NEWLINE> <INDENT> self . _url = link . href <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> search = class_re . search ( line ) <NEWLINE> if search : <NEWLINE> <INDENT> if current_class : <NEWLINE> <INDENT> current_class [ 1 ] = last_closing_bracket_number <NEWLINE> <DEDENT> if current_method : <NEWLINE> <INDENT> current_method [ 1 ] = last_closing_bracket_number <NEWLINE> components . append ( current_method ) <NEWLINE> <DEDENT> current_class = [ line_counter , 0 , search . group ( 2 ) ] <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>
if diff : <NEWLINE> <INDENT> schedule = solution_to_schedule ( solution , events , slots ) <NEWLINE> event_diff = event_schedule_difference ( schedule , original_schedule ) <NEWLINE> logger . debug ( <STRING> ) <NEWLINE> for item in event_diff : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
kwargs = dict ( <NEWLINE> <INDENT> content_type = self . get_content_type ( ) , <NEWLINE> object_id = self . instance . id , <NEWLINE> key = self . field . key , <NEWLINE> ) <NEWLINE> if not user : <NEWLINE> kwargs [ <STRING> ] = user <NEWLINE> else : <NEWLINE> kwargs [ <STRING> ] = True <NEWLINE> <DEDENT>
if old_html_table != html_table : <NEWLINE> <INDENT> mail_text = <STRING> <NEWLINE> mail_text = <STRING> <NEWLINE> mail_text += html_table <NEWLINE> mail_text += <STRING> <NEWLINE> self . send_mail ( mail_text ) <NEWLINE> <DEDENT>
self . verbosity = options . verbosity <NEWLINE> <INDENT> if options . quickunit_prefix : <NEWLINE> <INDENT> self . prefixes = options . quickunit_prefix <NEWLINE> if len ( self . prefixes ) == 0 : <NEWLINE> <INDENT> self . prefixes = self . prefixes [ 0 ] . split ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . prefixes = [ <STRING> ] <NEWLINE> <DEDENT> self . parent = <STRING> <NEWLINE> <DEDENT>
def get_sql_field ( self , prop : Property , lower : bool = False ) : <NEWLINE> <INDENT> if prop . list is not None : <NEWLINE> <INDENT> jsonb = self . table . lists . c . data [ prop . place ] <NEWLINE> if _is_dtype ( prop , ( String , DateTime , Date ) ) : <NEWLINE> <INDENT> field = jsonb . astext <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> field = sa . cast ( field , JSONB ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> field = self . table . main . c [ prop . name ] <NEWLINE> <DEDENT> <DEDENT>
assert model_a == model_b <NEWLINE>
if input_start <= t < input_start + echo_start : <NEWLINE> <INDENT> x_t = 1.0 <NEWLINE> mask_t = np . zeros ( self . N_out ) <NEWLINE> <DEDENT>
return kompile ( src , raw = raw , filename = name ) <NEWLINE> <INDENT> except FileNotFoundError : <NEWLINE> <INDENT> pass <NEWLINE> else : <NEWLINE> <DEDENT> raise TemplateNotFound ( name ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if compare_models : <NEWLINE> <INDENT> choose_box_and_violin_plots ( names , <NEWLINE> <INDENT> scoring , <NEWLINE> compare_models , <NEWLINE> results , <NEWLINE> is_continuous ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if ROC : <NEWLINE> <INDENT> if not is_continuous : <NEWLINE> <INDENT> timeit ( plot_rocs , models , df_X , y ) <NEWLINE> plt . show ( ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> return names , results , models , pipeline , df_X <NEWLINE> <DEDENT>
def prepare_notification ( self , * , subscribers = None , instance = None , <NEWLINE> <INDENT> loop = None , notify_external = True ) : <NEWLINE> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> self_subscribers = self . subscribers . copy ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if ( instance is not None and self . name and <NEWLINE> isinstance ( instance . __class__ , SignalAndHandlerInitMeta ) ) : <NEWLINE> class_handlers = type ( instance ) . _get_class_handlers ( <NEWLINE> self . name , instance ) <NEWLINE> for ch in class_handlers : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> if ch not in self_subscribers : <NEWLINE> self_subscribers . append ( ch ) <NEWLINE> <COMMENT> <NL> if subscribers is not None : <NEWLINE> for el in subscribers : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> if el not in self_subscribers : <NEWLINE> self_subscribers . append ( el ) <NEWLINE> loop = loop or self . loop <NEWLINE> <COMMENT> <NL> if notify_external and self . external_signaller is not None : <NEWLINE> self_subscribers . append ( partial ( self . ext_publish , instance , loop ) ) <NEWLINE> if self . _fnotify is None : <NEWLINE> fnotify = None <NEWLINE> else : <NEWLINE> if instance is None : <NEWLINE> fnotify = self . _fnotify <NEWLINE> else : <NEWLINE> fnotify = types . MethodType ( instance , self . _fnotify ) <NEWLINE> validator = self . _fvalidation <NEWLINE> if validator is not None and instance is not None : <NEWLINE> validator = types . MethodType ( validator , instance ) <NEWLINE> return Executor ( self_subscribers , owner = self , <NEWLINE> concurrent = SignalOptions . EXEC_CONCURRENT in self . flags , <NEWLINE> loop = loop , exec_wrapper = fnotify , <NEWLINE> fvalidation = validator ) <NEWLINE> <DEDENT>
def global_interpreter ( self , version ) : <NEWLINE> <INDENT> version_name = <STRING> % version <NEWLINE> if Path ( self . bin_path / <STRING> ) . exists ( ) : <NEWLINE> <INDENT> remove ( str ( self . bin_path / <STRING> ) ) <NEWLINE> <DEDENT> symlink ( str ( self . lib_path / version_name / <STRING> / <STRING> ) , str ( self . bin_path / <STRING> ) ) <NEWLINE> <DEDENT>
@ register . filter <NEWLINE> <INDENT> def order_links ( links ) : <NEWLINE> <INDENT> links_list = list ( links ) <NEWLINE> ordered_links = [ ] <NEWLINE> while links_list : <NEWLINE> <INDENT> minor = links_list [ 0 ] <NEWLINE> for link in links_list : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if ( link . link_type . ordering < minor . link_type . ordering ) : <NEWLINE> <INDENT> minor = link <NEWLINE> <DEDENT> <DEDENT> except TypeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> ordered_links . append ( link ) <NEWLINE> links_list . remove ( minor ) <NEWLINE> <DEDENT> return ordered_links <NEWLINE> <DEDENT> <DEDENT>
for topic in topics_list : <NEWLINE> <INDENT> if topic . projects . count ( ) : <NEWLINE> <INDENT> topics_list . append ( ( topic . id , topic . name ) ) <NEWLINE> return topics_list <NEWLINE> <DEDENT> <DEDENT>
request = dict ( get_default_request_parameters ( chosen_request_params ) ) <NEWLINE> <INDENT> check_review_timestamp = bool ( <STRING> in request . keys ( ) and request [ <STRING> ] != <STRING> ) <NEWLINE> if check_review_timestamp : <NEWLINE> <INDENT> current_date = datetime . datetime . now ( ) <NEWLINE> num_days = int ( request [ <STRING> ] ) <NEWLINE> date_threshold = current_date - datetime . timedelta ( days = num_days ) <NEWLINE> timestamp_threshold = datetime . datetime . timestamp ( date_threshold ) <NEWLINE> if verbose : <NEWLINE> <INDENT> if request [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> collection_keyword = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> collection_keyword = <STRING> <NEWLINE> <DEDENT> print ( <STRING> . format ( collection_keyword , <NEWLINE> <INDENT> timestamp_threshold ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _update_data_with_deltas ( self , packet_id , deltas ) : <NEWLINE> <INDENT> for delta_id in [ 0 , 1 ] : <NEWLINE> <COMMENT> <NL> <INDENT> sample_id = ( packet_id - 1 ) * 2 + delta_id + 1 <NEWLINE> <COMMENT> <NL> self . _last_eeg_data += np . array ( deltas [ delta_id ] ) <NEWLINE> self . _update_counts_and_enqueue ( <STRING> , sample_id ) <NEWLINE> <DEDENT> <DEDENT>
def get_fleet ( self ) : <NEWLINE> <INDENT> fleets_list = [ ] <NEWLINE> response = self . session . get ( <STRING> <NEWLINE> <INDENT> . format ( self . server_number , self . server_language ) ) <NEWLINE> <DEDENT> if response . status_code == 302 : <NEWLINE> <INDENT> fleets = response . text . split ( <STRING> ) <NEWLINE> del fleets [ 0 ] <NEWLINE> for fleet in fleets : <NEWLINE> <INDENT> fleet_id = fleet [ 0 : 30 ] . split ( <STRING> ) [ 0 ] <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> fleet_mission = int ( fleet [ marker + 19 : marker + 22 ] . split ( <STRING> ) [ 0 ] ) <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> fleet_arrival = datetime . strptime ( fleet [ marker + 35 : marker + 54 ] , <STRING> ) <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> origin_raw = fleet [ marker : marker + 180 ] <NEWLINE> origin_list = origin_raw . split ( <STRING> ) [ 1 ] . split ( <STRING> ) [ 0 ] . split ( <STRING> ) <NEWLINE> fleet_origin = const . coordinates ( origin_list [ 0 ] , origin_list [ 1 ] , origin_list [ 2 ] ) <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> destination_raw = fleet [ marker : marker + 200 ] <NEWLINE> destination_list = destination_raw . split ( <STRING> ) [ 1 ] . split ( <STRING> ) [ 0 ] . split ( <STRING> ) <NEWLINE> fleet_destination = const . coordinates ( destination_list [ 0 ] , destination_list [ 1 ] , destination_list [ 2 ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not value in self . filter_keys : <NEWLINE> <INDENT> raise ImproperlyConfigured ( <NEWLINE> <INDENT> <STRING> % ( key , self . filter_keys ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if view not in view . get_views ( ) : <NEWLINE> <INDENT> raise ImproperlyConfigured ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> % view <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
setattr ( ModelMixin , <NEWLINE> <INDENT> <STRING> % view_class . get_underscored_action_name ( ) , <NEWLINE> _get_url ) <NEWLINE> <DEDENT>
def read ( self , max_records = None ) : <NEWLINE> <INDENT> array = [ ] <NEWLINE> dictionary = { } <NEWLINE> index_column = self . index_column_number ( ) <NEWLINE> for ( count , entry ) in enumerate ( self . _tfh ) : <NEWLINE> <INDENT> array . append ( entry ) <NEWLINE> dictionary [ entry [ index_column ] ] = entry <COMMENT> <NEWLINE> if max_records and count >= max_records : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def __next__ ( self ) : <NEWLINE> <INDENT> row = next ( self . _consumer ) <NEWLINE> decoded_row = unpackb ( row . value ) <NEWLINE> if self . _end_time : <NEWLINE> <INDENT> self . verbose ( <STRING> ) <NEWLINE> self . verbose ( decoded_row ) <NEWLINE> count += 0 <NEWLINE> while True : <NEWLINE> <INDENT> count += 1 <NEWLINE> decoded_time = decoded_row [ self . _time_column ] <NEWLINE> decoded_time = self . decode_item ( decoded_time ) <NEWLINE> decoded_time = self . parse_time ( decoded_time ) <NEWLINE> if decoded_time >= self . _kafka_end_time : <NEWLINE> <INDENT> self . verbose ( <STRING> + str ( count ) + <STRING> ) <NEWLINE> raise StopIteration ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
results = { <STRING> : conns , <NEWLINE> <INDENT> <STRING> : ports } <NEWLINE> return ( self . _output_key , conns ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for i in range ( num_rounds ) : <NEWLINE> <COMMENT> <NL> <INDENT> m = max ( int ( client_num * C ) , 1 ) <NEWLINE> <COMMENT> <NL> S = np . array ( random . sample ( range ( client_num ) , client_num ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if nstart is None : <NEWLINE> <INDENT> import random <NEWLINE> x = dict ( [ ( n , random . random ( ) ) for n in G ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> x = nstart <NEWLINE> <COMMENT> <NL> <DEDENT> s = 1.0 / sum ( x . values ( ) ) <NEWLINE> for k in x : x [ k ] *= s <NEWLINE> nnodes = G . number_of_nodes ( ) <NEWLINE> <COMMENT> <NL> for i in range ( max_iter ) : <NEWLINE> <INDENT> xlast = x <NEWLINE> x = dict . fromkeys ( xlast . keys ( ) , 0 ) <NEWLINE> <COMMENT> <NL> for n in x : <NEWLINE> <INDENT> for nbr in G [ n ] : <NEWLINE> <INDENT> x [ n ] += xlast [ nbr ] * G [ n ] [ nbr ] <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> s = 1.0 / sum ( x . values ( ) ) <NEWLINE> for n in x : x [ n ] *= s <NEWLINE> <COMMENT> <NL> err = sum ( [ abs ( x [ n ] - xlast [ n ] ) for n in x ] ) <NEWLINE> if err < n * tol : <NEWLINE> <INDENT> return x <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
labelGenerator = _gen_node_label ( G ) <NEWLINE>
if nbunch is None : <NEWLINE> <INDENT> nbunch = G . nodes_iter ( ) <NEWLINE> for v in G : <COMMENT> <NEWLINE> if v in explored : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> fringe = [ v ] <COMMENT> <NEWLINE> while fringe : <NEWLINE> <INDENT> w = fringe [ - 1 ] <COMMENT> <NEWLINE> if w in explored : <COMMENT> <NEWLINE> <INDENT> fringe . pop ( ) <NEWLINE> continue <NEWLINE> <DEDENT> seen [ w ] = 1 <COMMENT> <NEWLINE> <COMMENT> <NL> new_nodes = [ ] <NEWLINE> for n in G [ w ] : <NEWLINE> <INDENT> if n not in explored : <NEWLINE> <INDENT> if n in seen : <COMMENT> <NEWLINE> <INDENT> raise nx . NetworkXUnfeasible ( <STRING> ) <NEWLINE> <DEDENT> new_nodes . append ( n ) <NEWLINE> <DEDENT> <DEDENT> if new_nodes : <COMMENT> <NEWLINE> <INDENT> fringe . extend ( new_nodes ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> explored [ w ] = 1 <NEWLINE> order_explored . insert ( 0 , w ) <COMMENT> <NEWLINE> fringe . pop ( ) <COMMENT> <NEWLINE> return order_explored <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def ambig_binary_crossentropy ( y_true , y_pred ) : <NEWLINE> <INDENT> non_ambig = K . cast ( ( y_true > - 0.5 ) , <STRING> ) <NEWLINE> return K . mean ( K . binary_crossentropy ( y_pred , y_true ) <NEWLINE> <INDENT> * non_ambig , axis = - 1 ) <NEWLINE> <DEDENT> <DEDENT>
def execute_from_command_line ( self , argv = None ) : <NEWLINE> <INDENT> if argv is None : <NEWLINE> <INDENT> argv = sys . argv <NEWLINE> <DEDENT> args = self . parser . parse_args ( argv [ 1 : ] ) <NEWLINE> self . nekumo . gateways = list ( self . parse_gateways ( args ) ) <NEWLINE> self . nekumo . ifaces = list ( self . parse_ifaces ( args ) ) <NEWLINE> if <STRING> in os . environ : <NEWLINE> <INDENT> loop = asyncio . get_event_loop ( ) <NEWLINE> loop . run_forever ( ) <NEWLINE> <DEDENT> <DEDENT>
if token == <STRING> or tk_end == len ( sql ) : <NEWLINE> <INDENT> sqls . append ( sql [ beg : tk_end ] ) <NEWLINE> beg = tk_end <NEWLINE> level = 0 <NEWLINE> status = <STRING> <NEWLINE> self . cte_dico = { } <NEWLINE> elif token == <STRING> and not cte_inline : <NEWLINE> if tk_value . lower ( ) == <STRING> : <NEWLINE> <INDENT> from_lvl [ level ] = True <NEWLINE> <DEDENT> elif from_lvl [ level ] : <NEWLINE> <INDENT> if last_other in ( <STRING> , <STRING> , <STRING> ) and ( <NEWLINE> tk_value in self . cte_dico ) : <NEWLINE> <COMMENT> <NL> <INDENT> bg , en , tknext = tk_end , tk_end , <STRING> <NEWLINE> while en < length and tknext == <STRING> : <NEWLINE> <INDENT> bg , ( en , tknext ) = en , self . get_token ( sql , en ) <NEWLINE> <COMMENT> <NL> <DEDENT> if sql [ bg : en ] . lower ( ) != <STRING> : <NEWLINE> <INDENT> sql2 = ( sql [ : end ] + <STRING> + self . cte_dico [ tk_value ] + <NEWLINE> <INDENT> <STRING> + tk_value + <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> sql2 = ( sql [ : end ] + <STRING> + self . cte_dico [ tk_value ] + <NEWLINE> <INDENT> <STRING> + <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
for table_ref in tables : <NEWLINE> <INDENT> table_sql = table_ref + <STRING> <NEWLINE> df = env [ table_ref ] <NEWLINE> df = self . _ensure_data_frame ( df , table_ref ) <NEWLINE> <COMMENT> <NL> pre_q = <STRING> % table_sql <NEWLINE> cur = self . _execute_sql ( pre_q ) <NEWLINE> self . _write_table ( table_sql , df , self . conn ) <NEWLINE> <COMMENT> <NL> for q_single in self . get_sqlsplit ( sql , True ) : <NEWLINE> if q_single . strip ( ) != <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> self . remove_tmp_tables ( <STRING> ) <NEWLINE> cur = self . _execute_cte ( q_single , env ) <NEWLINE> return cur <NEWLINE> <DEDENT> <DEDENT>
global MASK_PREDICTOR_HANDLER <NEWLINE> <INDENT> with LOCK : <NEWLINE> <INDENT> if MASK_PREDICTOR_HANDLER is None : <NEWLINE> <INDENT> MASK_PREDICTOR_HANDLER = MaskPredictor ( deepLearningModel , boxSize , gpus ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
with MaskPredictor ( deepLearningModelFname , boxSize , gpus = [ 0 ] ) as mp : <NEWLINE> <INDENT> mask = mp . predictMask ( mic ) <NEWLINE> <DEDENT>
def get_current_session ( self ) : <NEWLINE> <INDENT> if self . session [ <STRING> ] : <NEWLINE> <INDENT> if ( <NEWLINE> <INDENT> arrow . get ( self . session [ <STRING> ] ) . shift ( <NEWLINE> <INDENT> seconds = int ( self . config [ <STRING> ] ) <NEWLINE> <DEDENT> ) <NEWLINE> < arrow . utcnow ( ) <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> return self . session <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_history ( self , start_time , end_time ) : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for day in arrow . Arrow . range ( <STRING> , start_time , end_time ) : <NEWLINE> <INDENT> data = self . request ( <NEWLINE> <INDENT> self . DATA_ENDPOINT , <NEWLINE> method = <STRING> , <NEWLINE> params = { <STRING> : day . format ( <STRING> ) } , <NEWLINE> <DEDENT> ) <NEWLINE> result . append ( Day . _from_data ( start_time , data ) ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> rating_true = rating_true [ [ col_user , col_item , col_rating ] ] <NEWLINE> rating_pred = rating_true [ [ col_user , col_item , col_prediction ] ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for i in range ( len ( split_index ) ) : <NEWLINE> <INDENT> splits [ i ] [ <STRING> ] = i <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def from_string ( cls , s ) : <NEWLINE> <COMMENT> <NL> <INDENT> val = base32hex . b32decode ( s . upper ( ) ) <NEWLINE> value_check = [ 0 < x < 255 for x in val ] <NEWLINE> <DEDENT> <DEDENT>
if <STRING> in binding : <NEWLINE> <INDENT> if <STRING> not in datum : <NEWLINE> <INDENT> datum [ <STRING> ] = [ ] <NEWLINE> <DEDENT> for sf in aslist ( schema [ <STRING> ] ) : <NEWLINE> <INDENT> if isinstance ( sf , dict ) : <NEWLINE> <INDENT> sfpath = expression . do_eval ( sf , self . job , self . requirements , self . docpath , datum [ <STRING> ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sfpath = { <STRING> : substitute ( datum [ <STRING> ] , sf ) } <NEWLINE> <DEDENT> if isinstance ( sfpath , list ) : <NEWLINE> <INDENT> datum [ <STRING> ] . extend ( sfpath ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> datum [ <STRING> ] . append ( sfpath ) <NEWLINE> <DEDENT> self . files . append ( sfpath ) <NEWLINE> <DEDENT> <DEDENT>
def _draft2toDraft3dev1 ( doc , loader , baseuri ) : <COMMENT> <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if isinstance ( doc , dict ) : <NEWLINE> <INDENT> if <STRING> in doc : <NEWLINE> <INDENT> imp = urlparse . urljoin ( baseuri , doc [ <STRING> ] ) <NEWLINE> impLoaded = loader . fetch ( imp ) <NEWLINE> r = None <COMMENT> <NEWLINE> if isinstance ( impLoaded , list ) : <NEWLINE> <INDENT> r = { <STRING> : r } <NEWLINE> <DEDENT> elif isinstance ( impLoaded , dict ) : <NEWLINE> <INDENT> r = impLoaded <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> r [ <STRING> ] = imp <NEWLINE> _ , frag = urlparse . urldefrag ( imp ) <NEWLINE> if frag : <NEWLINE> <INDENT> frag = <STRING> + frag <NEWLINE> r = findId ( r , frag ) <NEWLINE> <DEDENT> return _draft2toDraft3dev1 ( r , loader , imp ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _resolve_idmap ( self , document , loader ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> for idmapField in loader . idmap : <NEWLINE> <INDENT> if ( idmapField in document ) : <NEWLINE> <INDENT> idmapFieldValue = document [ idmapField ] <NEWLINE> if ( isinstance ( idmapFieldValue , dict ) <NEWLINE> <INDENT> and <STRING> not in idmapFieldValue <NEWLINE> and <STRING> not in idmapFieldValue ) : <NEWLINE> ls = [ ] <NEWLINE> for k in sorted ( idmapFieldValue . keys ( ) ) : <NEWLINE> val = idmapFieldValue [ k ] <NEWLINE> v = None <COMMENT> <NEWLINE> if not isinstance ( v , dict ) : <NEWLINE> <INDENT> if idmapField in loader . mapPredicate : <NEWLINE> <INDENT> v = { loader . mapPredicate [ idmapField ] : val } <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise validate . ValidationException ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> , k , v ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v = val <NEWLINE> <DEDENT> v [ loader . idmap [ idmapField ] ] = k <NEWLINE> ls . append ( v ) <NEWLINE> document [ idmapField ] = ls <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
async_result = gevent . event . AsyncResult ( ) <NEWLINE> <INDENT> gevent . spawn ( self . _process_response , method , bufchan , <NEWLINE> <INDENT> timeout ) . link ( async_result ) <NEWLINE> <DEDENT> return async_result <NEWLINE> except : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> bufchan . close ( ) <NEWLINE> raise <NEWLINE> <DEDENT>
def get_name ( self , regex , path = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if path is None : <NEWLINE> <INDENT> path = self . path <NEWLINE> <DEDENT> return super ( CamImage , self ) . get_name ( path , regex ) <NEWLINE> <DEDENT>
highlight = 0 <NEWLINE> <INDENT> page = 0 <NEWLINE> state = <STRING> <NEWLINE> key = 0 <NEWLINE> try : <NEWLINE> <INDENT> while key != ord ( <STRING> ) : <NEWLINE> <INDENT> if windowsize [ 0 ] > 8 and windowsize [ 1 ] > 30 : <NEWLINE> <INDENT> win_l . clear ( ) <NEWLINE> win_l . border ( 0 ) <NEWLINE> win_r . clear ( ) <NEWLINE> win_r . border ( 0 ) <NEWLINE> win_l . addstr ( windowsize [ 0 ] - 1 , windowsize [ 1 ] // 2 - 9 , <STRING> + str ( page + 1 ) ) <NEWLINE> win_r . addstr ( windowsize [ 0 ] - 4 , windowsize [ 1 ] // 2 - 11 , <STRING> ) <NEWLINE> win_r . addstr ( windowsize [ 0 ] - 3 , windowsize [ 1 ] // 2 - 12 , <STRING> ) <NEWLINE> win_r . addstr ( windowsize [ 0 ] - 2 , windowsize [ 1 ] // 2 - 9 , <STRING> ) <NEWLINE> index = 0 <NEWLINE> if state == <STRING> : <NEWLINE> <INDENT> totalitems = len ( data [ <STRING> ] ) <NEWLINE> currentpage = data [ <STRING> ] [ maxitems * page : maxitems * ( page + 1 ) ] <NEWLINE> for i in currentpage : <NEWLINE> <INDENT> if index < maxitems : <NEWLINE> <INDENT> if index == highlight : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen , curses . A_REVERSE ) <NEWLINE> win_r . addnstr ( 2 , 3 , <STRING> + str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> win_r . addnstr ( 3 , 3 , <STRING> + str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen ) <NEWLINE> <DEDENT> <DEDENT> index += 1 <NEWLINE> <DEDENT> <DEDENT> if state == <STRING> : <NEWLINE> <INDENT> totalitems = len ( data [ <STRING> ] ) <NEWLINE> currentpage = data [ <STRING> ] [ maxitems * page : maxitems * ( page + 1 ) ] <NEWLINE> for i in currentpage : <NEWLINE> <INDENT> if index < maxitems : <NEWLINE> <INDENT> if index == highlight : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen , curses . A_REVERSE ) <NEWLINE> win_r . addnstr ( 2 , 3 , str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> win_r . addnstr ( 4 , 3 , <STRING> + str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> win_r . addstr ( 5 , 3 , <STRING> ) <NEWLINE> status = textwrap . wrap ( str ( i [ <STRING> ] [ <STRING> ] ) , windowsize [ 1 ] // 2 - 6 ) <NEWLINE> l_num = 7 <NEWLINE> for line in status : <NEWLINE> <INDENT> if l_num >= windowsize [ 0 ] - 2 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> win_r . addstr ( l_num , 4 , line ) <NEWLINE> l_num += 1 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen ) <NEWLINE> <DEDENT> <DEDENT> index += 1 <NEWLINE> <DEDENT> <DEDENT> win_l . refresh ( ) <NEWLINE> win_r . refresh ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stdscr . clear ( ) <NEWLINE> stdscr . addstr ( 0 , 0 , <STRING> ) <NEWLINE> stdscr . addstr ( 1 , 0 , <STRING> ) <NEWLINE> <DEDENT> key = stdscr . getch ( ) <NEWLINE> if key == curses . KEY_DOWN : <NEWLINE> <INDENT> if highlight + page * maxitems + 1 != totalitems : <NEWLINE> <INDENT> if highlight + 1 == maxitems : <NEWLINE> <INDENT> page += 1 <NEWLINE> highlight = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> highlight += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif key == curses . KEY_UP : <NEWLINE> <INDENT> if highlight == 0 and page > 0 : <NEWLINE> <INDENT> page -= 1 <NEWLINE> highlight = maxitems - 1 <NEWLINE> <DEDENT> elif highlight > 0 : <NEWLINE> <INDENT> highlight -= 1 <NEWLINE> <DEDENT> <DEDENT> elif key == curses . KEY_NPAGE and totalitems > ( page + 1 ) * maxitems : <NEWLINE> <INDENT> highlight = 0 <NEWLINE> page += 1 <NEWLINE> <DEDENT> elif key == curses . KEY_PPAGE and page > 0 : <NEWLINE> <INDENT> highlight = 0 <NEWLINE> page -= 1 <NEWLINE> <DEDENT> elif key == curses . KEY_RIGHT or key == 10 : <NEWLINE> <INDENT> if state == <STRING> : <NEWLINE> <INDENT> curses . nocbreak ( ) ; stdscr . keypad ( 0 ) ; curses . echo ( ) <NEWLINE> curses . endwin ( ) <NEWLINE> print ( <STRING> ) <NEWLINE> subprocess . call ( [ <STRING> , currentpage [ highlight ] [ <STRING> ] [ <STRING> ] , <STRING> ] ) <NEWLINE> stdscr = curses . initscr ( ) <NEWLINE> curses . noecho ( ) <NEWLINE> curses . cbreak ( ) <NEWLINE> stdscr . keypad ( 1 ) <NEWLINE> <DEDENT> elif state == <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> query = [ currentpage [ highlight ] [ <STRING> ] [ <STRING> ] , 0 ] <NEWLINE> data = query_twitch ( query [ 0 ] , query [ 1 ] ) <NEWLINE> state = <STRING> <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> <DEDENT> elif key == curses . KEY_LEFT : <NEWLINE> <INDENT> if state != <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> data = query_twitch ( <STRING> , 0 ) <NEWLINE> state = <STRING> <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> <DEDENT> elif key == ord ( <STRING> ) : <NEWLINE> <INDENT> searchbox = curses . newwin ( 3 , windowsize [ 1 ] - 4 , windowsize [ 0 ] // 2 - 1 , 2 ) <NEWLINE> searchbox . border ( 0 ) <NEWLINE> searchbox . addnstr ( 0 , 3 , <STRING> , windowsize [ 0 ] - 4 ) <NEWLINE> searchbox . refresh ( ) <NEWLINE> curses . echo ( ) <NEWLINE> s = searchbox . getstr ( 1 , 1 , windowsize [ 1 ] - 6 ) <NEWLINE> init_display ( stdscr ) <NEWLINE> query = [ s . decode ( <STRING> ) , 1 ] <NEWLINE> data = query_twitch ( query [ 0 ] , query [ 1 ] ) <NEWLINE> state = <STRING> <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> elif key == ord ( <STRING> ) : <NEWLINE> <INDENT> if state == <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> data = query_twitch ( query [ 0 ] , query [ 1 ] ) <NEWLINE> <DEDENT> elif state == <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> data = query_twitch ( <STRING> , 0 ) <NEWLINE> <DEDENT> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> elif key == curses . KEY_RESIZE : <NEWLINE> <INDENT> windowsize = init_display ( stdscr ) <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> curses . nocbreak ( ) ; stdscr . keypad ( 0 ) ; curses . echo ( ) <NEWLINE> curses . endwin ( ) <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def get_properties ( self , items ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _check_values ( ) <NEWLINE> item_widths = [ len ( item ) for item in items ] <NEWLINE> if not item_widths : <NEWLINE> <INDENT> column_widths , num_lines = [ ] , 0 <NEWLINE> <DEDENT> elif any ( width > self . max_line_width for width in item_widths ) : <NEWLINE> <INDENT> column_widths , num_lines = [ self . max_line_width ] , len ( item_widths ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> column_widths , num_lines = self . calculate_columns ( item_widths ) <NEWLINE> <DEDENT> return LineProperties ( column_widths , self . spacing , num_lines ) <NEWLINE> <DEDENT>
for attr_name , member in class_state . iteritems ( ) : <NEWLINE> <INDENT> if callable ( member ) : <NEWLINE> <INDENT> deps_used = parse_deps_used ( member ) <NEWLINE> invalid_ports = deps_used . difference ( needs_ports_defined ) . difference ( RESERVED_PORT_NAMES ) <NEWLINE> all_deps_used . update ( deps_used ) <NEWLINE> if invalid_ports : <NEWLINE> <INDENT> raise UnknownPort ( <STRING> . format ( <NEWLINE> <INDENT> class_name , <NEWLINE> attr_name , <NEWLINE> <STRING> . join ( sorted ( deps_used ) ) <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def copy_and_tag ( variable , role , name ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> copy = variable . copy ( ) <NEWLINE> copy . name = <STRING> . format ( <COMMENT> <NEWLINE> <INDENT> brick . name , self . name , name ) <NEWLINE> <DEDENT> annotations = getattr ( copy . tag , <STRING> , [ ] ) + [ brick , call ] <NEWLINE> copy . tag . annotations = annotations <NEWLINE> copy . tag . name = name <COMMENT> <NEWLINE> VariableRole . add_role ( variable , role ) <NEWLINE> return copy <NEWLINE> <DEDENT> <DEDENT>
def check_gaussian ( rng , mean , std , shape ) : <NEWLINE> <INDENT> weights = IsotropicGaussian ( mean , std ) . generate ( rng , shape ) <NEWLINE> assert weights . shape == shape <NEWLINE> assert weights . dtype == theano . config . floatX <NEWLINE> assert_allclose ( weights . mean ( ) , mean , atol = 1e-2 ) <NEWLINE> assert_allclose ( weights . std ( ) , std , atol = 1e-2 ) <NEWLINE> yield check_gaussian , rng , 0 , 1 , ( 500 , 600 ) <NEWLINE> yield check_gaussian , rng , 5 , 3 , ( 600 , 500 ) <NEWLINE> <DEDENT>
params = bricks . get_params ( ) <NEWLINE> <INDENT> for name in params . keys ( ) : <NEWLINE> <INDENT> if name not in params : <NEWLINE> <INDENT> logger . error ( <STRING> <NEWLINE> <INDENT> . format ( name ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
