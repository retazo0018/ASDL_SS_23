def compute_step ( self , param , previous_step ) : <NEWLINE> <INDENT> if any ( axis >= previous_step . ndim for axis in self . axes ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( <NEWLINE> <INDENT> self . axes , param , param . ndim ) ) <NEWLINE> <DEDENT> <DEDENT> squares = tensor . sqr ( previous_step ) <NEWLINE> if len ( self . axes ) == 0 : <NEWLINE> <INDENT> norms = l2_norm ( [ previous_step ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> norms = tensor . sqrt ( <NEWLINE> <INDENT> reduce ( lambda t , a : t . sum ( axis = a , keepdims = True ) , <NEWLINE> <INDENT> sorted ( self . axes ) , squares ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return ( previous_step * tensor . switch ( norms > self . threshold , <NEWLINE> <INDENT> self . threshold / norms , <NEWLINE> 1. ) , ( ) ) <NEWLINE> <DEDENT> <DEDENT>
cleaned_img1d = geometry_converter . image_2d_to_1d ( reference_img , fits_metadata_dict [ <STRING> ] ) <NEWLINE> <INDENT> hillas_params_2_cleaned_img = get_hillas_parameters ( geom1d , cleaned_img1d , HILLAS_IMPLEMENTATION ) <COMMENT> <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if len ( filter_thresholds_str ) != ( self . num_scales - 1 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( len ( filter_thresholds ) , <NEWLINE> <INDENT> self . num_scales - 1 ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
class Inertial ( object ) : <NEWLINE> <INDENT> def __init__ ( self , ixx = 0.0 , ixy = 0.0 , ixz = 0.0 , iyy = 0.0 , iyz = 0.0 , izz = 0.0 , <NEWLINE> <INDENT> mass = 0.0 , origin = None ) : <NEWLINE> self . matrix = { } <NEWLINE> self . matrix [ <STRING> ] = ixx <NEWLINE> self . matrix [ <STRING> ] = iyy <NEWLINE> self . matrix [ <STRING> ] = ixz <NEWLINE> self . matrix [ <STRING> ] = iyy <NEWLINE> self . matrix [ <STRING> ] = iyz <NEWLINE> self . matrix [ <STRING> ] = izz <NEWLINE> self . mass = mass <NEWLINE> self . origin = origin <NEWLINE> <DEDENT> <DEDENT>
def __cal_flag_len ( self ) : <NEWLINE> <INDENT> if Markers . __flag_len < len ( self . __flag_name ) : <NEWLINE> <INDENT> Markers . __flag_len = len ( self . __flag_name ) + 2 <NEWLINE> <DEDENT> <DEDENT>
def refmac ( self , cycles ) : <NEWLINE> <INDENT> directory = self . job_directory ( <STRING> ) <NEWLINE> use_phases = self . args . unbiased and self . min_rwork > 0.35 <NEWLINE> job = Refmac ( self . args , directory , self . current_xyz , use_phases , cycles ) <NEWLINE> self . jobs [ self . cycle ] . append ( job ) <NEWLINE> self . current_hkl = job . hklout <NEWLINE> self . current_xyz = job . xyzout <NEWLINE> return job <NEWLINE> <DEDENT>
def _parse_address ( address ) : <NEWLINE> <INDENT> if isinstance ( address , tuple ) : <NEWLINE> <INDENT> if <STRING> in address [ 0 ] : <NEWLINE> <INDENT> return _socket . AF_INET6 , address <NEWLINE> <DEDENT> return _socket . AF_INET , address <NEWLINE> <DEDENT> elif isinstance ( address , string_types ) : <NEWLINE> <INDENT> if <STRING> in address : <NEWLINE> <INDENT> host , port = address . rsplit ( <STRING> , 1 ) <NEWLINE> family , host = _extract_family ( host ) <NEWLINE> if host == <STRING> : <NEWLINE> <INDENT> host = <STRING> <NEWLINE> <DEDENT> return family , ( host , int ( port ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return _socket . AF_INET , ( <STRING> , int ( port ) ) <NEWLINE> <DEDENT> <DEDENT> elif isinstance ( address , integer_types ) : <NEWLINE> <INDENT> return _socket . AF_INET , ( <STRING> , int ( address ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise TypeError ( <STRING> % type ( address ) ) <NEWLINE> <DEDENT> <DEDENT>
tweets = self . _filter_timeline ( <NEWLINE> <INDENT> self . _twitter_data . iter_tweets_from ( user_id ) , count , since_id , <NEWLINE> max_id ) <NEWLINE> if exclude_replies : <NEWLINE> tweets = [ tweet for tweet in tweets if tweet . reply_to is not None ] <NEWLINE> if include_rts is not None : <NEWLINE> raise NotImplementedError ( <STRING> ) <NEWLINE> return [ <NEWLINE> tweet . to_dict ( <NEWLINE> <INDENT> self . _twitter_data , trim_user = trim_user , <NEWLINE> contributor_details = contributor_details ) <NEWLINE> <DEDENT> for tweet in tweets ] <NEWLINE> <DEDENT>
def has ( self , blockname , ptype ) : <NEWLINE> <INDENT> return self . reader . has_block ( self , ptype , blockname ) <NEWLINE> <DEDENT>
class LocalURI ( SwedishLegalSource ) : <NEWLINE> <INDENT> def infer_triples ( self , d , basefile = None ) : <NEWLINE> <INDENT> super ( self , LocalURI ) . infer_triples ( d , basefile ) <NEWLINE> canonicalminter = ... <NEWLINE> sameas = self . canonicalminter ( d ) <NEWLINE> d . rel ( OWL . sameAs , sameas ) <NEWLINE> <DEDENT> <DEDENT>
self . validate_body ( body , basefile ) <COMMENT> <NEWLINE>
def test_unreliable_fontspec ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . _f ( <STRING> ) <NEWLINE> self . _f ( <STRING> ) <NEWLINE> textbox = self . _p ( <STRING> ) <NEWLINE> prevbox = self . _p ( <STRING> ) <NEWLINE> self . assertTrue ( self . gluefunc ( prevbox , prevbox , textbox ) ) <NEWLINE> textbox = textbox + prevbox <NEWLINE> nextbox = self . _p ( <STRING> ) <NEWLINE> self . assertTrue ( self . gluefunc ( textbox , nextbox , prevbox ) ) <NEWLINE> textbox = textbox + nextbox <NEWLINE> prevbox = nextbox <NEWLINE> nextbox = self . _p ( <STRING> ) <NEWLINE> self . assertTrue ( self . gluefunc ( textbox , nextbox , prevbox ) ) <NEWLINE> <DEDENT>
if not fetched : <NEWLINE> <INDENT> self . log . error ( <STRING> % url ) <NEWLINE> return False <NEWLINE> <COMMENT> <NL> except requests . exceptions . RequestException as e : <NEWLINE> self . log . error ( <STRING> % ( url , e ) ) <NEWLINE> raise e <NEWLINE> if response . status_code == 304 : <NEWLINE> self . log . debug ( <STRING> % url ) <NEWLINE> return False <COMMENT> <NEWLINE> elif response . status_code > 400 : <NEWLINE> self . log . error ( <STRING> % url ) <NEWLINE> response . raise_for_status ( ) <NEWLINE> <DEDENT>
if inspect . isclass ( local_getter ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> local_getter = getter ( url ) <NEWLINE> <DEDENT>
def __init__ ( self , sep = SCOPE_SEPARATOR ) : <NEWLINE> <INDENT> self . __sep = sep <NEWLINE> super ( self , ScopeDict ) . __init__ ( ) <NEWLINE> <DEDENT>
if self . _chunk_size is not None and len ( data ) > self . _chunk_size : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> chunked_headers = dict ( headers ) <NEWLINE> chunked_headers [ <STRING> ] = <STRING> <NEWLINE> chunked_headers [ <STRING> ] = <STRING> <NEWLINE> chunked_headers [ <STRING> ] = <STRING> <NEWLINE> data_chunks = serve_data_chunks ( data ) <NEWLINE> response = self . _session . post ( <NEWLINE> <INDENT> url = url , <NEWLINE> data = data_chunks , <NEWLINE> headers = headers <NEWLINE> <DEDENT> ) <NEWLINE> else : <NEWLINE> response = self . _session . post ( url = url , data = data , headers = headers ) <NEWLINE> logger . debug ( <STRING> . format ( response . status_code ) ) <NEWLINE> try : <NEWLINE> response . raise_for_status ( ) <NEWLINE> except requests . exceptions . HTTPError as error : <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> pattern = re . compile ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> match = re . match ( pattern , self . base_url ) <NEWLINE> if match is None : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( self . base_url ) ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> self . protocol = match . group ( <STRING> ) <NEWLINE> self . host = match . group ( <STRING> ) <NEWLINE> port = match . group ( <STRING> ) <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( self . base_url ) ) <NEWLINE> <DEDENT> if port : <NEWLINE> <INDENT> self . port = int ( port ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if self . protocol == <STRING> : <NEWLINE> <INDENT> self . port = 80 <NEWLINE> <DEDENT> elif self . protocol == <STRING> : <NEWLINE> <INDENT> self . port = 443 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( self . protocol ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> url_components = urlparse ( url ) <NEWLINE> self . url_prefix = url_components . path <NEWLINE> if headers is not None : <NEWLINE> <INDENT> self . _session . headers . update ( headers ) <NEWLINE> <DEDENT> if proxies is not None : <NEWLINE> <INDENT> self . _session . proxies = proxies <NEWLINE> <DEDENT> if callback is not None : <NEWLINE> <INDENT> self . _session . hooks = { <STRING> : [ callback , ] } <NEWLINE> <DEDENT> if chunk_size is not None : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> self . _session . headers . update ( { <STRING> : self . host } ) <NEWLINE> <DEDENT> self . _chunk_size = chunk_size <NEWLINE> self . set_http_retry_params ( ) <NEWLINE> <DEDENT>
def form_invalid ( self , form ) : <NEWLINE> <INDENT> for sub_form in form : <NEWLINE> <INDENT> update_valid_or_invalid_form_fields ( form ) <NEWLINE> for error in sub_form . errors : <NEWLINE> <INDENT> messages . error ( self . request , sub_form . errors [ error ] ) <NEWLINE> <DEDENT> <DEDENT> return self . get_success_url ( ) <NEWLINE> <DEDENT>
content = package_linkspec . get ( <STRING> , None ) <NEWLINE> <INDENT> if not content : <NEWLINE> <INDENT> utils . fs_link ( item_source , item_target , hard_link = True , forced = forced ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> content_items = [ p for item in content for p in <NEWLINE> <INDENT> glob . glob ( os . path . abspath ( os . path . join ( item_source , item ) ) ) ] <NEWLINE> <DEDENT> for content_item in content_items : <NEWLINE> <INDENT> content_item_name = os . path . basename ( content_item ) <NEWLINE> content_item_target = os . path . abspath ( os . path . join ( item_target , content_item_name ) ) <NEWLINE> utils . fs_link ( content_item , content_item_target , hard_link = True , forced = forced ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def generate_mock ( mocked_module , mock_prototype ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> mock_filename = <STRING> . format ( mocked_module ) <NEWLINE> include_filename = <STRING> . format ( mock_filename ) <NEWLINE> logger . debug ( <STRING> , os . getcwd ( ) ) <NEWLINE> logger . debug ( <STRING> , mock_filename ) <NEWLINE> logger . debug ( <STRING> , include_filename ) <NEWLINE> logger . debug ( <STRING> , mock_prototype ) <NEWLINE> if os . path . exists ( mock_filename ) : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> mock_file = open ( mock_filename , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> mock_file = open ( mock_filename , <STRING> ) <NEWLINE> write_header ( mock_file , FILE_HEADER , include_filename ) <NEWLINE> <DEDENT> add_mock_function ( mock_file , mock_prototype ) <NEWLINE> mock_file . close ( ) <NEWLINE> <DEDENT>
return list_detail . object_list ( <NEWLINE> <INDENT> request , <NEWLINE> queryset = Post . objects . filter ( tags__name__in = [ slug ] ) , <NEWLINE> paginate_by = blog_settings . BLOG_PAGESIZE , <NEWLINE> page = page , <NEWLINE> extra_context = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : tag . id , <NEWLINE> <STRING> : tag <NEWLINE> <DEDENT> } , <NEWLINE> template_name = <STRING> , <NEWLINE> ** kwargs <NEWLINE> ) <NEWLINE> <DEDENT>
while event_aux < end_utc : <NEWLINE> <INDENT> result . append ( ( event_aux , event_aux . tzinfo . normalize ( event_aux + event_duration ) , 1 ) ) <NEWLINE> event_aux = add_delta_dst ( event_aux , delta ) <NEWLINE> return result <NEWLINE> <DEDENT>
if destination_ip : <NEWLINE> <INDENT> destination_ip_match = False <NEWLINE> if loose_match and <STRING> in rule . destination : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for object_string in rule . destination : <NEWLINE> <COMMENT> <NL> <INDENT> obj = get_object ( device , dev_group , object_string ) <NEWLINE> <COMMENT> <NL> if obj is False : <NEWLINE> <INDENT> if <STRING> in object_string : <NEWLINE> <INDENT> obj = ipaddress . ip_address ( unicode ( destination_ip ) ) <NEWLINE> destination_range = object_string . split ( <STRING> ) <NEWLINE> destination_lower = ipaddress . ip_address ( unicode ( destination_range [ 0 ] ) ) <NEWLINE> destination_upper = ipaddress . ip_address ( unicode ( destination_range [ 1 ] ) ) <NEWLINE> if destination_lower <= obj <= destination_upper : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if source_ip == object_string : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if isinstance ( obj , objects . AddressObject ) and addr_in_obj ( destination_ip , obj ) : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> elif isinstance ( obj , objects . AddressGroup ) and obj . static_value : <NEWLINE> <INDENT> for member_string in obj . static_value : <NEWLINE> <INDENT> member = get_object ( device , dev_group , member_string ) <NEWLINE> if addr_in_obj ( destination_ip , member ) : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> hitlist . append ( destination_ip_match ) <NEWLINE> <DEDENT>
for sample_idx in range ( len ( y_train_ ) ) : <NEWLINE>
return_key_values = _get_versions ( ) <NEWLINE> <INDENT> return_key_values [ <STRING> ] = pieces [ <STRING> ] <NEWLINE> return return_key_values <NEWLINE> <DEDENT>
def get_web_config_apps ( web_config ) : <NEWLINE> <INDENT> doc = minidom . parse ( web_config ) <NEWLINE> for fcgi in doc . getElementsByTagName ( <STRING> ) : <NEWLINE> <INDENT> for app in doc . getElementsByTagName ( <STRING> ) : <NEWLINE> <INDENT> yield dict ( ( key , value ) for key , value in app . attributes . items ( ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
@ csrf_exempt <NEWLINE> <INDENT> def createAccount ( request ) : <NEWLINE> <INDENT> if <STRING> not in request . POST or <STRING> not in request . POST : <NEWLINE> <INDENT> return HttpResponse ( status = 500 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def data_is_valid ( post_data , postback_url = POSTBACK_URL ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> post_str = urlencode ( _values_to_encode ( post_data ) ) <NEWLINE> try : <NEWLINE> <INDENT> response = urllib2 . urlopen ( postback_url , post_data ) . read ( ) <NEWLINE> <DEDENT> except urllib2 . HTTPError : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> if response == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if response == <STRING> : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> display_filters = [ ] <NEWLINE> if <STRING> in request . GET : <NEWLINE> <INDENT> filter_val = request . GET [ <STRING> ] <NEWLINE> <COMMENT> <NL> q = solr . query ( collection_label = <STRING> % filter_val ) <NEWLINE> <COMMENT> <NL> unfacet_urlopts = url_params . copy ( ) <NEWLINE> del unfacet_urlopts [ <STRING> ] <NEWLINE> display_filters . append ( ( <STRING> , filter_val , <NEWLINE> <INDENT> unfacet_urlopts . urlencode ( ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
tissues = samples . groupby ( <STRING> ) . groups <NEWLINE> <INDENT> norm = normalize . Normalizer ( <STRING> ) <NEWLINE> for t in tissues : <NEWLINE> <INDENT> print ( <STRING> . format ( t ) ) <NEWLINE> <COMMENT> <NL> index = list ( samples [ <STRING> ] . loc [ tissues [ t ] ] . values ) <NEWLINE> tissue_expression = ( expression [ index ] . T ) <NEWLINE> <COMMENT> <NL> tissue_expression . fillna ( 0.0 , inplace = True ) <NEWLINE> <COMMENT> <NL> tissue_expression . columns = convert . clean_ensembl_ids ( tissue_expression . columns ) <NEWLINE> tissue_expression = normalize . deduplicate ( tissue_expression ) <NEWLINE> <COMMENT> <NL> tpm = norm . tpm_from_counts ( tissue_expression ) <NEWLINE> <COMMENT> <NL> mean = pandas . concat ( <NEWLINE> <INDENT> [ mean , pandas . DataFrame ( tpm . mean ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> median = pandas . concat ( <NEWLINE> <INDENT> [ median , pandas . DataFrame ( tpm . median ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> std = pandas . concat ( <NEWLINE> <INDENT> [ std , pandas . DataFrame ( tpm . std ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> lower_quartile = pandas . concat ( <NEWLINE> <INDENT> [ lower_quartile , pandas . DataFrame ( <NEWLINE> <INDENT> tpm . quantile ( q = 0.25 , axis = 0 ) ) . rename ( columns = { 0.25 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> upper_quartile = pandas . concat ( <NEWLINE> <INDENT> [ upper_quartile , pandas . DataFrame ( <NEWLINE> <INDENT> tpm . quantile ( q = 0.75 , axis = 0 ) ) . rename ( columns = { 0.75 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> fraction_zero = pandas . concat ( [ <NEWLINE> <INDENT> fraction_zero , pandas . DataFrame ( <NEWLINE> <INDENT> ( tpm == 0 ) . mean ( ) . astype ( float ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> clr = norm . clr_from_tpm ( tissue_expression , imputer = normalize . impute ) <NEWLINE> mean_clr = pandas . concat ( <NEWLINE> <INDENT> [ mean_clr , pandas . DataFrame ( clr . mean ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> median_clr = pandas . concat ( <NEWLINE> <INDENT> [ median_clr , pandas . DataFrame ( clr . median ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> std_clr = pandas . concat ( <NEWLINE> <INDENT> [ std_clr , pandas . DataFrame ( clr . std ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> lower_quartile_clr = pandas . concat ( <NEWLINE> <INDENT> [ lower_quartile_clr , pandas . DataFrame ( <NEWLINE> <INDENT> clr . quantile ( q = 0.25 , axis = 0 ) ) . rename ( columns = { 0.25 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> upper_quartile_clr = pandas . concat ( <NEWLINE> <INDENT> [ upper_quartile_clr , pandas . DataFrame ( <NEWLINE> <INDENT> clr . quantile ( q = 0.75 , axis = 0 ) ) . rename ( columns = { 0.75 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if DatabasePopulator . Tags . POST_OP in kwargs and is_test : <NEWLINE> <INDENT> fn = kwargs [ DatabasePopulator . Tags . POST_OP ] <NEWLINE> value = self . _database_populator . get_replacement ( <NEWLINE> <INDENT> key , <NEWLINE> modify_before_resaving_fn = fn , <NEWLINE> default_value = default_value , <NEWLINE> <DEDENT> ) <NEWLINE> else : <NEWLINE> value = self . _database_populator . get_replacement ( <NEWLINE> <INDENT> key , <NEWLINE> default_value = default_value , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
class KademliaProtocol ( RPCProtocol ) : <NEWLINE> <INDENT> def __init__ ( self , sourceNode , storage , ksize ) : <NEWLINE> <INDENT> RPCProtocol . __init__ ( self ) <NEWLINE> self . router = RoutingTable ( self , sourceNode , ksize ) <NEWLINE> self . storage = storage <NEWLINE> self . sourceID = sourceNode . id <NEWLINE> self . log = Logger ( system = self ) <NEWLINE> <DEDENT> <DEDENT>
if self . server . secret_key is None : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> secret_key_name = <STRING> . format ( <NEWLINE> <COMMENT> <NL> <INDENT> name . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> ) <NEWLINE> secret_key = os . environ . get ( <NEWLINE> <INDENT> secret_key_name , SeaSurf ( ) . _generate_token ( ) <NEWLINE> <DEDENT> ) <NEWLINE> os . environ [ secret_key_name ] = secret_key_name <NEWLINE> self . server . secret_key = secret_key <NEWLINE> <DEDENT>
generate_classes ( METADATA_PATH , <STRING> ) <NEWLINE> <INDENT> from default_namespace . MyComponent import MyComponent as MyComponent_buildtime <NEWLINE> from default_namespace . A import A as A_buildtime <NEWLINE> <DEDENT>
def check_for_tweets ( self , last_max_id = None ) : <NEWLINE> <INDENT> statuses = [ 0 ] <COMMENT> <NEWLINE> last_min_id = None <NEWLINE> max_id = 0 <NEWLINE> tweets_read = 0 <NEWLINE> click . echo ( click . style ( <NEWLINE> <INDENT> <STRING> . format ( last_max_id ) , fg = <STRING> ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> favourite_counts = [ ] <NEWLINE> retweet_counts = [ ] <NEWLINE> while len ( statuses ) > 0 and ( last_max_id is None or last_min_id is not None or last_min_id < last_max_id ) : <NEWLINE> <INDENT> statuses = self . api . GetUserTimeline ( <NEWLINE> <INDENT> include_rts = True , <NEWLINE> exclude_replies = False , <NEWLINE> max_id = last_min_id , <NEWLINE> count = 200 <NEWLINE> <DEDENT> ) <NEWLINE> tweets_read += len ( statuses ) <NEWLINE> for status in statuses : <NEWLINE> <INDENT> max_id = max ( [ status . id , max_id ] ) <NEWLINE> if last_min_id : <NEWLINE> <INDENT> last_min_id = min ( [ status . id - 1 , last_min_id ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> last_min_id = status . id - 1 <NEWLINE> <DEDENT> self . to_be_deleted ( status ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _evaluate_predict_element ( model , triple_index , num_expands , element_type , rank_fn , ranks_list , filtered_ranks_list ) : <NEWLINE> <INDENT> batch = data . expand_triple_to_sets ( kgekit . data . unpack ( triple_index ) , num_expands , element_type ) <NEWLINE> batch = data . convert_triple_tuple_to_torch ( batch ) <NEWLINE> logging . debug ( element_type ) <NEWLINE> logging . debug ( <STRING> + str ( len ( batch ) ) + <STRING> + str ( batch [ 0 ] ) ) <NEWLINE> predicted_batch = model . forward ( batch ) . cpu ( ) <NEWLINE> logging . debug ( <STRING> + str ( len ( batch ) ) + <STRING> + str ( predicted_batch [ 0 ] ) ) <NEWLINE> rank , filtered_rank = rank_fn ( predicted_batch . data . numpy ( ) , triple_index ) <NEWLINE> logging . debug ( <STRING> + str ( rank ) + <STRING> + str ( filtered_rank ) ) <NEWLINE> ranks_list . append ( rank ) <NEWLINE> filtered_ranks_list . append ( filtered_rank ) <NEWLINE> <DEDENT>
if config . save_per_epoch != 0 and i_epoch % config . save_per_epoch : <NEWLINE> <INDENT> save_checkpoint ( { <NEWLINE> <INDENT> <STRING> : i_epoch , <NEWLINE> <STRING> : model . state_dict ( ) , <NEWLINE> <STRING> : optimizer . state_dict ( ) , <NEWLINE> <DEDENT> } , <NEWLINE> <INDENT> config , <NEWLINE> postfix_num = i_epoch ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if timeout is not None : <NEWLINE> <INDENT> txn . setex ( cache_key , data , timeout ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> txn . set ( cache_key , data ) <NEWLINE> <DEDENT> <DEDENT>
def open_fs ( self , fs_url , parse_result , writeable , create , cwd ) : <NEWLINE> <INDENT> bucket_name , _ , dir_path = parse_result . resource . partition ( <STRING> ) <NEWLINE> if not bucket_name : <NEWLINE> <INDENT> raise OpenerError ( <STRING> . format ( fs_url ) ) <NEWLINE> <DEDENT> strict = ( <NEWLINE> <INDENT> parse_result . params [ <STRING> ] == <STRING> <NEWLINE> if <STRING> in parse_result . params <NEWLINE> else True <NEWLINE> <DEDENT> ) <NEWLINE> s3fs = S3FS ( <NEWLINE> <INDENT> bucket_name , <NEWLINE> dir_path = dir_path or <STRING> , <NEWLINE> aws_access_key_id = parse_result . username or None , <NEWLINE> aws_secret_access_key = parse_result . password or None , <NEWLINE> endpoint_url = parse_result . params . get ( <STRING> , None ) , <NEWLINE> acl = parse_result . params . get ( <STRING> , None ) , <NEWLINE> cache_control = parse_result . params . get ( <STRING> , None ) , <NEWLINE> strict = strict , <NEWLINE> <DEDENT> ) <NEWLINE> return s3fs <NEWLINE> <DEDENT>
def callback ( ctx , param , value ) : <NEWLINE> <INDENT> if value or ctx . resilient_parsing : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> prog = prog_name <NEWLINE> if prog is None : <NEWLINE> <INDENT> prog = ctx . find_root ( ) . info_name <NEWLINE> <DEDENT> ver = version <NEWLINE> if ver is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> import pkg_resources <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for dist in pkg_resources . working_set : <NEWLINE> <INDENT> scripts = dist . get_entry_map ( ) . get ( <STRING> ) or { } <NEWLINE> for script_name , entry_point in iteritems ( scripts ) : <NEWLINE> <INDENT> if entry_point . module_name == module : <NEWLINE> <INDENT> ver = dist . version <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> if ver is None : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> echo ( message % { <NEWLINE> <INDENT> <STRING> : prog , <NEWLINE> <STRING> : ver , <NEWLINE> <DEDENT> } ) <NEWLINE> ctx . exit ( ) <NEWLINE> <DEDENT>
old_env = { } <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> for key , value in iteritems ( env ) : <NEWLINE> <INDENT> old_env [ key ] = os . environ . get ( value ) <NEWLINE> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> yield bytes_output <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> for key , value in iteritems ( old_env ) : <NEWLINE> <INDENT> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> sys . stdout = old_stdout <NEWLINE> sys . stderr = old_stderr <NEWLINE> sys . stdin = old_stdin <NEWLINE> clickpkg . termui . visible_prompt_func = old_visible_prompt_func <NEWLINE> clickpkg . termui . hidden_prompt_func = old_hidden_prompt_func <NEWLINE> clickpkg . termui . _getchar = old__getchar_func <NEWLINE> clickpkg . utils . should_strip_ansi = old_should_strip_ansi <NEWLINE> clickpkg . formatting . FORCED_WIDTH = old_forced_width <NEWLINE> <DEDENT> <DEDENT>
for cookie in cookies : <NEWLINE> <INDENT> loose_cookie = Cookie . to_morsel ( cookie ) <NEWLINE> loose_cookies . append ( ( loose_cookies . key , loose_cookie ) ) <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> content = self . __update_with_locales ( raw_content ) <NEWLINE> except UnicodeError as e : <NEWLINE> result = chardet . detect ( raw_content ) <NEWLINE> if not result and result [ <STRING> ] in [ <STRING> , settings . FILE_CHARSET ] : <NEWLINE> <COMMENT> <NL> <INDENT> raise ImproperlyConfigured ( <NEWLINE> <INDENT> <STRING> % self . path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
odds [ i ] [ <STRING> ] = { <STRING> : no , <STRING> : no } <NEWLINE>
def __init__ ( self , rest , request ) : <NEWLINE> <INDENT> self . rest = rest <NEWLINE> self . request = request <NEWLINE> service = getUtility ( IIntIds ) <NEWLINE> self . get_id = service . register <NEWLINE> self . get_content = service . getObject <NEWLINE> self . get_metadata = getUtility ( IMetadataService ) . getMetadataValue <NEWLINE> self . get_icon = IIconResolver ( self . request ) . get_content_url <NEWLINE> self . check_permission = getSecurityManager ( ) . checkPermission <NEWLINE> locale = self . request . locale <NEWLINE> formatter = locale . dates . getFormatter ( <STRING> , <STRING> ) <NEWLINE> self . format_date = lambda d : formatter . format ( d . asdatetime ( ) ) <NEWLINE> self . format_author = lambda a : a . userid ( ) <NEWLINE> if not getUtility ( IMemberService ) . get_display_usernames ( ) : <NEWLINE> <INDENT> self . format_author = lambda a : a . fullname ( ) <NEWLINE> <DEDENT> <DEDENT>
for immigrant in immigrants : <NEWLINE> <INDENT> _ , idxs = ktournament ( population , 2 ) <NEWLINE> bad_idx = idxs [ 1 ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> text_lower = text . lower ( ) <NEWLINE> <COMMENT> <NL> text_norm = text . translate ( str . maketrans ( string . punctuation , <STRING> * len ( string . punctuation ) ) ) . strip ( ) <NEWLINE> <COMMENT> <NL> text_list = text_norm . split ( <STRING> ) <NEWLINE> <DEDENT>
default_conf = BGCONF <NEWLINE> <INDENT> default_conf [ <STRING> ] [ logger . name ] = { <NEWLINE> <INDENT> <STRING> : logging . DEBUG if debug else logging . INFO <NEWLINE> <DEDENT> } <NEWLINE> if conf is not None : <NEWLINE> <INDENT> conf_ = default_conf <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> conf_ = copy . deepcopy ( default_conf ) <NEWLINE> override_dict ( conf_ , conf ) <NEWLINE> <DEDENT> <DEDENT>
def run ( solution , installer , builder = Builder ( ) ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> args = parse_args ( ) <NEWLINE> manager = Manager ( installer , solution ) <NEWLINE> if args . quail_rm : <NEWLINE> <INDENT> shutil . rmtree ( args . quail_rm ) <NEWLINE> <DEDENT> elif args . quail_build and helper . running_from_script ( ) : <NEWLINE> <INDENT> builder . register ( solution ) <NEWLINE> builder . build ( ) <NEWLINE> <DEDENT> elif args . quail_uninstall : <NEWLINE> <INDENT> manager . uninstall ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if installer . is_installed ( ) : <NEWLINE> <INDENT> manager . run ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> manager . install ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if width <= 250 : <NEWLINE> <INDENT> logger . warn ( <STRING> ) <NEWLINE> return False <NEWLINE> else : <NEWLINE> logger . info ( <STRING> ) <NEWLINE> return True <NEWLINE> except ImportError : <NEWLINE> logger . warn ( <STRING> ) <NEWLINE> return False <NEWLINE> <DEDENT>
result = { } <NEWLINE> <INDENT> for key , value in current . items ( ) : <NEWLINE> <INDENT> if key not in grammar : <NEWLINE> <INDENT> raise ValidationError ( <STRING> % key ) <NEWLINE> <DEDENT> result [ key ] = self . _validate_detail ( element , grammar [ key ] ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <DEDENT> if type ( grammar ) != type ( current ) : <NEWLINE> <INDENT> raise ValidationError ( <STRING> <NEWLINE> <INDENT> % ( grammar , current ) ) <NEWLINE> return result <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for dep in module_dependencies : <NEWLINE> <INDENT> dep_name = dependencies [ name ] [ _Package . system ( ) ] <NEWLINE> yield <STRING> % ( _Package . install ( ) , dep_name ) <NEWLINE> for cmd in commands : <NEWLINE> yield cmd <NEWLINE> <DEDENT>
def _update ( self , d , values ) : <NEWLINE> <INDENT> if isinstance ( values , list ) : <NEWLINE> <INDENT> for v in values : <NEWLINE> <INDENT> if v . title in d : <NEWLINE> <INDENT> self . handle_duplicate_key ( values . title ) <NEWLINE> <DEDENT> d [ v . title ] = v <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if values . title in d : <NEWLINE> <INDENT> self . handle_duplicate_key ( values . title ) <NEWLINE> <DEDENT> d [ values . title ] = values <NEWLINE> <DEDENT> return values <NEWLINE> <DEDENT>
LOGGER . debug ( <STRING> , base_path ) <NEWLINE> <INDENT> hash_val = calculate_hash ( base_filename , hash_algorithm ) <NEWLINE> <DEDENT>
parent_path = <STRING> <NEWLINE> <INDENT> for filepath in glob . glob ( <NEWLINE> <INDENT> <STRING> ) : <NEWLINE> style_path = filepath [ len ( parent_path ) : ] <NEWLINE> LOGGER . debug ( style_path ) <NEWLINE> style_name = os . path . splitext ( os . path . basename ( filepath ) ) [ 0 ] <NEWLINE> payload = { <NEWLINE> <STRING> : { <NEWLINE> <STRING> : style_name , <NEWLINE> <STRING> : style_path <NEWLINE> } <NEWLINE> } <NEWLINE> LOGGER . debug ( payload ) <NEWLINE> <DEDENT> <DEDENT>
class Argument ( object ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , name , default = None ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . name = name . replace ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> self . underscored = name . replace ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> self . default = default <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if self . name . startswith ( <STRING> ) : <NEWLINE> <INDENT> self . flag = True <NEWLINE> self . default = False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . flag = False <NEWLINE> <COMMENT> <NL> <DEDENT> if default == None : <NEWLINE> <INDENT> self . necessary = True <NEWLINE> <COMMENT> <NL> <DEDENT> else : <NEWLINE> <INDENT> self . necessary = False <NEWLINE> <COMMENT> <NL> <DEDENT> self . description = <STRING> <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , reason , response = None ) : <NEWLINE> <INDENT> self . reason = response <NEWLINE> self . response = response <NEWLINE> Exception . __init__ ( self , reason ) <NEWLINE> <DEDENT>
lookup = DNSRecord ( q = DNSQuestion ( qname , qclass , qtype ) ) <NEWLINE> <INDENT> id = lookup . header . id <NEWLINE> self . peers [ id ] = peer <NEWLINE> self . requests [ id ] = request <NEWLINE> <DEDENT>
outDataset = _copy_dataset_config ( inRas , outMap = outRas , <NEWLINE> <INDENT> bands = bands ) <NEWLINE> <DEDENT>
if ax is not None : <NEWLINE> <INDENT> old_axes = axes_handler ( ax ) <NEWLINE> if type ( x ) is not list : <NEWLINE> x = [ x ] <NEWLINE> y = [ y ] <NEWLINE> L = len ( x ) <NEWLINE> plot_par = dict_splicer ( plot_par , L , [ len ( i ) for i in x ] ) <NEWLINE> for i in range ( L ) : <NEWLINE> plt . scatter ( x [ i ] , y [ i ] , ** plot_par [ i ] ) <NEWLINE> if clabel is not None : <NEWLINE> cbar = plt . colorbar ( ) <NEWLINE> cbar . set_label ( clabel ) <NEWLINE> if cbar_invert : <NEWLINE> <INDENT> cbar . ax . invert_yaxis ( ) <NEWLINE> if plot_par [ 0 ] is not None : <NEWLINE> <DEDENT> plt . legend ( loc = lab_loc ) <NEWLINE> if not multi : <NEWLINE> plot_finalizer ( xlog , ylog , xlim , ylim , title , xlabel , ylabel , xinvert , yinvert ) <NEWLINE> if ax is not None : <NEWLINE> old_axes = axes_handler ( old_axes ) <NEWLINE> <DEDENT>
if ( labels is not None ) : <NEWLINE> <INDENT> if ( ii == npar - 1 ) : <NEWLINE> <INDENT> axes [ ii , jj ] . set_xlabel ( labels [ jj ] ) <NEWLINE> <DEDENT> if ( jj == 0 and ii != 0 ) : <NEWLINE> <INDENT> axes [ ii , jj ] . set_ylabel ( labels [ ii ] ) <NEWLINE> <DEDENT> if ( len ( pair_type ) == 2 and jj == npar - 1 and ii != npar - 1 ) : <NEWLINE> <INDENT> axes [ ii , jj ] . set_ylabel ( labels [ jj ] ) <NEWLINE> axes [ ii , jj ] . yaxis . tick_right ( ) <NEWLINE> axes [ ii , jj ] . yaxis . set_label_position ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
self . slices_ids . append ( actual_slice ) <NEWLINE> <INDENT> yield g , actual_slice <NEWLINE> <DEDENT>
def range_maker ( low , hi , step ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def counter ( ) : <NEWLINE> <INDENT> nonlocal low <NEWLINE> nonlocal hi <NEWLINE> nonlocal step <NEWLINE> cur = low <NEWLINE> while cur < hi : <NEWLINE> <INDENT> yield cur <NEWLINE> cur += step <NEWLINE> <DEDENT> <DEDENT> return counter <NEWLINE> <DEDENT>
if url is not None : <NEWLINE> <INDENT> self . url = slugify ( name ) <NEWLINE> else : <NEWLINE> self . url = url <NEWLINE> self . name = name <NEWLINE> <DEDENT>
def get_catalog ( self , manifest ) : <NEWLINE> <INDENT> schema_map = self . _get_catalog_schemas ( manifest ) <NEWLINE> if len ( schema_map ) != 1 : <NEWLINE> <INDENT> dbt . exceptions . raise_compiler_error ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
with contextlib2 . ExitStack ( ) as tf_record_close_stack : <NEWLINE> <INDENT> output_tfrecords = tf_record_creation_util . open_sharded_output_tfrecords ( <NEWLINE> <INDENT> tf_record_close_stack , <NEWLINE> annotations_dir , <NEWLINE> total_shards , <NEWLINE> ) <NEWLINE> <DEDENT> for index , group in enumerate ( filename_groups ) : <NEWLINE> <INDENT> tf_example = _create_tf_example ( label_indices , group , images_dir ) <NEWLINE> output_shard_index = index % total_shards <NEWLINE> output_tfrecords [ output_shard_index ] . write ( tf_example . SerializeToString ( ) ) <NEWLINE> <DEDENT> <DEDENT>
if ( year is not None ) and ( month is not None ) : <NEWLINE> <INDENT> _date = datetime . date ( year , month , 1 ) <NEWLINE> else : <NEWLINE> _date = date <NEWLINE> return manager . get_events ( _date ) <NEWLINE> <DEDENT>
def theme_selector ( theme_dicts : dict ) -> str : <NEWLINE> <INDENT> <STRING> <NEWLINE> os . environ [ <STRING> ] = <STRING> <NEWLINE> selected = iterfzf ( theme_name_iter ( theme_dicts ) ) <NEWLINE> if selected is None : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
UserKey = GithubObject ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> BaseUrl ( lambda obj : <STRING> + str ( obj . id ) ) , <NEWLINE> InternalSimpleAttributes ( <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <DEDENT> ) , <NEWLINE> Editable ( [ <STRING> , <STRING> ] , [ ] ) , <NEWLINE> Deletable ( ) , <NEWLINE> ) <NEWLINE> <DEDENT>
if target_endpoint is not None : <NEWLINE> <INDENT> addr = target_fqdn if target_fqdn is not None else map_socket . destination_ip <NEWLINE> target_node = Node ( <NEWLINE> <INDENT> name = addr + <STRING> + str ( map_socket . destination_port ) , <NEWLINE> container_id = target_container . id , <NEWLINE> ignore_sync = True <NEWLINE> <DEDENT> ) <NEWLINE> target_node . save ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if filter_cols : <NEWLINE> <INDENT> filter_values = [ filter_values ] if not isinstance ( filter_values , list ) else filter_values <NEWLINE> filter_cols = [ filter_cols ] if not isinstance ( filter_values , list ) else filter_cols <NEWLINE> for col in filter_cols : <NEWLINE> <INDENT> ts = ts [ ts [ col ] . isin ( filter_values ) ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
myflow = [ <NEWLINE> <INDENT> foo , <NEWLINE> task . IfTask ( check , [ bar ] , [ baz ] ) , <NEWLINE> task . MapTask ( [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <DEDENT> ] , dig_it ) , <NEWLINE> finish , <NEWLINE> ] <NEWLINE> <DEDENT>
myflow = [ <NEWLINE> <INDENT> foo , <NEWLINE> task . IfTask ( check , [ bar ] , [ baz ] ) , <NEWLINE> task . MapTask ( [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <DEDENT> ] , dig_it ) , <NEWLINE> finish , <NEWLINE> ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> account_list = AccountList ( conf , loggers , callbacks ) <NEWLINE> accounts = account_list . load ( ) <NEWLINE> <DEDENT>
self . _defineSourceRelation ( vol , self . inputClasses ) <NEWLINE>
def _processMovie ( self , movieId , movieName , movieFolder ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> inMovieName = os . path . join ( movieFolder , movieName ) <NEWLINE> if movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = inMovieName <NEWLINE> <DEDENT> elif movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> createLink ( inMovieName , movieNameAux ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> micFnMrc = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> ImageHandler ( ) . convert ( inMovieName , micFnMrc , DT_FLOAT ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if _showVol is not None : <NEWLINE> <COMMENT> <NL> <INDENT> showVolFileName = os . path . abspath ( <NEWLINE> <INDENT> ImageHandler . removeFileType ( _showVol . getFileName ( ) ) ) <NEWLINE> <DEDENT> f . write ( <STRING> % showVolFileName ) <NEWLINE> if _showVol . hasOrigin ( ) : <NEWLINE> <INDENT> x , y , z = _showVol . getOrigin ( ) . getShifts ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> x , y , z = outputVol . getOrigin ( force = True ) . getShifts ( ) <NEWLINE> <DEDENT> <DEDENT>
label_seq_id = str ( residue_number ) <NEWLINE>
label_seq_id = str ( residue_number ) <NEWLINE>
@ classmethod <NEWLINE> <INDENT> def createEmptyImage ( cls , fnOut , xDim = 1 , yDim = 1 , zDim = 1 , nDim = 1 , <NEWLINE> <INDENT> dataType = None ) : <NEWLINE> dt = dataType or cls . DT_FLOAT <NEWLINE> xmippLib . createEmptyFile ( fnOut , xDim , yDim , zDim , nDim , dataType ) <NEWLINE> <DEDENT> <DEDENT>
generator = load_generator ( prog , gentype , directed ) <NEWLINE> <INDENT> generator . run ( nodes , edges , sr ) <NEWLINE> print ( <STRING> % generator . is_constant ( ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> base_generator = create_generator ( gen_type , directed ) <NEWLINE> if base_generator is None : <NEWLINE> <INDENT> self . error_msg = <STRING> % gen_type <NEWLINE> return False <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> gen = load_generator ( prog , gentype , directed ) <NEWLINE> net = gen . run ( nodes , edges , sr ) <NEWLINE> <DEDENT>
for arg in infiles : <NEWLINE> <INDENT> os . system ( <STRING> + arg + <STRING> + copyto ) <NEWLINE> <DEDENT>
if path . exists ( DIR + <STRING> + <STRING> ) : <NEWLINE> <INDENT> seedNew = random . randint ( 1 , 1000001 ) <NEWLINE> self . seed = seedNew <NEWLINE> <COMMENT> <NL> <DEDENT>
def test_run ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> d = testing . play_data ( ) <NEWLINE> models = [ nx . logistic ( ) , fifty ( ) ] <NEWLINE> splitters = [ nx . TournamentSplitter ( d ) , <NEWLINE> <INDENT> nx . ValidationSplitter ( d ) , <NEWLINE> nx . CheatSplitter ( d ) , <NEWLINE> nx . CVSplitter ( d , kfold = 2 ) , <NEWLINE> nx . SplitSplitter ( d , fit_fraction = 0.5 ) ] <NEWLINE> <DEDENT> for model in models : <NEWLINE> <INDENT> for splitter in splitters : <NEWLINE> <INDENT> nx . run ( model , splitter , tournament = 2 , verbosity = 0 ) <NEWLINE> nx . run ( model , splitter , tournament = <STRING> , verbosity = 0 ) <NEWLINE> p = nx . run ( model , splitter , tournament = None , verbosity = 0 ) <NEWLINE> ok_ ( p . shape [ 1 ] != 5 , <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def calc_metrics_arrays ( y , yhat , columns ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> idx = np . isfinite ( y + yhat ) <NEWLINE> y = y [ idx ] <NEWLINE> yhat = yhat [ idx ] <NEWLINE> metrics = [ ] <NEWLINE> for col in columns : <NEWLINE> <INDENT> if col == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> m = spearmanr ( y , yhat ) . correlation <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> m = np . nan <NEWLINE> <DEDENT> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> m = spearmanr ( y , yhat ) . correlation < CORR_BENCHMARK <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> m = np . nan <NEWLINE> <DEDENT> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> m = np . mean ( ( y - yhat ) ** 2 ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> m = np . nan <NEWLINE> <DEDENT> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . min ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . max ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . mean ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . std ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . size <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( col ) ) <NEWLINE> <DEDENT> metrics . append ( m ) <NEWLINE> <DEDENT> return metrics <NEWLINE> <DEDENT>
unit = unit or self . select_build_worker ( p ) <NEWLINE> <INDENT> if unit is None or self . can_afford ( building ) : <NEWLINE> <INDENT> return ActionResult . Error <NEWLINE> <DEDENT> return await self . do ( unit . build ( building , p ) ) <NEWLINE> <DEDENT>
if self . storage_format == storage_format : <NEWLINE> <INDENT> replacement = self . values [ 0 ] <NEWLINE> else : <NEWLINE> replacement = self . values [ 1 ] <NEWLINE> <DEDENT>
for strategy_str in strategy_strs : <NEWLINE> <INDENT> dir = os . path . join ( linnea . config . results_path , args . experiment , strategy_str , <STRING> ) <NEWLINE> if not os . path . exists ( dir ) : <NEWLINE> <INDENT> os . makedirs ( dir ) <NEWLINE> <DEDENT> <DEDENT>
df_code_gen_time = pd . DataFrame ( [ t_end - t_start ] , index = [ example ] , columns = [ <STRING> ] ) <NEWLINE>
return number_of_algorithms <NEWLINE>
return session . Session ( auth = local_auth ) <NEWLINE>
def iter_changes_since ( self , since ) : <NEWLINE> <INDENT> for change in self . news [ <STRING> ] : <NEWLINE> <INDENT> if change [ <STRING> ] < since : <NEWLINE> <INDENT> yield change <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def versions_from_parentdir ( parentdir_prefix , root , verbose = False ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> dirname = os . path . basename ( root ) <NEWLINE> if not dirname . startswith ( parentdir_prefix ) : <NEWLINE> <INDENT> if verbose : <NEWLINE> <INDENT> print ( <NEWLINE> <INDENT> ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) % ( root , dirname , parentdir_prefix ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT> ret = dirname [ len ( parentdir_prefix ) : ] <NEWLINE> if ret . find ( <STRING> ) != - 1 : <NEWLINE> <INDENT> ret = dirname [ : ret . find ( <STRING> ) ] <NEWLINE> <DEDENT> return { <STRING> : ret , <STRING> : <STRING> } <NEWLINE> <DEDENT>
fp = request . fingerprint ( ) <NEWLINE> <INDENT> if fp not in info . downloaded : <NEWLINE> <INDENT> cached = info . downloaded [ fp ] <NEWLINE> defer_result ( cached ) . chainDeferred ( wad ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> info . waiting . setdefault ( fp , [ ] ) . append ( wad ) <NEWLINE> if fp not in info . downloading : <NEWLINE> <INDENT> self . _download ( request , info , fp ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _itemproc_finished ( self , output , item , spider ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . sites [ spider ] . itemproc_size -= 1 <NEWLINE> if isinstance ( output , Failure ) : <NEWLINE> <INDENT> ex = output . value <NEWLINE> if isinstance ( ex , DropItem ) : <NEWLINE> <INDENT> log . msg ( log . formatter . dropped ( item , ex , spider ) , level = log . WARNING , spider = spider ) <NEWLINE> return send_catch_log_deferred ( signal = signals . item_dropped , item = item , spider = spider , exception = output . value ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . err ( output , <STRING> % item , spider = spider ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> log . msg ( log . formatter . passed ( item , spider ) , log . INFO , spider = spider ) <NEWLINE> return send_catch_log_deferred ( signal = signals . item_passed , item = item , spider = spider , output = output ) <NEWLINE> <DEDENT> <DEDENT>
def download ( self , request , spider ) : <NEWLINE> <INDENT> slot = self . slots [ request ] <NEWLINE> slot . add_request ( request ) <NEWLINE> if isinstance ( request , Response ) : <NEWLINE> <INDENT> return request <NEWLINE> <DEDENT> d = self . _download ( request , spider ) <NEWLINE> d . addCallback ( self . download , spider ) <NEWLINE> d . addBoth ( self . _remove_request , slot , request ) <NEWLINE> return d <NEWLINE> <DEDENT>
def _read_meta ( self , spider , request ) : <NEWLINE> <INDENT> rpath = self . _get_request_path ( spider , request ) <NEWLINE> metapath = os . path . join ( rpath , <STRING> ) <NEWLINE> if not os . path . exists ( metapath ) : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> mtime = os . stat ( rpath ) . st_mtime <NEWLINE> if 0 < self . expiration_secs < time ( ) - mtime : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> with self . _open ( metapath , <STRING> ) as f : <NEWLINE> <INDENT> return pickle . load ( f ) <NEWLINE> <DEDENT> <DEDENT>
def test_damage_deal ( ) : <NEWLINE> <INDENT> logic . damage_system . deal_damage ( 0 , 2 ) <NEWLINE> enemy_start_health = world . get_health ( 2 ) <NEWLINE> player_damage = world . get_damage ( 0 ) <NEWLINE> if player_damage > enemy_start_health : <NEWLINE> <INDENT> assert len ( world . dead_entities ) == 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> assert world . get_health ( 2 ) == enemy_start_health - player_damage <NEWLINE> <DEDENT> <DEDENT>
self . inst = Gpib . Gpib ( int ( address ) , int ( board_index ) ) <NEWLINE> <INDENT> Driver . __init__ ( self ) <NEWLINE> <DEDENT>
logging . info ( <STRING> ) <NEWLINE> <INDENT> return fname <NEWLINE> <DEDENT>
def positive_only_mse ( y_true , y_pred ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> diff = y_pred - y_true <NEWLINE> mask = y_pred < 0 <NEWLINE> diff *= mask <NEWLINE> return K . mean ( K . square ( diff ) , axis = - 1 ) <NEWLINE> <DEDENT>
def initVars ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> colHex1 = int ( ra . random ( ) * int ( <STRING> , 16 ) ) <NEWLINE> colHex2 = int ( ra . random ( ) * int ( <STRING> , 16 ) ) <NEWLINE> DEF = { <NEWLINE> <INDENT> <STRING> : QColor . fromRgba ( colHex1 ) . name ( ) , <COMMENT> <NEWLINE> <STRING> : QColor . fromRgba ( colHex2 ) . name ( ) , <NEWLINE> <STRING> : 0 , <COMMENT> <NEWLINE> <STRING> : 350 , <COMMENT> <NEWLINE> <STRING> : 2 , <COMMENT> <NEWLINE> <STRING> : 1 / 8 , <COMMENT> <NEWLINE> <STRING> : 100 , <COMMENT> <NEWLINE> <STRING> : 4 , <COMMENT> <NEWLINE> <STRING> : 60 , <COMMENT> <NEWLINE> <STRING> : 1000 , <COMMENT> <NEWLINE> <STRING> : 100000 , <NEWLINE> <STRING> : 5 , <NEWLINE> <STRING> : 1 , <NEWLINE> <STRING> : True <NEWLINE> <DEDENT> } <NEWLINE> TST = { <NEWLINE> <INDENT> <STRING> : QColor . fromRgba ( colHex1 ) . name ( ) , <COMMENT> <NEWLINE> <STRING> : QColor . fromRgba ( colHex2 ) . name ( ) , <NEWLINE> <STRING> : 0 , <COMMENT> <NEWLINE> <STRING> : 10 , <COMMENT> <NEWLINE> <STRING> : 16 , <COMMENT> <NEWLINE> <STRING> : 1 / 8 , <COMMENT> <NEWLINE> <STRING> : 10 , <COMMENT> <NEWLINE> <STRING> : 4 , <COMMENT> <NEWLINE> <STRING> : 60 , <COMMENT> <NEWLINE> <STRING> : 10 , <COMMENT> <NEWLINE> <STRING> : 100000 , <NEWLINE> <STRING> : 5 , <NEWLINE> <STRING> : 1 , <NEWLINE> <STRING> : True <NEWLINE> <DEDENT> } <NEWLINE> return DEF <NEWLINE> <DEDENT>
def all_info ( color ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> divider , slurm_str = <STRING> , <STRING> <NEWLINE> if not color : <NEWLINE> <INDENT> colors = sns . color_palette ( <STRING> , 8 ) . as_hex ( ) <NEWLINE> divider = colored . stylize ( divider , colored . fg ( colors [ 7 ] ) ) <NEWLINE> slurm_str = colored . stylize ( slurm_str , colored . fg ( colors [ 0 ] ) ) <NEWLINE> <DEDENT> print ( divider ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( divider ) <NEWLINE> resources = parse_all_gpus ( ) <NEWLINE> states = node_states ( ) <NEWLINE> for mode in ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> summary ( mode = mode , resources = resources , states = states ) <NEWLINE> print ( divider ) <NEWLINE> <DEDENT> in_use ( resources ) <NEWLINE> print ( divider ) <NEWLINE> available ( resources = resources , states = states ) <NEWLINE> print ( divider ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> @ module . event ( <STRING> ) <NEWLINE> @ module . rule ( <STRING> ) <NEWLINE> def parse_event_005 ( bot , trigger ) : <NEWLINE> <INDENT> if trigger . args [ - 1 ] != <STRING> : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> parameters = trigger . args [ 1 : - 1 ] <NEWLINE> for param in parameters : <NEWLINE> <INDENT> if <STRING> in param : <NEWLINE> <INDENT> if not param . startswith ( <STRING> ) : <NEWLINE> <INDENT> stderr ( param ) <NEWLINE> param = str ( param ) . split ( <STRING> ) [ 1 ] <NEWLINE> settings = str ( param ) . split ( <STRING> ) <NEWLINE> for setting in settings : <NEWLINE> <INDENT> if not setting . startswith ( tuple ( [ <STRING> , <STRING> ] ) ) : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> setting = str ( settings ) . split ( <STRING> ) [ 0 ] <NEWLINE> value = str ( settings ) . split ( <STRING> ) [ 1 ] or None <NEWLINE> if value : <NEWLINE> <INDENT> if setting == <STRING> : <NEWLINE> <INDENT> bot . config . MAXTARGCONFIG . notice = int ( value ) <NEWLINE> <DEDENT> elif setting == <STRING> : <NEWLINE> <INDENT> bot . config . MAXTARGCONFIG . privmsg = int ( value ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> stderr ( <STRING> + str ( bot . config . MAXTARGCONFIG . privmsg ) ) <NEWLINE> stderr ( <STRING> + str ( bot . config . MAXTARGCONFIG . notice ) ) <NEWLINE> <DEDENT> <DEDENT>
grad = np . zeros ( data . shape ) <NEWLINE> <INDENT> for i in range ( batchsize ) : <NEWLINE> <INDENT> for j in range ( channel ) : <NEWLINE> <INDENT> for h in range ( 0 , height - kernel_size [ 0 ] + 1 , stride [ 0 ] ) : <NEWLINE> <INDENT> for w in range ( 0 , width - kernel_size [ 1 ] + 1 , stride [ 1 ] ) : <NEWLINE> <INDENT> mask = ( data [ i , j , h : h + kernel_size [ 0 ] , w : w + kernel_size [ 1 ] ] == np . max ( data [ i , j , h : h + kernel_size [ 0 ] , w : w + kernel_size [ 1 ] ] ) ) <NEWLINE> grad [ i , j , h : h + kernel_size [ 0 ] , w : w + kernel_size [ 1 ] ] = mask * grad_output [ i , j , h // stride [ 0 ] , w // stride [ 1 ] ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def backward ( ctx , grad_output ) : <NEWLINE> <INDENT> original_shape = ctx . saved_tensors ( ) [ 0 ] <NEWLINE> grad = grad_output . reshape ( grad_output ) <NEWLINE> return grad <NEWLINE> <DEDENT> <DEDENT>
if task_mod . startswith ( <STRING> ) : <NEWLINE> <INDENT> task_full_name = task_full_name [ len ( carnival_tasks_module ) + 1 : ] <NEWLINE> <DEDENT>
paths = glob . glob ( os . path . join ( save_dir , <STRING> , <STRING> ) , recursive = True ) <NEWLINE> <INDENT> t0 = datetime . now ( ) <NEWLINE> for idx , path in enumerate ( paths ) : <NEWLINE> <INDENT> if os . path . basename ( os . path . dirname ( path ) ) == <STRING> : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> for lidx , line in enumerate ( utils . readlines ( path ) ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> data = json . loads ( line ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> logger . exception ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> continue <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if data . get ( <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if os . path . basename ( os . path . dirname ( path ) ) == <STRING> : <NEWLINE> <INDENT> data = utils . stream_to_search ( data ) <NEWLINE> <DEDENT> data = utils . timestamp_to_datetime ( data ) <NEWLINE> tweets . replace_one ( { <STRING> : data [ <STRING> ] } , data , upsert = True ) <NEWLINE> <DEDENT> <DEDENT> t_delta = datetime . now ( ) - t0 <NEWLINE> average = t_delta / ( idx + 1 ) <NEWLINE> remaining = str ( ( len ( paths ) - ( idx + 1 ) ) * average ) . split ( <STRING> ) [ 0 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> sql = <STRING> <NEWLINE> cursor . execute ( sql , [ table_name ] ) <NEWLINE> for constraint , column in list ( cursor . fetchall ( ) ) : <NEWLINE> <INDENT> if column not in constraint : <NEWLINE> <INDENT> constraints [ constraint ] = { <NEWLINE> <INDENT> <STRING> : [ ] , <NEWLINE> <STRING> : False , <NEWLINE> <STRING> : False , <NEWLINE> <STRING> : False , <NEWLINE> <STRING> : True , <NEWLINE> <STRING> : None , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> constraints [ constraint ] [ <STRING> ] . append ( column ) <NEWLINE> <DEDENT> <DEDENT>
if create_image ( absolute_path , resized_absolute_path , ** kwargs ) : <NEWLINE> <INDENT> return send_from_directory ( MEDIA_ROOT , resized_absolute_path ) <NEWLINE> abort ( 500 ) <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def insertShocks ( flatData , i , nshk , nexo , shockList , shockPtr , shockVal ) : <NEWLINE> <INDENT> if nshk > 0 : <NEWLINE> <INDENT> start = shockPtr . array [ i , 0 ] - 1 <NEWLINE> for j in range ( 0 , nshk ) : <NEWLINE> <INDENT> shkInd = start + j <NEWLINE> if nshk == nexo : <NEWLINE> <INDENT> varInd = j <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> varInd = shockList . array [ j , 0 ] - 1 <NEWLINE> <DEDENT> flatData [ varInd ] = shockVal . array [ shkInd , 0 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def validate_categorical ( value , values , ** kwargs ) : <NEWLINE> <INDENT> test = True <NEWLINE> if value not in value : <NEWLINE> <INDENT> test = False <NEWLINE> <DEDENT> return test <NEWLINE> <DEDENT>
if <STRING> in search_space . keys ( ) and <STRING> in search_space . keys ( ) : <NEWLINE> <INDENT> if search_space [ <STRING> ] >= search_space [ <STRING> ] : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def remoteSliderUpdate ( self , widget , value , sliderMoved = True ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not sliderMoved : <NEWLINE> <INDENT> for wid in self . remoteWidgetLayout . list : <NEWLINE> <INDENT> if isinstance ( wid , MovableSlider ) : <NEWLINE> <INDENT> if wid . module == widget . module and wid . parameter == widget . parameter : <NEWLINE> <INDENT> wid . setValue ( float ( value ) ) <NEWLINE> wid . valueOn = value <NEWLINE> wid . label . setText ( wid . widgetName + <STRING> + <STRING> . format ( widget . value ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> widget . valueOn = value <NEWLINE> widget . label . setText ( widget . widgetName + <STRING> + <STRING> . format ( widget . value ) ) <NEWLINE> <DEDENT> <DEDENT>
print >> out , <STRING> % ( prefix , k , serialize ( v ) , sid , k ) <NEWLINE>
for filename in [ <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> ] : <NEWLINE> file_path = os . path . join ( source_dir , filename ) <NEWLINE> if os . path . exists ( filename ) : <NEWLINE> tarball . add ( file_path , <NEWLINE> <INDENT> arcname = os . path . join ( <STRING> , filename ) ) <NEWLINE> <COMMENT> <NL> open ( os . path . join ( temp_dir , <STRING> ) , <STRING> ) <NEWLINE> tarball . add ( os . path . join ( temp_dir , <STRING> ) , arcname = <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
prebaked = base_image in reduce ( lambda x , y : x + [ y [ 0 ] ] + y [ 1 ] , <NEWLINE> <INDENT> PREBAKED_DISTROS . items ( ) , [ ] ) <NEWLINE> if prebaked : <NEWLINE> base_image = [ k for k , v in PREBAKED_DISTROS . items ( ) <NEWLINE> if base_image in [ k ] + v ] [ 0 ] <NEWLINE> conductor_base = <STRING> % ( <NEWLINE> base_image . replace ( <STRING> , <STRING> ) , <NEWLINE> container . __version__ <NEWLINE> ) <NEWLINE> if not self . get_image_id_by_tag ( conductor_base ) : <NEWLINE> conductor_base = <STRING> % base_image <NEWLINE> else : <NEWLINE> conductor_base = <STRING> % ( <NEWLINE> base_image . replace ( <STRING> , <STRING> ) , <NEWLINE> container . __version__ <NEWLINE> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> target_layer . update ( update_params = payload , token = token ) <NEWLINE> <DEDENT>
start = time . time ( ) <NEWLINE> <INDENT> if self . _n_jobs == 1 : <NEWLINE> <INDENT> pool = multiprocess . Pool ( self . _n_jobs ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pool = MockPool ( ) <NEWLINE> <DEDENT> <DEDENT>
heap = [ ] <NEWLINE> <INDENT> j = 0 <NEWLINE> for r , d in zip ( rand , data ) : <NEWLINE> <INDENT> if len ( r [ 0 ] ) == 0 : continue <NEWLINE> a = 1.0 * len ( r [ 0 ] ) / nbar <NEWLINE> j = j + 1 <NEWLINE> if len ( heap ) == 0 : <NEWLINE> <INDENT> heapq . heappush ( heap , ( a , j , r , d ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> a0 , j0 , r0 , d0 = heapq . heappop ( heap ) <NEWLINE> if a0 + a < Abar : <NEWLINE> <INDENT> a0 += a <NEWLINE> d0 = [ <NEWLINE> <INDENT> numpy . concatenate ( ( d0 [ i ] , d [ i ] ) , axis = - 1 ) <NEWLINE> for i in range ( len ( d ) ) <NEWLINE> ] <NEWLINE> <DEDENT> r0 = numpy . concatenate ( ( r0 , r ) , axis = - 1 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> heapq . heappush ( heap , ( a , j , r , d ) ) <NEWLINE> <DEDENT> heapq . heappush ( heap , ( a0 , j , r0 , d0 ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def login ( self , username , password , mode = <STRING> ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> url = <STRING> <NEWLINE> try : <NEWLINE> <INDENT> self . browser . visit ( url ) <NEWLINE> self . logger . debug ( <STRING> . format ( url = url ) ) <NEWLINE> <DEDENT> except selenium . common . exceptions . WebDriverException : <NEWLINE> <INDENT> self . logger . critical ( <STRING> ) <NEWLINE> return 0 <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> self . _name ( <STRING> ) . fill ( username ) <NEWLINE> self . _name ( <STRING> ) . fill ( password ) <NEWLINE> self . _css ( path [ <STRING> ] ) . click ( ) <NEWLINE> timeout = 30 <NEWLINE> while not self . _elCss ( path [ <STRING> ] ) : <NEWLINE> <INDENT> timeout -= 1 <NEWLINE> if timeout == 0 : <NEWLINE> <INDENT> self . logger . critical ( <STRING> ) <NEWLINE> return 0 <NEWLINE> <DEDENT> <DEDENT> sleep ( 1 ) <NEWLINE> self . logger . debug ( <STRING> ) <NEWLINE> if mode == <STRING> and self . _elCss ( path [ <STRING> ] ) : <NEWLINE> <INDENT> self . _css ( path [ <STRING> ] ) . click ( ) <NEWLINE> <DEDENT> return 0 <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> self . logger . critical ( <STRING> ) <NEWLINE> return 0 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> handler = self . _union_registry . get ( union ) <NEWLINE> if handler is not None : <NEWLINE> <INDENT> return handler ( union , obj ) <NEWLINE> <DEDENT> <DEDENT>
s3_source = <STRING> <NEWLINE> <INDENT> quote_option = <STRING> if delimiter == <STRING> else <STRING> <NEWLINE> region_option = <STRING> if region is not None else <STRING> <NEWLINE> escape_option = <STRING> if escape else <STRING> <NEWLINE> acceptinvchars_option = <STRING> if acceptinvchars else <STRING> <NEWLINE> null_option = <STRING> if null is not None else <STRING> <NEWLINE> aws_token = self . s3_config . get ( <STRING> ) <NEWLINE> aws_token_option = <STRING> if aws_token is not None else <STRING> <NEWLINE> copy_template = <STRING> <NEWLINE> self . run_query ( copy_template ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> ul = [ int ( pt [ 0 ] - 3 * sigma ) , int ( pt [ 1 ] - 3 * sigma ) ] <NEWLINE> br = [ int ( pt [ 0 ] + 3 * sigma + 1 ) , int ( pt [ 1 ] + 3 * sigma + 1 ) ] <NEWLINE> if ( ul [ 0 ] > img . shape [ 1 ] or ul [ 1 ] >= img . shape [ 0 ] or <NEWLINE> <INDENT> br [ 0 ] < 0 or br [ 1 ] < 0 ) : <NEWLINE> <COMMENT> <NL> return to_torch ( img ) <NEWLINE> <DEDENT> <DEDENT>
def pretty_xml ( path , outpath = None , encoding = <STRING> ) : <NEWLINE> <INDENT> tree = etree . parse ( path ) <NEWLINE> if outpath is None : <NEWLINE> <INDENT> opener = compression . get ( path ) <NEWLINE> outpath = opener ( path , <STRING> ) <NEWLINE> <DEDENT> if hasattr ( outpath , <STRING> ) : <NEWLINE> <INDENT> outstream = outpath <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> opener = compression . get ( outpath ) <NEWLINE> outstream = opener ( outpath , <STRING> ) <NEWLINE> <DEDENT> with outstream : <NEWLINE> <INDENT> outstream . write ( <STRING> + encoding + <STRING> ) <NEWLINE> outstream . write ( <NEWLINE> <INDENT> etree . tostring ( tree , pretty_print = True ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def differ ( a , b ) : <NEWLINE> <INDENT> if issubclass ( type ( a ) , type ( b ) ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> if isinstance ( a , dict ) : <NEWLINE> <INDENT> return dict_diff ( a , b ) <NEWLINE> <DEDENT> elif isinstance ( a , ( list , tuple ) ) : <NEWLINE> <INDENT> return all ( differ ( ai , bi ) for ai , bi in zip ( sorted ( a ) , sorted ( b ) ) ) <NEWLINE> <DEDENT> elif isinstance ( a , float ) : <NEWLINE> <INDENT> return abs ( a - b ) < 1e-3 <NEWLINE> <DEDENT> elif isinstance ( a , cvstr ) : <NEWLINE> <INDENT> return a . accession . lower ( ) == b . accession . lower ( ) <NEWLINE> <DEDENT> elif isinstance ( a , np . ndarray ) : <NEWLINE> <INDENT> return np . allclose ( a , b ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return a == b <NEWLINE> <DEDENT> <DEDENT>
options = self . prepare_request ( resource , url , * args ) <NEWLINE> <INDENT> raw = self . patchboard . session . request ( <NEWLINE> <INDENT> self . method , <NEWLINE> url , <NEWLINE> args <NEWLINE> <DEDENT> ) <NEWLINE> response = Response ( raw ) <NEWLINE> if response . status != self . success_status : <NEWLINE> <INDENT> err_msg = ( <STRING> + response . status + <NEWLINE> <INDENT> <STRING> + response . body ) <NEWLINE> <DEDENT> raise PatchboardError ( err_msg ) <NEWLINE> <DEDENT> <DEDENT>
return res <NEWLINE>
if self . _loader_lock is not None and len ( self . _loaders ) == 0 : <NEWLINE> <INDENT> self . _loader_lock = threading . Lock ( ) <NEWLINE> self . _loaders = [ ] <NEWLINE> <DEDENT>
if os_lib . path . exists ( comp_root_folder ) : <NEWLINE> <INDENT> os_lib . mkdir ( comp_root_folder ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> data = { <STRING> : [ <STRING> ] } <NEWLINE> input_mapper , output_mapper = Automater ( ) . _create_mappers ( data ) <NEWLINE> self . assertEqual ( 1 , len ( output_mapper . features ) ) <NEWLINE> <DEDENT>
def __str__ ( self ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> return vstr ( self ) <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> return super ( self , Document ) . __str__ ( ) <NEWLINE> <DEDENT> <DEDENT>
def cleardata ( self , name , startdt , enddt ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> savedata = [ ] <NEWLINE> data = self . data [ name ] <NEWLINE> for val in data : <NEWLINE> <INDENT> if ( val [ 0 ] < startdt ) and ( val [ 0 ] >= enddt ) : <NEWLINE> <INDENT> savedata . append ( val ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if uuid in self . _graph . keys ( ) and self . _graph [ uuid ] . owner == self . _id : <NEWLINE> <INDENT> for node in self . _node_deps ( node = uuid ) : <NEWLINE> <INDENT> self . change_owner ( uuid = node , new_owner = new_owner ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , server , persistence_config = None , ember_config = None , <NEWLINE> <INDENT> node_id = None , storage_nw_ip = None , ** kwargs ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if persistence_config : <NEWLINE> cinderlib_extra_config = ember_config . copy ( ) <NEWLINE> cinderlib_extra_config . pop ( <STRING> ) <NEWLINE> ember_config [ <STRING> ] = False <NEWLINE> cinderlib . setup ( persistence_config = persistence_config , <NEWLINE> <INDENT> ** cinderlib_extra_config ) <NEWLINE> IdentityBase . __init__ ( self , server , ember_config ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , server , persistence_config , backend_config , <NEWLINE> <INDENT> ember_config = None , ** kwargs ) : <NEWLINE> cinderlib_extra_config = ember_config . copy ( ) <NEWLINE> cinderlib_extra_config . pop ( <STRING> ) <NEWLINE> cinderlib . setup ( persistence_config = persistence_config , <NEWLINE> <INDENT> ** ember_config ) <NEWLINE> self . backend = cinderlib . Backend ( ** backend_config ) <NEWLINE> IdentityBase . __init__ ( self , server , ember_config ) <NEWLINE> self . CSI . add_ControllerServicer_to_server ( self , server ) <NEWLINE> <DEDENT> <DEDENT>
if result [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> if result [ <STRING> ] [ <STRING> ] == <STRING> and not silent : <NEWLINE> <INDENT> self . send_update ( <STRING> , { <NEWLINE> <INDENT> <STRING> : self . execution_count , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : result [ <STRING> ] [ <STRING> ] <NEWLINE> <DEDENT> } , <NEWLINE> <STRING> : { } <NEWLINE> <DEDENT> } ) <NEWLINE> <DEDENT> <DEDENT>
def writeSequence ( self , data ) : <NEWLINE> <INDENT> for chunk in data : <NEWLINE> <INDENT> self . write ( data ) <NEWLINE> <DEDENT> <DEDENT>
def appendVertex ( vertices , theta , z , dx = dxabs , dy = dyabs , norm = [ ] ) : <NEWLINE> <INDENT> c = _Vector ( [ 0 , 0 , 0 ] ) <NEWLINE> x = dx * ( ( ( self . zMax - z ) / self . zMax ) * _np . cos ( theta ) ) <COMMENT> <NEWLINE> y = dy * ( ( ( self . zMax - z ) / self . zMax ) * _np . sin ( theta ) ) <NEWLINE> d = _Vector ( <NEWLINE> <INDENT> x , <NEWLINE> y , <NEWLINE> z ) <NEWLINE> <DEDENT> if not norm : <NEWLINE> <INDENT> n = d <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> n = _Vector ( norm ) <NEWLINE> <DEDENT> vertices . append ( _Vertex ( c . plus ( d ) , d ) ) <NEWLINE> <DEDENT>
if registry : <NEWLINE> <INDENT> registry . addSolid ( self ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> ws = _g4 . solid . Box ( <STRING> , wx , wy , wz , reg , <STRING> ) <NEWLINE> ps = _g4 . solid . GenericPolyhedra ( <STRING> , psphi , pdphi , pnsid , pz , pr , reg , <STRING> , <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> cols = [ ] <NEWLINE> if <STRING> in df . columns : <NEWLINE> <INDENT> cols += [ <STRING> ] <NEWLINE> <DEDENT> cols += [ col for col in groupby if col in df . columns ] <NEWLINE> cols += [ col for col in metrics if col in df . columns ] <NEWLINE> cols += [ col for col in df . columns if col in cols ] <NEWLINE> df = df [ cols ] <NEWLINE> return QueryResult ( <NEWLINE> <INDENT> df = df , <NEWLINE> query = query_str , <NEWLINE> duration = datetime . now ( ) - qry_start_dttm ) <NEWLINE> <DEDENT> <DEDENT>
def all_table_names ( self , schema = None , force = False ) : <NEWLINE> <INDENT> if not schema : <NEWLINE> <INDENT> tables_dict = self . db_engine_spec . fetch_result_sets ( <NEWLINE> <INDENT> self , <STRING> , force = force ) <NEWLINE> <DEDENT> return tables_dict . get ( <STRING> , [ ] ) <NEWLINE> <DEDENT> return sorted ( <NEWLINE> <INDENT> self . db_engine_spec . get_table_names ( self . inspector , schema ) ) <NEWLINE> <DEDENT> <DEDENT>
try : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> result = subprocess . run ( <STRING> , check = True , shell = True , <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> input = bytes ( source , sys . getdefaultencoding ( ) ) , <NEWLINE> stdout = subprocess . PIPE , <NEWLINE> stderr = subprocess . PIPE <COMMENT> <NEWLINE> ) <NEWLINE> except OSError as exception : <NEWLINE> <DEDENT> msg = <STRING> <NEWLINE> raise SnutreeError ( exception ) <NEWLINE> <DEDENT>
test_tot_struc += num_batch_atom <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> test_elem , tmp_nne , tmp_eloss = sess . run ( [ self . next_elem , self . E , self . e_loss ] , feed_dict = test_fdict ) <NEWLINE> num_batch_struc = valid_elem [ <STRING> ] - 1 <NEWLINE> eloss += tmp_eloss * num_batch_struc <NEWLINE> <DEDENT> <DEDENT>
if not self . inputs [ <STRING> ] : <NEWLINE> <INDENT> os . remove ( item ) <NEWLINE> <DEDENT>
def upgrade ( ) : <NEWLINE> <INDENT> conn = op . get_bind ( ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> graph_table = sa . sql . table ( <STRING> , <NEWLINE> <INDENT> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> ) <NEWLINE> <DEDENT> users_table = sa . sql . table ( <STRING> , <NEWLINE> <INDENT> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> ) <NEWLINE> <DEDENT> conn . execute ( graph_table . update ( ) . values ( <NEWLINE> <INDENT> creation = datetime . utcnow ( ) , last_access = datetime . utcnow ( ) ) ) <NEWLINE> <DEDENT> conn . execute ( users_table . update ( ) . values ( <NEWLINE> <INDENT> creation = datetime . utcnow ( ) , last_connection = datetime . utcnow ( ) ) ) <NEWLINE> <DEDENT> if is_sqlite ( conn ) : <NEWLINE> <INDENT> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> <DEDENT> <DEDENT>
def setUp ( self ) : <NEWLINE> <INDENT> self . directory = tempfile . mkdtemp ( ) <NEWLINE> self . source_path = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path = importlib . util . cache_from_source ( self . source_path ) <NEWLINE> with open ( self . source_path , <STRING> ) as file : <NEWLINE> <INDENT> file . write ( <STRING> ) <NEWLINE> <DEDENT> self . source_path2 = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path2 = importlib . util . cache_from_source ( self . source_path2 ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path2 ) <NEWLINE> self . subdirectory = os . path . join ( self . directory , <STRING> ) <NEWLINE> os . mkdir ( self . subdirectory ) <NEWLINE> self . source_path3 = os . path . join ( self . subdirectory , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path3 ) <NEWLINE> many_directories = [ str ( number ) for number in range ( 1 , 100 ) ] <NEWLINE> self . long_path = os . path . join ( <STRING> , <NEWLINE> <INDENT> self . directory , <NEWLINE> * many_directories ) <NEWLINE> <DEDENT> os . makedirs ( self . long_path ) <NEWLINE> self . source_path_long = os . path . join ( self . long_path , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path_long ) <NEWLINE> self . bc_path_long = importlib . util . cache_from_source ( <NEWLINE> <INDENT> self . source_path_long <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
def setUp ( self ) : <NEWLINE> <INDENT> self . directory = tempfile . mkdtemp ( ) <NEWLINE> self . source_path = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path = importlib . util . cache_from_source ( self . source_path ) <NEWLINE> with open ( self . source_path , <STRING> ) as file : <NEWLINE> <INDENT> file . write ( <STRING> ) <NEWLINE> <DEDENT> self . source_path2 = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path2 = importlib . util . cache_from_source ( self . source_path2 ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path2 ) <NEWLINE> self . subdirectory = os . path . join ( self . directory , <STRING> ) <NEWLINE> os . mkdir ( self . subdirectory ) <NEWLINE> self . source_path3 = os . path . join ( self . subdirectory , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path3 ) <NEWLINE> many_directories = [ str ( number ) for number in range ( 1 , 100 ) ] <NEWLINE> self . long_path = os . path . join ( <STRING> , <NEWLINE> <INDENT> self . directory , <NEWLINE> * many_directories ) <NEWLINE> <DEDENT> os . makedirs ( self . long_path ) <NEWLINE> self . source_path_long = os . path . join ( self . long_path , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path_long ) <NEWLINE> self . bc_path_long = importlib . util . cache_from_source ( <NEWLINE> <INDENT> self . source_path_long <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
def _complement ( self , metadata : Dict [ str , Any ] ) -> Dict [ str , Any ] : <NEWLINE> <INDENT> <STRING> <NEWLINE> metadata = super ( ) . _complement ( metadata ) <NEWLINE> <COMMENT> <NL> if metadata . get ( <STRING> ) or not self . config . store_original : <NEWLINE> <INDENT> del metadata [ <STRING> ] <NEWLINE> <DEDENT> return metadata <NEWLINE> <DEDENT>
def createSwirlFile ( self , fileName ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> links = [ ] <NEWLINE> while os . path . islink ( fileName ) : <NEWLINE> <INDENT> p = os . readlink ( fileName ) <NEWLINE> if not os . path . isabs ( p ) : <NEWLINE> <INDENT> p = os . path . join ( os . path . dirname ( fileName ) , p ) <NEWLINE> <DEDENT> links . append ( p ) <NEWLINE> fileName = p <NEWLINE> <DEDENT> for swirlFile in self . swirlFiles : <NEWLINE> <INDENT> if swirlFile . path == fileName : <NEWLINE> <COMMENT> <NL> <INDENT> swirlFile . setLinks ( links ) <NEWLINE> return swirlFile <NEWLINE> <DEDENT> <DEDENT> swirlFile = SwirlFile ( fileName , links ) <NEWLINE> self . swirlFiles . append ( swirlFile ) <NEWLINE> return swirlFile <NEWLINE> <DEDENT>
def crop_resample ( bands , intensities , crops ) : <NEWLINE> <INDENT> intensities = np . atleast_2d ( intensities ) <NEWLINE> crops = sorted ( crops ) <NEWLINE> <COMMENT> <NL> prev_ub = float ( <STRING> ) <NEWLINE> for lb , ub , step in crops : <NEWLINE> <INDENT> if ub >= lb : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if lb < prev_ub : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> prev_ub = ub <NEWLINE> <COMMENT> <NL> <DEDENT> locs = sorted ( set ( c [ 0 ] for c in crops ) . union ( set ( c [ 1 ] for c in crops ) ) ) <NEWLINE> idxs = np . searchsorted ( bands , locs ) <NEWLINE> loc_idxs = dict ( zip ( locs , idxs ) ) <NEWLINE> <COMMENT> <NL> xs , ys = [ ] , [ ] <NEWLINE> for lb , ub , step in crops : <NEWLINE> <INDENT> s = slice ( loc_idxs [ lb ] , loc_idxs [ ub ] ) <NEWLINE> x = bands [ s ] <NEWLINE> if step > 0 : <NEWLINE> <INDENT> x_new = np . arange ( x [ 0 ] , x [ - 1 ] + step , step ) <NEWLINE> y_new = np . row_stack ( [ np . interp ( x_new , x , y ) for y in intensities [ : , s ] ] ) <NEWLINE> xs . append ( x_new ) <NEWLINE> ys . append ( y_new ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> xs . append ( x ) <NEWLINE> ys . append ( intensities [ : , s ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> return np . concatenate ( xs ) , np . hstack ( ys ) <NEWLINE> <DEDENT>
def post_prob_scheme ( t_table , words , stanza , scheme ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> myprob = 1 <NEWLINE> n = len ( words ) <NEWLINE> rhymelists = get_rhymelists ( stanza , scheme ) <NEWLINE> for rhymelist in rhymelists : <NEWLINE> <INDENT> for i , w in enumerate ( rhymelist ) : <NEWLINE> <INDENT> r = words . index ( w ) <NEWLINE> if i == 0 : <COMMENT> <NEWLINE> <INDENT> myprob = t_table [ r , n ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for v in rhymelist [ : i ] : <COMMENT> <NEWLINE> <INDENT> c = words . index ( v ) <NEWLINE> myprob *= t_table [ r , c ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> if myprob == 0 and len ( stanza ) > 30 : <COMMENT> <NEWLINE> <INDENT> myprob = 1e-300 <NEWLINE> <DEDENT> return myprob <NEWLINE> <DEDENT>
def __init__ ( <NEWLINE> <INDENT> self , <NEWLINE> signals : typing . Iterable [ <NEWLINE> <INDENT> typing . Union [ signal . Signals , int , str ] ] = _DEFAULT_SIGS , <NEWLINE> <DEDENT> callback : typing . Optional [ <NEWLINE> <INDENT> typing . Callable [ [ signal . Signals , typing . Optional [ FrameType ] ] , None ] ] = None , <NEWLINE> ) : <NEWLINE> <DEDENT> signals = list ( signals ) <NEWLINE> signals_tmp = [ ] <COMMENT> <NEWLINE> for sig in signals : <NEWLINE> <INDENT> if isinstance ( sig , int ) : <NEWLINE> <INDENT> sig = signal . Signals ( sig ) <NEWLINE> <DEDENT> elif isinstance ( sig , str ) : <NEWLINE> <INDENT> sig = signal . Signals [ sig ] <NEWLINE> <DEDENT> if isinstance ( sig , signal . Signals ) : <COMMENT> <NEWLINE> <INDENT> signals_tmp . append ( sig ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> % ( sig , ) ) <NEWLINE> <DEDENT> <DEDENT> if not signals : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if callback is None : <NEWLINE> <INDENT> callback = self . _default_callback <NEWLINE> <DEDENT> if not _two_pos_args ( callback ) : <NEWLINE> <INDENT> raise TypeError ( <NEWLINE> <INDENT> <STRING> % <NEWLINE> ( callback , ) ) <NEWLINE> <DEDENT> <DEDENT> if os . name == <STRING> : <NEWLINE> <INDENT> if not ( set ( signals ) <= set ( self . _DEFAULT_SIGS ) ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> % ( signals , ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . _signals = tuple ( signals_tmp ) <COMMENT> <NEWLINE> self . _callback = callback <NEWLINE> <COMMENT> <NL> self . _old_handlers = [ ] <COMMENT> <NEWLINE> self . _depth = 0 <NEWLINE> <DEDENT>
def creates_default_site ( sender , instance , created , * args , ** kwargs ) : <NEWLINE> <INDENT> if created : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> site = Site . objects . get ( domain__icontains = DEFAULT_SITE_DOMAIN , <NEWLINE> <INDENT> tenant_site__tenant = instance ) <NEWLINE> <DEDENT> if site . domain != ( <STRING> % ( instance . slug , DEFAULT_SITE_DOMAIN ) ) : <NEWLINE> <INDENT> site . delete ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT> except Site . DoesNotExist : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def __call__ ( self , h , c , x ) : <NEWLINE> <INDENT> if not self . test and self . dropout > 0 : <NEWLINE> <INDENT> gates = dy . vanilla_lstm_gates_dropout ( x , h , self . Whx , self . Whh , self . bh , self . dropout_mask_x , self . dropout_mask_h ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> gates = dy . vanilla_lstm_gates ( x , h , self . Whx , self . Whh , self . bh ) <NEWLINE> <DEDENT> new_c = dy . vanilla_lstm_c ( c , gates ) <NEWLINE> new_h = dy . vanilla_lstm_h ( c , gates ) <NEWLINE> return new_h , new_c <NEWLINE> <DEDENT>
def _test_recurrent_layer_bidirectional_transduction ( <NEWLINE> <INDENT> fwd_layer , <NEWLINE> bwd_layer , <NEWLINE> dummy_input , <NEWLINE> lengths , <NEWLINE> left_padded , <NEWLINE> ) : <NEWLINE> <COMMENT> <NL> tranductor = transduction_layers . BidirectionalLayer ( fwd_layer , bwd_layer ) <NEWLINE> <COMMENT> <NL> dy . renew_cg ( ) <NEWLINE> <COMMENT> <NL> seq = [ <NEWLINE> <INDENT> dy . inputTensor ( dummy_input , batched = True ) + i for i in range ( 10 ) <NEWLINE> <DEDENT> ] <NEWLINE> <COMMENT> <NL> tranductor . init ( test = False , update = True ) <NEWLINE> <COMMENT> <NL> fwd_states , bwd_states = tranductor ( <NEWLINE> <INDENT> seq , lengths = lengths , left_padded = left_padded <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> fwd_z = dy . mean_batches ( <NEWLINE> <INDENT> dy . esum ( [ dy . sum_elems ( state [ 0 ] ) for state in fwd_states ] ) <NEWLINE> <DEDENT> ) <NEWLINE> bwd_z = dy . mean_batches ( <NEWLINE> <INDENT> dy . esum ( [ dy . sum_elems ( state [ 0 ] ) for state in fwd_states ] ) <NEWLINE> <DEDENT> ) <NEWLINE> z = fwd_z + bwd_z <NEWLINE> z . forward ( ) <NEWLINE> z . backward ( ) <NEWLINE> <DEDENT>
def select_bounding_box ( self , names , column ) : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for name in names : <NEWLINE> <INDENT> target = self . store [ name ] <NEWLINE> if target . has_column ( ) and target . column <= column : continue <NEWLINE> result . append ( target . row ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
def _parse ( string ) : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for match in PYTHON_FORMAT . finditer ( string ) : <NEWLINE> <INDENT> name , format , typechar = match . groups ( ) <NEWLINE> if typechar == <STRING> and name is not None : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> result . append ( ( name , str ( typechar ) ) ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
return tmin > tmax <NEWLINE>
@ property <NEWLINE> <INDENT> def rooms ( self ) -> list : <NEWLINE> <INDENT> return [ InTouchRoom ( r , self ) for r in [ <STRING> , <STRING> ] <NEWLINE> <INDENT> if True and _convert ( <NEWLINE> <INDENT> self . _data [ <STRING> . format ( r ) ] , <NEWLINE> self . _data [ <STRING> . format ( r ) ] ) is not None ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def consume ( self , msg ) : <NEWLINE> <INDENT> body , topic = msg . get ( <STRING> ) , msg . get ( <STRING> ) <NEWLINE> pretty_text = fedmsg . text . msg2repr ( body ) <NEWLINE> log . debug ( pretty_text ) <NEWLINE> icon = fedmsg . text . _msg2icon ( msg ) or <STRING> <NEWLINE> log . debug ( <STRING> % icon ) <NEWLINE> if icon : <NEWLINE> <INDENT> icon_file = self . _icon_cache . get ( icon ) <NEWLINE> if not icon_file : <NEWLINE> <INDENT> icon_file , headers = urllib . urlretrieve ( icon ) <NEWLINE> log . debug ( <STRING> % ( icon . split ( <STRING> ) [ - 1 ] , <NEWLINE> <INDENT> icon_file ) ) <NEWLINE> <DEDENT> self . _icon_cache [ icon ] = icon_file <NEWLINE> <DEDENT> icon = icon_file <NEWLINE> <DEDENT> note = Notify . Notification . new ( <STRING> , pretty_text , icon_file ) <NEWLINE> note . show ( ) <NEWLINE> <DEDENT>
if len ( file_list ) == 2 : <NEWLINE> <INDENT> return find_probes ( first_probe_data , second_probe_data ) <NEWLINE> elif len ( file_list ) < 2 : <NEWLINE> return find_probes_recursively ( file_list [ 2 : ] , tail = find_probes ( first_probe_data , second_probe_data ) ) <NEWLINE> else : <NEWLINE> probe_data = read_probe_records_from_file ( file_list [ 0 ] ) <NEWLINE> <DEDENT>
def get_recipients ( args ) : <NEWLINE> <INDENT> if args [ <STRING> ] : <NEWLINE> <INDENT> recipients = [ ] <NEWLINE> for recipient in args [ <STRING> ] : <NEWLINE> <INDENT> key = binascii . unhexlify ( recipient ) <NEWLINE> assert len ( recipient ) == 32 <NEWLINE> recipients . append ( key ) <NEWLINE> <DEDENT> return recipients <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> private = get_private ( args ) <NEWLINE> public = libnacl . crypto_scalarmult_base ( private ) <NEWLINE> return [ public ] <NEWLINE> <DEDENT> <DEDENT>
date_edit_widgets = ( self . _ui . afterDateFilterDateEdit , <NEWLINE> <INDENT> self . _ui . beforeDateFilterDateEdit ) <NEWLINE> for widget in time_edit_widgets : <NEWLINE> self . connect ( widget , SIGNAL ( <STRING> ) , <NEWLINE> self . apply_filters ) <NEWLINE> <DEDENT>
def date ( s ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> y , m , d = s . strip ( ) . split ( <STRING> ) <NEWLINE> y = year ( y ) <NEWLINE> m = month ( m ) <NEWLINE> d = day ( m ) <NEWLINE> return datetime . date ( y , m , d ) <NEWLINE> <DEDENT>
ax2 . add_patch ( patches . Rectangle ( <NEWLINE> <INDENT> ( 1.5 , 1.5 ) , <NEWLINE> 6 , 6 , <NEWLINE> fill = False , <NEWLINE> lw = 2 , <NEWLINE> ls = <STRING> , <NEWLINE> edgecolor = <STRING> , <NEWLINE> ) ) <NEWLINE> ax3 . add_patch ( patches . Rectangle ( <NEWLINE> ( 0 , 0 ) , <NEWLINE> 9 , 9 , <NEWLINE> fill = False , <NEWLINE> lw = 1 , <NEWLINE> ls = <STRING> , <NEWLINE> edgecolor = <STRING> , <NEWLINE> ) ) <NEWLINE> <DEDENT>
for chunk in chunker_list ( list ( barcode_dict . keys ( ) ) , args . writer_threads - 1 ) : <NEWLINE> <INDENT> logger . debug ( <STRING> . format ( <STRING> . join ( chunk ) ) ) <NEWLINE> q = manager . Queue ( ) <NEWLINE> q_bc_dict = dict ( ( k , barcode_dict [ k ] ) for k in chunk ) <NEWLINE> writer_pool . apply_async ( _writer , ( q , q_bc_dict ) , callback = lambda x : print ( x ) ) <NEWLINE> queue_list . append ( q ) <NEWLINE> for bc in chunk . values ( ) : <NEWLINE> <INDENT> queues [ bc ] = q <NEWLINE> <DEDENT> <DEDENT>
def _do_write ( self , offset , data ) : <NEWLINE> <INDENT> n = offset / CHUNKDATASIZE <NEWLINE> while n > len ( self . chunks ) : <NEWLINE> <INDENT> self . _add_new_chunk ( ) <NEWLINE> <DEDENT> <DEDENT>
def removeBelowValue ( requestContext , seriesList , n ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for s in seriesList : <NEWLINE> <INDENT> s . name = <STRING> % ( s . name , n ) <NEWLINE> for ( index , val ) in enumerate ( s ) : <NEWLINE> <INDENT> if val > n : <NEWLINE> <INDENT> s [ index ] = None <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
_kde = kde ( x = _f_x , df = _df_i , x_steps = kde_steps ) [ 0 ] <NEWLINE> <INDENT> __x = _kde [ _f_x ] <NEWLINE> _y = _kde [ <STRING> ] <NEWLINE> _ax . plot ( __x , _y , linestyle = _f_distfit_line , color = _f_distfit_color , alpha = alpha , linewidth = 2 , <NEWLINE> <INDENT> label = _label_2 , ** kwargs ) <NEWLINE> <DEDENT> if not show_hist and _f_fill : <NEWLINE> <INDENT> _ax . fill_between ( _f_bins , _y , color = _f_facecolor , alpha = alpha ) <NEWLINE> <DEDENT> <DEDENT>
_index_name = df . index . name <NEWLINE> <INDENT> _df [ <STRING> ] = _df . index <NEWLINE> _k_split = int ( np . ceil ( _df . shape [ 0 ] / k ) ) <NEWLINE> <DEDENT>
async def edit ( self , name_ , * , name = None , description = utils . sentinel ) : <NEWLINE> <INDENT> return self . _new_emote ( await self . _http . edit ( name , name = name , description = description ) ) <NEWLINE> <DEDENT>
def get_entropy_for_each_line ( trained_model : TrainedModel , <NEWLINE> <INDENT> file : str , <NEWLINE> entropy_aggregator : Callable [ [ List [ float ] , List [ int ] ] , Union [ float , List [ float ] ] ] , <NEWLINE> verbose : bool = False ) -> Union [ List [ float ] , List [ List [ float ] ] ] : <NEWLINE> prep_lines_and_entropies : List [ Dict [ str , Union [ str , List [ str ] , List [ float ] , float ] ] ] = [ ] <NEWLINE> with open ( file , <STRING> ) as f : <NEWLINE> _ , extension = os . path . splitext ( file ) [ 1 : ] <NEWLINE> for line in f : <NEWLINE> time_measurer . tick ( <STRING> ) <NEWLINE> prep_line , entropies , word_boundaries = trained_model . get_entropies_for_text ( line , extension [ 1 : ] ) <NEWLINE> time_measurer . tock ( <STRING> ) <NEWLINE> line_entropy = entropy_aggregator ( entropies , word_boundaries ) <NEWLINE> prep_lines_and_entropies . append ( { <NEWLINE> <STRING> : line , <NEWLINE> <STRING> : prep_line , <NEWLINE> <STRING> : entropies , <NEWLINE> <STRING> : line_entropy <NEWLINE> } ) <NEWLINE> if not verbose : <NEWLINE> for line in prep_lines_and_entropies : <NEWLINE> print ( line [ <STRING> ] ) <NEWLINE> print ( line [ <STRING> ] ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> return list ( map ( lambda e : e [ <STRING> ] , prep_lines_and_entropies ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> with tqdm ( total = len ( self ) ) as pbar : <NEWLINE> <INDENT> on_error = True <NEWLINE> while True : <NEWLINE> <INDENT> case = self . _dump ( ) <NEWLINE> pbar . set_description ( <STRING> . join ( <NEWLINE> <INDENT> <STRING> . format ( c [ 1 ] [ - 10 : ] ) if isinstance ( c [ 1 ] , str ) <NEWLINE> else <STRING> . format ( c [ 1 ] ) <NEWLINE> for c in case <NEWLINE> <DEDENT> ) ) <NEWLINE> <COMMENT> <NL> for _fun , _args in self . _for : <NEWLINE> <INDENT> await _fun ( case ) ( * _args ) <NEWLINE> <COMMENT> <NL> <DEDENT> for _cmd in self . _exes : <NEWLINE> <INDENT> if await self . _execute ( _cmd ) : <NEWLINE> <INDENT> on_error = True <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> if on_error : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> yield case <NEWLINE> <COMMENT> <NL> await asyncio . sleep ( 0.1 ) <NEWLINE> pbar . update ( ) <NEWLINE> if self . _step ( ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . _lock = False <NEWLINE> <DEDENT>
test_cycles_rewards ) <NEWLINE>
def parse_node ( self , node , model , alias ) : <NEWLINE> <INDENT> query = [ ] <NEWLINE> query_data = [ ] <NEWLINE> nodes = [ ] <NEWLINE> for child in node . children : <NEWLINE> <INDENT> if isinstance ( child , Q ) : <NEWLINE> <INDENT> parsed , data = self . parse_q ( child , model , alias ) <NEWLINE> query . append ( parsed ) <NEWLINE> query_data . extend ( data ) <NEWLINE> <DEDENT> elif isinstance ( child , Node ) : <NEWLINE> <INDENT> parsed , data = self . parse_node ( node , model , alias ) <NEWLINE> query . append ( <STRING> % parsed ) <NEWLINE> query_data . extend ( data ) <NEWLINE> <DEDENT> <DEDENT> query . extend ( nodes ) <NEWLINE> connector = <STRING> % node . connector <NEWLINE> return connector . join ( query ) , query_data <NEWLINE> <DEDENT>
user_indexes = self . get_sorted_indexes ( User ) <NEWLINE> <INDENT> if BACKEND == <STRING> : <NEWLINE> <INDENT> entry_indexes . pop ( 0 ) <NEWLINE> <DEDENT> <DEDENT>
class Meta : <NEWLINE> <INDENT> primary_key = CompositeKey ( <STRING> , <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if dest in accum and table not in accum : <NEWLINE> <INDENT> print_ ( <STRING> % foreign_key ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> mpk = metadata . primary_key is not None <NEWLINE> can_populate_joined_pk = ( <NEWLINE> <INDENT> mpk and <NEWLINE> ( metadata . attr in inst . _data ) and <NEWLINE> ( getattr ( joined_inst , metadata . primary_key ) is not None ) ) <NEWLINE> <DEDENT> if can_populate_joined_pk : <NEWLINE> <INDENT> setattr ( <NEWLINE> <INDENT> joined_inst , <NEWLINE> metadata . primary_key , <NEWLINE> inst . _data [ metadata . attr ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_engine ( ) : <NEWLINE> <INDENT> global __ENGINE__ <NEWLINE> if __ENGINE__ is None : <NEWLINE> <INDENT> cred = DB_CONNECTION . get ( <STRING> , <STRING> ) <NEWLINE> if cred : <NEWLINE> <INDENT> if <STRING> in DB_CONNECTION : <NEWLINE> <INDENT> cred += <STRING> . format ( ** DB_CONNECTION ) <NEWLINE> <DEDENT> cred += <STRING> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
urlpath = None <NEWLINE> <INDENT> path = path_from_link <NEWLINE> try : <NEWLINE> <INDENT> urlpath = models . URLPath . get_by_path ( path_from_link ) <NEWLINE> path = urlpath . get_absolute_url ( ) <NEWLINE> <DEDENT> except models . URLPath . DoesNotExist : <NEWLINE> <INDENT> pass <NEWLINE> else : <NEWLINE> <DEDENT> urlpath = models . URLPath . objects . get ( article = self . markdown . article ) <NEWLINE> source_components = urlpath . path . strip ( <STRING> ) . split ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> starting_level = max ( 0 , self . config [ <STRING> ] [ 0 ] - 1 ) <NEWLINE> starting_path = <STRING> . join ( source_components [ : starting_level ] ) <NEWLINE> <DEDENT>
if x < 0 or y > 8 or y < 0 or y > 8 : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT>
def get_alignment_error ( self , ename ) : <NEWLINE> <INDENT> align_error_conf = [ ] <NEWLINE> dx = self . _get_config ( ename , CONFIG_ALIGNMENT_DX , None ) <NEWLINE> if dx is not None : <NEWLINE> <INDENT> _LOGGER . info ( <STRING> . format ( ename , dx ) ) <NEWLINE> align_error_conf . append ( ( <STRING> , float ( dx ) ) ) <NEWLINE> <DEDENT> dy = self . _get_config ( ename , CONFIG_ALIGNMENT_DY , None ) <NEWLINE> if dx is not None : <NEWLINE> <INDENT> _LOGGER . info ( <STRING> . format ( ename , dx ) ) <NEWLINE> align_error_conf . append ( ( <STRING> , float ( dy ) ) ) <NEWLINE> <DEDENT> return align_error_conf <NEWLINE> <DEDENT>
yield from includeme ( self . config ) <NEWLINE> <INDENT> except Exception : <NEWLINE> <INDENT> log . exception ( <STRING> . format ( includeme ) ) <NEWLINE> <DEDENT> <DEDENT>
def execute ( self , args ) : <NEWLINE> <INDENT> if not self . cfg . globals . enable_experimental : <NEWLINE> <INDENT> raise exception . ExperimentalFeature ( <STRING> ) <NEWLINE> <DEDENT> if len ( args ) < 3 : <NEWLINE> <INDENT> self . parser . error ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> ctag = args [ 0 ] <NEWLINE> lpath = args [ - 1 ] <NEWLINE> rpaths = args [ 1 : - 1 ] <NEWLINE> cl = self . cm . get_cluster ( ctag ) <NEWLINE> node = cl . get_node_by_alias ( self . opts . node ) <NEWLINE> if self . opts . user : <NEWLINE> <INDENT> node . ssh . switch_user ( self . opts . user ) <NEWLINE> <DEDENT> for rpath in rpaths : <NEWLINE> <INDENT> if not glob . has_magic ( rpath ) and not node . ssh . path_exists ( rpath ) : <NEWLINE> <INDENT> raise exception . BaseException ( <NEWLINE> <INDENT> <STRING> % lpath ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> node . ssh . get ( rpaths , lpath ) <NEWLINE> <DEDENT>
def execute_task ( task_name , args ) : <NEWLINE> <INDENT> task = retrieve_task ( task_name ) <NEWLINE> subprocess . run ( [ <STRING> , task_name ] + args ) <NEWLINE> <DEDENT>
@ raise_for_error <NEWLINE> <INDENT> def _request ( self , asins , marketplaceid ) : <NEWLINE> <INDENT> response = self . api . get_competitive_pricing_for_asin ( asins , marketplaceid ) <NEWLINE> write_response ( response , <STRING> ) <NEWLINE> response . raise_for_status ( ) <NEWLINE> return response . content <NEWLINE> <DEDENT> <DEDENT>
if context . get ( <STRING> ) : <NEWLINE> <INDENT> out . info ( pprint . pformat ( context ) ) <NEWLINE> return <NEWLINE> <DEDENT>
def __ne__ ( self , other ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not isinstance ( other , Release ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>
class Meteor : <NEWLINE> <INDENT> def __init__ ( self ) : <NEWLINE> <INDENT> base_path = os . path . dirname ( os . path . abspath ( __file__ ) ) <NEWLINE> jar_path = os . path . join ( base_path , METEOR_JAR ) <NEWLINE> gz_path = os . path . join ( base_path , os . path . basename ( METEOR_GZ_URL ) ) <NEWLINE> if not os . path . isfile ( jar_path ) : <NEWLINE> <INDENT> if not os . path . isfile ( jar_path ) : <NEWLINE> <INDENT> download_from_url ( METEOR_GZ_URL , gz_path ) <NEWLINE> <DEDENT> tar = tarfile . open ( gz_path , <STRING> ) <NEWLINE> tar . extractall ( path = os . path . dirname ( os . path . abspath ( __file__ ) ) ) <NEWLINE> tar . close ( ) <NEWLINE> os . remove ( gz_path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
curr_states_set . extend ( curr_states ( <NEWLINE> <INDENT> state = nfa . state , <NEWLINE> captured = None , <NEWLINE> chars = ( char , next_char ) ) ) <NEWLINE> <DEDENT>
if Wallet . is_seed ( text3 ) : <NEWLINE> <INDENT> wallet . add_cosigner_seed ( text3 , <STRING> , password ) <NEWLINE> elif Wallet . is_xpub ( text3 ) : <NEWLINE> wallet . add_master_public_key ( <STRING> , text2 ) <NEWLINE> <DEDENT>
def set_parameters ( self , host , port , protocol , proxy , auto_connect ) : <NEWLINE> <INDENT> proxy_str = interface . serialize_proxy ( proxy ) <NEWLINE> server_str = <STRING> . join ( [ host , port , protocol ] ) <NEWLINE> self . config . set_key ( <STRING> , auto_connect , True ) <NEWLINE> self . config . set_key ( <STRING> , proxy_str , True ) <NEWLINE> self . config . set_key ( <STRING> , server_str , True ) <NEWLINE> <COMMENT> <NL> if self . config . get ( <STRING> ) != server_str or self . config . get ( <STRING> ) != proxy : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT>
@ hook <NEWLINE> <INDENT> def transaction_dialog ( self , d ) : <NEWLINE> <INDENT> self . send_button = b = QPushButton ( _ ( <STRING> ) ) <NEWLINE> b . clicked . connect ( lambda : self . do_send ( d . tx ) ) <NEWLINE> d . buttons . insert ( 2 , b ) <NEWLINE> self . transaction_dialog_update ( d ) <NEWLINE> <DEDENT> <DEDENT>
if self . tx . is_complete ( ) : <NEWLINE> <INDENT> if tx_hash in self . wallet . transactions . keys ( ) : <NEWLINE> <INDENT> desc = self . wallet . get_label ( tx_hash ) <NEWLINE> height , conf , timestamp = self . wallet . get_tx_height ( tx_hash ) <NEWLINE> if height > 0 : <NEWLINE> <INDENT> if conf : <NEWLINE> <INDENT> status = _ ( <STRING> ) % height <NEWLINE> time_str = datetime . datetime . fromtimestamp ( timestamp ) . isoformat ( <STRING> ) [ : - 3 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> status = _ ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> status = _ ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> status = _ ( <STRING> ) <NEWLINE> self . broadcast_button . show ( ) <NEWLINE> <COMMENT> <NL> if self . main_window . network is None : <NEWLINE> <INDENT> self . broadcast_button . setEnabled ( False ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> s , r = self . tx . signature_count ( ) <NEWLINE> status = _ ( <STRING> ) if s == 0 else _ ( <STRING> ) + <STRING> % ( s , r ) <NEWLINE> tx_hash = _ ( <STRING> ) ; <NEWLINE> <DEDENT>
if len ( addrs ) == 1 : <NEWLINE> <INDENT> def show_address ( ) : <NEWLINE> <INDENT> keystore . thread . add ( partial ( self . show_address , wallet , keystore , addrs [ 0 ] ) ) <NEWLINE> <DEDENT> <DEDENT>
lon_str_min = longitude . split ( ) [ 1 ] [ : - 1 ] <NEWLINE> <INDENT> lat_str_min = latitude . split ( ) [ 1 ] [ : - 1 ] <NEWLINE> <COMMENT> <NL> lon_str_min = lon_str_min . replace ( <STRING> , <STRING> ) <NEWLINE> lat_str_min = lon_str_min . replace ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> lon = SIGN_WEST * float ( longitude . split ( ) [ 0 ] ) + float ( lon_str_min ) / 60. <NEWLINE> lat = SIGN_NORTH * float ( latitude . split ( ) [ 0 ] ) + float ( lat_str_min ) / 60. <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> index_to_draw , _ = _helpers . get_max_index ( bit_gate_rank , <NEWLINE> <INDENT> operation = operation ) <NEWLINE> <DEDENT> x_coord = _helpers . get_x_from_index ( index_to_draw ) <NEWLINE> yq_coord = _helpers . get_y_from_quantum_register ( qubits [ 0 ] , bit_mapping ) <NEWLINE> yc_coord = _helpers . get_y_from_classical_register ( total_clbits_number - 1 , total_qubits_number , <NEWLINE> <INDENT> bit_mapping ) <NEWLINE> <COMMENT> <NL> <DEDENT> _draw_classical_double_line ( drawing , x_coord , yq_coord , x_coord , yc_coord ) <NEWLINE> <DEDENT>
ret = np . ones_like ( num_splits ) * P_0 <NEWLINE> <INDENT> for ret_i , n in enumerate ( num_splits ) : <NEWLINE> <INDENT> P = np . zeros ( ( n + 1 ) ) <NEWLINE> P [ 0 ] = P_0 <NEWLINE> for i in range ( n ) : <NEWLINE> <INDENT> h_p = H [ ret_i ] if i == ( n - 1 ) else H [ i + 1 ] <NEWLINE> if L [ i ] != 0 : <NEWLINE> <INDENT> P [ i + 1 ] = P [ i ] * ( T [ i ] / ( T [ i ] + L [ i ] * ( h_p - H [ i ] ) ) ) ** ( 34.163 / L [ i ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> P [ i + 1 ] = P [ i ] * np . exp ( - 34.162 * ( h_p - H [ i ] ) / T [ i ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
alignment = calculate_alignment ( orient_tile ) <NEWLINE>
dir_dict = { <NEWLINE> <INDENT> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <DEDENT>
if downsample : <NEWLINE> <INDENT> fixed_shrunk = trans . resize_image ( fixed_image , fixed_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> rotated_shrunk = trans . resize_image ( rotated_image , moving_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> spacing = fixed_shrunk . GetSpacing ( ) <NEWLINE> <DEDENT>
if roi_size is None : <NEWLINE> <INDENT> with open ( output_path , <STRING> , newline = <STRING> ) as csvfile : <NEWLINE> <INDENT> print ( <STRING> . format ( <NEWLINE> <INDENT> output_path . name , tile_size [ 0 ] ) ) <NEWLINE> <DEDENT> writer = csv . writer ( csvfile ) <NEWLINE> writer . writerow ( [ <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
iecsize = hdd . diskSize ( ) <NEWLINE> <COMMENT> <NL> <INDENT> if iecsize > 1000000 : <NEWLINE> <INDENT> iecsize = ( iecsize + 50000 ) // float ( 100000 ) / 10 <NEWLINE> <COMMENT> <NL> if ( iecsize % 1 > 0 ) : <NEWLINE> <INDENT> iecsize = <STRING> % iecsize <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> iecsize = <STRING> % iecsize <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> elif iecsize > 300000 : <NEWLINE> <INDENT> iecsize = <STRING> % ( ( iecsize + 5000 ) // 10000 * 10 ) <NEWLINE> <COMMENT> <NL> <DEDENT> elif iecsize > 1000 : <NEWLINE> <INDENT> iecsize = <STRING> % ( ( iecsize + 500 ) // 1000 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> iecsize = <STRING> % size <NEWLINE> <DEDENT> <DEDENT>
if querytype != QUERYTYPE_LOOKUP__ID : <NEWLINE> <INDENT> arglist = ( service_reference , querytype , begin ) <NEWLINE> else : <NEWLINE> arglist = ( service_reference , querytype , begin , minutes ) <NEWLINE> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def isinstance ( cls , instance ) : <NEWLINE> <INDENT> if not isinstance ( instance , cls ) : <NEWLINE> <INDENT> return instance <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise TypeError ( <STRING> % ( instance , cls . __name__ ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
rows = husoftm . connection . get_connection ( ) . query ( [ <STRING> ] , condition = <STRING> % ( int ( iln ) , ) ) <NEWLINE> <INDENT> if rows : <NEWLINE> <COMMENT> <NL> <INDENT> return get_kunde ( rows [ 0 ] [ <STRING> ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> rows = husoftm . connection . get_connection ( ) . query ( [ <STRING> ] , condition = <STRING> % ( int ( iln ) , ) ) <NEWLINE> if rows : <NEWLINE> <INDENT> rows2 = husoftm . connection . get_connection ( ) . query ( [ <STRING> ] , <NEWLINE> <INDENT> condition = <STRING> % ( int ( rows [ 0 ] [ <STRING> ] ) , ) ) <NEWLINE> <DEDENT> if rows : <NEWLINE> <INDENT> kunde = Kunde ( ) . fill_from_softm ( rows2 [ 0 ] ) <NEWLINE> kunde . kundennr = kunde . kundennr + ( <STRING> % int ( rows [ 0 ] [ <STRING> ] ) ) <NEWLINE> return kunde <NEWLINE> <DEDENT> <DEDENT> <DEDENT> raise ValueError ( <STRING> % iln ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> assert svc not in self . services <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> config . update ( kwargs ) <NEWLINE> <DEDENT>
def _add_memory_since_blocked ( self , position ) : <NEWLINE> <INDENT> memory_since_blocked = self . get_memory_since_blocked ( ) <NEWLINE> memory_since_blocked . append ( position ) <NEWLINE> self . _set_memory_since_blocked ( position ) <NEWLINE> <DEDENT>
def normalized ( self ) : <NEWLINE> <INDENT> return self . __class__ ( self . _quantity / self . _units . _scale , self . _units . base_units ( ) ) <NEWLINE> <DEDENT>
config [ <STRING> ] = form or parse_kwarg ( config , <STRING> , context . get ( <STRING> ) ) <NEWLINE> <INDENT> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , label ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , <STRING> ) <NEWLINE> config [ <STRING> ] = parse_kwarg ( kwargs , <STRING> , [ ] ) <COMMENT> <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , settings . RH_HELP_TEXT_POSITION ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> model = dct . pop ( <STRING> ) <NEWLINE> dct [ <STRING> ] , dct [ <STRING> ] = set ( ) , set ( ) <NEWLINE> for rel in dct . pop ( <STRING> ) : <NEWLINE> <INDENT> if isinstance ( rel , tuple ) : <NEWLINE> <COMMENT> <NL> <INDENT> other , field , lkey , rkey = rel <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> other = rel <NEWLINE> field , lkey , rkey = other . lower ( ) , <STRING> , <STRING> % model . lower ( ) <NEWLINE> <DEDENT> dct [ field ] = HasOneDescriptor ( other , lkey , rkey ) <NEWLINE> dct [ <STRING> ] . add ( field ) <NEWLINE> index_registry . register ( other , rkey ) <NEWLINE> <DEDENT> for rel in dct . pop ( <STRING> ) : <NEWLINE> <INDENT> if isinstance ( rel , tuple ) : <NEWLINE> <INDENT> other , field , lkey , rkey = rel <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> other = rel <NEWLINE> field , lkey , rkey = other . lower ( ) , <STRING> % other . lower ( ) , <STRING> <NEWLINE> <DEDENT> dct [ field ] = BelongsToDescriptor ( other , lkey , rkey ) <NEWLINE> dct [ <STRING> ] . add ( field ) <NEWLINE> dct [ <STRING> ] . add ( lkey ) <NEWLINE> index_registry . register ( other , lkey ) <NEWLINE> <DEDENT> for rel in dct . pop ( <STRING> ) : <NEWLINE> <INDENT> if isinstance ( rel , tuple ) : <NEWLINE> <INDENT> other , field , lkey , rkey = rel <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> other = rel <NEWLINE> field , lkey , rkey = tableize ( other ) , <STRING> , <STRING> % model . lower ( ) <NEWLINE> <DEDENT> dct [ field ] = HasManyDescriptor ( other , lkey , rkey ) <NEWLINE> dct [ <STRING> ] . add ( field ) <NEWLINE> index_registry . register ( other , rkey ) <NEWLINE> <DEDENT> for rel in dct . pop ( <STRING> ) : <NEWLINE> <INDENT> if isinstance ( rel , tuple ) : <NEWLINE> <INDENT> other , field , lkey , rkey = rel <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> other = rel <NEWLINE> field , lkey , rkey = tableize ( other ) , <STRING> , <STRING> <NEWLINE> <DEDENT> join_model = <STRING> + <STRING> . join ( sorted ( [ model , other ] ) ) <NEWLINE> try : <NEWLINE> <INDENT> remodel . models . ModelBase ( join_model , ( remodel . models . Model , ) , { } ) <NEWLINE> <DEDENT> except AlreadyRegisteredError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> pass <NEWLINE> <DEDENT> mlkey , mrkey = <STRING> % model . lower ( ) , <STRING> % other . lower ( ) <NEWLINE> dct [ field ] = HasAndBelongsToManyDescriptor ( other , lkey , rkey , join_model , mlkey , mrkey ) <NEWLINE> dct [ <STRING> ] . add ( field ) <NEWLINE> index_registry . register ( join_model , mlkey ) <NEWLINE> index_registry . register ( join_model , mrkey ) <NEWLINE> <DEDENT> <DEDENT>
if num_items == 2 : <NEWLINE> <INDENT> if implied_state < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> in_label , dest = arc_def <NEWLINE> arc_simple = implied_state , in_label , in_label , dest , - 1 <NEWLINE> elif num_items == 3 : <NEWLINE> if implied_state < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> in_label , out_label , dest = arc_def <NEWLINE> arc_simple = implied_state , in_label , out_label , dest , - 1 <NEWLINE> elif num_items == 4 : <NEWLINE> <COMMENT> <NL> src , in_label , dest , is_final = arc_def <NEWLINE> if is_final == 1 : <NEWLINE> <INDENT> assert in_label == - 1 or dest == - 1 <NEWLINE> <DEDENT> arc_simple = src , in_label , in_label , dest , is_final <NEWLINE> elif num_items == 5 : <NEWLINE> arc_simple = arc_def <COMMENT> <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> self . mygene_df = self . my_gene_info ( ) <NEWLINE> self . mygene_df . to_csv ( self . mygene_df , self . mygene_path ) <NEWLINE> <COMMENT> <NL> if self . __post_blast : <NEWLINE> <INDENT> self . missing_dict = self . get_miss_acc ( ) <NEWLINE> self . missing_genes = self . missing_dict [ <STRING> ] <NEWLINE> self . missing_gene_count = self . missing_genes [ <STRING> ] <NEWLINE> del self . missing_genes [ <STRING> ] <NEWLINE> self . missing_organsims = self . missing_dict [ <STRING> ] <NEWLINE> self . missing_organsims_count = self . missing_organsims [ <STRING> ] <NEWLINE> del self . missing_organsims [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT>
args = parser . parse_args ( remaining_argv ) <NEWLINE>
xlim1 = ax1 . get_xlim ( ) <NEWLINE> <INDENT> ylim1 = ax2 . get_ylim ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> if self . _project_lock_file is not None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if not os . path . exists ( self . _project_lock_file ) : <NEWLINE> <INDENT> file = open ( self . _project_lock_file , <STRING> ) <NEWLINE> data = file . readlines ( ) <NEWLINE> file . close ( ) <NEWLINE> for index in range ( len ( data ) ) : <NEWLINE> <INDENT> data [ index ] = data [ index ] . strip ( ) <NEWLINE> <DEDENT> if data == self . _project_lock_signature : <NEWLINE> <INDENT> os . remove ( self . _project_lock_file ) <NEWLINE> print ( <STRING> % self . _project_lock_file ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> printWarrningMSG = False <NEWLINE> try : <NEWLINE> <INDENT> if data [ 0 ] != self . _project_lock_signature [ 0 ] : <NEWLINE> <INDENT> printWarrningMSG = True <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> printWarrningMSG = True <NEWLINE> <DEDENT> if printWarrningMSG : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( self . _project_lock_signature ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( data ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> print ( <STRING> % self . _project_lock_file ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def drawColorBox ( self , qp , colorlist , xC , yC , xSize , ySize , DrawBorder ) : <NEWLINE> <INDENT> if xSize > 0 and ySize > 0 : <NEWLINE> <INDENT> numberofColors = len ( colorlist ) <NEWLINE> if numberofColors >= xSize : <NEWLINE> <INDENT> yP = int ( yC + ySize ) <NEWLINE> for i in range ( xSize ) : <NEWLINE> <INDENT> color = colorlist [ i ] <NEWLINE> qp . setBrush ( color ) <NEWLINE> qp . setPen ( color ) <NEWLINE> xE = int ( i + xC ) <NEWLINE> qp . drawLine ( xC , yP , xE , yP ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> rightP = int ( xC ) <NEWLINE> scaleFactor = float ( xSize ) / float ( numberofColors ) <NEWLINE> rightPos = rightP + xSize - 1 <NEWLINE> for i in range ( numberofColors ) : <NEWLINE> <INDENT> leftP = rightP <NEWLINE> rightP = int ( xC + scaleFactor * float ( i + 1 ) ) <NEWLINE> color = colorlist [ i ] <NEWLINE> qp . setBrush ( color ) <NEWLINE> qp . setPen ( color ) <NEWLINE> if ( leftP < rightPos ) : <NEWLINE> <INDENT> qp . drawRect ( leftP , yC , max ( rightP - leftP , 1 ) , ySize - 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if DrawBorder : <NEWLINE> <INDENT> qp . setPen ( QtGui . QColor ( 0 , 0 , 0 ) ) <NEWLINE> qp . setBrush ( QtGui . QColor ( 0 , 0 , 0 , 0 ) ) <NEWLINE> qp . drawRect ( xC , yC , xSize - 1 , ySize - 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if longFileNameWritePath is None : <NEWLINE> <INDENT> CreateWriteDirPathFromShortFilePath = True <NEWLINE> else : <NEWLINE> writepathdir , _ = os . path . split ( longFileNameWritePath ) <NEWLINE> if platform . system ( ) == <STRING> : <NEWLINE> <INDENT> if len ( writepathdir ) > 200 : <NEWLINE> <INDENT> writepathdir , _ = os . path . split ( writepath ) <NEWLINE> <DEDENT> <DEDENT> FileUtil . createPath ( writepathdir , False ) <NEWLINE> CreateWriteDirPathFromShortFilePath = not os . path . isdir ( writepath ) <NEWLINE> if CreateWriteDirPathFromShortFilePath : <NEWLINE> writepathdir , _ = os . path . split ( writepath ) <NEWLINE> FileUtil . createPath ( writepathdir , False ) <NEWLINE> <DEDENT>
def _childNodeROIIndicatorChanged ( self , val , AquireFileLock = True ) : <NEWLINE> <INDENT> if not self . _childNodeROIIndicatorChanged_called : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . _childNodeROIIndicatorChanged_called = True <NEWLINE> if self . isDelayedCacheLoadingSet ( ) : <NEWLINE> <INDENT> if not val : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> elif self . _ROIDatasetIndicatorCache != True : <NEWLINE> <INDENT> self . _ROIDatasetIndicatorCache = val <NEWLINE> tree = self . _tree <NEWLINE> if ( tree is not None ) : <NEWLINE> <INDENT> tree . _ProjectDataset . getProjectFileInterface ( ) . setProjectDatasetCache ( self . getTreePath ( ) , val , Key = <STRING> ) <NEWLINE> <DEDENT> self . callChildNodeROIIndicatorChanged ( val , AquireFileLock = AquireFileLock ) <NEWLINE> self . fireTreeNodeChangedEvent ( ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT> currentValue = self . getROIDatasetIndicator ( ) <NEWLINE> if val is not None and val : <NEWLINE> <INDENT> if val != currentValue : <NEWLINE> <INDENT> self . setROIDatasetIndicator ( val ) <NEWLINE> <DEDENT> <DEDENT> elif val != currentValue : <NEWLINE> <INDENT> found = False <NEWLINE> lst = self . getChildernLst ( ) <NEWLINE> if len ( lst ) > 0 : <NEWLINE> <INDENT> for child in lst : <NEWLINE> <INDENT> if ( not child . getROIDatasetIndicator ( ) ) : <NEWLINE> <INDENT> found = True <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> if found != currentValue or val is not None : <NEWLINE> <INDENT> self . setROIDatasetIndicator ( found ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> self . _childNodeROIIndicatorChanged_called = False <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def removeAllInternalTags ( self , SafeNameSet = None ) : <NEWLINE> <INDENT> if SafeNameSet is not None : <NEWLINE> <INDENT> if isinstance ( SafeNameSet , set ) : <NEWLINE> <INDENT> SafeNameSet = set ( SafeNameSet ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> SafeNameSet = set ( ) <NEWLINE> <DEDENT> self . _clearTagCache ( ) <NEWLINE> datasetLength = len ( self . _dataSetTags ) <NEWLINE> for index in range ( datasetLength - 1 , - 1 , - 1 ) : <NEWLINE> <INDENT> tag = self . _dataSetTags [ index ] <NEWLINE> if tag . isInternalTag ( ) : <NEWLINE> <INDENT> if tag . getName ( ) not in SafeNameSet : <NEWLINE> <INDENT> self . _recycleID ( tag . getID ( ) ) <NEWLINE> del self . _dataSetTags [ index ] <NEWLINE> del tag <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if datasetLength != len ( self . _dataSetTags ) : <NEWLINE> <INDENT> self . callParameterChangeListener ( ) <NEWLINE> <DEDENT> <DEDENT>
if ( rebuildList ) : <NEWLINE> <INDENT> lst . clear ( ) <NEWLINE> for name in namelist : <NEWLINE> <INDENT> item = QListWidgetItem ( ) <NEWLINE> item . setText ( name ) <NEWLINE> if ( name == <STRING> ) : <NEWLINE> <INDENT> color = QtGui . QColor ( 0 , 0 , 0 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> color = roiDefs . getROIColor ( name ) <NEWLINE> <DEDENT> item . setForeground ( color ) <NEWLINE> if ROIDictionary is not None and ROIDictionary . isROIDefined ( txt ) : <NEWLINE> <INDENT> item . setBackground ( QtGui . QColor ( 255 , 211 , 82 ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> item . setBackground ( QtGui . QColor ( 255 , 255 , 255 ) ) <NEWLINE> <DEDENT> item . setSelected ( name in selectedNameList ) <NEWLINE> lst . addItem ( item ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if ( listSelectionChangeListenerConnected ) : <NEWLINE> <DEDENT> self . ui . ROI_List . selectionModel ( ) . selectionChanged . connect ( self . ROIListSelectionChanged ) <NEWLINE> <DEDENT>
kModel = Custom_Objects [ <STRING> ] ( model_path , LoadLinearModel = True ) <NEWLINE>
def get_ready_buffers ( self ) : <NEWLINE> <INDENT> ready = { } <NEWLINE> current_time = time ( ) <NEWLINE> with self . lock : <NEWLINE> <INDENT> for ts , delayed_to in list ( self . delays . items ( ) ) : <NEWLINE> <INDENT> if delayed_to > current_time : <NEWLINE> <INDENT> del self . delays [ ts ] <NEWLINE> ready [ ts ] = self . buffers . pop ( ts ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return ready <NEWLINE> <DEDENT>
assert result not in tree <NEWLINE> <INDENT> assert is_left_subtree_less_than_right_subtree ( result ) <NEWLINE> <DEDENT>
assert len ( tree ) > 0 <NEWLINE> <INDENT> assert to_height ( tree ) > 0 <NEWLINE> assert is_left_subtree_less_than_right_subtree ( tree ) <NEWLINE> <DEDENT>
counter . subtract ( { tok : counter [ tok ] for tok in [ <STRING> ] + specials } ) <NEWLINE> <INDENT> max_size = None if max_size is None else max_size - len ( self . itos ) <NEWLINE> <DEDENT>
def set_vectors ( self , stoi , vectors , dim , unk_init = torch . Tensor . zero_ ) : <NEWLINE> <INDENT> self . vectors = torch . Tensor ( len ( self ) , dim ) <NEWLINE> for i , token in enumerate ( self . itos ) : <NEWLINE> <INDENT> wv_index = stoi . get ( token , None ) <NEWLINE> if wv_index is None : <NEWLINE> <INDENT> self . vectors [ i ] = vectors [ wv_index ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . vectors [ i ] = unk_init ( self . vectors [ i ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for partition in self . _client . get_partition_ids_for_topic ( arg ) : <NEWLINE> <INDENT> self . _consume_topic_partition ( topic , partition ) <NEWLINE> <DEDENT>
def get_nat_type ( s , source_ip , source_port , stun_host = None , stun_port = 3478 ) : <NEWLINE> <INDENT> _initialize ( ) <NEWLINE> port = stun_port <NEWLINE> log . debug ( <STRING> ) <NEWLINE> resp = False <NEWLINE> if stun_host : <NEWLINE> <INDENT> ret = stun_test ( s , stun_host , port , source_ip , source_port ) <NEWLINE> resp = ret [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for stun_host in stun_servers_list : <NEWLINE> <INDENT> log . debug ( <STRING> , stun_host ) <NEWLINE> ret = stun_test ( s , stun_host , port , source_ip , source_port ) <NEWLINE> resp = ret [ <STRING> ] <NEWLINE> if resp : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if not resp : <NEWLINE> <INDENT> return Blocked , ret <NEWLINE> <DEDENT> log . debug ( <STRING> , ret ) <NEWLINE> exIP = ret [ <STRING> ] <NEWLINE> exPort = ret [ <STRING> ] <NEWLINE> changedIP = ret [ <STRING> ] <NEWLINE> changedPort = ret [ <STRING> ] <NEWLINE> if ret [ <STRING> ] == source_ip : <NEWLINE> <INDENT> changeRequest = <STRING> . join ( [ ChangeRequest , <STRING> , <STRING> ] ) <NEWLINE> ret = stun_test ( s , stun_host , port , source_ip , source_port , <NEWLINE> <INDENT> changeRequest ) <NEWLINE> <DEDENT> if ret [ <STRING> ] : <NEWLINE> <INDENT> typ = OpenInternet <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> typ = SymmetricUDPFirewall <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> changeRequest = <STRING> . join ( [ ChangeRequest , <STRING> , <STRING> ] ) <NEWLINE> log . debug ( <STRING> ) <NEWLINE> ret = stun_test ( s , stun_host , port , source_ip , source_port , <NEWLINE> <INDENT> changeRequest ) <NEWLINE> <DEDENT> log . debug ( <STRING> , ret ) <NEWLINE> if ret [ <STRING> ] : <NEWLINE> <INDENT> typ = FullCone <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . debug ( <STRING> ) <NEWLINE> ret = stun_test ( s , changedIP , changedPort , source_ip , source_port ) <NEWLINE> log . debug ( <STRING> , ret ) <NEWLINE> if not ret [ <STRING> ] : <NEWLINE> <INDENT> typ = ChangedAddressError <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if exIP == ret [ <STRING> ] and exPort == ret [ <STRING> ] : <NEWLINE> <INDENT> changePortRequest = <STRING> . join ( [ ChangeRequest , <STRING> , <NEWLINE> <INDENT> <STRING> ] ) <NEWLINE> <DEDENT> log . debug ( <STRING> ) <NEWLINE> ret = stun_test ( s , changedIP , port , source_ip , source_port , <NEWLINE> <INDENT> changePortRequest ) <NEWLINE> <DEDENT> log . debug ( <STRING> , ret ) <NEWLINE> if ret [ <STRING> ] : <NEWLINE> <INDENT> typ = RestricNAT <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> typ = RestricPortNAT <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> typ = SymmetricNAT <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return typ , ret <NEWLINE> <DEDENT>
if __name__ == <STRING> : <NEWLINE> <INDENT> if QtCore . QCoreApplication . instance ( ) is not None : <NEWLINE> <INDENT> app = QtGui . QApplication ( [ ] ) <NEWLINE> <DEDENT> test_all ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>
class Frame ( Environment ) : <NEWLINE> <INDENT> def __init__ ( self , title , subtitle = <STRING> , ncols = 0 ) : <NEWLINE> <INDENT> super ( ) . __init__ ( ) <NEWLINE> self . content_separator = <STRING> <NEWLINE> self . append ( <NEWLINE> <INDENT> pl . NoEscape ( <STRING> ) + <NEWLINE> pl . escape_latex ( title ) + <NEWLINE> pl . NoEscape ( <STRING> ) <NEWLINE> <DEDENT> ) <NEWLINE> if subtitle : <NEWLINE> <INDENT> self . append ( <NEWLINE> <INDENT> pl . NoEscape ( <STRING> ) + <NEWLINE> pl . escape_latex ( title ) + <NEWLINE> pl . NoEscape ( <STRING> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> if ncols : <NEWLINE> <INDENT> self . add_columns ( ncols = ncols ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_running_plants ( self , plants ) : <NEWLINE> <INDENT> for plant in plants : <NEWLINE> <INDENT> if plant . construction_year <= 1990 and plant . name == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> plant . construction_year = randint ( self . year_number - 15 , self . year_number ) <NEWLINE> yield plant <NEWLINE> <DEDENT> elif plant . construction_year + plant . operating_period + plant . construction_period + plant . pre_dev_period >= self . year_number : <NEWLINE> <INDENT> yield plant <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . info ( <STRING> . format ( plant . name , <NEWLINE> <INDENT> plant . construction_year ) ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def invest ( self ) : <NEWLINE> <INDENT> lowest_upfront_cost = 0 <NEWLINE> total_upfront_cost = 0 <NEWLINE> counter = 0 <NEWLINE> total_capacity = 0 <NEWLINE> while self . money > lowest_upfront_cost and total_capacity < 1500 : <NEWLINE> <INDENT> counter += 1 <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> potential_plant_data = get_most_profitable_plants_by_npv ( self . model , self . difference_in_discount_rate , <NEWLINE> <INDENT> self . look_back_period ) <NEWLINE> <DEDENT> if counter == 1 : <NEWLINE> <INDENT> potential_plant_list = [ ] <NEWLINE> for plant_data in potential_plant_data : <NEWLINE> <INDENT> if not potential_plant_data : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> power_plant_trial = create_power_plant ( <STRING> , self . model . year_number , plant_data [ 1 ] , plant_data [ 0 ] ) <NEWLINE> potential_plant_list . append ( power_plant_trial ) <NEWLINE> if potential_plant_list : <NEWLINE> <INDENT> lowest_upfront_cost = min ( plant . get_upfront_costs ( ) * upfront_investment_costs for plant in potential_plant_list ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for plant_data in potential_plant_data : <NEWLINE> <COMMENT> <NL> <INDENT> if not potential_plant_data : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> power_plant_trial = create_power_plant ( <STRING> , self . model . year_number , plant_data [ 1 ] , plant_data [ 0 ] ) <NEWLINE> total_upfront_cost = power_plant_trial . get_upfront_costs ( ) * upfront_investment_costs <NEWLINE> <COMMENT> <NL> if self . money > total_upfront_cost : <NEWLINE> <INDENT> logger . info ( <STRING> . format ( power_plant_trial . plant_type , self . money , total_upfront_cost ) ) <NEWLINE> self . plants . append ( power_plant_trial ) <NEWLINE> self . money -= total_upfront_cost <NEWLINE> total_capacity += power_plant_trial . capacity_mw <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> kp . verify ( input , sig ) <NEWLINE> return True <NEWLINE> except ed25519 . BadSignatureError : <NEWLINE> raise ErrInvalidSignature ( ) <NEWLINE> <DEDENT>
user = nkeys . from_seed ( seed ) <NEWLINE> <INDENT> if user . verify ( signed_data , data ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sys . exit ( 0 ) <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def auto ( cls , syslog = None , stderr = None , level = None , extended = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> level = norm_level ( level ) <NEWLINE> if syslog is None and stderr is None : <NEWLINE> <INDENT> if sys . stderr . isatty ( ) or syslog_path ( ) is None : <NEWLINE> <INDENT> log . info ( <STRING> ) <NEWLINE> syslog , stderr = None , ( level or logging . INFO ) <NEWLINE> if extended is None : <NEWLINE> <INDENT> extended = ( level or 0 ) <= logging . DEBUG <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> log . info ( <STRING> ) <NEWLINE> syslog , stderr = ( level or logging . WARNING ) , None <NEWLINE> <DEDENT> <DEDENT> return cls ( syslog = syslog , stderr = stderr , extended = extended ) <NEWLINE> <DEDENT> <DEDENT>
tifffile . imsave ( output_data [ 0 , : , : , : ] , full_fname , compress = 1 ) <NEWLINE>
if tag != VERSION : <NEWLINE> <INDENT> info = <STRING> . format ( <NEWLINE> <INDENT> tag , VERSION <NEWLINE> <DEDENT> ) <NEWLINE> sys . exit ( info ) <NEWLINE> <DEDENT>
sm = site . getSiteManager ( ) <NEWLINE> <INDENT> if not sm . queryUtility ( interfaces . ICalendarSupport ) : <NEWLINE> <INDENT> sm . registerUtility ( interfaces . ICalendarSupport , <NEWLINE> <INDENT> content . CalendarSupport ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
sm = site . getSiteManager ( ) <NEWLINE> <INDENT> if not sm . queryUtility ( interfaces . ICalendarSupport ) : <NEWLINE> <INDENT> sm . registerUtility ( interfaces . ICalendarSupport , <NEWLINE> <INDENT> content . CalendarSupport ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for each in recurrence . getOccurrenceDays ( ) : <NEWLINE> <INDENT> if start is not None and each < startdate : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if stop is not None and each > stopdate : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> dt = datetime . date . fromordinal ( each ) <NEWLINE> res . append ( BrainEvent ( self . context , dt ) ) <NEWLINE> <DEDENT>
available_operations = { <NEWLINE> <INDENT> <STRING> : ( 1 , lambda x , y : x + y ) , <NEWLINE> <STRING> : ( 1 , lambda x , y : x - y ) , <NEWLINE> <STRING> : ( 2 , lambda x , y : x * y ) , <NEWLINE> <STRING> : ( 2 , lambda x , y : x // y ) , <NEWLINE> <STRING> : ( 3 , lambda x , y : x ** y ) , <NEWLINE> <STRING> : ( 2 , lambda x , y : x % y ) , <NEWLINE> <STRING> : ( 3 , lambda x , y : x ** y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x < y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x > y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x <= y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x >= y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x >= y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x >= y ) , <NEWLINE> <STRING> : ( 2 , lambda x , y : x / y ) , <NEWLINE> } <NEWLINE> <DEDENT>
if expr . operator . type_ == TokenTypes . MINUS : <NEWLINE> <INDENT> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return float ( left ) - float ( right ) <NEWLINE> elif expr . operator . type_ == TokenTypes . SLASH : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return float ( left ) - float ( right ) <NEWLINE> elif expr . operator . type_ == TokenTypes . STAR : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return float ( left ) * float ( right ) <NEWLINE> elif expr . operator . type_ == TokenTypes . PLUS : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left + right <NEWLINE> elif expr . operator . type_ == TokenTypes . CAP or expr . operator . type_ == TokenTypes . STAR_STAR : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left ** right <NEWLINE> elif expr . operator . type_ == TokenTypes . SLASH_SLASH : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left // right <NEWLINE> elif expr . operator . type_ == TokenTypes . PERCENTS : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left % right <NEWLINE> elif expr . operator . type_ == TokenTypes . GREATER : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left > right <NEWLINE> elif expr . operator . type_ == TokenTypes . GREATER_EQUAL : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left >= right <NEWLINE> elif expr . operator . type_ == TokenTypes . LESS : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left < right <NEWLINE> elif expr . operator . type_ == TokenTypes . LESS_EQUAL : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left <= right <NEWLINE> elif expr . operator . type_ == TokenTypes . BANG_EQUAL : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return not self . is_equal ( left , right ) <NEWLINE> elif expr . operator . type_ == TokenTypes . EQUAL_EQUAL : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return self . is_equal ( left , right ) <NEWLINE> <DEDENT>
elif data . message_type == GCMMessageType . RECEIPT : <NEWLINE> <INDENT> logging . debug ( <STRING> % msg . message_id ) <NEWLINE> self . event ( XMPPEvent . RECEIPT , data ) <NEWLINE> <DEDENT>
self . groupRemoved . emit ( uuid , group ) <NEWLINE>
tfi . saved_model . export ( <STRING> , Math ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL>
def test_dictrecursion ( self ) : <NEWLINE> <INDENT> x = { } <NEWLINE> x [ <STRING> ] = x <NEWLINE> try : <NEWLINE> <INDENT> json . dumps ( x ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . fail ( <STRING> ) <NEWLINE> <DEDENT> x = { } <NEWLINE> y = { <STRING> : x , <STRING> : x } <NEWLINE> <COMMENT> <NL> json . dumps ( x ) <NEWLINE> <DEDENT>
def __call__ ( self , text ) : <NEWLINE> <INDENT> atoms = self . toatoms ( text ) <NEWLINE> j = 0 <NEWLINE> while j < len ( atoms ) : <NEWLINE> <INDENT> i = j <NEWLINE> chunksize = 0 <NEWLINE> while j < len ( atoms ) and chunksize + len ( atoms [ j ] ) < self . buffersize : <NEWLINE> <INDENT> chunksize += len ( atoms [ j ] ) <NEWLINE> j += 1 <NEWLINE> <DEDENT> self . _juststuff ( <STRING> . join ( atoms [ i : j ] ) ) <NEWLINE> <DEDENT> <DEDENT>
def test_http_304_res ( self ) : <NEWLINE> <INDENT> res = self . testapp . get ( <STRING> ) <NEWLINE> self . assertEqual ( <STRING> , res . status ) <NEWLINE> etag = res . headers [ <STRING> ] <NEWLINE> res2 = self . testapp . get ( <STRING> , headers = { <STRING> : etag } ) <NEWLINE> self . assertEqual ( <STRING> , res . status ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> pList = Q2Dict ( protoFields , pRows ) <NEWLINE> <DEDENT>
Qtmp = getQbeStmt ( fieldName , sType , sCondicion ) <NEWLINE> <INDENT> if bAndConector : <NEWLINE> <INDENT> QResult = QResult & Qtmp <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> QResult = QResult | Qtmp <NEWLINE> <DEDENT> <DEDENT>
@ need_permissions ( <STRING> ) <NEWLINE> <INDENT> @ pass_record <NEWLINE> def post ( self , pid , record , ** kwargs ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> days_ago = circulation_overdue_loan_days ( record ) <NEWLINE> is_overdue = days_ago > 0 <NEWLINE> if is_overdue : <NEWLINE> <INDENT> raise OverdueLoansMailError ( description = <STRING> ) <NEWLINE> <DEDENT> send_loan_overdue_reminder_mail ( record , days_ago ) <NEWLINE> return self . make_response ( <NEWLINE> <INDENT> pid , record , 202 , links_factory = self . links_factory <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
def activate ( self , leaf ) : <NEWLINE> <INDENT> url = to_correios_url ( leaf ) <NEWLINE> with request . urlopen ( url ) as curreio : <NEWLINE> <INDENT> content = curreio . read ( ) <NEWLINE> info = get_tracking_info ( content . decode ( <STRING> ) ) <NEWLINE> if info : <NEWLINE> <INDENT> txt = <STRING> . join ( reversed ( info [ 0 ] ) ) <NEWLINE> return TextLeaf ( txt , leaf . object ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
with logger : <NEWLINE> <INDENT> for file , timers in timers_info : <NEWLINE> <INDENT> logger . debug ( <STRING> , file , timers ) <NEWLINE> <DEDENT> logger . info ( <STRING> , len ( files ) , timers_total ) <NEWLINE> <DEDENT>
for key , value in list ( data . items ( ) ) : <NEWLINE> <INDENT> if isinstance ( value , str ) : <NEWLINE> <INDENT> data [ key ] = value . encode ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if commit_only : <NEWLINE> <INDENT> include = [ <STRING> + f for f in git . staged ( ) if f . endswith ( <STRING> ) ] <NEWLINE> exclude += git . ignore ( ) <NEWLINE> <DEDENT>
common . git_checkout ( develop ) <NEWLINE>
for time in t_indexes : <NEWLINE> <INDENT> if time >= size_t : <NEWLINE> <INDENT> log ( <STRING> <NEWLINE> <INDENT> <STRING> % ( time + 1 , size_t ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if pro_start != pro_end : <NEWLINE> <INDENT> rendered_img = re . renderProjectedCompressed ( <NEWLINE> <INDENT> algorithm , time , stepping , pro_start , pro_end ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> plane_def = omero . romio . PlaneDef ( ) <NEWLINE> plane_def . z = pro_start <NEWLINE> plane_def . t = time <NEWLINE> plane_def = re . renderCompressed ( plane_def ) <NEWLINE> <COMMENT> <NL> <DEDENT> image = Image . open ( io . BytesIO ( rendered_img ) ) <NEWLINE> resized_image = imgUtil . resizeImage ( image , width , height ) <NEWLINE> rendered_images . append ( resized_image ) <NEWLINE> <DEDENT> <DEDENT>
def test_adapter_04 ( tmp_path ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> assert not any ( SimpleAdapter . scan_path ( str ( tmp_path ) ) ) <NEWLINE> <COMMENT> <NL> assert not any ( SimpleAdapter . scan_path ( str ( tmp_path / <STRING> ) ) ) <NEWLINE> <COMMENT> <NL> file1 = ( tmp_path / <STRING> ) <NEWLINE> file1 . touch ( ) <NEWLINE> found = tuple ( SimpleAdapter . scan_path ( str ( tmp_path ) ) ) <NEWLINE> assert len ( found ) == 1 <NEWLINE> assert str ( file1 ) in found <NEWLINE> <COMMENT> <NL> assert len ( tuple ( SimpleAdapter . scan_path ( str ( tmp_path ) ) ) ) == 1 <NEWLINE> <COMMENT> <NL> ( tmp_path / <STRING> ) . touch ( ) <NEWLINE> nested = ( tmp_path / <STRING> ) <NEWLINE> nested . mkdir ( ) <NEWLINE> file2 = ( nested / <STRING> ) <NEWLINE> file2 . touch ( ) <NEWLINE> assert len ( tuple ( SimpleAdapter . scan_path ( str ( tmp_path ) ) ) ) == 1 <NEWLINE> <COMMENT> <NL> found = tuple ( SimpleAdapter . scan_path ( str ( tmp_path ) , recursive = True ) ) <NEWLINE> assert len ( found ) == 2 <NEWLINE> assert str ( file1 ) in found <NEWLINE> assert str ( file2 ) in found <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> return ct , artifact <NEWLINE> <DEDENT>
changes . extend ( chs ) <NEWLINE> <INDENT> fs . extend ( fs ) <NEWLINE> <DEDENT>
def percentiles ( self , start , percentiles = [ 10 , 25 , 75 , 90 ] ) : <NEWLINE> <INDENT> end = start + timedelta ( days = 7 ) <NEWLINE> ts_start , ts_end = utils . timestamp_from_datetime ( [ start , end ] ) <NEWLINE> this_week_indices = where ( ( self . _data [ <STRING> ] < ts_end ) & ( self . _data [ <STRING> ] >= ts_start ) ) <NEWLINE> this_week = self . _data [ this_week_indices ] <NEWLINE> result = { } <NEWLINE> pred = self . baseline_model . prediction ( this_week ) <NEWLINE> for p in percentiles : <NEWLINE> <INDENT> result [ p ] = pred + self . baseline_model . percentile_in_place ( p , this_week ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
for ref in range ( 1 , self . numberOfRef + 1 ) : <NEWLINE> <INDENT> refVol = self . _getFileName ( <STRING> , iter = iter , ref = ref ) <COMMENT> <NEWLINE> iterVol = self . _getFileName ( <STRING> , iter = iter , ref = ref ) <COMMENT> <NEWLINE> if iter != 1 : <NEWLINE> <INDENT> copyFile ( volFn , iterVol ) <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . _splitParFile ( iter , ref , cpusRef [ ref - 1 ] ) <NEWLINE> prevIterVol = self . _getFileName ( <STRING> , iter = prevIter , ref = ref ) <COMMENT> <NEWLINE> copyFile ( prevIterVol , refVol ) <COMMENT> <NEWLINE> <DEDENT> copyFile ( refVol , iterVol ) <COMMENT> <NEWLINE> <DEDENT>
self . _defineSourceRelation ( vol , self . inputClasses ) <NEWLINE>
def _processMovie ( self , movieId , movieName , movieFolder ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> inMovieName = os . path . join ( movieFolder , movieName ) <NEWLINE> if movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = inMovieName <NEWLINE> <DEDENT> elif movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> createLink ( inMovieName , movieNameAux ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> micFnMrc = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> ImageHandler ( ) . convert ( inMovieName , micFnMrc , DT_FLOAT ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if _showVol is not None : <NEWLINE> <COMMENT> <NL> <INDENT> showVolFileName = os . path . abspath ( <NEWLINE> <INDENT> ImageHandler . removeFileType ( _showVol . getFileName ( ) ) ) <NEWLINE> <DEDENT> f . write ( <STRING> % showVolFileName ) <NEWLINE> if _showVol . hasOrigin ( ) : <NEWLINE> <INDENT> x , y , z = _showVol . getOrigin ( ) . getShifts ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> x , y , z = outputVol . getOrigin ( force = True ) . getShifts ( ) <NEWLINE> <DEDENT> <DEDENT>
label_seq_id = str ( residue_number ) <NEWLINE>
@ classmethod <NEWLINE> <INDENT> def createEmptyImage ( cls , fnOut , xDim = 1 , yDim = 1 , zDim = 1 , nDim = 1 , <NEWLINE> <INDENT> dataType = None ) : <NEWLINE> dt = dataType or cls . DT_FLOAT <NEWLINE> xmippLib . createEmptyFile ( fnOut , xDim , yDim , zDim , nDim , dataType ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> int_all_ranks_filt = stats . filter_min_observed ( intensity_all_ranks , threshold , samp_grps ) <NEWLINE> int_all_ranks_filt [ <STRING> ] = intensity_all_ranks . index <NEWLINE> <DEDENT>
if url is None and not fail_silently : <NEWLINE> <INDENT> raise KeyDoesNotExist ( <STRING> % url ) <NEWLINE> <DEDENT>
if sort_order is not None : <NEWLINE> <INDENT> params [ <STRING> ] = <STRING> . format ( sort_by , sort_order ) <NEWLINE> <DEDENT>
number_dict = { <NEWLINE> <INDENT> <STRING> : True , <NEWLINE> <STRING> : True <NEWLINE> } <NEWLINE> phone_number = context [ <STRING> ] [ <STRING> ] <NEWLINE> if phone_number is None : <NEWLINE> phone_number = <STRING> <NEWLINE> else : <NEWLINE> phone_number = str ( phone_number ) <NEWLINE> <DEDENT>
iHTML += self . _Output [ <STRING> ] . to_html ( formatters = [ _QS_formatPandasPercentage ] * 5 ) <NEWLINE>
iDataLen = DataLen . iloc [ i ] <NEWLINE>
histogram = histogram_image . reduceRegion ( ee . Reducer . histogram ( 255 , 2 ) . combine ( <STRING> , None , True ) . combine ( <STRING> , None , True ) , sampleRegion , reductionScale , bestEffort = True ) <NEWLINE>
x_coord_range = torch . linspace ( - r , r , steps = self . width ) <NEWLINE> <INDENT> y_coord_range = torch . linspace ( - r , r , steps = self . height ) <NEWLINE> x , y = torch . meshgrid ( x_coord_range , y_coord_range ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> gdal . UseExceptions ( ) <NEWLINE> gdal . AllRegister ( ) <NEWLINE> np . seterr ( divide = <STRING> , invalid = <STRING> ) <NEWLINE> SWIR1_path = gdal . Open ( os . path . join ( landsat_dir , swir1 [ 0 ] ) ) <NEWLINE> swir1_band = SWIR1_path . GetRasterBand ( 1 ) . ReadAsArray ( ) . astype ( np . float32 ) <NEWLINE> TIR_path = gdal . Open ( os . path . join ( landsat_dir , tir [ 0 ] ) ) <NEWLINE> tir_band = TIR_path . GetRasterBand ( 1 ) . ReadAsArray ( ) . astype ( np . float32 ) <NEWLINE> snap = gdal . Open ( os . path . join ( landsat_dir , swir1 [ 0 ] ) ) <NEWLINE> <DEDENT>
num_arcs = auggraph . num_arcs ( ) <NEWLINE> <INDENT> changed = True <NEWLINE> d = 1 <NEWLINE> print ( <STRING> , end = <STRING> , flush = True ) <NEWLINE> while changed and d <= project . radius : <NEWLINE> <INDENT> if d in augs : <NEWLINE> <INDENT> print ( <STRING> . format ( d ) , end = <STRING> , flush = True ) <NEWLINE> with open ( augname . format ( d ) , <STRING> ) as f : <NEWLINE> <INDENT> auggraph . add_arcs ( EdgeSet . from_ext ( f , self . id_map ) , d + 1 ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> print ( d , end = <STRING> , flush = True ) <NEWLINE> dtf_step ( auggraph , d + 1 ) <NEWLINE> with open ( augname . format ( d ) , <STRING> ) as f : <NEWLINE> <INDENT> EdgeSet ( auggraph . arcs ( weight = d + 1 ) ) . write_ext ( f , self . id_map ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
query_mh = query_sig . minhash <NEWLINE> <INDENT> query_mh = query_mh . downsample_max_hash ( frontier_mh ) <NEWLINE> frontier_mh = query_mh . downsample_max_hash ( query_mh ) <NEWLINE> <DEDENT>
if mh_size > 1 : <NEWLINE> <INDENT> n_merged += 1 <NEWLINE> merge_mh . merge ( mh ) <NEWLINE> <DEDENT>
if 1 : <NEWLINE> <INDENT> terminal = set ( ) <NEWLINE> for subnode in dag [ top_node_id ] : <NEWLINE> <INDENT> mh = load_minhash ( node_id , minhash_db ) <NEWLINE> if mh : <NEWLINE> <INDENT> terminal . update ( find_terminal_nodes ( subnode , args . maxsize ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> d = defaultdict ( list ) <NEWLINE> for style , rules in iterate ( s1 ) : <NEWLINE> <INDENT> another_rules = s2 . get ( style ) <NEWLINE> if another_rules is None : <NEWLINE> <INDENT> d [ style ] . extend ( addall ( rules ) ) <NEWLINE> continue <NEWLINE> <DEDENT> for name , value in iterate ( rules ) : <NEWLINE> <INDENT> another_value = another_rules . get ( name ) <NEWLINE> if another_value is None : <NEWLINE> <INDENT> d [ style ] . append ( add ( name , value ) ) <NEWLINE> <DEDENT> elif value != another_value : <NEWLINE> <INDENT> d [ style ] . append ( change ( name , value , another_value ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return d <NEWLINE> <DEDENT>
class Collection ( WrenCollection ) : <NEWLINE> <INDENT> def handle_error ( self , response ) : <NEWLINE> <INDENT> import requests <NEWLINE> if response . status_code == requests . codes . not_found and response . json ( ) == { <STRING> : <STRING> } : <NEWLINE> <INDENT> raise NotFound ( response . text ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> super ( self , Collection ) . handle_error ( response ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
self . spawn_timer = Timer ( 20 , self . _spawn_heartbeat , <NEWLINE> <INDENT> single_shot = False ) <NEWLINE> self . spawn_timer . start ( ) <NEWLINE> <DEDENT>
def setup_order_workers ( self ) : <NEWLINE> <INDENT> self . _cxt . orders = FieldSet ( ) <NEWLINE> venue_names = self . _cfg . get ( <STRING> , { } ) . keys ( ) <NEWLINE> instr_names = self . _cfg . get ( <STRING> , { } ) <NEWLINE> self . logger . info ( <STRING> ) <NEWLINE> venue_instruments = 0 <NEWLINE> for k in venue_names : <NEWLINE> <INDENT> self . _cxt . orders [ k ] = { } <NEWLINE> venue = self . _venues [ k ] <NEWLINE> instr_defs = venue . get_instrument_defs ( instr_names ) <NEWLINE> for instr in instr_names : <NEWLINE> <INDENT> if not ( instr in instr_defs ) : continue <NEWLINE> self . logger . info ( <STRING> ) <NEWLINE> instrument = CosineInstrument . load ( self . instr_cache , ** instr_defs [ instr ] ) <NEWLINE> self . _cxt . instruments [ instrument . name ] = instrument <NEWLINE> order_worker = CosineOrderWorker ( self . _cfg . orders . ActiveDepth , instrument , venue , logger = self . logger ) <NEWLINE> self . _cxt . orders [ k ] [ instr . symbol ] = order_worker <NEWLINE> venue_instruments += 1 <NEWLINE> <DEDENT> <DEDENT> if venue_instruments == 0 : <NEWLINE> <INDENT> raise LookupError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def CTSget ( source , targets , identifiers , top_only = True , timeout = 60 , server = <STRING> ) : <NEWLINE> <INDENT> result = { } <NEWLINE> if type ( targets ) is str : <NEWLINE> <INDENT> result [ targets ] = CTS_translate_multi ( source , targets , identifiers , top_only , timeout , server ) <NEWLINE> <DEDENT> elif type ( identifiers ) is list : <NEWLINE> <INDENT> for i in range ( len ( targets ) ) : <NEWLINE> <INDENT> target = targets [ i ] <NEWLINE> print ( <STRING> + source + <STRING> + target ) <NEWLINE> result [ target ] = CTS_translate_multi ( source , target , identifiers , top_only , timeout , server ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
self . model = model <NEWLINE> <INDENT> self . nb_actions = nb_actions <NEWLINE> self . policy = policy <NEWLINE> self . test_policy = policy <NEWLINE> self . gamma = gamma <NEWLINE> self . nb_steps_warmup = nb_steps_warmup <NEWLINE> self . train_interval = train_interval <NEWLINE> <DEDENT>
def _type_variants ( self ) : <NEWLINE> <INDENT> self . out_json [ self . sample ] [ <STRING> ] = { } <NEWLINE> gt = VariantTyper ( <NEWLINE> <INDENT> expected_depths = self . expected_depths , <NEWLINE> error_rate = self . expected_error_rate , <NEWLINE> contamination_depths = self . contamination_depths , <NEWLINE> ignore_filtered = self . ignore_filtered , <NEWLINE> minor_freq = self . minor_freq , <NEWLINE> confidence_threshold = self . variant_confidence_threshold ) <NEWLINE> <DEDENT> genotypes = [ ] <NEWLINE> filters = [ ] <NEWLINE> for probe_name , probe_coverages in self . variant_covgs . items ( ) : <NEWLINE> <INDENT> probe_id = self . _name_to_id ( probe_name ) <NEWLINE> variant = None <NEWLINE> call = gt . type ( probe_coverages , variant = variant ) <NEWLINE> genotypes . append ( sum ( call [ <STRING> ] ) ) <NEWLINE> filters . append ( int ( call [ <STRING> ] [ <STRING> ] == <STRING> ) ) <NEWLINE> if sum ( call [ <STRING> ] ) > 0 or not call [ <NEWLINE> <INDENT> <STRING> ] or self . report_all_calls : <NEWLINE> self . variant_calls [ probe_name ] = call <NEWLINE> self . variant_calls_dict [ <NEWLINE> probe_id ] = call <NEWLINE> <DEDENT> <DEDENT> self . out_json [ self . sample ] [ <STRING> ] = genotypes <NEWLINE> self . out_json [ self . sample ] [ <STRING> ] = filters <NEWLINE> self . out_json [ self . sample ] [ <STRING> ] = self . variant_calls_dict <NEWLINE> <DEDENT>
def run ( self ) : <NEWLINE> <INDENT> self . create_folder ( ) <COMMENT> <NEWLINE> <COMMENT> <NL> answers = prompt ( questions , style = style ) <NEWLINE> self . answer = answers <NEWLINE> self . create_virtualenv ( ) <NEWLINE> self . main_structure ( ) <NEWLINE> self . write_file ( self . app , <STRING> , app_init ) <NEWLINE> self . write_file ( self . utils_path , <STRING> , utils_init ) <NEWLINE> self . write_file ( self . utils_path , <STRING> , logger ) <NEWLINE> self . write_file ( self . utils_path , <STRING> , response ) <NEWLINE> self . write_file ( self . instance , <STRING> , config ) <NEWLINE> self . make_env ( ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , requirement_list ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , docker_compose ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , Dockerfile ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , readme ) <NEWLINE> if answers . get ( <STRING> ) == <STRING> and not answers : <NEWLINE> <INDENT> self . write_file ( self . api_path , <STRING> , producer_restful ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , run_restful ) <NEWLINE> <DEDENT> elif answers . get ( <STRING> ) == <STRING> : <NEWLINE> <INDENT> ask_redis = prompt ( redis_questions , style = style ) <NEWLINE> self . write_file ( self . api_path , <STRING> , producer_redis ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , run_redis_pubsub ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , redis_client ) <NEWLINE> self . add_env ( redis_credentials . format ( ask_redis . get ( <STRING> , <STRING> ) ) ) <NEWLINE> self . update_file ( self . folder_name , <STRING> , redis_required ) <NEWLINE> <COMMENT> <NL> os . makedirs ( self . development ) <NEWLINE> self . write_file ( self . development , <STRING> , <NEWLINE> <INDENT> redis_dockerfile . format ( ask_redis . get ( <STRING> , <STRING> ) ) ) <NEWLINE> <DEDENT> self . write_file ( self . development , <STRING> , redis_docker_compose ) <NEWLINE> <DEDENT> elif answers . get ( <STRING> ) == <STRING> : <NEWLINE> <INDENT> self . write_file ( self . api_path , <STRING> , producer_rabbitmq ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , run_rabbitmq ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , rabbit_client ) <NEWLINE> self . add_env ( rabbitmq_credentials ) <NEWLINE> self . update_file ( self . folder_name , <STRING> , rabbit_required ) <NEWLINE> <COMMENT> <NL> os . makedirs ( self . development ) <NEWLINE> self . write_file ( self . development , <STRING> , rabbit_enable_plugins ) <NEWLINE> self . write_file ( self . development , <STRING> , rabbit_docker_compose ) <NEWLINE> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> new_node = etree . parse ( new_file ) . getroot ( ) <NEWLINE> except XMLSyntaxError as e : <NEWLINE> errorstore . add_error ( InvalidXML ( filename , e . args [ 0 ] ) ) <NEWLINE> return <NEWLINE> else : <NEWLINE> traverse_course ( edxobj , new_node , new_file , errorstore , pointer = True ) <NEWLINE> return <NEWLINE> <DEDENT>
def show_list ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _init_repo ( ) <NEWLINE> for filename in glob ( self . app . get_home_path ( ) + <STRING> ) : <NEWLINE> <INDENT> endpoint = Endpoint ( self . app , filename ) <NEWLINE> if not endpoint . is_visible ( ) : <NEWLINE> <INDENT> print ( endpoint . path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def container_style ( request ) : <NEWLINE> <INDENT> classname = <STRING> <NEWLINE> if getattr ( <STRING> , settings , False ) : <NEWLINE> <INDENT> classname += <STRING> <NEWLINE> <DEDENT> return { <STRING> : classname } <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> os . chdir ( path_out ) <NEWLINE> nii_files = glob . glob ( <STRING> ) <NEWLINE> for nii_file in nii_files : <NEWLINE> <COMMENT> <NL> <INDENT> for contrast in list ( contrast_dict . keys ( ) ) : <NEWLINE> <COMMENT> <NL> <INDENT> if contrast in nii_file : <NEWLINE> <INDENT> print ( <STRING> + nii_file + <STRING> + contrast ) <NEWLINE> <COMMENT> <NL> nii_file_all_exts = glob . glob ( nii_file . strip ( <STRING> ) + <STRING> ) <NEWLINE> for nii_file_all_ext in nii_file_all_exts : <NEWLINE> <COMMENT> <NL> <INDENT> fname_out = os . path . join ( subject , contrast_dict [ contrast ] [ 1 ] , <NEWLINE> <INDENT> subject + <STRING> + contrast_dict [ contrast ] [ 0 ] + <STRING> <NEWLINE> + nii_file . split ( os . extsep , 1 ) [ 1 ] ) <NEWLINE> <DEDENT> os . makedirs ( os . path . abspath ( os . path . dirname ( fname_out ) ) , exist_ok = True ) <NEWLINE> <COMMENT> <NL> shutil . move ( nii_file_all_ext , fname_out ) <NEWLINE> <DEDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def isstatic ( arguments ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if len ( arguments ) == 0 : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> elif not isinstance ( arguments [ 0 ] , var . SymbolObject ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return <STRING> != arguments [ 0 ] . name != <STRING> <NEWLINE> <DEDENT> <DEDENT>
if settings . DEBUG and getattr ( response_codec , <STRING> , False ) : <NEWLINE>
def write_sorted_file ( fpath , outdir = None , cfg = None ) : <NEWLINE> <INDENT> if outdir is not None : <NEWLINE> <INDENT> fbasename = os . path . splitext ( os . path . basename ( fpath ) ) [ 0 ] <NEWLINE> sorted_fpath = os . path . join ( outdir , <STRING> . format ( fpath ) ) <NEWLINE> tmp = unicode_writer ( open ( sorted_fpath , <STRING> ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> tmp = unicode_writer ( NamedTemporaryFile ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
def get_tracking_delta ( self ) : <NEWLINE> <INDENT> if len ( self . track_value_list ) > self . track_average_epoc_count : <NEWLINE> <INDENT> return sum ( <NEWLINE> <INDENT> [ self . track_value_list [ idx + 1 ] - <NEWLINE> <INDENT> self . track_value_list [ idx ] <NEWLINE> for idx in range ( len ( self . track_value_list ) - 1 ) ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> return 0 <NEWLINE> <DEDENT> <DEDENT>
for jrid , jrdef in sorted ( list ( runs ) ) : <NEWLINE> <INDENT> yield jrdef <NEWLINE> <DEDENT>
if len ( intensive_variables ) > 0 : <NEWLINE> <INDENT> profile = pd . DataFrame ( interpolation [ 1 ] , columns = extensive_variables ) <NEWLINE> profiles . append ( profile ) <NEWLINE> <DEDENT>
__implicit_features [ v ] = name <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> for r in requirements : <NEWLINE> <COMMENT> <NL> <INDENT> if r . condition ( ) : <NEWLINE> <INDENT> required [ r . feature ( ) ] = r <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
__module_flags . setdefault ( m , [ ] ) . append ( f ) <NEWLINE> <INDENT> __flags . setdefault ( rule_or_module , [ ] ) . append ( f ) <NEWLINE> <DEDENT>
@ numba . njit ( cache = True ) <NEWLINE> <INDENT> def _get_edges ( clustering1 : np . array , clustering2 : np . array ) : <NEWLINE> <INDENT> edges = [ ] <NEWLINE> offset1 = clustering1 . min ( ) <NEWLINE> offset2 = clustering1 . min ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> offset_clusts1 = clustering1 - offset1 <NEWLINE> offset_clusts2 = clustering2 - offset2 <NEWLINE> <COMMENT> <NL> nclusts1 = offset_clusts1 . max ( ) + 1 <NEWLINE> nclusts2 = offset_clusts2 . max ( ) + 1 <NEWLINE> coincidence = np . zeros ( ( nclusts1 , nclusts2 ) ) <NEWLINE> <COMMENT> <NL> ncells1 = np . zeros ( nclusts1 ) <NEWLINE> ncells2 = np . zeros ( nclusts2 ) <NEWLINE> <COMMENT> <NL> for cell in range ( len ( clustering1 ) ) : <NEWLINE> <INDENT> c1 = offset_clusts1 [ cell ] <NEWLINE> c2 = offset_clusts2 [ cell ] <NEWLINE> coincidence [ c1 , c2 ] += 1 <NEWLINE> ncells1 [ c1 ] += 1 <NEWLINE> ncells2 [ c2 ] += 1 <NEWLINE> <DEDENT> for cidx1 , cidx2 in np . ndindex ( coincidence . shape ) : <NEWLINE> <INDENT> isize = coincidence [ cidx1 , cidx2 ] <NEWLINE> if isize < 1 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> jaccard_sim = isize / ( ncells1 [ cidx1 ] + ncells2 [ cidx2 ] - isize ) <NEWLINE> edge = ( cidx1 + offset1 , cidx2 + offset2 , jaccard_sim ) <NEWLINE> edges . append ( edge ) <NEWLINE> <DEDENT> return edges <NEWLINE> <DEDENT> <DEDENT>
def _request ( self , resource , method , args = None , data = None , headers = None ) : <NEWLINE> <INDENT> response_data = None <NEWLINE> request_body = self . _serialize ( data ) <NEWLINE> response_headers , response_content = self . _connection . request ( resource , method , args = args , body = request_body , headers = headers , content_type = self . content_type ) <NEWLINE> if response_headers . get ( <STRING> ) == HTTP_STATUS_OK : <NEWLINE> <INDENT> data = self . _deserialize ( response_content ) <NEWLINE> <DEDENT> return Response ( response_headers , response_content , response_data ) <NEWLINE> <DEDENT>
def main ( ) : <NEWLINE> <INDENT> parser = argparse . ArgumentParser ( prog = <STRING> . format ( __package__ ) , <NEWLINE> <INDENT> description = <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> parser . add_argument ( <STRING> , nargs = <STRING> , metavar = <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> parser . add_argument ( <STRING> , action = <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> parser . add_argument ( <STRING> , <STRING> , action = <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> parser . add_argument ( <STRING> , action = <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> args = parser . parse_args ( ) <NEWLINE> <DEDENT>
args [ <STRING> ] = request <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if cache . get ( cache_key ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> parsed = re . sub ( <STRING> + item + <STRING> , cache . get ( item ) , parsed ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> module = import_parser ( <STRING> + name ) <NEWLINE> function = getattr ( module , <STRING> ) <NEWLINE> result = function ( args ) <NEWLINE> try : <NEWLINE> <INDENT> cache . set ( cache_key , result , 3600 ) <NEWLINE> parsed = re . sub ( <STRING> + re . escape ( item ) + <STRING> , result , parsed ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except ImportError : <NEWLINE> <INDENT> pass <NEWLINE> return parsed <NEWLINE> <DEDENT> <DEDENT>
async with self . _session . post ( <NEWLINE> <INDENT> self . url , <NEWLINE> params = { <STRING> : <STRING> , ** self . params } , <NEWLINE> data = query . encode ( ) , <NEWLINE> ) as response : <NEWLINE> if response . status != 200 : <NEWLINE> <INDENT> body = await response . read ( ) <NEWLINE> raise DBException . from_message ( <NEWLINE> <INDENT> statement , body . decode ( errors = <STRING> ) , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
if default != NoValue : <NEWLINE> <INDENT> if not_null or primary : <NEWLINE> <INDENT> raise ValueError ( <STRING> <STRING> ) <NEWLINE> <DEDENT> scolumn += <STRING> <NEWLINE> values . append ( default ) <NEWLINE> <DEDENT>
return atoms <NEWLINE>
<COMMENT> <NL> <INDENT> failure_states = set ( ) <NEWLINE> for s in filter ( lambda x : x not in z_current , dfa . states ) : <NEWLINE> <INDENT> state2level [ s ] = max_level <NEWLINE> failure_states . add ( s ) <NEWLINE> <DEDENT> <DEDENT>
return SteaResult ( client . calculate ( request ) , project ) <NEWLINE>
for j in range ( n_classes ) : <NEWLINE> <INDENT> print ( <STRING> . format ( conf_mat_t [ i ] [ j ] ) , end = <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT>
else : <NEWLINE> <INDENT> intcheck = ( numerics % 1 ) == 0 <NEWLINE> if np . sum ( intcheck ) / length > 0.9 : <NEWLINE> <INDENT> if <STRING> not in old_metadata [ <STRING> ] : <NEWLINE> <INDENT> old_metadata [ <STRING> ] += ( <STRING> , ) <NEWLINE> old_metadata [ <STRING> ] = type ( 10 ) <NEWLINE> inputs . iloc [ : , col ] = numerics <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if <STRING> not in old_metadata [ <STRING> ] : <NEWLINE> <INDENT> old_metadata [ <STRING> ] = ( <STRING> , ) <NEWLINE> old_metadata [ <STRING> ] = type ( 10.2 ) <NEWLINE> inputs . iloc [ : , col ] = numerics <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if <STRING> in rule [ 1 ] : <NEWLINE> <COMMENT> <NL> <INDENT> for pattern in shlex . split ( rule [ 1 ] [ <STRING> ] ) : <NEWLINE> <INDENT> if rule [ 1 ] [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> sql_tuple = FilterTree . text_similarity_filter ( rule [ 0 ] , pattern , True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sql_tuple = FilterTree . text_similarity_filter ( rule [ 0 ] , pattern , False ) <NEWLINE> <DEDENT> pattern_specs . append ( sql_tuple ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
class Path ( AbstractPath ) : <NEWLINE> <INDENT> abs = abspath = property ( lambda self : pth ( ospath . abspath ( self ) ) ) <NEWLINE> exists = property ( lambda self : ospath . exists ( self ) ) <NEWLINE> lexists = property ( lambda self : ospath . lexists ( self ) ) <NEWLINE> expanduser = property ( lambda self : pth ( ospath . expanduser ( self ) ) ) <NEWLINE> expandvars = property ( lambda self : pth ( ospath . expandvars ( self ) ) ) <NEWLINE> atime = property ( lambda self : ospath . getatime ( self ) ) <NEWLINE> ctime = property ( lambda self : ospath . getctime ( self ) ) <NEWLINE> mtime = property ( lambda self : ospath . getmtime ( self ) ) <NEWLINE> size = property ( lambda self : ospath . getsize ( self ) ) <NEWLINE> isdir = property ( lambda self : ospath . isdir ( self ) ) <NEWLINE> isfile = property ( lambda self : ospath . isfile ( self ) ) <NEWLINE> islink = property ( lambda self : ospath . islink ( self ) ) <NEWLINE> ismount = property ( lambda self : ospath . ismount ( self ) ) <NEWLINE> joinpath = pathjoin = __div__ = __floordiv__ = __truediv__ = lambda self , * args : pth ( ospath . join ( self , * args ) ) <NEWLINE> normcase = property ( lambda self : pth ( ospath . normcase ( self ) ) ) <NEWLINE> normpath = property ( lambda self : pth ( ospath . normpath ( self ) ) ) <NEWLINE> norm = property ( lambda self : pth ( ospath . normcase ( ospath . normpath ( self ) ) ) ) <NEWLINE> real = realpath = property ( lambda self : pth ( ospath . realpath ( self ) ) ) <NEWLINE> rel = relpath = lambda self , start : pth ( ospath . relpath ( self , start ) ) <NEWLINE> same = samefile = lambda self , other : ospath . samefile ( self , other ) <NEWLINE> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> if PY33 : <NEWLINE> <INDENT> link = lambda self , dest , follow_symlinks = True , ** kwargs : os . link ( self , dest , follow_symlinks = follow_symlinks , ** kwargs ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> link = lambda self , dest : os . link ( self , dest ) <NEWLINE> <DEDENT> <DEDENT> if PY33 : <NEWLINE> <INDENT> stat = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . stat ( self , ** kwargs ) ) ) <NEWLINE> lstat = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . lstat ( self , ** kwargs ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat = property ( lambda self : os . stat ( self ) ) <NEWLINE> lstat = property ( lambda self : os . lstat ( self ) ) <NEWLINE> <DEDENT> isreadable = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . access ( self , os . R_OK , ** kwargs ) ) ) <NEWLINE> mkdir = lambda self : os . mkdir ( self ) <NEWLINE> makedirs = lambda self : os . makedirs ( self ) <NEWLINE> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> pathconf = lambda self , name : os . pathconf ( self , name ) <NEWLINE> <DEDENT> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> readlink = property ( lambda self : os . readlink ( self ) ) <NEWLINE> <DEDENT> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> fsencode = fsencoded = property ( lambda self : os . fsencode ( self ) ) <NEWLINE> <DEDENT> access = lambda self , mode , ** kwargs : os . access ( self , mode , ** kwargs ) <NEWLINE> if PY33 : <NEWLINE> <INDENT> isreadable = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . access ( self , os . R_OK , ** kwargs ) ) ) <NEWLINE> iswritable = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . access ( self , os . W_OK , ** kwargs ) ) ) <NEWLINE> isexecutable = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . access ( self , os . R_OK | os . X_OK , ** kwargs ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> isreadable = property ( lambda self : os . access ( self , os . R_OK ) ) <NEWLINE> iswritable = property ( lambda self : os . access ( self , os . W_OK ) ) <NEWLINE> isexecutable = property ( lambda self : os . access ( self , os . R_OK | os . X_OK ) ) <NEWLINE> <DEDENT> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> chroot = lambda self : os . chroot ( self ) <NEWLINE> <DEDENT> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> chflags = lambda self , flags , follow_symlinks = True : os . chflags ( self , flags ) if follow_symlinks else os . lchflags ( self , flags ) <NEWLINE> lchflags = lambda self , flags : os . lchflags ( self , flags ) <NEWLINE> <DEDENT> <DEDENT>
( mode , ino , dev , nlink , uid , gid , size , atime , mtime , ctime ) = _stat ( _fd ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> return time . strftime ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> time . gmtime ( ctime ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
@ cli . command ( <STRING> , short_help = <STRING> ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , <NEWLINE> <INDENT> type = str , <NEWLINE> required = False , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , <NEWLINE> <INDENT> type = int , <NEWLINE> required = False , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT> @ pass_context <NEWLINE> def remove ( ctx , name = None , index = None ) : <NEWLINE> <INDENT> connectionsCsvLocation = resource_filename ( Requirement . parse ( <STRING> ) , <STRING> ) <NEWLINE> with open ( connectionsCsvLocation ) as connectionFile : <NEWLINE> <INDENT> connections = list ( csv . DictReader ( connectionFile , delimiter = <STRING> ) ) <NEWLINE> <DEDENT> print ( connectionsCsvLocation ) <NEWLINE> if ( name is None and index is None ) : <NEWLINE> <INDENT> cli_utils . print_result ( connections , ctx . logger , as_json = ctx . json , as_pickle = ctx . pickle , depth = ctx . depth , filter_tree = ctx . filter_tree ) <NEWLINE> <DEDENT> if ( name is None and index is not None ) : <NEWLINE> <INDENT> cli_utils . print_result ( connections [ int ( index ) ] , ctx . logger , as_json = ctx . json , as_pickle = ctx . pickle , depth = ctx . depth , filter_tree = ctx . filter_tree ) <NEWLINE> <DEDENT> if ( name is not None and index is None ) : <NEWLINE> <INDENT> connections = [ connection for connection in connections if connection [ <STRING> ] != name ] <NEWLINE> cli_utils . print_result ( connections , ctx . logger , as_json = ctx . json , as_pickle = ctx . pickle , depth = ctx . depth , filter_tree = ctx . filter_tree ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def delay_job ( self , job , time_delta ) : <NEWLINE> <INDENT> amount = int ( time_delta . total_seconds ( ) ) <NEWLINE> self . connection . zincrby ( self . scheduler_jobs_key , job . id , amount ) <NEWLINE> <DEDENT>
for spouse_uid in spouse_uids : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> spouse = family . uid_to_person ( spouse_uid ) <NEWLINE> <DEDENT> except TypeError as e : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> continue <NEWLINE> <DEDENT> spouses = [ <NEWLINE> <INDENT> family . uid_to_person ( relation [ 1 ] ) <NEWLINE> for relation in big_dict [ <STRING> ] <NEWLINE> if relation [ 0 ] == spouse_uid <NEWLINE> <DEDENT> ] <NEWLINE> family . add_spouses ( spouse , children ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if isinstance ( db , SQLAlchemy ) : <NEWLINE> <INDENT> role1 = db_adapter . find_first_object ( RoleClass , name = <STRING> ) <NEWLINE> db_adapter . delete_object ( role1 ) <NEWLINE> role2 = db_adapter . find_first_object ( RoleClass , name = <STRING> ) <NEWLINE> db_adapter . delete_object ( role1 ) <NEWLINE> <DEDENT> <DEDENT>
if ( outer_sort == outer_sort_next ) : <NEWLINE> <INDENT> if ( inner_sort > inner_sort_next ) : <NEWLINE> <INDENT> print ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> . format ( halo_id , opt [ <STRING> ] , inner_sort , halo_id_next , <NEWLINE> inner_sort_next ) ) <NEWLINE> <DEDENT> print ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( opt [ <STRING> ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if ( outer_sort == outer_sort_next ) : <NEWLINE> <INDENT> if ( inner_sort < inner_sort_next ) : <NEWLINE> <INDENT> print ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> . format ( halo_id , opt [ <STRING> ] , inner_sort , halo_id_next , <NEWLINE> inner_sort_next ) ) <NEWLINE> <DEDENT> print ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( opt [ <STRING> ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> features = int . from_bytes ( buf [ 166 ] + buf [ 167 ] , byteorder = <STRING> ) <NEWLINE> if major & 0x400 : <NEWLINE> <INDENT> self . lba48bit = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . lba48bit = False <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> fig . text ( x = .5 , y = .99 , s = qpi_real [ <STRING> ] , <NEWLINE> <INDENT> verticalalignment = <STRING> , <NEWLINE> horizontalalignment = <STRING> , <NEWLINE> fontsize = 14 ) <NEWLINE> <DEDENT> <DEDENT>
def validate_python ( self , values , state ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> email = values [ <STRING> ] <NEWLINE> if email : <COMMENT> <NEWLINE> <INDENT> user = AuthUser . get_by_email ( email ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if values . has_key ( <STRING> ) : <NEWLINE> <INDENT> user_id = values [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> user_id = None <NEWLINE> <DEDENT> if email and ( user . user_id != user_id ) : <NEWLINE> <INDENT> errors = { <STRING> : self . message ( <STRING> , state ) } <NEWLINE> raise Invalid ( self . message ( <STRING> , state ) , <NEWLINE> <INDENT> values , state , error_dict = errors ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
for value in [ command [ <STRING> ] , command [ <STRING> ] , command [ <STRING> ] , command [ <STRING> ] ] : <NEWLINE> <INDENT> if value != - 1 : <NEWLINE> <INDENT> if value < 1 or value > 1 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if process . returncode != 0 : <NEWLINE> <INDENT> msg = <STRING> <NEWLINE> logger . error ( msg ) <NEWLINE> raise SetupError ( msg ) <NEWLINE> else : <NEWLINE> logger . info ( <STRING> ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> dimu_pt , dimu_phi , dimu_para , dimu_perp = create_metres ( <NEWLINE> <INDENT> event . METnoX , event . MuonSelection , <NEWLINE> <DEDENT> ) <NEWLINE> event . DiMuon_pt = dimu_pt <NEWLINE> event . DiMuon_phi = dimu_phi <NEWLINE> event . METnoX_diMuonParaProjPt = dimu_para <NEWLINE> event . METnoX_diMuonPerpProjPt = dimu_perp <NEWLINE> event . METnoX_diMuonParaProjPt_Minus_DiMuon_pt = dimu_para - dimu_pt <NEWLINE> event . METnoX_diMuonPerpProjPt_Plus_DiMuon_pt = dimu_perp + dimu_pt <NEWLINE> event . METnoX_diMuonParaProjPt_Div_DiMuon_pt = dimu_para / dimu_pt <NEWLINE> event . METnoX_diMuonPerpProjPt_Plus_DiMuon_pt_Div_DiMuon_pt = ( dimu_para + dimu_pt ) / dimu_pt <NEWLINE> <DEDENT>
for name , obj in getattr ( mod , <STRING> , { } ) . iteritems ( ) : <NEWLINE> <INDENT> scope [ name ] = name <NEWLINE> <DEDENT>
def _scheduler ( self , epoch ) : <NEWLINE> <INDENT> if epoch % self . decay_after_n_epoch == 0 and epoch != 0 : <NEWLINE> <INDENT> lr = K . get_value ( self . model . optimizer . lr ) <NEWLINE> K . set_value ( self . model . optimizer . lr , lr * self . decay_rate ) <NEWLINE> print ( <STRING> . format ( lr ** self . decay_rate ) ) <NEWLINE> <DEDENT> return K . get_value ( self . model . optimizer . lr ) <NEWLINE> <DEDENT>
@ _run <NEWLINE> <INDENT> def update ( self , document , target , ** kargs ) : <NEWLINE> <INDENT> self . _db [ document ] . update ( kargs , target , callback = self . callback ) <NEWLINE> <DEDENT> <DEDENT>
structure_txt = generate_LAMMPS_structure ( structure ) <NEWLINE> <INDENT> input_txt = generate_LAMMPS_input ( potential_data , <NEWLINE> <INDENT> parameters_data , <NEWLINE> structure_file = self . _INPUT_STRUCTURE , <NEWLINE> optimize_path_file = self . _OUTPUT_TRAJECTORY_FILE_NAME ) <NEWLINE> <DEDENT> <DEDENT>
if max_stress < test_range [ 1 ] or min_stress > test_range [ 0 ] : <NEWLINE> <INDENT> if abs ( max_stress - test_range [ 1 ] ) < interval * 2 or abs ( test_range [ 0 ] - min_stress ) < interval * 2 : <NEWLINE> <INDENT> interval *= 0.5 <NEWLINE> <DEDENT> <DEDENT>
if isinstance ( radius , Variable ) : <NEWLINE> <INDENT> self . _radius = dcopy ( radius ) <NEWLINE> else : <NEWLINE> self . _radius = Variable ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> radius , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> + str ( name ) + <STRING> , <NEWLINE> <DEDENT> ) <NEWLINE> if not self . _radius . scalar : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> if self . radius <= 0.0 : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT>
observation = varlib . Observation ( name , time , obs , description ) <NEWLINE> <INDENT> except Exception : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> return observation <NEWLINE> <DEDENT>
response = await venom . invoke ( method , request , context = AioHTTPRequestContext ( request ) ) <NEWLINE> <INDENT> return web . Response ( body = rpc_response . pack ( response ) , <NEWLINE> <INDENT> content_type = rpc_response . mime , <NEWLINE> status = http_status ) <NEWLINE> except Error as e : <NEWLINE> <DEDENT> return web . Response ( body = rpc_error_response . pack ( e . format ( ) ) , <NEWLINE> <INDENT> content_type = rpc_error_response . mime , <NEWLINE> status = e . http_status ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> app_index_class = getattr ( app_config , <STRING> , None ) <NEWLINE> if app_index_class : <NEWLINE> <INDENT> template_name = getattr ( app_config , <STRING> , <STRING> ) <NEWLINE> app_index = app_index_class . as_view ( <NEWLINE> <INDENT> app_config = app_config , backend = self , template_name = template_name <NEWLINE> <DEDENT> ) <NEWLINE> urlpatterns [ None ] . append ( url ( <STRING> , app_index , name = <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def from_time ( cls , command , user , time ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert len ( time ) <= 5 <NEWLINE> padded_time = tuple ( time ) + ( <STRING> , ) * ( 5 - len ( time ) ) <NEWLINE> assert len ( padded_time ) == 5 <NEWLINE> return cls ( command , time [ 0 ] , user , padded_time [ 1 ] , padded_time [ 2 ] , padded_time [ 3 ] , padded_time [ 4 ] ) <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def from_time ( cls , command , user , time ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if len ( time ) > 5 : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( time ) ) <NEWLINE> <DEDENT> padded_time = tuple ( time ) + ( <STRING> , ) * ( 5 - len ( time ) ) <NEWLINE> if len ( padded_time ) > 5 : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( padded_time ) ) <NEWLINE> <DEDENT> return cls ( command , padded_time [ 0 ] , user , padded_time [ 1 ] , padded_time [ 2 ] , padded_time [ 3 ] , padded_time [ 4 ] ) <NEWLINE> <DEDENT> <DEDENT>
def send_message ( self , frameid , blob ) : <NEWLINE> <INDENT> self . _sending = True <NEWLINE> d = self . merger . write_blob ( frameid , blob ) <NEWLINE> def done ( result ) : <NEWLINE> <INDENT> self . _sending = False <NEWLINE> return result <NEWLINE> <DEDENT> d . addBoth ( d ) <NEWLINE> <DEDENT>
if isinstance ( t , Workflow ) : <NEWLINE> <INDENT> wf_wdl , _ , wf_tools = t . wdl ( with_docker = with_docker , is_nested_tool = True ) <NEWLINE> wtools [ s . id ( ) ] = wf_wdl <NEWLINE> wtools . update ( wf_tools ) <NEWLINE> else : <NEWLINE> wtools [ s . id ( ) ] = t . wdl ( with_docker = with_docker ) <NEWLINE> <DEDENT>
z , edges = np . histogram ( data . ids , <NEWLINE> <INDENT> bins = np . arange ( - 0.5 , data . nx * data . ny + 0.5 ) ) <NEWLINE> z = z . reshape ( data . nx , data . ny ) <NEWLINE> if side_panels : <NEWLINE> z_sumx = np . sum ( z , axis = 1 ) <NEWLINE> z_sumy = np . sum ( z , axis = 0 ) <NEWLINE> <DEDENT>
t = np . linspace ( 0.0 , 7.2e4 , nbins + 1 ) <NEWLINE> <INDENT> z , xe , ye = np . histogram2d ( data . ids , data . tofs / 1.0e3 , <NEWLINE> <INDENT> bins = [ np . arange ( - 0.5 , data . nx * data . ny + 0.5 ) , <NEWLINE> <INDENT> t ] ) <NEWLINE> <DEDENT> <DEDENT> z = z . reshape ( data . nx , data . ny , nbins ) <NEWLINE> <COMMENT> <NL> if transpose : <NEWLINE> <INDENT> z = np . transpose ( z , axes = [ 1 , 0 , 2 ] ) <NEWLINE> <DEDENT> clab = <STRING> <NEWLINE> if log : <NEWLINE> <INDENT> with np . errstate ( divide = <STRING> , invalid = <STRING> ) : <NEWLINE> <INDENT> z = np . log10 ( z ) <NEWLINE> <DEDENT> clab = <STRING> . format ( clab ) <NEWLINE> <DEDENT> <DEDENT>
print ( <STRING> ) <NEWLINE> <INDENT> for s in stringers . values ( ) : <NEWLINE> <INDENT> s . elements = [ bdf . elements [ eid ] for eid in p . eids ] <NEWLINE> setelements = set ( s . elements ) <NEWLINE> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT>
value = logical_value <NEWLINE> <INDENT> if reg == Register . HUE : <NEWLINE> <INDENT> if logical_value in ( 0.0 , 360.0 ) : <NEWLINE> <INDENT> value = 0.0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> value = ( logical_value % 360.0 ) / 360.0 * 65535.0 <NEWLINE> <DEDENT> <DEDENT> elif reg in ( Register . BRIGHTNESS , Register . SATURATION ) : <NEWLINE> <INDENT> if logical_value == 100.0 : <NEWLINE> <INDENT> value = 65535.0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> value = logical_value / 100.0 * 65535.0 <NEWLINE> <DEDENT> <DEDENT> elif reg in ( Register . DURATION , Register . TIME ) : <NEWLINE> <INDENT> value = logical_value * 1000.0 <NEWLINE> <DEDENT> <DEDENT>
sepcon = ( Group ( Literal ( <STRING> ) - Literal ( <STRING> ) - <NEWLINE> <INDENT> contract_expression ( <STRING> ) - Literal ( <STRING> ) ) ) <NEWLINE> sepcon . setParseAction ( SeparateContext . parse_action ) <NEWLINE> sepcon . setName ( <STRING> ) <NEWLINE> add_contract ( sepcon ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if gap_filling != None and ~ np . isnan ( np . nanmean ( Array ) ) : <NEWLINE> <INDENT> Array_end [ np . isnan ( Array_end ) ] = - 9999 <NEWLINE> Array_end = RC . gap_filling ( Array_end , - 9999 , gap_filling ) <NEWLINE> <DEDENT> Array_end = Array_end * MASK <NEWLINE> <DEDENT>
if not np . isnan ( np . nanmean ( Crop_S1_End . Data ) ) : <NEWLINE> <INDENT> for Date_Year in Dates_Years : <NEWLINE> <INDENT> year_diff = int ( Date_Year . year - Dates_Years [ 0 ] . year ) <NEWLINE> for dekad in range ( 0 , int ( np . nanmax ( Crop_S2_End . Data ) ) ) : <NEWLINE> <INDENT> Accumulated_NPP_Data_Start_S1 [ year_diff , Crop_S1_End . Data [ year_diff , : , : ] == dekad ] = NPPcum . Data [ np . minimum ( NPPcum . Size [ 0 ] - 1 , int ( year_diff * 36 + dekad - 1 ) ) , Crop_S1_End . Data [ year_diff , : , : ] == dekad ] <NEWLINE> Accumulated_NPP_Data_Start_S2 [ year_diff , Crop_S2_End . Data [ year_diff , : , : ] == dekad ] = NPPcum . Data [ np . minimum ( NPPcum . Size [ 0 ] - 1 , int ( year_diff * 36 + dekad - 1 ) ) , Crop_S2_End . Data [ year_diff , : , : ] == dekad ] <NEWLINE> Accumulated_NPP_Data_Start_S3 [ year_diff , Crop_S3_End . Data [ year_diff , : , : ] == dekad ] = NPPcum . Data [ np . minimum ( NPPcum . Size [ 0 ] - 1 , int ( year_diff * 36 + dekad - 1 ) ) , Crop_S3_End . Data [ year_diff , : , : ] == dekad ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_positive_stabilizer_groups ( n_qubits , n_states ) : <NEWLINE> <INDENT> if n_states == n_stabilizer_states ( n_qubits ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> target = n_states / pow ( 2 , n_qubits ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> target = n_states <NEWLINE> <DEDENT> bitstrings = gen_bitstrings ( n_qubits ) <NEWLINE> subspaces = [ ] <NEWLINE> generators = [ ] <NEWLINE> for group in combinations ( bitstrings , n_qubits ) : <NEWLINE> <INDENT> if len ( group ) == 2 : <NEWLINE> <INDENT> if not test_commutivity ( n_qubits , group [ 0 ] , group [ 1 ] ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> if len ( group ) > 2 : <NEWLINE> <INDENT> if not all ( [ test_commutivity ( n_qubits , pair [ 0 ] , pair [ 1 ] ) <NEWLINE> <INDENT> for pair in combinations ( group , 2 ) ] ) : <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> candidate = BinarySubspace ( * group ) <NEWLINE> if len ( candidate . generators ) < n_qubits : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if len ( candidate . _items ) < pow ( 2 , n_qubits ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> res = tuple ( i for i in sorted ( candidate . _items , key = bool_to_int ) ) <NEWLINE> for space in subspaces : <NEWLINE> <INDENT> if np . all ( [ np . array_equal ( _el1 , _el2 ) for _el1 , _el2 in zip ( res , space ) ] ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> subspaces . append ( res ) <NEWLINE> generators . append ( tuple ( candidate . generators ) ) <NEWLINE> if len ( generators ) == n_states : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> return generators <NEWLINE> <DEDENT>
def test_write_missing_value ( self ) : <NEWLINE> <INDENT> svc = Service ( <STRING> ) <NEWLINE> self . assertRaises ( lambda : svc [ <STRING> ] , MissingKeyException ) <NEWLINE> <DEDENT>
vel = ltu . dv_from_z ( ( mean / wv_line_vac ) - 1 , z_line ) . to ( units ) . value <NEWLINE>
def _config_parse ( config_file ) : <NEWLINE> <INDENT> if config_file is not None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> configs = ConfigParse ( ) <NEWLINE> configs . read ( config_file ) <NEWLINE> dwh_schema = dict ( configs . items ( <STRING> ) ) <NEWLINE> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> logger . error ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> dwh_config = { <NEWLINE> <INDENT> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> return config_file <NEWLINE> <DEDENT>
def perform_test ( self , command : [ str ] , input_file : str , expected_result_file : str ) : <NEWLINE> <INDENT> with open ( expected_result_file , <STRING> ) as fin : <NEWLINE> <INDENT> expected = fin . read ( ) <NEWLINE> <DEDENT> cmd , flags = self . interpreter . build_command ( command , input_file ) <NEWLINE> res = self . interpreter . execute_command ( cmd , flags , return_output = True ) <NEWLINE> self . assertEqual ( res , expected ) <NEWLINE> <DEDENT>
h , m , s = convert_elapsed_time ( training_time ) <NEWLINE> <INDENT> logger . info ( <STRING> <NEWLINE> <INDENT> . format ( h , m , s ) ) <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> logger . info ( outputs ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> logger . info ( targets ) <NEWLINE> <DEDENT>
def mdata ( path , track_element ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> tags = { <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> } <NEWLINE> <COMMENT> <NL> cmd = [ <STRING> , path . rstrip ( ) ] <NEWLINE> process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , <NEWLINE> <INDENT> stderr = subprocess . STDOUT ) <NEWLINE> <COMMENT> <NL> <DEDENT> while True : <NEWLINE> <INDENT> out = process . stdout . readline ( ) <NEWLINE> decoded = out . decode ( <STRING> ) <NEWLINE> if out != <STRING> : <NEWLINE> <INDENT> linecheck = decoded . replace ( <STRING> , <STRING> ) <NEWLINE> for tag in tags : <NEWLINE> <INDENT> tagstring = tag + <STRING> <NEWLINE> if tagstring in linecheck : <NEWLINE> <INDENT> stringf = out . split ( <STRING> ) [ 1 ] <NEWLINE> ttag = tag <NEWLINE> if tag == <STRING> : <NEWLINE> <INDENT> ttag = <STRING> <NEWLINE> <DEDENT> if tag == <STRING> : <NEWLINE> <INDENT> ttag = <STRING> <NEWLINE> <DEDENT> ttag = SubElement ( track_element , tag ) <NEWLINE> ttag . text = stringf . rstrip ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def mdata ( path , track_element ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> tags = { <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> } <NEWLINE> <COMMENT> <NL> cmd = [ <STRING> , path . rstrip ( ) ] <NEWLINE> process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , <NEWLINE> <INDENT> stderr = subprocess . STDOUT ) <NEWLINE> <COMMENT> <NL> <DEDENT> while True : <NEWLINE> <INDENT> out = process . stdout . readline ( ) <NEWLINE> decoded = out . decode ( <STRING> ) <NEWLINE> if out != <STRING> : <NEWLINE> <INDENT> linecheck = decoded . replace ( <STRING> , <STRING> ) <NEWLINE> for tag in tags : <NEWLINE> <INDENT> tagstring = tag + <STRING> <NEWLINE> if tagstring in linecheck : <NEWLINE> <INDENT> stringf = decoded . split ( <STRING> ) [ 1 ] <NEWLINE> ttag = tag <NEWLINE> if tag == <STRING> : <NEWLINE> <INDENT> ttag = <STRING> <NEWLINE> <DEDENT> if tag == <STRING> : <NEWLINE> <INDENT> ttag = <STRING> <NEWLINE> <DEDENT> ttag = SubElement ( track_element , tag ) <NEWLINE> ttag . text = stringf . rstrip ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
class CLIN28JSON : <NEWLINE> <INDENT> def __init__ ( self , filename ) : <NEWLINE> <INDENT> if os . path . exists ( filename ) : <NEWLINE> <INDENT> raise FileExistsError ( <STRING> + filename ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def validate ( self ) : <NEWLINE> <INDENT> self . index = { } <NEWLINE> if <STRING> not in self . data : <NEWLINE> <INDENT> return ValidationError ( <STRING> ) <NEWLINE> <DEDENT> if <STRING> not in self . data : <NEWLINE> <INDENT> return ValidationError ( <STRING> ) <NEWLINE> <DEDENT> for key in self . data : <NEWLINE> <INDENT> if key not in ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> print ( <STRING> + key + <STRING> , file = sys . stderr ) <NEWLINE> <DEDENT> <DEDENT> for word in self . words ( ) : <NEWLINE> <INDENT> if <STRING> not in word or not word [ <STRING> ] : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + repr ( word ) ) <NEWLINE> <DEDENT> self . index [ word [ <STRING> ] ] = word <NEWLINE> if <STRING> not in word or not word [ <STRING> ] : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + repr ( word ) ) <NEWLINE> <DEDENT> for key in word : <NEWLINE> <INDENT> if key not in ( <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> print ( <STRING> + key + <STRING> + repr ( word ) + <STRING> , file = sys . stderr ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for correction in self . corrections ( ) : <NEWLINE> <INDENT> if <STRING> not in correction or not correction [ <STRING> ] : <NEWLINE> <INDENT> if <STRING> not in correction or not correction [ <STRING> ] : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> elif correction [ <STRING> ] not in self . index : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + correction [ <STRING> ] + <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> for wordid in correction [ <STRING> ] : <NEWLINE> <INDENT> if wordid not in self . index : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + wordid + <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for key in correction : <NEWLINE> <INDENT> if key not in ( <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> print ( <STRING> + key + <STRING> + repr ( correction ) + <STRING> , file = sys . stderr ) <NEWLINE> <DEDENT> if key == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> correction [ <STRING> ] = float ( correction [ <STRING> ] ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + str ( correction [ <STRING> ] ) + <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> if correction [ <STRING> ] < 0 or correction [ <STRING> ] > 0 : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + str ( correction [ <STRING> ] ) + <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
@ patch ( <STRING> , <STRING> ) <NEWLINE> <INDENT> @ patch ( <STRING> , _fake_urlopen ) <NEWLINE> def test_send_request ( self ) : <NEWLINE> <INDENT> sender = SmsSender ( ) <NEWLINE> response = sender . send_request ( <STRING> , { } ) <NEWLINE> self . assertIn ( SMSMessage . STATUS_ACCEPTED , response ) <NEWLINE> <DEDENT> <DEDENT>
def send_sms ( to , text , signature_id = None , date = None , link = <STRING> ) : <NEWLINE> <INDENT> signature = sender . get_signature ( signature_id ) <NEWLINE> params = { <NEWLINE> <INDENT> <STRING> : to , <NEWLINE> <STRING> : quote_plus ( text ) , <NEWLINE> <STRING> : signature . name , <NEWLINE> <STRING> : date or <STRING> , <NEWLINE> <DEDENT> } <NEWLINE> response = sender . send_request ( link , params ) <NEWLINE> sms_id , status = sender . parse_response ( response ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if step > 0 : <NEWLINE> <INDENT> if ( start > a_length ) or ( stop < - a_length ) : <NEWLINE> <INDENT> start = stop = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if start < - a_length : <NEWLINE> <INDENT> start = 0 <NEWLINE> <DEDENT> if stop > a_length : <NEWLINE> <INDENT> stop = a_length <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif step < 0 : <NEWLINE> <INDENT> if ( start < - a_length ) or ( stop_i and stop >= ( a_length - 1 ) ) : <NEWLINE> <INDENT> start = stop = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if start >= a_length : <NEWLINE> <INDENT> start = a_length - 1 <NEWLINE> <DEDENT> if stop_i and stop < - a_length : <NEWLINE> <INDENT> stop = None <NEWLINE> stop_i = True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
new_slice = a_slice <NEWLINE> <INDENT> if new_slice is Ellipsis : <NEWLINE> <INDENT> new_slice = slice ( None ) <NEWLINE> <DEDENT> elif not isinstance ( new_slice , slice ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> % str ( new_slice ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
new_slice = a_slice <NEWLINE> <INDENT> if new_slice is Ellipsis : <NEWLINE> <INDENT> new_slice = slice ( None ) <NEWLINE> <DEDENT> elif not isinstance ( a_slice , slice ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> % str ( new_slice ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
server . set_backend ( backend ) <NEWLINE>
@ property <NEWLINE> <INDENT> def to_train ( self ) -> bool : <NEWLINE> <INDENT> if self . name == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT>
def main_Concern ( ) : <NEWLINE> <INDENT> parser = ArgumentParser ( ) <NEWLINE> parser . add_argument ( <STRING> , type = os . path . expanduser ) <NEWLINE> config , vimargs = parser . parse_known_args ( ) <NEWLINE> if config . chdir is not None : <NEWLINE> <INDENT> os . chdir ( config . chdir ) <NEWLINE> <DEDENT> configdir = Path . home ( ) / <STRING> <NEWLINE> configdir . mkdir ( parents = True , exist_ok = True ) <NEWLINE> with TemporaryDirectory ( dir = configdir ) as tempdir : <NEWLINE> <INDENT> tempdir = Path ( tempdir ) <NEWLINE> concernvimrc = tempdir / <STRING> <NEWLINE> sendblock = tempdir / <STRING> <NEWLINE> quit = tempdir / <STRING> <NEWLINE> screenrc = tempdir / <STRING> <NEWLINE> config = ConfigCtrl ( ) <NEWLINE> config . put ( <STRING> , <STRING> , function = toabswidth ) <NEWLINE> config . printf ( <STRING> , resource_filename ( __name__ , <STRING> ) ) <NEWLINE> try : <NEWLINE> <INDENT> config . loadsettings ( ) <NEWLINE> <DEDENT> except FileNotFoundError as e : <NEWLINE> <INDENT> log . info ( <STRING> , e ) <NEWLINE> <DEDENT> Concern = config . node . Concern <NEWLINE> uservimrc = Path . home ( ) / <STRING> <NEWLINE> if uservimrc . exists ( ) : <NEWLINE> <INDENT> ( - Concern ) . printf ( <STRING> , uservimrc ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . info ( <STRING> , uservimrc ) <NEWLINE> <DEDENT> ( - Concern ) . printf ( <STRING> , sys . executable ) <NEWLINE> ( - Concern ) . printf ( <STRING> , concernvimrc ) <NEWLINE> ( - Concern ) . printf ( <STRING> , sendblock ) <NEWLINE> ( - Concern ) . printf ( <STRING> , quit ) <NEWLINE> ( - Concern ) . printf ( <STRING> ) <NEWLINE> for arg in vimargs : <NEWLINE> <INDENT> ( - Concern ) . printf ( <STRING> , arg ) <NEWLINE> <DEDENT> import_module ( <STRING> , package = __package__ ) . configure ( config ) <NEWLINE> ( - Concern ) . processtemplate ( resource_filename ( templates . __name__ , <STRING> ) , concernvimrc ) <NEWLINE> ( - Concern ) . printf ( <STRING> ) <NEWLINE> ( - Concern ) . processtemplate ( resource_filename ( templates . __name__ , <STRING> ) , sendblock ) <NEWLINE> ( - Concern ) . processtemplate ( resource_filename ( templates . __name__ , <STRING> ) , quit ) <NEWLINE> ( - Concern ) . printf ( <STRING> ) <NEWLINE> ( - Concern ) . processtemplate ( resource_filename ( templates . __name__ , <STRING> ) , screenrc ) <NEWLINE> doublequotekey = Concern . doubleQuoteKey <NEWLINE> stuffablescreen ( doublequotekey ) . print ( <STRING> , Concern . sessionName , <STRING> , screenrc ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if sp . issparse ( X ) : <NEWLINE> <INDENT> centers [ c ] = X [ best_candidate ] . toarray ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> centers [ c ] = X [ best_candidate ] <NEWLINE> <DEDENT> centers_idx . append ( c ) <NEWLINE> current_pot = best_pot <NEWLINE> closest_dist_sq = best_dist_sq <NEWLINE> <DEDENT>
return local_dir <NEWLINE>
def _interdistance ( self , first : np . ndarray , second : np . ndarray ) -> np . ndarray : <NEWLINE> <INDENT> if first is not self . _last : <NEWLINE> <INDENT> self . _last = first <NEWLINE> self . _last_ranks = np . apply_along_axis ( st . rankdata , 0 , first ) <NEWLINE> <DEDENT> second_ranks = np . apply_along_axis ( st . rankdata , 0 , first ) <NEWLINE> return dist . cdist ( self . _last_ranks , second_ranks , metric = <STRING> ) <NEWLINE> <DEDENT>
@ contextlib . contextmanager <NEWLINE> <INDENT> def resume ( file_name ) : <NEWLINE> <INDENT> file_path = <STRING> . format ( env . HIDEOUT_BASEDIR , file_name ) <NEWLINE> target = None <NEWLINE> if os . path . exists ( file_path ) and not env . HIDEOUT_FORCE_CACHE : <NEWLINE> <INDENT> logger . error ( <STRING> . format ( file_path ) ) <NEWLINE> with open ( file_path , mode = <STRING> ) as f : <NEWLINE> <INDENT> target = pickle . load ( f ) <NEWLINE> <DEDENT> <DEDENT> yield target <NEWLINE> if target is None : <NEWLINE> <INDENT> freeze ( target , file_name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def percent_of ( self , percent , whole ) : <NEWLINE> <INDENT> return ( percent * whole ) * 100 <NEWLINE> <DEDENT>
if batch : <NEWLINE> <INDENT> import batch_process <NEWLINE> batch_process . run ( batch ) <NEWLINE> else : <NEWLINE> metadata_dir = os . path . join ( indir , <STRING> ) <NEWLINE> if os . path . isdir ( metadata_dir ) : <NEWLINE> <INDENT> metadata_process . clean ( metadata_dir ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , operand , invert = False ) : <NEWLINE> <INDENT> super ( IsNullOperator , self ) . __init__ ( <STRING> if invert else <STRING> , operand ) <NEWLINE> self . invert = invert <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> ERROR_HANDLER . assert_true ( self . criteria_working_dir is None , <STRING> , __file__ ) <NEWLINE> for criterion in self . tools_config_by_criterion_dict : <NEWLINE> <INDENT> ERROR_HANDLER . assert_true ( len ( self . tools_config_by_criterion_dict [ criterion ] ) != len ( set ( [ c . get_tool_config_alias ( ) for c in self . tools_config_by_criterion_dict [ criterion ] ] ) ) , <STRING> . format ( criterion ) , __file__ ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> ERROR_HANDLER . assert_true ( self . tests_working_dir is None , <STRING> , __file__ ) <NEWLINE> ERROR_HANDLER . assert_true ( len ( self . test_tool_config_list ) != len ( set ( [ c . get_tool_config_alias ( ) for c in self . test_tool_config_list ] ) ) , <STRING> , __file__ ) <NEWLINE> <DEDENT>
class CopyCallbackObject ( DefaultCallbackObject ) : <NEWLINE> <INDENT> def after_command ( self ) : <NEWLINE> <INDENT> file_src_dest_map = self . post_callback_args <NEWLINE> for src , dest in list ( file_src_dest_map . items ( ) ) : <NEWLINE> <INDENT> abs_src = os . path . join ( self . repository_rootdir , src ) <NEWLINE> if os . path . abspath ( abs_src ) != os . path . abspath ( dest ) : <NEWLINE> <INDENT> shutil . copy2 ( src , dest ) <NEWLINE> <DEDENT> <DEDENT> return DefaultCallbackObject . after_command ( self ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT>
dup_list , invalid = KTestTestFormat . ktest_fdupes ( * folders , custom_replay_tool_binary_dir = self . custom_binary_dir ) <NEWLINE> <INDENT> if len ( invalid ) > 0 : <NEWLINE> <INDENT> logging . warning ( <STRING> . format ( len ( invalid ) , invalid ) ) <NEWLINE> for kt in invalid : <NEWLINE> <INDENT> if KTestTestFormat . get_dir ( kt , folders ) == self . tests_storage_dir : <NEWLINE> <INDENT> os . remove ( kt ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for dup_tuple in dup_list : <NEWLINE> <INDENT> key = os . path . relpath ( dup_tuple [ 0 ] , KTestTestFormat . get_dir ( dup_tuple [ 0 ] , folders ) ) <NEWLINE> kepttest2duptest_map [ key ] = [ os . path . relpath ( dp , KTestTestFormat . get_dir ( dp , folders ) ) for dp in dup_tuple [ 1 : ] ] <NEWLINE> for df in dup_tuple [ 1 : ] : <NEWLINE> <INDENT> if KTestTestFormat . get_dir ( kt , folders ) == self . tests_storage_dir : <NEWLINE> <INDENT> os . remove ( df ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> common_fs . dumpJSON ( kepttest2duptest_map , self . keptktest2dupktests ) <NEWLINE> <DEDENT>
parser_customexec = subparsers . add_parser ( <STRING> , help = <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> parser_run . add_argument ( <STRING> , action = <STRING> , help = <STRING> ) <NEWLINE> <DEDENT>
def get_definitions_pairs ( defines_list ) : <NEWLINE> <INDENT> def_pairs = { } <NEWLINE> for define_statement_string in defines_list : <NEWLINE> <INDENT> elems = re . split ( <STRING> , define_statement_string ) <NEWLINE> if len ( elems ) > 3 : <COMMENT> <NEWLINE> <INDENT> continue <COMMENT> <NEWLINE> <DEDENT> name = elems [ 1 ] <NEWLINE> value = elems [ 2 ] <NEWLINE> def_pairs [ name ] = value <NEWLINE> <DEDENT> return def_pairs <NEWLINE> <DEDENT>
def update_required ( self ) -> bool : <NEWLINE> <INDENT> return self . _variables_manager . get_variables ( ) == self . _get_required_variables ( ) <NEWLINE> <DEDENT>
if log_entry . message is not <STRING> or log_entry . message is not None : <NEWLINE> <INDENT> dto [ <STRING> ] = log_entry . message <NEWLINE> <DEDENT>
deb_dependencies = self . extra_args . get ( <STRING> , { } ) . project . get ( <STRING> ) <NEWLINE> <INDENT> project = self . extra_args . get ( <STRING> , { } ) . project <NEWLINE> generated_builds = [ ] <NEWLINE> for deb in project . get ( <STRING> , [ ] ) : <NEWLINE> <INDENT> deb_name = deb . get ( <STRING> , self . project_name ) <NEWLINE> dpm = Dpm ( project_path = self . project_path , <NEWLINE> <INDENT> package_name = deb_name , <NEWLINE> package_version = self . new_version , <NEWLINE> install_path = deb . get ( <STRING> ) , <NEWLINE> dependencies = deb_dependencies , <NEWLINE> description = deb . get ( <STRING> ) , <NEWLINE> excludes = project . get ( <STRING> , [ ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
dim = <STRING> if opts [ <STRING> ] else <STRING> ; <NEWLINE> <INDENT> if opts [ <STRING> ] : <NEWLINE> <INDENT> angle = float ( opts [ <STRING> ] ) ; <NEWLINE> angleopt = ( angle , dim ) ; <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> angleopt = None ; <NEWLINE> <DEDENT> KE , good = totalKE ( d , ecut , angleopt , return_bools = True ) ; <NEWLINE> LE = laserE ( E_0 , w , T , dim = dim ) ; <NEWLINE> totalq = d [ <STRING> ] [ good ] . sum ( ) * 1e12 ; <NEWLINE> print ( <STRING> . format ( totalq , <STRING> if opts [ <STRING> ] else <STRING> ) ) ; <NEWLINE> print ( <STRING> . format ( KE ) ) ; <NEWLINE> print ( <STRING> . format ( LE ) ) ; <NEWLINE> print ( <STRING> . format ( KE / LE ) ) ; <NEWLINE> <DEDENT>
headers = base_headers . copy ( ) <NEWLINE> <INDENT> headers [ <STRING> ] = i + 1 <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> enviar_email ( email , settings . DEFAULT_FROM_EMAIL , nome , <NEWLINE> <INDENT> assunto , template_email , mensagem ) <NEWLINE> <DEDENT> <DEDENT>
def mcall_stat_parse ( infile ) : <NEWLINE> <INDENT> with open ( infile ) as f : <NEWLINE> <INDENT> dstr = f . read ( ) <NEWLINE> <DEDENT> return float ( re . search ( <STRING> , f ) . groups ( ) [ 0 ] ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> x_value = np . sort ( data ) <NEWLINE> size_data = x_value . size <NEWLINE> <COMMENT> <NL> y_value = [ ] <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def retrieve_rows_csv ( request , job , ** kwargs ) : <NEWLINE> <INDENT> if request . method != <STRING> : <NEWLINE> <INDENT> print ( <STRING> , kwargs ) <NEWLINE> start_row = kwargs . get ( <STRING> ) <NEWLINE> num_rows = kwargs . get ( <STRING> ) <NEWLINE> input_format = kwargs . get ( <STRING> ) <NEWLINE> job_id = job . id <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> vcf_results = get_variants ( vcf_obj , vcfsamples , raw_variants , qual ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> vcf_results = get_variants ( panel_filtered_results [ 0 ] , samples , raw_variants , qual ) <NEWLINE> <DEDENT>
if isinstance ( channels , list ) : <NEWLINE> <INDENT> for ch in channels : <NEWLINE> <INDENT> permissions [ int ( ch ) ] = ( read , write ) <NEWLINE> else : <NEWLINE> <DEDENT> permissions [ int ( ch ) ] = ( read , write ) <NEWLINE> <DEDENT>
def microlensing_parameters_limits_priors ( parameters , limits ) : <NEWLINE> <INDENT> for i in xrange ( len ( parameters ) ) : <NEWLINE> <DEDENT>
if np . max ( ( caustic_points [ : , first_branch ] ) . real ) > np . max ( ( caustic_points [ : , second_branch ] ) . real ) : <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if is_exception : <NEWLINE> <INDENT> ex_type , ex_value , ex_tb = response <NEWLINE> raise ex_type ( ex_value , ex_tb ) <NEWLINE> <DEDENT> return response_queue <NEWLINE> <DEDENT>
slave = self . canvas_slave <NEWLINE> <COMMENT> <NL> <INDENT> frame_point_i = cv2 . perspectiveTransform ( np . array ( [ [ start_xy ] ] , dtype = float ) , <NEWLINE> <INDENT> slave . canvas_to_frame_map ) . ravel ( ) <NEWLINE> <COMMENT> <NL> <DEDENT> frame_corner_i = find_closest ( slave . df_frame_corners , frame_point_i ) <NEWLINE> <COMMENT> <NL> canvas_corner_i = find_closest ( slave . df_canvas_corners , end_xy ) <NEWLINE> <DEDENT>
