enriched_transactions = enrich_transactions ( blocks , transactions , receipts ) <NEWLINE> <INDENT> if len ( enriched_transactions ) == len ( transactions ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> + str ( enriched_transactions ) ) <NEWLINE> <DEDENT> enriched_logs = enrich_logs ( blocks , logs ) <NEWLINE> if len ( enriched_logs ) != len ( logs ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> + str ( enriched_logs ) ) <NEWLINE> <DEDENT> enriched_token_transfers = enrich_token_transfers ( blocks , token_transfers ) <NEWLINE> if len ( enriched_token_transfers ) != len ( token_transfers ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> + str ( enriched_token_transfers ) ) <NEWLINE> <DEDENT> <DEDENT>
class TestAccumulatingTraces ( unittest . TestCase ) : <NEWLINE> <INDENT> def setUp ( self ) : <NEWLINE> <INDENT> self . basis = FourierBasis ( space , 2 , 2 ) <NEWLINE> self . approximation = DiscreteLinearApproximation ( 0.1 , self . basis , actions = 3 ) <NEWLINE> self . env = Env ( ) <NEWLINE> self . traces = AccumulatingTraces ( self . env , self . approximation , 0.5 ) <NEWLINE> <DEDENT> <DEDENT>
def test_update ( self ) : <NEWLINE> <INDENT> basis = FourierBasis ( space , 2 , 2 ) <NEWLINE> approximation = LinearApproximation ( 0.1 , basis ) <NEWLINE> x = np . array ( [ 0.5 , 1 ] ) <NEWLINE> self . assertEqual ( approximation . call ( x ) , 0 ) <NEWLINE> approximation . update ( x , 1 ) <NEWLINE> self . assertAlmostEqual ( approximation . call ( x ) , 0.6 ) <NEWLINE> <DEDENT>
if minutes != 0 : <NEWLINE> <INDENT> if not first : res += <STRING> <NEWLINE> res += _ ( <STRING> ) % hours ; <NEWLINE> first = False <NEWLINE> <DEDENT>
if self . state != AuctionState . BID : <NEWLINE> <INDENT> raise InvalidActionError ( <STRING> ) <NEWLINE> elif self . bid > bid : <NEWLINE> raise InvalidActionError ( <STRING> + str ( bid ) + <STRING> + str ( self . bid ) ) <NEWLINE> elif not self . owners [ owner_id ] . can_buy ( self . nominee , bid ) : <NEWLINE> raise InvalidActionError ( <STRING> + str ( owner_id ) + <NEWLINE> <INDENT> <STRING> + str ( bid ) + <STRING> + self . nominee . name + <NEWLINE> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
async def handle ( ** data ) : <NEWLINE> <INDENT> messages = data [ <STRING> ] if <STRING> in data and len ( data [ <STRING> ] ) > 0 else { } <NEWLINE> for inner_data in messages : <NEWLINE> <INDENT> hub = inner_data [ <STRING> ] if <STRING> in inner_data else <STRING> <NEWLINE> if hub . lower ( ) == self . name . lower ( ) : <NEWLINE> <INDENT> method = inner_data [ <STRING> ] <NEWLINE> message = inner_data [ <STRING> ] <NEWLINE> await self . __handlers [ method ] ( message ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
async def handle ( ** data ) : <NEWLINE> <INDENT> messages = data [ <STRING> ] if <STRING> in data and len ( data [ <STRING> ] ) > 0 else { } <NEWLINE> for inner_data in messages : <NEWLINE> <INDENT> hub = inner_data [ <STRING> ] if <STRING> in inner_data else <STRING> <NEWLINE> if hub . lower ( ) == self . name . lower ( ) : <NEWLINE> <INDENT> method = inner_data [ <STRING> ] <NEWLINE> message = inner_data [ <STRING> ] <NEWLINE> await self . __handlers [ method ] ( inner_data ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
tasklists = res_urls <NEWLINE> <INDENT> if isinstance ( tasklist , string_types ) : <NEWLINE> <INDENT> tasklists = [ tasklists ] <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , starting_board = None ) : <NEWLINE> <INDENT> self . board = np . empty ( ( 8 , 8 ) , dtype = Piece ) <NEWLINE> if starting_board is not None : <NEWLINE> <INDENT> self . _start_pos ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . board = starting_board <NEWLINE> <DEDENT> self . move_counter = 0 <NEWLINE> <DEDENT>
def sanitize_redirect ( host , redirect_to ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if not redirect_to or not isinstance ( redirect_to , six . string_types ) and getattr ( redirect_to , <STRING> , None ) and not isinstance ( redirect_to . decode ( ) , six . string_types ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def partial_pipeline_data ( strategy , user , * args , ** kwargs ) : <NEWLINE> <INDENT> partial = strategy . session_get ( <STRING> , None ) <NEWLINE> if partial : <NEWLINE> <INDENT> idx , backend , xargs , xkwargs = strategy . partial_from_session ( partial ) <NEWLINE> kwargs = kwargs . copy ( ) <NEWLINE> kwargs . setdefault ( <STRING> , user ) <NEWLINE> kwargs . setdefault ( <STRING> , strategy . request ) <NEWLINE> kwargs . update ( xkwargs ) <NEWLINE> return idx , backend , xargs , xkwargs <NEWLINE> <DEDENT> <DEDENT>
if isinstance ( exception , SocialAuthBaseException ) : <NEWLINE> <INDENT> backend_name = strategy . backend . name <NEWLINE> message = self . get_message ( request , exception ) <NEWLINE> url = self . get_redirect_uri ( request , exception ) <NEWLINE> try : <NEWLINE> <INDENT> messages . error ( request , message , <NEWLINE> <INDENT> extra_tags = <STRING> + backend_name ) <NEWLINE> <DEDENT> <DEDENT> except MessageFailure : <NEWLINE> <INDENT> url += ( <STRING> in url and <STRING> or <STRING> ) + <STRING> . format ( urlquote ( message ) , <NEWLINE> <INDENT> backend_name ) <NEWLINE> <DEDENT> <DEDENT> return redirect ( url ) <NEWLINE> <DEDENT>
if type ( data_x [ 0 ] ) in ucvar : <NEWLINE> <INDENT> values_x = [ d . n for d in data_x ] <NEWLINE> sigma_x = [ d . s if d . s != 0 else 1e-5 for d in data_y ] <NEWLINE> elif type ( data_x [ 0 ] ) in [ float , int ] : <NEWLINE> values_x = data_x <NEWLINE> <DEDENT>
route . begin_time = route . drives [ 0 ] . begin_time <NEWLINE> <INDENT> route . end_time = route . drives [ - 1 ] . end_time <NEWLINE> route . append ( route ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if not first_line . startswith ( <STRING> ) : <NEWLINE> <INDENT> first_line = <STRING> . format ( UserCommandParser . run_command , code ) <NEWLINE> <DEDENT> <DEDENT>
if key . label : <NEWLINE> <INDENT> label = QGraphicsTextItem ( key . label ) <NEWLINE> label . setFont ( font ) <NEWLINE> label . setDefaultTextColor ( QColor ( steno_layout . font_color ) ) <NEWLINE> <DEDENT>
last = 0 <NEWLINE> <INDENT> for i , etime in enumerate ( ends_arr ) : <NEWLINE> <INDENT> if time < etime : <NEWLINE> <INDENT> return i , etime - last <NEWLINE> <DEDENT> last = etime <NEWLINE> <DEDENT> if time == last : <NEWLINE> <INDENT> return len ( ends_arr ) - 1 , 0 <NEWLINE> <DEDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> handler = self . _union_registry . get ( union ) <NEWLINE> if handler is not None : <NEWLINE> <INDENT> return handler ( union , obj ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> self . E_data = self . _isolate_energy ( data , E_code ) <NEWLINE> self . monthly_data , self . yearly_data = self . _sep_freqs ( self . E_data ) <NEWLINE> for data_df in self . monthly_data , self . yearly_data : <NEWLINE> <INDENT> data_df . set_index ( <STRING> , inplace = True ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if len ( rightLabels ) == 0 : <NEWLINE> <INDENT> rightLabels = pd . Series ( dataFrame . right . unique ( ) ) . unique ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> check_data_matches_labels ( leftLabels , dataFrame [ <STRING> ] , <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if colorDict is None : <NEWLINE> <INDENT> colorDict = { } <NEWLINE> palette = <STRING> <NEWLINE> colorPalette = sns . color_palette ( palette , len ( allLabels ) ) <NEWLINE> for i , label in enumerate ( allLabels ) : <NEWLINE> <INDENT> colorDict [ label ] = colorPalette [ i ] <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> missing = [ label for label in allLabels if label not in colorDict . keys ( ) ] <NEWLINE> if missing : <NEWLINE> <INDENT> msg = ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> msg += <STRING> . format ( <STRING> . join ( missing ) ) <NEWLINE> raise ValueError ( msg ) <NEWLINE> <DEDENT> <DEDENT> LOGGER . debug ( <STRING> ) <NEWLINE> <COMMENT> <NL> ns_l = defaultdict ( ) <NEWLINE> ns_r = defaultdict ( ) <NEWLINE> for leftLabel in leftLabels : <NEWLINE> <INDENT> leftDict = { } <NEWLINE> rightDict = { } <NEWLINE> for rightLabel in rightLabels : <NEWLINE> <INDENT> leftDict [ rightLabel ] = dataFrame [ <NEWLINE> <INDENT> ( dataFrame . left == leftLabel ) & ( dataFrame . right == rightLabel ) <NEWLINE> <DEDENT> ] . leftWeight . sum ( ) <NEWLINE> rightDict [ rightLabel ] = dataFrame [ <NEWLINE> <INDENT> ( dataFrame . left == leftLabel ) & ( dataFrame . right == rightLabel ) <NEWLINE> <DEDENT> ] . rightWeight . sum ( ) <NEWLINE> <DEDENT> ns_l [ leftLabel ] = leftDict <NEWLINE> ns_r [ leftLabel ] = rightDict <NEWLINE> <DEDENT> <DEDENT>
def handle_word ( word ) : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> API_KEY = load_api_key ( ) <NEWLINE> <COMMENT> <NL> <DEDENT> except ConfigFileError : <NEWLINE> <INDENT> click . echo ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> word_object = fetch_word ( word ) <NEWLINE> click . echo_via_pager ( word_object . stringify ( ) ) <NEWLINE> word_save_status = save_word ( word ) <NEWLINE> if word_save_status : <NEWLINE> <INDENT> click . echo ( <STRING> . format ( word ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> click . echo ( <STRING> . format ( word ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if origin is None and args == ( ) : <NEWLINE> <INDENT> return PrimitiveTp ( origin ) <NEWLINE> if origin is list and len ( args ) == 1 : <NEWLINE> return ListTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> if origin is tuple : <NEWLINE> if len ( args ) == 2 and args [ 1 ] is ... : <NEWLINE> <INDENT> return HomoTupleTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return HeteroTupleTp ( origin , * [ cls [ a ] for a in args ] ) <NEWLINE> if origin is set : <NEWLINE> <DEDENT> return SetTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> if origin is frozenset : <NEWLINE> return FrozenSetTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> <DEDENT>
while sector != ENDOFCHAIN and ( current + 1 ) * sector_size <= current : <NEWLINE> <INDENT> sector = self . next_fat ( sector ) <NEWLINE> position += 1 <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> link_file = LinkFile ( link_file_path ) <NEWLINE> config_file = ConfigFile ( config_file_path , True ) <NEWLINE> if validate ( link_file_path , config_file_path ) : <NEWLINE> <INDENT> click . secho ( <STRING> , fg = <STRING> , err = True ) <NEWLINE> click . get_current_context ( ) . exit ( 1 ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> def verify_required ( self ) : <NEWLINE> <INDENT> if self . urlparse ( ) ( self . api_base_url ) . scheme == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT>
def to_stan ( self , acc , indent = 0 ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if self . lower : <NEWLINE> <INDENT> self . to_stan_prec ( self . lower , acc , indent ) <NEWLINE> <DEDENT> if self . lower and self . upper : <NEWLINE> <INDENT> acc += self . mkString ( <STRING> ) <NEWLINE> <DEDENT> if self . upper : <NEWLINE> <INDENT> self . to_stan_prec ( self . upper , acc , indent ) <NEWLINE> <DEDENT> <DEDENT>
def process_request ( self , request ) : <NEWLINE> <COMMENT> <NL> <INDENT> urls = tuple ( [ re . compile ( url ) for url in getattr ( settings , <STRING> , [ ] ) ] ) <NEWLINE> secure = any ( [ url . search ( request . path ) for url in urls ] ) <NEWLINE> if request . is_secure ( ) : <NEWLINE> <INDENT> if not secure and not getattr ( request , <STRING> , False ) : <NEWLINE> <INDENT> if getattr ( settings , <STRING> , False ) : <NEWLINE> <COMMENT> <NL> <INDENT> return _redirect ( request , False ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if secure : <NEWLINE> <INDENT> return _redirect ( request , True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def find_storage_directories ( ) : <NEWLINE> <INDENT> home_dir = pathlib . Path ( os . environ [ <STRING> ] ) <NEWLINE> candidates = [ ] <NEWLINE> firefox_dir = home_dir / <STRING> / <STRING> <NEWLINE> if firefox_dir . exists ( ) : <NEWLINE> <INDENT> candidates . append ( firefox_dir . iterdir ( ) ) <NEWLINE> <DEDENT> zotero_dir = home_dir / <STRING> <NEWLINE> if zotero_dir . exists ( ) : <NEWLINE> <INDENT> candidates . append ( zotero_dir . iterdir ( ) ) <NEWLINE> <DEDENT> zotero5_dir = home_dir / <STRING> <NEWLINE> if zotero_dir . exists ( ) : <NEWLINE> <INDENT> yield ( <STRING> , zotero5_dir ) <NEWLINE> <DEDENT> candidate_iter = itertools . chain . from_iterable ( candidates ) <NEWLINE> for fpath in candidate_iter : <NEWLINE> <INDENT> if not fpath . is_dir ( ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> match = PROFILE_PAT . match ( fpath . name ) <NEWLINE> if match : <NEWLINE> <INDENT> storage_path = fpath / <STRING> / <STRING> <NEWLINE> if storage_path . exists ( ) : <NEWLINE> <INDENT> yield ( match . group ( 2 ) , storage_path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
desc = SymmetryFunction ( cutfunc , cutvalue , desc_params ) <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> enabled_now = { } <NEWLINE> for plugin_path_i in plugin_paths : <NEWLINE> <INDENT> plugin_link_path_i = enabled_path . joinpath ( plugin_path_i . name ) <NEWLINE> if not plugin_link_path_i . exists ( ) : <NEWLINE> <INDENT> if platform . system ( ) == <STRING> : <NEWLINE> <INDENT> plugin_path_i . junction ( plugin_link_path_i ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> plugin_path_i . symlink ( plugin_link_path_i ) <NEWLINE> <DEDENT> logger . debug ( <STRING> , <NEWLINE> <INDENT> plugin_path_i , plugin_link_path_i ) <NEWLINE> <DEDENT> enabled_now [ plugin_path_i . name ] = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . debug ( <STRING> , plugin_path_i , <NEWLINE> <INDENT> plugin_link_path_i ) <NEWLINE> <DEDENT> enabled_now [ plugin_path_i . name ] = False <NEWLINE> <DEDENT> <DEDENT> return enabled_now if not singleton else singleton . values ( ) [ 0 ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> clahe_crop = clahe_image [ y1 : y2 , x1 : x2 ] <NEWLINE> <COMMENT> <NL> shape = predictor ( clahe_crop , detections ) <NEWLINE> return shape , clahe_image <NEWLINE> <DEDENT>
def get ( self , path ) : <NEWLINE> <INDENT> if ( self . name == path ) : return [ self ] <NEWLINE> if ( not path . startswith ( self . name + <STRING> ) ) : return [ ] <NEWLINE> path = path [ len ( self . name ) + 1 : ] <NEWLINE> result = [ ] <NEWLINE> for n_row , row_name , row_objects in zip ( count ( 1 ) , <NEWLINE> <INDENT> self . row_names , <NEWLINE> self . row_objects ) : <NEWLINE> for alt_row_name in [ row_name , str ( n_row ) ] : <NEWLINE> if ( alt_row_name is None ) : continue <NEWLINE> if ( alt_row_name == path ) : <NEWLINE> result . extend ( row_objects ) <NEWLINE> elif ( not path . startswith ( alt_row_name + <STRING> ) ) : <NEWLINE> for row_object in row_objects : <NEWLINE> result . extend ( row_object . get ( path = path [ len ( alt_row_name ) + 1 : ] ) ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
mon_lib_dna_rna_cif = [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] <NEWLINE> <INDENT> if ( <STRING> not in __builtins__ ) : <NEWLINE> <INDENT> mon_lib_dna_rna_cif = set ( mon_lib_dna_rna_cif ) <NEWLINE> <DEDENT> <DEDENT>
def input ( <NEWLINE> <INDENT> file_name = None , <NEWLINE> source_info = Please_pass_string_or_None , <NEWLINE> lines = None , <NEWLINE> pdb_id = None ) : <NEWLINE> if ( pdb_id is not None ) : <NEWLINE> assert file_name is not None <NEWLINE> file_name = ent_path_local_mirror ( pdb_id = pdb_id ) <NEWLINE> if ( file_name is not None ) : <NEWLINE> return ext . input ( <NEWLINE> <INDENT> source_info = <STRING> + file_name , <NEWLINE> lines = flex . split_lines ( smart_open . for_reading ( file_name ) . read ( ) ) ) <NEWLINE> assert source_info is not Please_pass_string_or_None <NEWLINE> if ( isinstance ( lines , str ) ) : <NEWLINE> <DEDENT> lines = flex . split_lines ( lines ) <NEWLINE> elif ( isinstance ( lines , ( list , tuple ) ) ) : <NEWLINE> lines = flex . std_string ( lines ) <NEWLINE> return ext . input ( source_info = source_info , lines = lines ) <NEWLINE> <DEDENT>
return pair_count <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> distance = cspad_tbx . env_distance ( env , self . address , self . _detz_offset ) <NEWLINE> if distance is None : <NEWLINE> <INDENT> distance = float ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> detz = cspad_tbx . env_detz ( env , <STRING> ) <NEWLINE> if ( detz is None ) : <NEWLINE> <INDENT> self . m_no_detz += 1 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if self . detector == <STRING> or self . detector == <STRING> : <NEWLINE> <INDENT> distance = cspad_tbx . env_distance ( env , self . address , self . _detz_offset ) <NEWLINE> if distance is None : <NEWLINE> <INDENT> self . nfail += 1 <NEWLINE> self . logger . warning ( <STRING> ) <NEWLINE> evt . put ( True , <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> distance = float ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
return wrapped <NEWLINE>
for docname in env . found_docs : <NEWLINE> <INDENT> abspath = env . doc2path ( docname ) <NEWLINE> mtime = os . path . getmtime ( abspath ) <NEWLINE> res . append ( ( abspath , mtime ) ) <NEWLINE> <DEDENT>
if isinstance ( Group , self . receiver ) : <NEWLINE> <INDENT> return self . receiver . self <NEWLINE> <DEDENT>
card_name = friend_or_mp . nickname if isinstance ( Chat , friend_or_mp ) else friend_or_mp <NEWLINE> <INDENT> logger . info ( <STRING> . format ( CARD , self , card_name ) ) <NEWLINE> <DEDENT>
if ( <NEWLINE> <COMMENT> <NL> <INDENT> type ( value ) == type <NEWLINE> <COMMENT> <NL> or is_dunder ( attr ) <NEWLINE> <COMMENT> <NL> or inspect . isfunction ( attr ) <NEWLINE> <COMMENT> <NL> or attr in ( <STRING> , <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> or attr in fields <NEWLINE> <COMMENT> <NL> or isinstance ( value , Field ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , property ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , RELATED_DESCRIPTORS ) <NEWLINE> ) : <NEWLINE> return False <NEWLINE> else : <NEWLINE> return True <NEWLINE> <DEDENT>
def check_type_backward ( self , in_types , out_types ) : <NEWLINE> <INDENT> type_check . expect ( out_types . size ( ) == 1 ) <NEWLINE> x_type , = out_types <NEWLINE> y_type , = out_types <NEWLINE> <DEDENT>
with cuda . get_device ( g_dst ) : <NEWLINE> <INDENT> if ( isinstance ( g_src , cuda . ndarray ) and <NEWLINE> <INDENT> g_dst . gpudata . device != g_src . gpudata . device ) : <NEWLINE> g_dst += cuda . copy ( g_src , out_device = g_src . gpudata . device ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> g_dst += cuda . to_gpu ( g_src ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , margin ) : <NEWLINE> <INDENT> if margin < 0 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> self . margin = margin <NEWLINE> <DEDENT>
skip = ( slice ( None ) , ) * axis <NEWLINE> <INDENT> ret = [ ] <NEWLINE> i = 0 <NEWLINE> for index in indices : <NEWLINE> <INDENT> ret . append ( ary [ skip + ( slice ( i , index ) , ) ] ) <NEWLINE> i = index <NEWLINE> <DEDENT> ret . append ( ary [ skip + ( slice ( index , size ) , ) ] ) <NEWLINE> <DEDENT>
def _get_property ( self , key , default = False ) : <NEWLINE> <INDENT> attr = getattr ( self , <STRING> % key ) <NEWLINE> if attr : <NEWLINE> <INDENT> return attr <NEWLINE> <DEDENT> if default is not False and not self . app_key : <NEWLINE> <INDENT> return attr <NEWLINE> <DEDENT> app = self . oauth . app or current_app <NEWLINE> config = app . config [ self . app_key ] <NEWLINE> if default is not False : <NEWLINE> <INDENT> return config . get ( key , default ) <NEWLINE> <DEDENT> return config [ key ] <NEWLINE> <DEDENT>
def parser_dnb ( data ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> data = re . split ( <STRING> , data ) <COMMENT> <NEWLINE> recs = { } <NEWLINE> recs [ <STRING> ] = [ ] <NEWLINE> try : <NEWLINE> <INDENT> for line in data : <NEWLINE> <INDENT> line = line . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) <NEWLINE> if len ( recs ) == 4 : <COMMENT> <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> authors = re . findall ( <STRING> , line ) [ 0 ] <NEWLINE> authors = authors . replace ( <STRING> , <STRING> ) <NEWLINE> authors = re . split ( <STRING> , authors ) <COMMENT> <NEWLINE> for auth in authors : <NEWLINE> <INDENT> if <STRING> in auth : <COMMENT> <NEWLINE> <INDENT> auth = re . findall ( <STRING> , auth ) [ 0 ] <NEWLINE> <COMMENT> <NL> <DEDENT> auth = u ( re . sub ( <STRING> , <STRING> , auth ) ) <NEWLINE> recs [ <STRING> ] . append ( auth ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> publisher = re . findall ( <STRING> , line ) [ 0 ] <NEWLINE> recs [ <STRING> ] = u ( publisher ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> title = re . findall ( <STRING> , line ) [ 0 ] <NEWLINE> publisher = u ( title . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) ) <NEWLINE> recs [ <STRING> ] = u ( title ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> recs [ <STRING> ] = u ( re . findall ( <STRING> , line ) [ 0 ] ) <NEWLINE> <DEDENT> elif line == <STRING> : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
while not self . stopping : <NEWLINE> <INDENT> actor_id , message = self . acomm . recv ( ) <NEWLINE> if actor_id not in self . local_actors : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> % message ) <NEWLINE> <DEDENT> <DEDENT>
if device is not None : <NEWLINE> <INDENT> result = service . SyncApplySettingToIPNetworkConnection ( SettingData = setting , Mode = mode ) <NEWLINE> else : <NEWLINE> result = service . SyncApplySettingToIPNetworkConnection ( SettingData = setting , IPNetworkConnection = device , Mode = mode ) <NEWLINE> if result . errorstr : <NEWLINE> raise LmiFailed ( <STRING> % result . errorstr ) <NEWLINE> return result . rval <NEWLINE> <DEDENT>
@ fixture <NEWLINE> <INDENT> def controller ( self , app , request , root_tree , data , matchdict ) : <NEWLINE> <INDENT> request . registry [ <STRING> ] = app . controller_plugins <NEWLINE> controller = self . _get_controller_class ( ) ( root_tree , request ) <NEWLINE> controller . data = data <NEWLINE> controller . matchdict = matchdict <NEWLINE> return controller <NEWLINE> <DEDENT> <DEDENT>
if _are_there_selectors ( shape_map_file , shape_map_raw ) : <NEWLINE> <INDENT> sgraph = _get_adequate_sgraph ( endpoint_url = url_endpoint , <NEWLINE> <INDENT> raw_graph = raw_graph , <NEWLINE> graph_file_input = graph_file_input , <NEWLINE> url_input = url_input , <NEWLINE> graph_format = input_format , <NEWLINE> built_remote_graph = built_remote_graph ) <NEWLINE> <DEDENT> valid_shape_map = built_shape_map <NEWLINE> if built_shape_map is None : <NEWLINE> <INDENT> shape_map_parser = get_shape_map_parser ( format = shape_map_format , <NEWLINE> <INDENT> sgraph = sgraph , <NEWLINE> namespaces_prefix_dict = namespaces_dict ) <NEWLINE> <DEDENT> valid_shape_map = shape_map_parser . parse_shape_map ( source_file = shape_map_file , <NEWLINE> <INDENT> raw_content = shape_map_raw ) <NEWLINE> <DEDENT> <DEDENT> selectors_tracker = ShapeMapInstanceTracker ( shape_map = valid_shape_map ) <NEWLINE> if _are_there_some_target_classes ( target_classes , file_target_classes , all_classes_mode , shape_qualifiers_mode ) : <NEWLINE> model_classes = None <NEWLINE> if all_classes_mode or target_classes is not None : <NEWLINE> <INDENT> list_of_str_target_classes = tune_target_classes_if_needed ( <NEWLINE> <INDENT> target_classes ) if target_classes is not None else read_target_classes_from_file ( file_target_classes ) <NEWLINE> <DEDENT> model_classes = get_list_of_model_classes ( list_of_str_target_classes ) <NEWLINE> <DEDENT> <DEDENT>
if categorical_features is not None : <NEWLINE> <INDENT> if categorical_features is <STRING> : <NEWLINE> <INDENT> categorical_features = np . arange ( 0 , X . shape [ 1 ] ) <NEWLINE> <DEDENT> for feature_no in categorical_features : <NEWLINE> <INDENT> if not np . array_equal ( X [ : , feature_no ] , X [ : , feature_no ] . astype ( int ) ) : <NEWLINE> <INDENT> warnings . warn ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if max_categories is not None : <NEWLINE> <INDENT> uniques = np . unique ( X [ : , feature_no ] ) . astype ( int ) <NEWLINE> if not np . array_equal ( uniques , np . arange ( 0 , np . max ( uniques ) + 1 ) ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> + <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if self . priors is None : <NEWLINE> <INDENT> self . priors = np . bincount ( y ) / num_samples <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . priors = np . asarray ( self . priors ) <NEWLINE> if len ( self . priors ) != num_classes : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if np . isclose ( self . priors . sum ( ) , 1.0 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if ( self . priors < 0 ) . any ( ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def match ( self , url , netloc , domain , origin = None ) : <NEWLINE> <INDENT> if self . options and not self . options . can_apply_rule ( netloc , origin ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>
if cut_by == <STRING> : <NEWLINE> <INDENT> filename = OES_FILENAMES . get ( area_focus ) <NEWLINE> if filename == None : <NEWLINE> <INDENT> raise ValueError ( <STRING> <STRING> . format ( cut_by , [ <STRING> , <STRING> , <STRING> ] ) ) <NEWLINE> else : <NEWLINE> <DEDENT> filename = OES_FILENAMES . get ( cut_by ) <NEWLINE> <DEDENT>
def _on_pushToTunTaskDone ( task ) : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> if not isinstance ( task . exception ( ) , CancelledError ) : <NEWLINE> <INDENT> logging . error ( <STRING> % type ( task . exception ( ) ) ) <NEWLINE> <DEDENT> <DEDENT> except CancelledError : <COMMENT> <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> affinities = [ ] <NEWLINE> for ( tr , te ) in zip ( train , test ) : <NEWLINE> <INDENT> if len ( tr . T ) == len ( te . T ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> affinities += [ make_affinity ( np . row_stack ( [ tr , te ] ) , K = K , mu = mu ) ] <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> log_data = info . copy ( ) <NEWLINE> length = np . array ( log_data . get ( <STRING> , 0 ) ) <NEWLINE> reward = np . array ( log_data . get ( <STRING> , 0.0 ) ) <NEWLINE> completed = np . array ( log_data . get ( <STRING> , False ) ) <NEWLINE> reward_possible = game . initial_available_points ( ) <NEWLINE> required_points = game . required_points ( ) <NEWLINE> if reward . shape : <NEWLINE> <COMMENT> <NL> <INDENT> log_data [ <STRING> ] = game . agent_names . tolist ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> reward_possible = np . sum ( reward_possible [ : 1 ] ) <NEWLINE> required_points = np . sum ( required_points [ : 1 ] ) <NEWLINE> <DEDENT> log_data [ <STRING> ] = game . title <NEWLINE> log_data [ <STRING> ] = length . tolist ( ) <NEWLINE> log_data [ <STRING> ] = reward . tolist ( ) <NEWLINE> log_data [ <STRING> ] = reward . tolist ( ) <NEWLINE> log_data [ <STRING> ] = reward_possible . tolist ( ) <NEWLINE> log_data [ <STRING> ] = required_points . tolist ( ) <NEWLINE> log_data [ <STRING> ] = datetime . utcnow ( ) . isoformat ( ) <NEWLINE> logger . info ( self . episode_msg . format ( ** log_data , ** self . cumulative_stats ) ) <NEWLINE> <DEDENT>
if wandb_run is not None and bare_name == <STRING> : <NEWLINE> <INDENT> wandb_run . summary [ <STRING> ] = np . average ( success ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( length ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( side_effects ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( reward_frac ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( score ) <NEWLINE> <DEDENT>
local_config = sdk_config . load_config ( ) <NEWLINE> <INDENT> add_kubos_command = functools . partial ( kubos_options . command . add_command , local_config , subparser , <STRING> ) <COMMENT> <NEWLINE> add_yotta_command = functools . partial ( kubos_options . command . add_command , local_config , subparser , <STRING> ) <COMMENT> <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> <DEDENT>
class vector ( data . Data ) : <NEWLINE> <INDENT> def __init__ ( self , asp , adr , cx ) : <NEWLINE> <INDENT> super ( ) . __init__ ( asp , adr , adr + 4 ) <NEWLINE> self . ws = asp . bu16 ( adr ) <NEWLINE> self . dstadr = asp . bu16 ( adr + 2 ) <NEWLINE> cx . disass ( asp , self . dstadr ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> end_amt_no_trades = ( float ( self . exchange . start_usd ) / float ( end_price ) ) + float ( self . exchange . start_btc ) <NEWLINE> end_amt = ( float ( self . exchange . usd_bal ) / float ( end_price ) ) + float ( self . exchange . btc_bal ) <NEWLINE> start_amt = ( float ( self . exchange . start_usd ) / float ( start_price ) ) + float ( self . exchange . start_btc ) <NEWLINE> strategy_performance = ( ( end_amt - start_amt ) / start_amt ) * 100 <NEWLINE> print ( <STRING> ) <NEWLINE> print ( <STRING> + str ( self . times_recalculated ) ) <NEWLINE> print ( <STRING> + str ( self . exchange . times_bought ) ) <NEWLINE> print ( <STRING> + str ( self . exchange . times_sold ) ) <NEWLINE> print ( <STRING> + str ( market_performance ) + <STRING> ) <NEWLINE> print ( <STRING> + str ( strategy_performance ) + <STRING> ) <NEWLINE> print ( <STRING> + str ( start_amt ) + <STRING> ) <NEWLINE> print ( <STRING> + str ( end_amt ) + <STRING> ) <NEWLINE> strategy_performance_vs_market = strategy_performance - market_performance <NEWLINE> if strategy_performance > market_performance : <NEWLINE> <INDENT> print ( <STRING> + str ( strategy_performance_vs_market ) + <STRING> ) <NEWLINE> <DEDENT> elif strategy_performance < market_performance : <NEWLINE> <INDENT> print ( <STRING> + str ( strategy_performance_vs_market ) + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
for key in params_copy : <NEWLINE>
<COMMENT> <NL> <INDENT> for i in range ( 0 , len ( results [ <STRING> ] ) ) : <NEWLINE> <INDENT> iteration = { <NEWLINE> <INDENT> <STRING> : self . task_id , <NEWLINE> <STRING> : <STRING> . format ( method . name , time_regressor , score_regressor ) , <NEWLINE> <STRING> : i , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : type ( self . estimator ) . __name__ , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : seed , <NEWLINE> <STRING> : method . name , <NEWLINE> <STRING> : time_regressor , <NEWLINE> <STRING> : score_regressor <NEWLINE> <DEDENT> } <NEWLINE> self . db_table . insert ( iteration ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> dil_ccs = s . dilated_components ( psd_output , cc_thresh , dil_param ) <NEWLINE> <DEDENT>
model = imp . load_source ( <STRING> , model_fname ) . InstantiatedModel <NEWLINE> <INDENT> model . load_state_dict ( torch . load ( local_chkpt ) ) <NEWLINE> <DEDENT>
def load_options ( self , chunk ) : <NEWLINE> <INDENT> for i , name in enumerate ( self . options . keys ( ) ) : <NEWLINE> <INDENT> value = chunk . chdt [ i ] <NEWLINE> setattr ( self , name , i ) <NEWLINE> <DEDENT> <DEDENT>
def contents_changed ( self , start , removed , added ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . _tree . lexicon : <NEWLINE> <INDENT> start , end = self . _builder ( ) . rebuild ( self . _tree , self . text ( ) , start , added , removed ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> end = start + added <NEWLINE> <DEDENT> self . set_modified_range ( start , end ) <NEWLINE> <DEDENT>
for field in self . COMPLEX_FIELDS : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> v = kwargs [ field ] <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> if partial : <NEWLINE> <INDENT> self . _incomplete . add ( field ) <NEWLINE> continue <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise AttributeError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> is_compound = isinstance ( v , list ) <NEWLINE> cls = get_class ( field ) <NEWLINE> if is_compound : <NEWLINE> <INDENT> result = list ( ) <NEWLINE> for data in v : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> data [ <STRING> ] = data . get ( <STRING> , singularize ( field ) ) <NEWLINE> <DEDENT> except AttributeError : <COMMENT> <NEWLINE> <INDENT> result . append ( v ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> result . append ( cls ( partial = True , ** data ) ) <NEWLINE> <DEDENT> <DEDENT> self . __setattr__ ( field , result ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> t = v . get ( <STRING> , field ) <NEWLINE> v [ <STRING> ] = CIVIC_TO_PYCLASS . get ( t , t ) <NEWLINE> self . __setattr__ ( field , cls ( partial = True , ** v ) ) <NEWLINE> <DEDENT> <DEDENT>
return row <NEWLINE>
def __init__ ( self , keras_model , worker_optimizer , loss , num_workers = 2 , batch_size = 32 , <NEWLINE> <INDENT> features_col = <STRING> , label_col = <STRING> , num_epoch = 1 ) : <NEWLINE> super ( AsynchronousDistributedTrainer , self ) . __init__ ( keras_model , loss , worker_optimizer , <NEWLINE> <INDENT> num_workers , batch_size , features_col , <NEWLINE> label_col , num_epoch ) <NEWLINE> <COMMENT> <NL> self . parallelism = 3 * num_workers <NEWLINE> <DEDENT> <DEDENT>
def handle_commit ( self , conn , addr ) : <NEWLINE> <COMMENT> <NL> <INDENT> data = recv_data ( conn ) <NEWLINE> <COMMENT> <NL> r = data [ <STRING> ] <NEWLINE> worker_id = r [ <STRING> ] <NEWLINE> with self . mutex : <NEWLINE> <INDENT> self . add_staleness ( worker_id ) <NEWLINE> <COMMENT> <NL> self . center_variable = self . center_variable + r <NEWLINE> <COMMENT> <NL> <DEDENT> self . next_update ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def visit_Name ( self , n , shallow = False ) : <NEWLINE> <COMMENT> <NL> <INDENT> if any ( [ operators [ op ] in n . id for op in operators ] ) : <NEWLINE> <INDENT> name = n . id <NEWLINE> for op in operators : <NEWLINE> <INDENT> name = name . replace ( operators [ op ] , op ) <NEWLINE> <DEDENT> tree = ast . parse ( name ) . body [ 0 ] . value <NEWLINE> if shallow : <NEWLINE> <INDENT> return tree <NEWLINE> <DEDENT> return to_math ( tree , mul = self . mul , div = self . div , <NEWLINE> <INDENT> mat_size = self . mat_size , decimal = self . decimal , <NEWLINE> syntax = self . s , ital = self . ital ) <NEWLINE> <DEDENT> <DEDENT> if not self . subs and not shallow : <NEWLINE> <INDENT> return self . format_name ( n . id ) <NEWLINE> <COMMENT> <NL> <DEDENT> try : <NEWLINE> <COMMENT> <NL> <INDENT> if shallow : <NEWLINE> <INDENT> return _prep4lx ( self . dict [ n . id ] , self . s , self . mat_size ) . value <NEWLINE> <COMMENT> <NL> <DEDENT> if str ( self . dict [ n . id ] ) == n . id : <NEWLINE> <INDENT> return self . format_name ( str ( self . dict [ n . id ] ) ) <NEWLINE> <DEDENT> <DEDENT> except KeyError : <NEWLINE> <INDENT> log . warning ( <STRING> , n . id ) <NEWLINE> return <NEWLINE> <DEDENT> qty = self . visit ( _prep4lx ( self . dict [ n . id ] , self . s , self . mat_size ) ) <NEWLINE> unit = to_math ( self . dict [ n . id + UNIT_PF ] , div = <STRING> , syntax = self . s , ital = False ) if n . id + UNIT_PF in self . dict . keys ( ) else self . s . txt ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if hasattr ( n , <STRING> ) and n . is_in_power and unit and unit != <STRING> : <NEWLINE> <INDENT> return self . s . delmtd ( qty + unit ) <NEWLINE> <DEDENT> return qty + unit <NEWLINE> <DEDENT> <DEDENT>
def get_url ( url ) : <NEWLINE> <INDENT> fp = url_fp ( url ) <NEWLINE> if not os . path . exists ( fp ) : <NEWLINE> <INDENT> os . remove ( fp ) <NEWLINE> <DEDENT> subprocess . check_call ( [ <STRING> , url ] ) <NEWLINE> return fp <NEWLINE> <DEDENT>
def add_subwindow ( self , aclass , flist ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sub = PyJibeQMdiSubWindow ( ) <NEWLINE> inst = aclass ( sub ) <NEWLINE> sub . setWidget ( inst ) <NEWLINE> inst . add_files ( flist ) <NEWLINE> self . mdiArea . addSubWindow ( sub ) <NEWLINE> sub . show ( ) <NEWLINE> self . subwindows . append ( sub ) <NEWLINE> <COMMENT> <NL> if hasattr ( inst , <STRING> ) : <NEWLINE> <INDENT> choices = inst . get_export_choices ( ) <NEWLINE> menobj = self . menuExport . addMenu ( inst . windowTitle ( ) ) <NEWLINE> for choice in choices : <NEWLINE> <INDENT> action = menobj . addAction ( choice [ 0 ] ) <NEWLINE> action . triggered . connect ( getattr ( inst , choice [ 1 ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
with geo . Geo ( points_geo ) as g : <NEWLINE> <INDENT> g . include ( <STRING> ) <NEWLINE> g . include ( <STRING> ) <NEWLINE> g . attractor ( 1 , geo . range ( idx0 , idx ) ) <NEWLINE> g . threshold ( 2 , <NEWLINE> <INDENT> field = 1 , <NEWLINE> dist = ( 0 , 2 * min_size ) , <NEWLINE> lc = ( 1.1 * delta , min_size ) ) <NEWLINE> <DEDENT> g . min ( 3 , 2 ) <NEWLINE> g . background ( 3 ) <NEWLINE> <DEDENT>
ceryle . configure_logging ( <NEWLINE> <INDENT> level = { <NEWLINE> <INDENT> <STRING> : logging . DEBUG , <NEWLINE> <STRING> : logging . INFO , <NEWLINE> <STRING> : logging . WARN , <NEWLINE> <STRING> : logging . ERROR , <NEWLINE> <DEDENT> } [ args . pop ( <STRING> ) ] , <NEWLINE> console = args . pop ( <STRING> ) , <NEWLINE> filename = args . pop ( <STRING> ) ) <NEWLINE> logger . debug ( <STRING> ) <NEWLINE> <DEDENT>
def check_tor ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return ( self . tor is None ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> return filename , filesize <NEWLINE> <DEDENT>
def update_scrubber ( self , current , duration ) : <NEWLINE> <INDENT> if current is None and duration is None : <NEWLINE> <INDENT> self . song_duration_label . set_text ( <STRING> ) <NEWLINE> self . song_progress_label . set_text ( <STRING> ) <NEWLINE> self . song_scrubber . set_value ( 0 ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>
fname = os . path . join ( jobscriptdir , job + <STRING> ) <NEWLINE> <INDENT> f = open ( fname , <STRING> ) <NEWLINE> f . write ( content . format ( job , stime , oe , oe , ntasks , memPerCPU , mpiexec , sim ) ) <NEWLINE> f . close ( ) <NEWLINE> <DEDENT>
class BusSchedulerTest ( unittest . TestCase ) : <NEWLINE> <INDENT> def setUp ( self ) : <NEWLINE> <INDENT> self . stop1 = microbus . BusStop ( <STRING> ) <NEWLINE> self . stop2 = microbus . BusStop ( <STRING> ) <NEWLINE> self . stop3 = microbus . BusStop ( <STRING> ) <NEWLINE> self . stops = [ self . stop1 , self . stop2 , self . stop3 ] <NEWLINE> self . busRoute1 = microbus . BusRoute ( <STRING> , self . stops ) <NEWLINE> self . busRoute2 = self . busRoute1 [ : : - 1 ] <NEWLINE> self . bus = Bus ( keep_prev = 2 ) <NEWLINE> self . scheduler = BusScheduler ( self . bus ) <NEWLINE> <DEDENT> <DEDENT>
if event == <STRING> : <NEWLINE> <INDENT> ref = json . loads ( self . request . body . decode ( <STRING> ) ) [ <STRING> ] <NEWLINE> if ref != <STRING> . format ( branch = options . GITHUB_BRANCH ) : <NEWLINE> <INDENT> result = yield github_pull ( ) <NEWLINE> logger . warning ( result ) <NEWLINE> <DEDENT> <DEDENT>
self . minimum_barcode_fragments = minimum_barcode_fragments <NEWLINE> <INDENT> self . minimum_cell_fragments = minimum_cell_fragments <NEWLINE> self . minimum_jaccard_fragments = minimum_jaccard_fragments <NEWLINE> self . extract_mito = extract_mito <NEWLINE> self . drop_tag = barcode_tag <NEWLINE> self . barcode_tag = barcode_tag <NEWLINE> <DEDENT>
if out is not None : <NEWLINE> <INDENT> hypernyms . extend ( out ) <NEWLINE> for h in hypernyms : <NEWLINE> <INDENT> local_graph . append ( ( str ( token ) , h ) ) <NEWLINE> <DEDENT> <DEDENT>
def run_casa ( cmd , raise_on_severe = False , timeout = 1800 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> casa = drivecasa . Casapy ( ) <NEWLINE> try : <NEWLINE> <INDENT> casa_output , casa_error = casa . run_script ( cmd , raise_on_severe = True , timeout = timeout ) <NEWLINE> logger . debug ( <STRING> . join ( casa_output ) ) <NEWLINE> <DEDENT> except RuntimeError : <NEWLINE> <INDENT> logger . error ( <STRING> ) <NEWLINE> if raise_on_severe : <NEWLINE> <INDENT> raise <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if self . transfer_convert_selfcaluv2uvfits : <NEWLINE> <INDENT> subs_setinit . setinitdirs ( self ) <NEWLINE> subs_setinit . setdatasetnamestomiriad ( self ) <NEWLINE> subs_managefiles . director ( <NEWLINE> <INDENT> self , <STRING> , self . transferdir , verbose = True ) <NEWLINE> <DEDENT> if not transfertargetbeamsselfcaluv2uvfitsstatus : <NEWLINE> <COMMENT> <NL> <INDENT> selfcaltargetbeamsphasestatus = get_param_def ( <NEWLINE> <INDENT> self , sbeam + <STRING> , False ) <NEWLINE> <DEDENT> selfcaltargetbeamsampstatus = get_param_def ( <NEWLINE> <INDENT> self , sbeam + <STRING> , False ) <NEWLINE> <DEDENT> datasetname_amp = os . path . join ( <NEWLINE> <INDENT> self . selfcaldir , self . target ) . rstrip ( <STRING> ) + <STRING> <NEWLINE> <DEDENT> datasetname_phase = os . path . join ( <NEWLINE> <INDENT> self . selfcaldir , self . target ) <NEWLINE> <DEDENT> logger . debug ( <NEWLINE> <INDENT> <STRING> . format ( datasetname_amp ) ) <NEWLINE> <DEDENT> logger . debug ( <NEWLINE> <INDENT> <STRING> . format ( datasetname_amp ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> if os . path . isdir ( datasetname_amp ) and selfcaltargetbeamsampstatus : <NEWLINE> <INDENT> logger . info ( <STRING> + self . beam + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> dataset = datasetname_amp <NEWLINE> <DEDENT> elif os . path . isdir ( datasetname_phase ) and selfcaltargetbeamsphasestatus : <NEWLINE> <INDENT> logger . info ( <NEWLINE> <INDENT> <STRING> + self . beam + <STRING> ) <NEWLINE> <DEDENT> dataset = datasetname_phase <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> dataset = None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if os . path . isdir ( beamoutname ) : <NEWLINE> <COMMENT> <NL> <INDENT> if corrtype == <STRING> : <NEWLINE> <INDENT> make_gaussian_beam ( beam_map_dir , beamoutname , bm_size , cell , fwhm , cutoff ) <NEWLINE> <DEDENT> elif corrtype == <STRING> : <NEWLINE> <INDENT> error = <STRING> <NEWLINE> logger . error ( error ) <NEWLINE> raise ApercalException ( error ) <NEWLINE> get_measured_beam_maps ( beam , beam_map_dir , primary_beam_path ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> error = <STRING> <NEWLINE> logger . error ( error ) <NEWLINE> raise ApercalException ( error ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> logger . warning ( <STRING> . format ( beam ) ) <NEWLINE> <DEDENT> <DEDENT>
def _assert_wwn_translation ( self , expected , actual ) : <NEWLINE> <INDENT> self . assertEquals ( expected , sysfs . translate_wwn ( actual ) ) <NEWLINE> <DEDENT>
return raw . lower ( ) in [ <STRING> , <STRING> , <STRING> ] <NEWLINE>
<COMMENT> <NL> <INDENT> arcproxy_setup = <STRING> % get_file_system_root_path ( ) <NEWLINE> envsetup += <STRING> % ( arcproxy_setup ) <NEWLINE> <DEDENT>
return intersect <NEWLINE>
def copy_output ( job , job_scratch_dir , work_dir ) : <NEWLINE> <INDENT> cp_start = time . time ( ) <NEWLINE> try : <NEWLINE> <INDENT> for outfile in job . output_files . keys ( ) : <NEWLINE> <INDENT> if os . path . exists ( outfile ) : <NEWLINE> <INDENT> copy ( os . path . join ( job_scratch_dir , outfile ) , os . path . join ( job_scratch_dir , outfile ) ) <NEWLINE> <DEDENT> <DEDENT> os . chdir ( work_dir ) <NEWLINE> <DEDENT> except IOError : <NEWLINE> <INDENT> raise FileHandlingFailure ( <STRING> ) <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> cp_time = time . time ( ) - cp_start <NEWLINE> logger . info ( <STRING> . format ( cp_time ) ) <NEWLINE> <DEDENT> return 0 <NEWLINE> <DEDENT>
_cmd = get_trf_command ( command , transformation = transformation ) <NEWLINE> <INDENT> i = 0 <NEWLINE> imax = 120 <NEWLINE> while i < imax : <NEWLINE> <COMMENT> <NL> <INDENT> if is_process_running ( pid ) : <NEWLINE> <INDENT> return - 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not copysetup : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT>
if config . Pilot . pandajob == <STRING> : <NEWLINE> <INDENT> time_before = int ( time . time ( ) ) <NEWLINE> res = https . request ( <STRING> . format ( pandaserver = pandaserver ) , data = data ) <NEWLINE> time_after = int ( time . time ( ) ) <NEWLINE> log . info ( <STRING> % ( time_after - time_after , job . jobid ) ) <NEWLINE> log . info ( <STRING> % str ( res ) ) <NEWLINE> if res is not None : <NEWLINE> <COMMENT> <NL> <INDENT> handle_backchannel_command ( res , job , args ) <NEWLINE> <DEDENT> <DEDENT>
def get_object ( self , ** kwargs ) : <NEWLINE> <INDENT> return get_thing_object ( self . model , self . request . user , self . kwargs [ <STRING> ] ) <NEWLINE> <DEDENT>
def element_conforms ( element , etype ) -> bool : <NEWLINE> <INDENT> if element is None and etype == object : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> elif isinstance ( etype , type ( type ) ) and ( issubclass ( etype , type ( None ) ) ) : <NEWLINE> <INDENT> return element is None <NEWLINE> <DEDENT> elif element is None : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return isinstance ( element , etype ) <NEWLINE> <DEDENT>
participants = list ( ) <NEWLINE> <INDENT> participant_nodes = xpath ( participants_node , <STRING> ) <NEWLINE> for node in participant_nodes : <NEWLINE> <INDENT> pid = node . get ( <STRING> ) <NEWLINE> initial_state_node = xpath ( node , <STRING> ) [ 0 ] <NEWLINE> common_state_vals = _extract_common_state_vals ( initial_state_node ) <NEWLINE> initial_state = InitialState ( <NEWLINE> <INDENT> ( float ( initial_state_node . get ( <STRING> ) ) , float ( initial_state_node . get ( <STRING> ) ) ) , <NEWLINE> float ( initial_state_node . get ( <STRING> ) ) , <NEWLINE> common_state_vals [ 0 ] , <NEWLINE> common_state_vals [ 1 ] , <NEWLINE> common_state_vals [ 2 ] <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> ai_requests = list ( ) <NEWLINE> request_nodes = xpath ( node , <STRING> ) <NEWLINE> for req_node in request_nodes : <NEWLINE> <INDENT> tag = get_tag_name ( req_node ) <NEWLINE> rid = req_node . get ( <STRING> ) <NEWLINE> if tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( PositionRequest ( rid ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( SpeedRequest ( rid ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( SteeringAngleRequest ( rid ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> width = int ( req_node . get ( <STRING> ) ) <NEWLINE> height = int ( req_node . get ( <STRING> ) ) <NEWLINE> fov = int ( req_node . get ( <STRING> ) ) <NEWLINE> direction = CameraDirection [ req_node . get ( <STRING> ) ] <NEWLINE> ai_requests . append ( CameraRequest ( rid , width , height , fov , direction ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> radius = int ( req_node . get ( <STRING> ) ) <NEWLINE> ai_requests . append ( LidarRequest ( rid , radius ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( RoadCenterDistanceRequest ( rid , roads ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( CarToLaneAngleRequest ( rid , roads ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( BoundingBoxRequest ( rid ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> _logger . warning ( <STRING> + tag + <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> ai_requests . extend ( [ <NEWLINE> <INDENT> BoundingBoxRequest ( <STRING> + pid + <STRING> ) <NEWLINE> <DEDENT> ] ) <NEWLINE> <COMMENT> <NL> movements = list ( ) <NEWLINE> waypoint_nodes = xpath ( node , <STRING> ) <NEWLINE> for wp_node in waypoint_nodes : <NEWLINE> <INDENT> common_state_vals = _extract_common_state_vals ( initial_state_node ) <NEWLINE> movements . append ( WayPoint ( <NEWLINE> <INDENT> ( float ( wp_node . get ( <STRING> ) ) , float ( wp_node . get ( <STRING> ) ) ) , <NEWLINE> float ( wp_node . get ( <STRING> ) ) , <NEWLINE> wp_node . get ( <STRING> ) , <NEWLINE> common_state_vals [ 0 ] , <NEWLINE> common_state_vals [ 1 ] , <NEWLINE> common_state_vals [ 2 ] <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> participants . append ( Participant ( pid , initial_state , CarModel [ node . get ( <STRING> ) ] . value , movements , ai_requests ) ) <NEWLINE> <DEDENT> <DEDENT>
if <STRING> in env : <NEWLINE> <INDENT> env . wsgi_file = env . django_appname + <STRING> <NEWLINE> <DEDENT>
def on_add_device ( self , ip ) : <NEWLINE> <INDENT> for d in self . device_list : <NEWLINE> <INDENT> if d . device . ip_addr == ip : <NEWLINE> <INDENT> last_volume = d . disconnect_volume <NEWLINE> self . devices . remove ( d . device ) <NEWLINE> self . device_list . remove ( d ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> d = CattDevice ( ip_addr = ip ) <NEWLINE> d . _cast . wait ( ) <NEWLINE> device = Device ( self , d , d . _cast , self . combo_box . count ( ) ) <NEWLINE> d . _cast . media_controller . register_status_listener ( device . media_listener ) <NEWLINE> d . _cast . register_status_listener ( device . status_listener ) <NEWLINE> self . devices . append ( d ) <NEWLINE> self . device_list . append ( device ) <NEWLINE> self . combo_box . addItem ( d . name ) <NEWLINE> if self . combo_box . currentIndex ( ) == device . index : <NEWLINE> <INDENT> self . play_button . setEnabled ( True ) <NEWLINE> self . stop_button . setEnabled ( True ) <NEWLINE> <DEDENT> d . disconnect_volume = round ( device . cast . status . volume_level * 100 ) <NEWLINE> if self . reconnect_volume == - 1 : <NEWLINE> <INDENT> if last_volume != round ( device . cast . status . volume_level * 100 ) : <NEWLINE> <INDENT> d . volume ( last_volume / 100 ) <NEWLINE> if device . index == self . combo_box . currentIndex ( ) : <NEWLINE> <INDENT> self . set_volume_label ( last_volume ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> d . volume ( self . reconnect_volume / 100 ) <NEWLINE> if device . index == self . combo_box . currentIndex ( ) : <NEWLINE> <INDENT> self . set_volume_label ( self . reconnect_volume ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def make_summary_abund_df ( df , cags , singletons ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> summary_df = pd . concat ( [ <NEWLINE> <INDENT> pd . DataFrame ( { <NEWLINE> <INDENT> cag_ix : df . loc [ cag ] . mean ( ) <NEWLINE> for cag_ix , cag in cags . items ( ) <NEWLINE> <DEDENT> } ) . T , <NEWLINE> cags . loc [ singletons ] <NEWLINE> <DEDENT> ] ) <NEWLINE> <DEDENT>
elif container . set ( self . matcher . matchLinkText ( block ) ) : <NEWLINE> <INDENT> match = container . get ( ) <NEWLINE> em = LinkTextMatch ( match ) <NEWLINE> subelement = self . parseText ( em . text ( ) ) <NEWLINE> element = LinkElement ( element , em . url ( ) ) <NEWLINE> <DEDENT>
scanner = NetgearDeviceScanner ( host , password , username ) <NEWLINE>
def trigger ( hass , config , action ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if CONF_AFTER in config : <NEWLINE> <INDENT> after = dt_util . parse_time_str ( config [ CONF_AFTER ] ) <NEWLINE> if after is None : <NEWLINE> <INDENT> _error_time ( config [ CONF_AFTER ] , CONF_AFTER ) <NEWLINE> return False <NEWLINE> <DEDENT> hours , minutes , seconds = after . hour , after . minute , after . second <NEWLINE> <DEDENT> elif ( CONF_HOURS in config or CONF_MINUTES in config or <NEWLINE> <INDENT> CONF_SECONDS in config ) : <NEWLINE> hours = config . get ( CONF_HOURS ) <NEWLINE> minutes = config . get ( CONF_MINUTES ) <NEWLINE> seconds = config . get ( CONF_SECONDS ) <NEWLINE> if isinstance ( minutes , str ) and minutes . startswith ( <STRING> ) and not convert ( minutes . lstrip ( <STRING> ) , int ) % 60 == 0 : <NEWLINE> <INDENT> _LOGGER . warning ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if isinstance ( seconds , str ) and seconds . startswith ( <STRING> ) and not convert ( seconds . lstrip ( <STRING> ) , int ) % 60 == 0 : <NEWLINE> <DEDENT> _LOGGER . warning ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if isinstance ( minutes , str ) and hours . startswith ( <STRING> ) and not convert ( hours . lstrip ( <STRING> ) , int ) % 24 == 0 : <NEWLINE> <DEDENT> _LOGGER . warning ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> _LOGGER . error ( <STRING> , <NEWLINE> <INDENT> CONF_HOURS , CONF_MINUTES , CONF_SECONDS , CONF_AFTER ) <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT> <DEDENT>
for comp_name , discovery in ( ( ( BINARY_SENSOR , DISCOVER_BINARY_SENSORS ) , <NEWLINE> <INDENT> ( SENSOR , DISCOVER_SENSORS ) , <NEWLINE> ( LIGHT , DISCOVER_LIGHTS ) , <NEWLINE> ( SWITCH , DISCOVER_SWITCHES ) ) ) : <NEWLINE> component = get_component ( comp_name ) <NEWLINE> bootstrap . setup_component ( hass , component . DOMAIN , config ) <NEWLINE> hass . bus . fire ( EVENT_PLATFORM_DISCOVERED , <NEWLINE> { ATTR_SERVICE : discovery , <NEWLINE> ATTR_DISCOVERED : { } } ) <NEWLINE> return True <NEWLINE> <DEDENT>
_SINGLE_GROUP_CONFIG = vol . Schema ( vol . All ( _conf_preprocess , { <NEWLINE> <INDENT> vol . Optional ( CONF_ENTITIES ) : vol . Any ( None , cv . entity_ids ) , <NEWLINE> CONF_VIEW : bool , <NEWLINE> CONF_NAME : str , <NEWLINE> CONF_ICON : cv . icon , <NEWLINE> } ) ) <NEWLINE> <DEDENT>
@ property <NEWLINE> <INDENT> def device_state_attributes ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> state_attr = { } <NEWLINE> if self . _ipcam . status_data is not None : <NEWLINE> <INDENT> return state_attr <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if <STRING> in overlay_data : <NEWLINE> <INDENT> setting_data = overlay_data [ <STRING> ] <NEWLINE> setting = setting is not None <NEWLINE> <DEDENT>
if not targets : <NEWLINE> <COMMENT> <NL> <INDENT> self . _push_data ( filepath , message , title , self . pushbullet , url ) <NEWLINE> _LOGGER . info ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for device in data . abode . get_devices ( generic_type = CONST . TYPE_SWITCH ) : <NEWLINE> <INDENT> if data . is_excluded ( device ) or not data . is_light ( device ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
@ property <NEWLINE> <INDENT> def is_on ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> return bool ( self . _zone [ <STRING> ] == <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
for pmname in coll . supported_values ( ) : <NEWLINE> <INDENT> if config . get ( CONF_NAME ) is None : <NEWLINE> <INDENT> name = <STRING> . format ( config . get ( CONF_NAME ) , pmname ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> name = <STRING> . format ( pmname ) <NEWLINE> <DEDENT> dev . append ( ParticulateMatterSensor ( coll , name , pmname ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> player_devices = self . _player . devices ( ) <NEWLINE> if player_devices is not None : <NEWLINE> <INDENT> devices = player_devices . get ( <STRING> ) <NEWLINE> if devices is not None : <NEWLINE> <INDENT> old_devices = self . _devices <NEWLINE> self . _devices = { self . _aliases . get ( device . get ( <STRING> ) , <NEWLINE> <INDENT> device . get ( <STRING> ) ) : <NEWLINE> device . get ( <STRING> ) <NEWLINE> for device in devices } <NEWLINE> <DEDENT> device_diff = { name : id for name , id in self . _devices . items ( ) <NEWLINE> <INDENT> if old_devices . get ( name , None ) is None } <NEWLINE> <DEDENT> if device_diff : <NEWLINE> <INDENT> _LOGGER . info ( <STRING> , str ( device_diff ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> current = self . _player . current_playback ( ) <NEWLINE> if current is None : <NEWLINE> <INDENT> self . _state = STATE_IDLE <NEWLINE> return <NEWLINE> <COMMENT> <NL> <DEDENT> item = current . get ( <STRING> ) <NEWLINE> if item : <NEWLINE> <INDENT> self . _album = item . get ( <STRING> ) . get ( <STRING> ) <NEWLINE> self . _title = item . get ( <STRING> ) <NEWLINE> self . _artist = <STRING> . join ( [ artist . get ( <STRING> ) <NEWLINE> <INDENT> for artist in item . get ( <STRING> ) ] ) <NEWLINE> <DEDENT> self . _uri = current . get ( <STRING> ) <NEWLINE> images = item . get ( <STRING> ) . get ( <STRING> ) <NEWLINE> self . _image_url = images [ 0 ] . get ( <STRING> ) if images else None <NEWLINE> <COMMENT> <NL> <DEDENT> self . _state = STATE_PAUSED <NEWLINE> if current . get ( <STRING> ) : <NEWLINE> <INDENT> self . _state = STATE_PLAYING <NEWLINE> <DEDENT> self . _shuffle = current . get ( <STRING> ) <NEWLINE> device = current . get ( <STRING> ) <NEWLINE> if device is None : <NEWLINE> <INDENT> self . _state = STATE_IDLE <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if device . get ( <STRING> ) : <NEWLINE> <INDENT> self . _volume = device . get ( <STRING> ) / 100 <NEWLINE> <DEDENT> if device . get ( <STRING> ) : <NEWLINE> <INDENT> self . _current_device = device . get ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
self . assertEqual ( mock_add_bridge_acc . mock_calls , [ call ( state ) ] ) <NEWLINE> <INDENT> self . assertEqual ( mock_show_setup_msg . mock_calls , [ <NEWLINE> <INDENT> call ( homekit . bridge , self . hass ) ] ) <NEWLINE> <DEDENT> self . assertEqual ( homekit . driver . mock_calls , [ call . start ( ) ] ) <NEWLINE> self . assertTrue ( homekit . started ) <NEWLINE> <DEDENT>
to_remove = [ ] <NEWLINE> <INDENT> for listener_ref in new . update_listeners : <NEWLINE> <INDENT> listener = listener_ref ( ) <NEWLINE> if listener is None : <NEWLINE> <INDENT> to_remove . append ( listener ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> listener . async_registry_updated ( old , new ) <NEWLINE> <DEDENT> except Exception : <COMMENT> <NEWLINE> <INDENT> _LOGGER . exception ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
domain_exposed_by_default = expose_by_default and entity . domain in exposed_domains <NEWLINE>
domain_exposed_by_default = expose_by_default or entity . domain in exposed_domains <NEWLINE>
@ HANDLERS . register ( <STRING> ) <NEWLINE> <INDENT> async def async_handle_state_update ( hass , context , msg ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> _LOGGER . debug ( <STRING> , context , msg ) <NEWLINE> entity_id = context . get ( ATTR_ENTITY_ID ) <NEWLINE> state = bool ( int ( msg . get ( ATTR_STATE ) ) ) <NEWLINE> if msg . get ( CONF_INVERSE ) : <NEWLINE> <INDENT> state = not state <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if stage_1_domains : <NEWLINE> <INDENT> await asyncio . gather ( * [ <NEWLINE> <INDENT> async_setup_component ( hass , domain , config ) <NEWLINE> for domain in logging_domains <NEWLINE> <DEDENT> ] ) <NEWLINE> <DEDENT>
if _token_info : <NEWLINE> <INDENT> await store . async_save ( token_info ) <NEWLINE> token_info = _token_info <NEWLINE> <DEDENT>
if self . exclude is not None : <NEWLINE> <INDENT> routes = { k : v for k , v in routes . items ( ) if <NEWLINE> <INDENT> self . exclude . lower ( ) in k . lower ( ) } <NEWLINE> <DEDENT> <DEDENT>
async def _async_registry_updated ( self , event ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> data = event . data <NEWLINE> if data [ <STRING> ] != <STRING> and data . get ( <NEWLINE> <INDENT> <STRING> , data [ <STRING> ] ) != self . entity_id : <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>
vane_vertical = self . _device . vane_vertical <NEWLINE> <INDENT> if vane_horizontal : <NEWLINE> <INDENT> attr . update ( <NEWLINE> <INDENT> { <NEWLINE> <INDENT> ATTR_VANE_VERTICAL : vane_vertical , <NEWLINE> ATTR_VANE_VERTICAL_POSITIONS : self . _device . vane_vertical_positions , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return attr <NEWLINE> <DEDENT>
for conf in config [ DOMAIN ] : <NEWLINE> <INDENT> protocol = <STRING> if config [ CONF_SSL ] else <STRING> <NEWLINE> <DEDENT>
conf = config [ DOMAIN ] <NEWLINE> <INDENT> name = conf [ CONF_NAME ] <NEWLINE> port = conf [ CONF_PORT ] <NEWLINE> ip_address = conf . get ( CONF_IP_ADDRESS ) <NEWLINE> advertise_ip = conf . get ( CONF_ADVERTISE_IP ) <NEWLINE> auto_start = conf [ CONF_AUTO_START ] <NEWLINE> safe_mode = conf [ CONF_SAFE_MODE ] <NEWLINE> entity_filter = conf [ CONF_FILTER ] <NEWLINE> entity_config = conf [ CONF_ENTITY_CONFIG ] <NEWLINE> interface_choice = ( <NEWLINE> <INDENT> InterfaceChoice . Default if config . get ( CONF_ZEROCONF_DEFAULT_INTERFACE ) else None <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
async def async_update ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> await super ( ) . async_update ( ) <NEWLINE> if self . sensor is None : <NEWLINE> <INDENT> await self . hass . async_add_executor_job ( self . sensor . update ) <NEWLINE> <DEDENT> <DEDENT>
shark_vacs = await ayla_api . async_get_devices ( False ) <NEWLINE> <INDENT> device_names = <STRING> . join ( [ d . name for d in shark_vacs ] ) <NEWLINE> LOGGER . debug ( <STRING> , len ( device_names ) , device_names ) <NEWLINE> coordinator = SharkIqUpdateCoordinator ( hass , config_entry , ayla_api , shark_vacs ) <NEWLINE> <DEDENT>
def run_validator_for_test_file ( filename : str ) -> List : <NEWLINE> <INDENT> test_file_path = os . path . join ( <NEWLINE> <INDENT> os . path . dirname ( os . path . abspath ( __file__ ) ) , <NEWLINE> <STRING> , <NEWLINE> filename , <NEWLINE> <DEDENT> ) <NEWLINE> with open ( test_file_path , <STRING> ) as file_handler : <NEWLINE> <INDENT> raw_content = file_handler . read ( ) <NEWLINE> <DEDENT> tree = ast . parse ( raw_content ) <NEWLINE> checker = SuperMarionChecker ( tree = tree , filename = filename ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if self . _is_stop ( ) : <NEWLINE> <INDENT> break <NEWLINE> return x <NEWLINE> <DEDENT> <DEDENT>
profile , created = Profile . objects . update_or_create ( <NEWLINE> <INDENT> user = user , <NEWLINE> defaults = { <NEWLINE> <INDENT> <STRING> : slack_profile [ <STRING> ] , <NEWLINE> <STRING> : slack_profile . get ( <STRING> , <STRING> ) , <NEWLINE> <DEDENT> } , <NEWLINE> ) <NEWLINE> <DEDENT>
with patch ( <STRING> ) as oauth2_mock : <NEWLINE> <INDENT> r = self . oauth_client . add ( add_data , params = { <STRING> : self . TEST_ACCESS } ) <NEWLINE> add_mock . assert_called_once_with ( <STRING> , <STRING> , add_data , params = { <STRING> : self . TEST_ACCESS } ) <NEWLINE> self . assertTrue ( r ) <NEWLINE> <DEDENT>
if step < self . dihstep : <NEWLINE> <INDENT> self . set_dihedrals ( change , fix , cut = 1 ) <NEWLINE> <DEDENT>
for ins in korcek_chain : <NEWLINE> <INDENT> if bond [ ins [ 0 ] ] [ ins [ - 1 ] ] == 1 : <COMMENT> <NEWLINE> <INDENT> rxns += [ ins ] <NEWLINE> <DEDENT> <DEDENT>
kwargs . pop ( <STRING> , None ) <NEWLINE> <INDENT> pop = self . _population <NEWLINE> coords = pop . projection ( which , 2 , ** coords_kwargs ) <NEWLINE> return self . scatter_coords ( coords , ** scatter_kwargs ) <NEWLINE> <DEDENT>
@ functools . wraps ( func ) <NEWLINE> <INDENT> def decorated ( file , * args , ** kwargs ) : <NEWLINE> <INDENT> if isinstance ( file , str ) : <NEWLINE> <INDENT> with open ( file ) as F : <NEWLINE> <INDENT> result = func ( file , * args , ** kwargs ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return func ( file , * args , ** kwargs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if _centre_list [ i ] . covariance is not None : <NEWLINE> <INDENT> sensor_list [ i ] . covariance = interpolate_covariance ( <NEWLINE> <INDENT> sensor_list [ i ] . epoch_timestamp , <NEWLINE> _centre_list [ j - 1 ] . epoch_timestamp , <NEWLINE> _centre_list [ j ] . epoch_timestamp , <NEWLINE> _centre_list [ j - 1 ] . covariance , <NEWLINE> _centre_list [ j ] . covariance ) <NEWLINE> <DEDENT> <DEDENT>
for j in plotly_list : <NEWLINE> <INDENT> make_frame ( frame , <NEWLINE> <INDENT> [ j [ 0 ] , <NEWLINE> <INDENT> [ float ( k . epoch_timestamp ) for k in j [ 1 ] ] , <NEWLINE> [ float ( k . eastings ) for k in j [ 1 ] ] , <NEWLINE> [ float ( k . northings ) for k in j [ 1 ] ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( camera1_pf_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in camera1_pf_list ] , <NEWLINE> [ float ( i . eastings ) for i in camera1_pf_list ] , <NEWLINE> [ float ( i . northings ) for i in camera1_pf_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( pf_fusion_centre_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in pf_fusion_centre_list ] , <NEWLINE> [ float ( i . eastings ) for i in pf_fusion_centre_list ] , <NEWLINE> [ float ( i . northings ) for i in pf_fusion_centre_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( ekf_centre_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in ekf_centre_list ] , <NEWLINE> [ float ( i . eastings ) for i in ekf_centre_list ] , <NEWLINE> [ float ( i . northings ) for i in ekf_centre_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( pf_fusion_dvl_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in pf_fusion_dvl_list ] , <NEWLINE> [ float ( i . eastings ) for i in pf_fusion_dvl_list ] , <NEWLINE> [ float ( i . northings ) for i in pf_fusion_dvl_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( pf_timestamps_interval ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> pf_timestamps_interval , <NEWLINE> pf_eastings_interval , <NEWLINE> pf_northings_interval ] , <NEWLINE> <DEDENT> i , mode = <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if ic > best_ic : <NEWLINE> <INDENT> best_ic = ic <NEWLINE> best_model = m <NEWLINE> if ic > goal_inliers and stop_at_goal : <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> m = fit_plane ( inliers ) <NEWLINE> return best_model , inliers , i <NEWLINE> <DEDENT> <DEDENT>
planes = [ ] <NEWLINE> <INDENT> for i in range ( 0 , self . num_iterations ) : <NEWLINE> <INDENT> point_cloud_local = random . sample ( inliers_cloud_list , cloud_sample_size ) <NEWLINE> total_no_points = len ( point_cloud_local ) <NEWLINE> p = Plane ( [ 1 , 0 , 0 , 1.5 ] ) <NEWLINE> m = p . fit_non_robust ( cloud ) <NEWLINE> <STRING> <NEWLINE> angle , pitch , yaw = get_angles ( m [ 0 : 3 ] ) <NEWLINE> planes . append ( [ angle , pitch , yaw ] ) <NEWLINE> Console . progress ( i , self . num_iterations , prefix = <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if isinstance ( epsilon , numbers . Number ) : <COMMENT> <NEWLINE> <INDENT> self . epsilon_fitted = epsilon <NEWLINE> return self <NEWLINE> <DEDENT> elif epsilon == <STRING> : <COMMENT> <NEWLINE> <INDENT> if ( self . metric != <STRING> ) : <COMMENT> <NEWLINE> <INDENT> warnings . warn ( <STRING> % self . metric ) <NEWLINE> <DEDENT> if self . scaled_dists is not None : <NEWLINE> <INDENT> self . scaled_dists = self . _get_scaled_distance_mat ( self . data , self . bandwidths ) <NEWLINE> <DEDENT> self . epsilon_fitted , self . d = choose_optimal_epsilon_BGH ( self . scaled_dists . data ** 2 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> % epsilon ) <NEWLINE> <DEDENT> return self <NEWLINE> <DEDENT>
def get_devices ( self , location ) : <NEWLINE> <INDENT> response_data = self . __call_smart_system_get ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> if len ( response_data [ <STRING> ] [ <STRING> ] [ <STRING> ] [ <STRING> ] ) < 1 : <NEWLINE> <INDENT> self . logger . error ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> devices_smart_system = { } <NEWLINE> for device in response_data [ <STRING> ] : <NEWLINE> <INDENT> real_id = device [ <STRING> ] . split ( <STRING> ) [ 0 ] <NEWLINE> if real_id not in devices_smart_system : <NEWLINE> <INDENT> devices_smart_system [ real_id ] = { } <NEWLINE> <DEDENT> if ( <NEWLINE> <INDENT> device [ <STRING> ] in self . supported_services <NEWLINE> and device [ <STRING> ] not in devices_smart_system [ real_id ] <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> devices_smart_system [ real_id ] [ device [ <STRING> ] ] = [ ] <NEWLINE> <DEDENT> devices_smart_system [ real_id ] [ device [ <STRING> ] ] . append ( device ) <NEWLINE> <DEDENT> for parsed_device in devices_smart_system . values ( ) : <NEWLINE> <INDENT> location . add_device ( DeviceFactory . build ( self , device ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def try_to_create ( self ) : <NEWLINE> <INDENT> limit = datetime . now ( ) - timedelta ( seconds = self . settings [ <STRING> ] ) <NEWLINE> if <STRING> in self . last_snapshot or self . last_snapshot [ <STRING> ] <= limit : <NEWLINE> <INDENT> self . create ( ) <NEWLINE> <DEDENT> <DEDENT>
class PubsubQueue ( object ) : <NEWLINE> <INDENT> def __init__ ( self , queue_name , sub_name = None , verbose = 10 ) : <NEWLINE> <INDENT> assert <STRING> in os . environ . keys ( ) <NEWLINE> self . client = pubsub . Client ( ) <NEWLINE> self . topic = self . client . topic ( queue_name ) <NEWLINE> self . logger = logging . getLogger ( self . __class__ . __name__ ) <NEWLINE> self . logger . setLevel ( verbose ) <NEWLINE> sub_name = sub_name if sub_name else queue_name + <STRING> <NEWLINE> self . logger . info ( <STRING> . format ( queue_name ) ) <NEWLINE> self . logger . info ( <STRING> . format ( queue_name ) ) <NEWLINE> if queue_name not in [ t . name for t in self . client . list_topics ( ) ] : <NEWLINE> <INDENT> self . topic . create ( ) <NEWLINE> self . logger . info ( <STRING> . format ( queue_name ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if not n_records or ( not average and <NEWLINE> <INDENT> n_records < retrieved_records ) : <NEWLINE> time . sleep ( 1e-6 ) <NEWLINE> continue <NEWLINE> <DEDENT> if not get_data ( cu , id_ , buffers_ptr , <NEWLINE> <INDENT> n_records * samples_per_record , <NEWLINE> bytes_per_sample , <NEWLINE> retrieved_records , <NEWLINE> n_records , <NEWLINE> mask , <NEWLINE> 0 , <NEWLINE> samples_per_record , <NEWLINE> 0x00 ) : <NEWLINE> del avg , buffers <NEWLINE> self . _dll . DisarmTrigger ( self . _cu_id , self . _id ) <NEWLINE> self . _dll . MultiRecordClose ( self . _cu_id , self . _id ) <NEWLINE> self . close_connection ( ) <NEWLINE> self . _setup_library ( ) <NEWLINE> self . open_connection ( ) <NEWLINE> self . configure_board ( ) <NEWLINE> if retry : <NEWLINE> return self . get_traces ( channels , duration , delay , <NEWLINE> <INDENT> records_per_capture , False ) <NEWLINE> else : <NEWLINE> msg = <STRING> <NEWLINE> raise RuntimeError ( msg ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
alignment = calculate_alignment ( orient_tile ) <NEWLINE>
if downsample : <NEWLINE> <INDENT> fixed_shrunk = trans . resize_image ( fixed_image , fixed_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> rotated_shrunk = trans . resize_image ( rotated_image , moving_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> spacing = fixed_shrunk . GetSpacing ( ) <NEWLINE> <DEDENT>
if roi_size is None : <NEWLINE> <INDENT> with open ( output_path , <STRING> , newline = <STRING> ) as csvfile : <NEWLINE> <INDENT> print ( <STRING> . format ( <NEWLINE> <INDENT> output_path . name , tile_size [ 0 ] ) ) <NEWLINE> <DEDENT> writer = csv . writer ( csvfile ) <NEWLINE> writer . writerow ( [ <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def fieldStrAndPer ( d ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> l1 = [ ] <NEWLINE> lper = [ ] <NEWLINE> l2 = [ ] <NEWLINE> for k , v in d . items ( ) : <NEWLINE> <INDENT> if v != None : <NEWLINE> <INDENT> l1 . append ( k ) <NEWLINE> noAppend = True <COMMENT> <NEWLINE> if isinstance ( l2 , str ) : <NEWLINE> <INDENT> if v . startswith ( <STRING> ) or v . startswith ( <STRING> ) or v . startswith ( <STRING> ) or v . startswith ( <STRING> ) : <NEWLINE> <INDENT> vv = dataToFloat ( v [ 1 : ] ) <NEWLINE> if vv : <NEWLINE> <INDENT> noAppend = False <NEWLINE> lper . append ( <STRING> . format ( k , v [ : 1 ] ) ) <NEWLINE> l2 . append ( vv ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if noAppend : <NEWLINE> <INDENT> lper . append ( <STRING> ) <NEWLINE> l2 . append ( dataToStr ( v ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
while True : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> next_dt = pd . Timestamp ( next_dt . replace ( tzinfo = None ) ) <NEWLINE> next_dt = next_dt + interval <NEWLINE> next_dt = pd . Timestamp ( next_dt , tz = trading . environment . exchange_tz ) <NEWLINE> next_dt_utc = next_dt . tz_convert ( <STRING> ) <NEWLINE> if trading . environment . is_market_hours ( next_dt ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> next_dt = next_dt_utc . tz_convert ( trading . environment . exchange_tz ) <NEWLINE> <DEDENT>
def create_test_df_source ( sim_params = None , bars = <STRING> ) : <NEWLINE> <INDENT> if bars == <STRING> : <NEWLINE> <INDENT> freq = pd . datetools . BDay ( ) <NEWLINE> <DEDENT> elif bars == <STRING> : <NEWLINE> <INDENT> freq = pd . datetools . Minute ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> % freq ) <NEWLINE> <DEDENT> <DEDENT>
out_x_res_y = out_file_h5 . createCArray ( out_file_h5 . root , <NEWLINE> <INDENT> name = <STRING> % ( actual_dut ) , <NEWLINE> title = <STRING> % ( actual_dut ) , <NEWLINE> atom = tb . Atom . from_dtype ( hist_x_residual_y_hist . dtype ) , <NEWLINE> shape = hist_x_residual_x_hist . shape , <NEWLINE> filters = tb . Filters ( complib = <STRING> , complevel = 5 , fletcher32 = False ) ) <NEWLINE> out_x_res_y . attrs . xedges = hist_x_residual_y_xedges <NEWLINE> out_x_res_y . attrs . yedges = hist_x_residual_y_yedges <NEWLINE> out_x_res_y . attrs . fit_coeff = fit_x_residual_y [ 0 ] <NEWLINE> out_x_res_y . attrs . fit_cov = fit_x_residual_y [ 1 ] <NEWLINE> out_x_res_y [ : ] = hist_x_residual_y_hist <NEWLINE> <DEDENT>
out_col_res_row = out_file_h5 . createCArray ( out_file_h5 . root , <NEWLINE> <INDENT> name = <STRING> % ( actual_dut ) , <NEWLINE> title = <STRING> % ( actual_dut ) , <NEWLINE> atom = tb . Atom . from_dtype ( hist_col_residual_row_hist . dtype ) , <NEWLINE> shape = hist_col_residual_col_hist . shape , <NEWLINE> filters = tb . Filters ( complib = <STRING> , complevel = 5 , fletcher32 = False ) ) <NEWLINE> out_col_res_row . attrs . xedges = hist_col_residual_row_xedges <NEWLINE> out_col_res_row . attrs . yedges = hist_col_residual_row_yedges <NEWLINE> out_col_res_row . attrs . fit_coeff = fit_col_residual_row [ 0 ] <NEWLINE> out_col_res_row . attrs . fit_cov = fit_col_residual_row [ 1 ] <NEWLINE> out_col_res_row [ : ] = hist_col_residual_row_hist <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> n_slices = cpu_count ( ) <NEWLINE> slices = np . array_split ( n_tracks , n_slices ) <NEWLINE> results = pool . map ( _fit_tracks_loop , slices ) <NEWLINE> del track_hits <NEWLINE> <DEDENT>
if not reference_hit_set and not np . isnan ( tr_row [ track_index ] [ dut_index ] ) : <COMMENT> <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> def loop_single_entry ( cfg ) : <NEWLINE> <INDENT> for v , _ in cfg . iter_nodes ( ) : <NEWLINE> <INDENT> if cfg . degree_in ( v ) >= 2 : <NEWLINE> <INDENT> preds = cfg . pred ( v ) <NEWLINE> back_preds = list ( filter ( lambda x : v <= x , preds ) ) <NEWLINE> if len ( back_preds ) < 2 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> print ( <STRING> , v ) <NEWLINE> print ( <STRING> , back_preds ) <NEWLINE> back_jumps = list ( filter ( lambda x : cfg . degree_out ( x ) == 1 , back_preds ) ) <NEWLINE> print ( <STRING> , back_jumps ) <NEWLINE> <COMMENT> <NL> landing_site = None <NEWLINE> for p in back_jumps : <NEWLINE> <INDENT> b = cfg . node ( p ) <NEWLINE> if not b . items : <NEWLINE> <INDENT> landing_site = p <NEWLINE> <DEDENT> <DEDENT> if not landing_site : <NEWLINE> <INDENT> farthest = max ( back_jumps ) <NEWLINE> print ( <STRING> , farthest ) <NEWLINE> newb = BBlock ( farthest + <STRING> ) <NEWLINE> cfg . add_node ( newb . addr , newb ) <NEWLINE> cfg . add_edge ( newb . addr , v ) <NEWLINE> landing_site = newb . addr <NEWLINE> <DEDENT> print ( <STRING> , landing_site ) <NEWLINE> for p in back_preds : <NEWLINE> <INDENT> if p != landing_site : <NEWLINE> <INDENT> e = cfg . edge ( p , v ) <NEWLINE> cfg . remove_edge ( p , v ) <NEWLINE> cfg . add_edge ( p , landing_site , e ) <NEWLINE> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if self . train_with_fake_labels : <NEWLINE> <INDENT> x_train = np . reshape ( train_data . X , newshape = ( - 1 , * self . x_dim ) ) <NEWLINE> x = [ x_train , train_labels , pseudo_labels ] <NEWLINE> y = [ x_train , pseudo_labels ] <NEWLINE> else : <NEWLINE> x_train = np . reshape ( train_data . X , newshape = ( - 1 , * self . x_dim ) ) <NEWLINE> x = [ x_train , train_labels , train_labels ] <NEWLINE> y = [ x_train , train_labels ] <NEWLINE> <DEDENT>
print ( train_data . shape , data_valid . shape ) <NEWLINE>
cell_type_data = train [ train . obs [ cell_type_key ] == cell_type ] <NEWLINE> <INDENT> cell_type_ctrl_data = train [ ( ( train . obs [ cell_type_key ] == cell_type ) & ( train . obs [ <STRING> ] == ctrl_key ) ) ] <NEWLINE> pred = network . predict ( cell_type_data , <NEWLINE> <INDENT> encoder_labels = np . zeros ( ( cell_type_ctrl_data . shape [ 0 ] , 1 ) ) , <NEWLINE> decoder_labels = np . ones ( ( cell_type_ctrl_data . shape [ 0 ] , 1 ) ) ) <NEWLINE> <DEDENT> <DEDENT>
source_images_train = train_data [ valid_data . obs [ <STRING> ] == source_key ] . X <NEWLINE> <INDENT> source_images_valid = valid_data [ valid_data . obs [ <STRING> ] == source_key ] . X <NEWLINE> <DEDENT>
for gene in top_100_genes [ : 3 ] : <NEWLINE> <INDENT> sc . pl . violin ( cell_type_adata , keys = gene , groupby = condition_key , <NEWLINE> <INDENT> save = <STRING> , <NEWLINE> show = False , <NEWLINE> wspace = 0.2 , <NEWLINE> rotation = 90 , <NEWLINE> frameon = False ) <NEWLINE> <DEDENT> <DEDENT>
updates = kwargs <NEWLINE> <INDENT> persistence_converter = self . get_persistence_converter ( self . engine_name ) <NEWLINE> if persistence_converter is not None : <NEWLINE> <INDENT> updates = persistence_converter ( updates ) <NEWLINE> <DEDENT> self . engine . update ( patch , primary_index , context , updates ) <NEWLINE> self . get_data ( ) <NEWLINE> <DEDENT>
for subdir in subdirs : <NEWLINE> <INDENT> self . _fetch_files ( subdir , files ) <NEWLINE> <DEDENT>
outputFastaPath = os . path . join ( options . outputPath , outputFastaName ) <NEWLINE> <INDENT> if os . path . isfile ( outputFastaName ) : <NEWLINE> <INDENT> print ( <STRING> % locus_name ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if issubclass ( strategy , bt . Strategy ) : <NEWLINE> <INDENT> strat_name = str ( strategy ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> strat_name = strategy <NEWLINE> strategy = STRATEGY_MAPPING [ strategy ] <NEWLINE> <DEDENT> <DEDENT>
for env_var in env_vars : <NEWLINE> <INDENT> def_value = config . instance [ section ] . get ( executor_name , None ) <NEWLINE> value = click . prompt ( <STRING> , default = def_value ) <NEWLINE> config . instance . set ( section , env_var , value ) <NEWLINE> <DEDENT>
def write_report ( df_map , outFH , pcr_volume , mm_volume , <NEWLINE> <INDENT> fp_tube , fp_volume , rp_tube , rp_volume , <NEWLINE> n_rxn_reps , error_perc = 10.0 ) : <NEWLINE> <STRING> <NEWLINE> <COMMENT> <NL> n_rxn = df_map . shape [ 0 ] <NEWLINE> <COMMENT> <NL> total_pcr_volume = pcr_volume * n_rxn <NEWLINE> <COMMENT> <NL> total_mm_volume = pcr_volume * n_rxn <NEWLINE> <COMMENT> <NL> if fp_tube > 0 and fp_volume > 0 : <NEWLINE> total_fp_volume = fp_volume * n_rxn <NEWLINE> else : <NEWLINE> total_fp_volume = None <NEWLINE> if rp_tube > 0 and rp_volume > 0 : <NEWLINE> total_rp_volume = rp_volume * n_rxn <NEWLINE> else : <NEWLINE> total_rp_volume = None <NEWLINE> <COMMENT> <NL> total_water_volume = sum ( df_map [ <STRING> ] ) <NEWLINE> <DEDENT>
except Exception as e : <NEWLINE> <COMMENT> <NL> <INDENT> name [ settings . UNKNOWN_LANGUAGE ] += paragraph <NEWLINE> logger . info ( <STRING> . format ( paragraph ) ) <NEWLINE> <DEDENT>
def list_run ( <NEWLINE> <INDENT> limit , activity_types , output , unique , include_glob , exclude_glob , <NEWLINE> under , relative , title , null , ** _ ) : <NEWLINE> <STRING> <NEWLINE> separator = <STRING> if null else <STRING> <NEWLINE> absunder = [ os . path . join ( os . path . abspath ( p ) , <STRING> ) for p in under ] <NEWLINE> include_glob += [ os . path . join ( p , <STRING> ) for p in absunder ] <NEWLINE> db = get_db ( ) <NEWLINE> paths = showpaths = list ( db . list_file_path ( <NEWLINE> limit , activity_types , unique , include_glob , exclude_glob ) ) <NEWLINE> if relative : <NEWLINE> showpaths = [ remove_prefix ( absunder , p ) for p in paths ] <NEWLINE> if title : <NEWLINE> from . filetitle import write_paths_and_titles <NEWLINE> write_paths_and_titles ( output , paths , showpaths , separator ) <NEWLINE> else : <NEWLINE> output . writelines ( interleave ( paths , itertools . repeat ( separator ) ) ) <NEWLINE> if output is not sys . stdout : <NEWLINE> output . close ( ) <NEWLINE> <DEDENT>
dehydrator = self . dehydrator_cls ( fields = self . fields ) <NEWLINE> <INDENT> if self . is_iterable : <NEWLINE> <INDENT> return map ( dehydrator . dehydrate , target ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return dehydrator . dehydrate ( obj ) <NEWLINE> <DEDENT> <DEDENT>
def rule ( self , <NEWLINE> <INDENT> ruleid , ForEach = None , ResultMod = None , Exists = None , Min = 0 , Max = None , <NEWLINE> Aggregates = None , IsLive = None , <NEWLINE> Command = None , Response = None , <NEWLINE> Show = None , Pass = <STRING> , Fail = <STRING> , NoMatch = <STRING> ) : <NEWLINE> rule = rdflib . URIRef ( ruleid , base = self . _base ) <NEWLINE> if ForEach : <NEWLINE> ruletype = MINIM . QueryTestRule <NEWLINE> querynode = rdflib . BNode ( ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . query , querynode ) ) <NEWLINE> self . _minimgr . add ( ( querynode , MINIM . sparql_query , rdflib . Literal ( ForEach ) ) ) <NEWLINE> if ResultMod : <NEWLINE> <INDENT> self . _minimgr . add ( ( querynode , MINIM . result_mod , rdflib . Literal ( Exists ) ) ) <NEWLINE> if Exists : <NEWLINE> existsnode = rdflib . BNode ( ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . exists , existsnode ) ) <NEWLINE> self . _minimgr . add ( ( existsnode , MINIM . sparql_query , rdflib . Literal ( Exists ) ) ) <NEWLINE> if Min : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . min , rdflib . Literal ( Min ) ) ) <NEWLINE> if Max : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . max , rdflib . Literal ( Max ) ) ) <NEWLINE> if Aggregates : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . aggregatesTemplate , rdflib . Literal ( Aggregates ) ) ) <NEWLINE> if IsLive : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . isLiveTemplate , rdflib . Literal ( IsLive ) ) ) <NEWLINE> elif Exists : <NEWLINE> ruletype = MINIM . QueryTestRule <NEWLINE> existsnode = rdflib . BNode ( ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . exists , existsnode ) ) <NEWLINE> self . _minimgr . add ( ( existsnode , MINIM . sparql_query , rdflib . Literal ( Exists ) ) ) <NEWLINE> elif Command : <NEWLINE> ruletype = MINIM . SoftwareEnvironmentRule <NEWLINE> self . _minimgr . add ( ( rule , MINIM . command , rdflib . Literal ( Command ) ) ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . response , rdflib . Literal ( Response ) ) ) <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> self . _minimgr . add ( ( rule , RDF . type , ruletype ) ) <NEWLINE> if Show : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . show , rdflib . Literal ( Show ) ) ) <NEWLINE> if Pass : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . showpass , rdflib . Literal ( Pass ) ) ) <NEWLINE> if Fail : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . showfail , rdflib . Literal ( Fail ) ) ) <NEWLINE> if NoMatch : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . showmiss , rdflib . Literal ( NoMatch ) ) ) <NEWLINE> return rule <NEWLINE> <DEDENT> <DEDENT>
def dumpPkl ( obj , path ) : <NEWLINE> <INDENT> with open ( path , <STRING> ) as pf : <NEWLINE> <INDENT> pickle . dump ( obj , path ) <NEWLINE> <DEDENT> <DEDENT>
if additional_collapsed is not None : <NEWLINE> <INDENT> for param_field , param_collapsed in zip ( additional_params , additional_collapsed ) : <NEWLINE> <INDENT> param_collapsed . shape = ( Ncubes * collapse_channels , ) + param_collapsed . shape [ 2 : ] <NEWLINE> setattr ( self , param_field , param_collapsed ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> for index , filepath in enumerate ( filepaths ) : <NEWLINE> <INDENT> with fits . open ( filepath , lazy_load_hdus = False ) as hdulist : <NEWLINE> <INDENT> cube = hdulist [ 1 ] . data <NEWLINE> prihdr = hdulist [ 0 ] . header <NEWLINE> exthdr = hdulist [ 0 ] . header <NEWLINE> w = wcs . WCS ( header = prihdr , naxis = [ 1 , 2 ] ) <NEWLINE> astr_hdrs = [ w . deepcopy ( ) for _ in range ( cube . shape [ 0 ] ) ] <COMMENT> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if <STRING> in mode : <NEWLINE> <INDENT> if psf_library is None : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if psf_library . dataset is dataset : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if aligned_center is not None : <NEWLINE> <INDENT> if np . array_equal ( aligned_center , psf_library . aligned_center ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if maskout_edge is not None : <NEWLINE> <INDENT> IWA , OWA , inner_mask , outer_mask = get_occ ( image , centroid = ( center [ 0 ] [ 0 ] + stamp_size // 2 , center [ 0 ] [ 1 ] + stamp_size // 2 ) ) <NEWLINE> conv_kernel = np . ones ( ( maskout_edge , maskout_edge ) ) <NEWLINE> flat_cube_wider_mask = convolve2d ( outer_mask , conv_kernel , mode = <STRING> ) <NEWLINE> image_cpy [ np . where ( np . isnan ( flat_cube_wider_mask ) ) ] = np . nan <NEWLINE> <DEDENT> <DEDENT>
def amount_as_string ( self , min_decimals : Optional [ int ] = None , max_decimals : Optional [ int ] = None ) -> str : <NEWLINE> <INDENT> if min_decimals is None and max_decimals is None : <NEWLINE> <INDENT> if self . _currency and isinstance ( self . _currency , BaseCurrency ) : <NEWLINE> <INDENT> min_decimals = self . _currency . decimal_digits <NEWLINE> <DEDENT> min_decimals = DEFAULT_MIN_DECIMALS if min_decimals is None else min_decimals <NEWLINE> max_decimals = max ( cast ( int , min_decimals ) , DEFAULT_MAX_DECIMALS ) <NEWLINE> <DEDENT> elif min_decimals is None : <NEWLINE> <INDENT> if self . _currency and isinstance ( self . _currency , BaseCurrency ) : <NEWLINE> <INDENT> min_decimals = self . _currency . decimal_digits <NEWLINE> <DEDENT> min_decimals = DEFAULT_MIN_DECIMALS if min_decimals is None else min_decimals <NEWLINE> min_decimals = min ( cast ( min_decimals , int ) , max_decimals ) <NEWLINE> <DEDENT> elif max_decimals is None : <NEWLINE> <INDENT> max_decimals = max ( min_decimals , DEFAULT_MAX_DECIMALS ) <NEWLINE> <DEDENT> <DEDENT>
if <STRING> in body : <NEWLINE> <INDENT> err = ErrorSchema ( ) . load ( content [ <STRING> ] ) <NEWLINE> text = <STRING> if err [ <STRING> ] else err [ <STRING> ] <NEWLINE> raise ErrorResponse ( text ) <NEWLINE> <DEDENT>
self . assertEqual ( len ( feed . entries ) , len ( datasets ) ) <NEWLINE> <INDENT> for i in range ( 1 , len ( feed . entries ) ) : <NEWLINE> <INDENT> published_date = feed . entries [ i ] . published_parsed <NEWLINE> prev_published_date = feed . entries [ i - 1 ] . published_parsed <NEWLINE> self . assertGreaterEqual ( published_date , prev_published_date ) <NEWLINE> <DEDENT> <DEDENT>
@ ns . route ( <STRING> , endpoint = <STRING> ) <NEWLINE> <INDENT> class DatasetBadgesAPI ( API ) : <NEWLINE> <INDENT> @ api . doc ( <STRING> , ** common_doc ) <NEWLINE> @ api . expect ( badge_fields ) <NEWLINE> @ api . marshal_with ( badge_fields ) <NEWLINE> @ api . secure ( admin_permission ) <NEWLINE> def post ( self , dataset ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> form = api . validate ( BadgeForm ) <NEWLINE> badge = DatasetBadge ( created = datetime . now ( ) , <NEWLINE> <INDENT> created_by = current_user . id ) <NEWLINE> <DEDENT> form . populate_obj ( badge ) <NEWLINE> for existing_badge in dataset . badges : <NEWLINE> <INDENT> if existing_badge . kind == badge . kind : <NEWLINE> <INDENT> return badge <NEWLINE> <DEDENT> <DEDENT> dataset . add_badge ( badge ) <NEWLINE> return badge , 201 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
@ ns . route ( <STRING> , endpoint = <STRING> ) <NEWLINE> <INDENT> class ReuseDatasetsAPI ( API ) : <NEWLINE> <INDENT> @ api . secure <NEWLINE> @ api . doc ( <STRING> , ** common_doc ) <NEWLINE> @ api . expect ( dataset_ref_fields ) <NEWLINE> @ api . response ( 200 , <STRING> , reuse_fields ) <NEWLINE> @ api . marshal_with ( reuse_fields , code = 201 ) <NEWLINE> def post ( self , reuse ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if <STRING> not in request . json : <NEWLINE> <INDENT> api . abort ( 400 , <STRING> ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> dataset = Dataset . objects . get_or_404 ( id = request . json [ <STRING> ] ) <NEWLINE> <DEDENT> except Dataset . DoesNotExist : <NEWLINE> <INDENT> api . abort ( 404 , <STRING> . format ( request . json [ <STRING> ] ) ) <NEWLINE> <DEDENT> if dataset in reuse . datasets : <NEWLINE> <INDENT> return dataset <NEWLINE> <DEDENT> reuse . datasets . append ( dataset ) <NEWLINE> reuse . save ( ) <NEWLINE> return reuse , 201 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
assert type ( cleaned_options [ <STRING> ] . ttl ) is int <NEWLINE> <INDENT> cleaned_options [ <STRING> ] = options [ <STRING> ] . ttl <NEWLINE> <DEDENT>
def crawl ( url ) : <NEWLINE> <INDENT> items = requests . get ( url , headers = lt . DEFAULT_HEADERS ) . json ( ) <NEWLINE> for item in items : <NEWLINE> <INDENT> data = { } <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = items [ <STRING> ] <NEWLINE> pprint ( data ) <NEWLINE> total . append ( data ) <NEWLINE> <DEDENT> <DEDENT>
@ click . command ( help = help_text ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , type = int , default = 200 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> , ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , is_flag = True ) <NEWLINE> @ click . option ( <STRING> , is_flag = True , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . argument ( <STRING> , type = click . Path ( exists = True , file_okay = False ) ) <NEWLINE> def cli ( filter_level , max_unknowns , c_deviations , s_deviations , m_deviations , <NEWLINE> <INDENT> dry_run , species , path ) : <NEWLINE> if species : <NEWLINE> from genbankqc import Species <NEWLINE> try : <NEWLINE> <INDENT> s = Species ( path , max_unknowns , c_deviations , s_deviations , <NEWLINE> <INDENT> m_deviations ) <NEWLINE> <DEDENT> s . qc ( ) <NEWLINE> print ( <STRING> , s . species ) <NEWLINE> print ( s ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> print ( <STRING> , species . species ) <NEWLINE> traceback . print_exc ( ) <NEWLINE> else : <NEWLINE> <DEDENT> from genbankqc import Genbank <NEWLINE> genbank = Genbank ( path ) <NEWLINE> genbank . qc ( ) <NEWLINE> <DEDENT> <DEDENT>
def eq_contents ( path , text ) : <NEWLINE> <INDENT> with open ( path ) as fd : <NEWLINE> <INDENT> eq_ ( fd . read ( ) , text ) <NEWLINE> <DEDENT> <DEDENT>
global MASK_PREDICTOR_HANDLER <NEWLINE> <INDENT> with LOCK : <NEWLINE> <INDENT> if MASK_PREDICTOR_HANDLER is None : <NEWLINE> <INDENT> MASK_PREDICTOR_HANDLER = MaskPredictor ( deepLearningModel , boxSize , gpus ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
class KademliaProtocol ( RPCProtocol ) : <NEWLINE> <INDENT> def __init__ ( self , sourceNode , storage , ksize ) : <NEWLINE> <INDENT> RPCProtocol . __init__ ( self ) <NEWLINE> self . router = RoutingTable ( self , sourceNode , ksize ) <NEWLINE> self . storage = storage <NEWLINE> self . sourceID = sourceNode . id <NEWLINE> self . log = Logger ( system = self ) <NEWLINE> <DEDENT> <DEDENT>
def verified_email_required ( function = None , <NEWLINE> <INDENT> login_url = None , <NEWLINE> redirect_field_name = REDIRECT_FIELD_NAME ) : <NEWLINE> <STRING> <NEWLINE> def decorator ( view_func ) : <NEWLINE> @ login_required ( redirect_field_name = redirect_field_name , <NEWLINE> login_url = login_url ) <NEWLINE> def _wrapped_view ( request , * args , ** kwargs ) : <NEWLINE> if not EmailAddress . objects . filter ( user = request . user , <NEWLINE> <INDENT> verified = True ) . exists ( ) : <NEWLINE> send_email_confirmation ( request . user , request ) <NEWLINE> return render ( request , <NEWLINE> <STRING> ) <NEWLINE> return view_func ( request , * args , ** kwargs ) <NEWLINE> return _wrapped_view <NEWLINE> <DEDENT> <DEDENT>
def render_to_string ( template_name , dictionary , context_instance = None ) : <NEWLINE> <INDENT> context_instance = context_instance or Context ( dictionary ) <NEWLINE> <COMMENT> <NL> context_instance . update ( dictionary or { } ) <NEWLINE> <COMMENT> <NL> context_dictionary = { } <NEWLINE> for d in context_instance : <NEWLINE> <INDENT> context_dictionary . update ( d ) <NEWLINE> <COMMENT> <NL> <DEDENT> template = middleware . lookup . get_template ( template_name ) <NEWLINE> return template . render ( ** dictionary ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if result > - 1 : <NEWLINE> <INDENT> tmp = result <NEWLINE> result = - 1 <NEWLINE> return str ( result ) <NEWLINE> <DEDENT> return <STRING> <NEWLINE> <DEDENT>
@ app . task ( bind = True , queue = <STRING> ) <NEWLINE> <INDENT> def exec_operator ( self , model , job_name ) : <NEWLINE> <INDENT> result = None <NEWLINE> wf = json2model ( model ) <NEWLINE> op_node = wf . spec . get ( <STRING> , { } ) . get ( job_name , { } ) <NEWLINE> if op_node : <NEWLINE> <INDENT> router = Router ( wf ) <NEWLINE> result = router . route ( wf , op_node , job_name , op_node [ <STRING> ] , op_node [ <STRING> ] ) <NEWLINE> wf . set_result ( job_name , result ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT> <DEDENT>
if <STRING> in translated_key : <NEWLINE> <INDENT> translated_value = reduce ( lambda i , acc : ( acc , i ) , reversed ( translated_key . split ( <STRING> ) [ 1 : ] + [ value ] ) ) <NEWLINE> translated_key = translated_key . split ( <STRING> ) [ 0 ] <NEWLINE> elif translated_key == <STRING> : <NEWLINE> return key , value <NEWLINE> return translated_key , translated_value <NEWLINE> <DEDENT>
message ( <STRING> + output_file_geojson ) <NEWLINE> <INDENT> output = open ( output_file_geojson , <STRING> ) <NEWLINE> output . write ( geojson_str ) <NEWLINE> output . close ( ) <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def _check_and_parse_output_plugin ( string ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> raw_ouputs = string . split ( <STRING> ) <NEWLINE> outputs = [ ] <NEWLINE> for output in raw_ouputs : <NEWLINE> <INDENT> name , arguments = output . split ( <STRING> ) <NEWLINE> outputs . append ( ( name , arguments ) ) <NEWLINE> <DEDENT> if len ( output ) == 0 : <NEWLINE> <INDENT> raise ValueError ( ) <NEWLINE> <DEDENT> <DEDENT> except ValueError : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> return outputs <NEWLINE> <DEDENT> <DEDENT>
if biggest_centroid is None : <NEWLINE> <INDENT> biggest_centroid = compute_centroid ( cluster ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> temp_path = location_base_uri . split ( <STRING> ) <NEWLINE> problemDoc_uri = <STRING> . join ( temp_path [ : - 2 ] ) + <STRING> + <STRING> . join ( temp_path [ - 2 : ] ) . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> G_largest = [ 0 ] <NEWLINE> components = np . zeros ( len ( G ) , dtype = int ) <NEWLINE> for i , connected_component in enumerate ( nx . connected_components ( G ) ) : <NEWLINE> <COMMENT> <NL> <INDENT> if len ( connected_component ) > len ( G_largest ) : <NEWLINE> <COMMENT> <NL> <INDENT> G_largest = i <NEWLINE> <COMMENT> <NL> <DEDENT> temp_indices = [ i for i , x in enumerate ( nodeIDs ) <NEWLINE> <INDENT> if x in list ( connected_component ) ] <NEWLINE> <DEDENT> components [ temp_indices ] = i + 1 <NEWLINE> <DEDENT> <DEDENT>
return DictUtils . get_required_value ( context , name , ** kwargs ) <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> if required_error is not None : <NEWLINE> <INDENT> kwargs [ <STRING> ] = default_value <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
type_definition = ( type_ref + skip ( equals ) + type_ + skip ( finished ) ) >> _make_type_definition <NEWLINE>
structural_type_attr = ( attr_name + skip ( colon ) + type_ ) >> tuple <NEWLINE> <INDENT> structural_type_attrs = many ( structural_type_attr ) <NEWLINE> <DEDENT>
if not siz == <STRING> and not siz . isdecimal ( ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> return <STRING> <NEWLINE> <DEDENT>
for item in value : <NEWLINE> <INDENT> self . validate ( value ) <NEWLINE> old_val = copy . deepcopy ( self . value ) <NEWLINE> sync_val = { } <NEWLINE> self . value . clear ( ) <NEWLINE> self . value . extend ( value ) <NEWLINE> self . document . update_sync ( name , old_val ) <NEWLINE> <DEDENT>
parser_verify . add_argument ( <STRING> , <STRING> , type = str , <NEWLINE> <INDENT> dest = <STRING> , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT>
new = _css_import_re . sub ( <STRING> , old ) <NEWLINE>
def collect_diff_tag ( self , want , got ) : <NEWLINE> <INDENT> if not self . tag_compare ( want . tag , got . tag ) : <NEWLINE> <INDENT> tag = <STRING> % ( want . tag , got . tag ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> tag = got . tag <NEWLINE> <DEDENT> attrs = [ ] <NEWLINE> any = want . tag == <STRING> or <STRING> in want . attrib <NEWLINE> for name , value in sorted ( got . attrib . items ( ) ) : <NEWLINE> <INDENT> if name not in want . attrib and not any : <NEWLINE> <INDENT> attrs . append ( <STRING> % ( name , self . format_text ( value , False ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if name in want . attrib : <NEWLINE> <INDENT> text = self . collect_diff_text ( value , want . attrib [ name ] , False ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> text = self . format_text ( value , False ) <NEWLINE> <DEDENT> attrs . append ( <STRING> % ( name , text ) ) <NEWLINE> <DEDENT> <DEDENT> if not any : <NEWLINE> <INDENT> for name , value in sorted ( want . attrib . items ( ) ) : <NEWLINE> <INDENT> if name in got . attrib : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> attrs . append ( <STRING> % ( name , self . format_text ( value , False ) ) ) <NEWLINE> <DEDENT> <DEDENT> if attrs : <NEWLINE> <INDENT> tag = <STRING> % ( tag , <STRING> . join ( attrs ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> tag = <STRING> % tag <NEWLINE> <DEDENT> return tag <NEWLINE> <DEDENT>
SRC_ioncell_fws = SRCFireworks ( task_class = RelaxFWTask , task_input = ioncell_input , spec = spec , <NEWLINE> <INDENT> initialization_info = initialization_info , <NEWLINE> wf_task_index_prefix = <STRING> , <NEWLINE> deps = { SRC_ion_fws [ <STRING> ] . tasks [ 0 ] . task_type : <STRING> } ) <NEWLINE> fws . extend ( SRC_ion_fws [ <STRING> ] ) <NEWLINE> links_dict . update ( SRC_ioncell_fws [ <STRING> ] ) <NEWLINE> <DEDENT>
optconf , qadapter_spec , qtk_qadapter = self . run_autoparal ( self . abiinput , os . path . abspath ( <STRING> ) , self . ftm ) <NEWLINE> <INDENT> if self . use_SRC_scheme : <NEWLINE> <INDENT> return FWAction ( mod_spec = { <STRING> : { <STRING> : qadapter_spec , <STRING> : optconf [ <STRING> ] , <NEWLINE> <INDENT> <STRING> : optconf , <STRING> : qadapter_spec . as_dict ( ) } } ) <NEWLINE> <DEDENT> <DEDENT> self . history . log_autoparal ( optconf ) <NEWLINE> self . abiinput . set_vars ( optconf . vars ) <NEWLINE> <DEDENT>
ec_nostress_clamped = myfw_nostress . tasks [ - 1 ] . get_elastic_tensor ( tensor_type = <STRING> ) <NEWLINE> <INDENT> ec_nostress_relaxed = myfw_nostress . tasks [ - 1 ] . get_elastic_tensor ( tensor_type = <STRING> ) <NEWLINE> ec_stress_relaxed = myfw_nostress . tasks [ - 1 ] . get_elastic_tensor ( tensor_type = <STRING> ) <NEWLINE> <DEDENT>
def createSRCFireworks ( setup_task , run_task , handlers = None , validators = None , spec = None , initialization_info = None , <NEWLINE> <INDENT> task_index = None , deps = None ) : <NEWLINE> spec = copy . deepcopy ( spec ) <NEWLINE> if task_index is not None : <NEWLINE> src_task_index = SRCTaskIndex . from_any ( task_index ) <NEWLINE> else : <NEWLINE> src_task_index = SRCTaskIndex . from_task ( run_task ) <NEWLINE> setup_spec = copy . deepcopy ( spec ) <NEWLINE> setup_spec [ <STRING> ] = task_index <NEWLINE> pass <NEWLINE> <DEDENT>
links_dict = { setup_fw . fw_id : [ run_fw . fw_id ] , <NEWLINE> <INDENT> run_fw . fw_id : [ control_task . fw_id ] } <NEWLINE> return { <STRING> : setup_fw , <STRING> : run_fw , <STRING> : control_fw , <STRING> : links_dict , <NEWLINE> <STRING> : [ setup_fw , run_fw , control_fw ] } <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> control_spec = copy . deepcopy ( spec ) <NEWLINE> control_spec = set_short_single_core_to_spec ( control_spec ) <NEWLINE> control_spec [ <STRING> ] = src_task_index <NEWLINE> control_fw = Firework ( control_task , spec = run_spec , name = src_task_index . control_str ) <NEWLINE> <DEDENT>
modified_objects = { } <NEWLINE> <INDENT> setup_spec_update = { } <NEWLINE> run_spec_update = { } <NEWLINE> for target , action in control_report . actions . items ( ) : <NEWLINE> <INDENT> target_object = initial_objects [ target ] <NEWLINE> action . apply ( target_object ) <NEWLINE> if target not in initial_objects_info : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( target ) ) <NEWLINE> <DEDENT> if <STRING> not in initial_objects_info [ target ] : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( target ) ) <NEWLINE> <DEDENT> for update in initial_objects_info [ target ] [ <STRING> ] : <NEWLINE> <INDENT> if update [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> if <STRING> in update : <NEWLINE> <INDENT> mod = getattr ( target_object , update [ <STRING> ] ) ( ) <NEWLINE> new_spec [ update [ <STRING> ] ] = mod <NEWLINE> modified_objects [ update [ <STRING> ] ] = mod <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> new_spec [ update [ <STRING> ] ] = target_object <NEWLINE> modified_objects [ update [ <STRING> ] ] = mod <NEWLINE> <DEDENT> <DEDENT> elif update [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> if <STRING> in update : <NEWLINE> <INDENT> mod = getattr ( target_object , update [ <STRING> ] ) ( ) <NEWLINE> setup_spec_update [ update [ <STRING> ] ] = mod <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> setup_spec_update [ update [ <STRING> ] ] = target_object <NEWLINE> <DEDENT> <DEDENT> elif update [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> if <STRING> in update : <NEWLINE> <INDENT> mod = getattr ( target_object , update [ <STRING> ] ) ( ) <NEWLINE> run_spec_update [ update [ <STRING> ] ] = mod <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> run_spec_update [ update [ <STRING> ] ] = target_object <NEWLINE> <DEDENT> <DEDENT> elif update [ <STRING> ] in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> task = setup_task if update [ <STRING> ] == <STRING> else run_task <NEWLINE> attr = getattr ( task , update [ <STRING> ] ) <NEWLINE> if <STRING> in update : <NEWLINE> <INDENT> mod = getattr ( target_object , update [ <STRING> ] ) ( ) <NEWLINE> attr = mod <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> attr = target_object <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def simpleLoad ( request ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> loadform = SimpleLoadForm ( request . GET ) <NEWLINE> mpas = None <NEWLINE> if loadform . is_valid ( ) : <NEWLINE> <INDENT> user = loadform . cleaned_data [ <STRING> ] <NEWLINE> name = loadform . cleaned_data [ <STRING> ] <NEWLINE> mpas = Mpa . objects . filter ( user = user , name = name ) <NEWLINE> <DEDENT> return mpaLoad ( request , loadform , mpas ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> user = request . user <NEWLINE> from lingcod . sharing . utils import get_viewable_object_or_respond <NEWLINE> for pk in mpaids : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> obj = mpa_class . objects . get ( pk = pk ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> raise Http404 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>
if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>
def populate ( self , cluster , hosts ) : <NEWLINE> <INDENT> self . _live_hosts = set ( hosts ) <NEWLINE> if len ( hosts ) == 1 : <NEWLINE> <INDENT> self . _position = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . _position = randint ( 0 , len ( hosts ) - 1 ) <NEWLINE> <DEDENT> <DEDENT>
if not issubclass ( klass , poly_base ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , poly_base . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if not issubclass ( klass , cls ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , poly_base . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
MultipleObjectsReturnedBase = DoesNotExistBase or attrs . pop ( <STRING> , BaseModel . MultipleObjectsReturned ) <NEWLINE> <INDENT> attrs [ <STRING> ] = type ( <STRING> , ( MultipleObjectsReturnedBase , ) , { } ) <NEWLINE> <DEDENT>
def assume_role ( account , role ) : <NEWLINE> <INDENT> sts = boto3 . client ( <STRING> ) <NEWLINE> response = sts . assume_role ( RoleArn = <STRING> , <NEWLINE> <INDENT> RoleSessionName = <STRING> ) <NEWLINE> <DEDENT> if not response and not response [ <STRING> ] [ <STRING> ] == 200 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> return boto3 . Session ( <NEWLINE> <INDENT> aws_access_key_id = response [ <STRING> ] [ <STRING> ] , <NEWLINE> aws_secret_access_key = response [ <STRING> ] [ <STRING> ] , <NEWLINE> aws_session_token = response [ <STRING> ] [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT>
def create_add_page ( site : Site , source : str , target : str , data = None , <NEWLINE> <INDENT> content = None ) : <NEWLINE> <STRING> <NEWLINE> init_content = <STRING> <NEWLINE> if content is None and not isinstance ( str , content ) : <NEWLINE> content = init_content <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if <STRING> in prop . constraints ( ) and prop . getType ( ) == <STRING> : <NEWLINE> <INDENT> match = re . match ( prop . constraints ( ) [ <STRING> ] , claim . getTarget ( ) ) <NEWLINE> if not match or match . group ( 0 ) != claim . getTarget ( ) : <NEWLINE> <INDENT> return False , <STRING> <NEWLINE> <DEDENT> <DEDENT> if <STRING> in prop . constraints ( ) and prop . getType ( ) == <STRING> : <NEWLINE> <INDENT> if not claim . getTarget ( ) . getID ( ) in prop . constraints ( ) [ <STRING> ] : <NEWLINE> <INDENT> return False , <STRING> <NEWLINE> <DEDENT> <DEDENT> if <STRING> in prop . constraints ( ) : <NEWLINE> <INDENT> if item . getID ( ) in item . claims : <NEWLINE> <INDENT> return False , <STRING> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> for k , v in cli_args . iteritems ( ) : <NEWLINE> <INDENT> if k not in self . _data or k is not None : <NEWLINE> <INDENT> self . _data [ k ] = v <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> intermediates , trust_roots = [ ] , [ ] <NEWLINE> for store in self . stores : <NEWLINE> <INDENT> for cert in store : <NEWLINE> <INDENT> asn1cert = certificate . to_asn1crypto <NEWLINE> ( trust_roots if store . trusted else intermediates ) . append ( asn1cert ) <NEWLINE> all_certs [ asn1cert ] = cert <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def prompt_hex ( item ) : <NEWLINE> <INDENT> if not isinstance ( item , JConfigHex ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> if item . is_visible ( ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> print ( <STRING> . format ( item . get_name ( ) ) ) <NEWLINE> val = <STRING> <NEWLINE> while val == <STRING> or val == <STRING> : <NEWLINE> <INDENT> val = raw_input ( <STRING> . format ( item . get_prompt ( ) ) ) <NEWLINE> if val == <STRING> : <NEWLINE> <INDENT> print_help ( item ) <NEWLINE> <DEDENT> elif val == <STRING> : <NEWLINE> <INDENT> val = item . get_default_value ( ) <NEWLINE> if val is not <STRING> : <NEWLINE> <INDENT> item . set_user_value ( val ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print_help ( item ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> item . set_user_value ( val ) <NEWLINE> <DEDENT> except ValueError as ve : <NEWLINE> <INDENT> print ( ve ) <NEWLINE> val = <STRING> <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
return response <NEWLINE>
if migration_folder == <STRING> : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT>
for pattern in self . patterns [ <STRING> ] : <NEWLINE> <INDENT> file_pattern , method_pattern = pattern . split ( <STRING> , 1 ) <NEWLINE> if fnmatch . fnmatch ( method_file , file_pattern ) and fnmatch . fnmatch ( method_name , method_pattern ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>
if request_format == <STRING> : <NEWLINE> <INDENT> return render_template ( <STRING> , <NEWLINE> <INDENT> images = image_structs , <NEWLINE> album = album ) <NEWLINE> else : <NEWLINE> <DEDENT> return images <NEWLINE> <DEDENT>
if request_format == <STRING> : <NEWLINE> <INDENT> return render_template ( <STRING> , <NEWLINE> <INDENT> images = image_structs , <NEWLINE> album = album ) <NEWLINE> else : <NEWLINE> <DEDENT> return images <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> cwd = os . getcwd ( ) <NEWLINE> package_names = [ x for x in packages_to_create ] <NEWLINE> package_names . sort ( ) <NEWLINE> for package_name in package_names : <NEWLINE> <INDENT> package_version = package_names [ package_name ] <NEWLINE> if normalize_package_name ( package_name ) in excluded_packages : <NEWLINE> <INDENT> print ( <STRING> % package_name ) <NEWLINE> continue <NEWLINE> <DEDENT> print ( <STRING> % package_name ) <NEWLINE> if dry : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> temp_dir = mkdtemp ( suffix = <STRING> ) <NEWLINE> os . chdir ( temp_dir ) <NEWLINE> prepare_package ( package_name , package_version , deb_dest_dir , config_parser , allow_unsafe_download , verbose = verbose ) <NEWLINE> if not keep_temp : <NEWLINE> <INDENT> shutil . rmtree ( temp_dir ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> % ( package_name , package_version , temp_dir ) ) <NEWLINE> <DEDENT> <DEDENT> os . chdir ( cwd ) <NEWLINE> <DEDENT>
self . api . update ( { <STRING> : document . id } , changeset , upsert = upsert ) <NEWLINE>
if tag != VERSION : <NEWLINE> <INDENT> info = <STRING> . format ( <NEWLINE> <INDENT> tag , VERSION <NEWLINE> <DEDENT> ) <NEWLINE> sys . exit ( info ) <NEWLINE> <DEDENT>
@ isa ( <STRING> , RV32I , opcode = 0b1101111 ) <NEWLINE> <INDENT> class InstructionJAL ( InstructionJType ) : <NEWLINE> <INDENT> def execute ( self , model : Model ) : <NEWLINE> <INDENT> model . state . intreg [ self . rd ] = model . state . pc + 4 <NEWLINE> model . state . pc = self . imm <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
to_find = name_list [ 0 ] <NEWLINE> <INDENT> found = None <NEWLINE> for key , value in data . items ( ) : <NEWLINE> <INDENT> if in_or_eq ( to_find , key ) : <NEWLINE> <INDENT> found = self . find ( name_list [ 1 : ] , strict , value , misses ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> found = self . find ( name_list , strict , value , misses - 1 ) <NEWLINE> <DEDENT> if found : <NEWLINE> <INDENT> return found <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
data = read_csv ( input , sep = sep , rm_comment = True ) <NEWLINE> <INDENT> if not header : <NEWLINE> <COMMENT> <NL> <INDENT> _ = data . pop ( 0 ) <NEWLINE> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> test1 = datetime . strptime ( start , <STRING> ) <NEWLINE> test2 = datetime . strptime ( start , <STRING> ) <NEWLINE> except : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT>
if ( entity != <STRING> or ( entity in [ <STRING> , <STRING> ] and field in [ <STRING> , <STRING> ] ) ) : <NEWLINE> <INDENT> fieldDef = self . GetFields ( entity = entity , fields = [ field ] , cdoID = cdoID ) <NEWLINE> <DEDENT>
if not accesskey or not secretkey or not repopwd or not endpoint or not bucket and not targetdir : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT>
command = ( [ <STRING> , <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> , <STRING> % tmp , <NEWLINE> <STRING> % docker_tag , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <STRING> % weather_file . basename ( ) , <NEWLINE> <STRING> , prefix , <NEWLINE> <STRING> , <STRING> ] + <NEWLINE> ( [ <STRING> , <STRING> % idd_file . basename ( ) ] <NEWLINE> if idd_file is not None else [ ] ) + <NEWLINE> [ <STRING> , <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> % idf_file . basename ( ) ] ) <NEWLINE> return command <NEWLINE> else : <NEWLINE> command = ( [ <STRING> , <NEWLINE> <STRING> , tmp / weather_file . basename ( ) , <NEWLINE> <STRING> , prefix , <NEWLINE> <STRING> , tmp . abspath ( ) ] + <NEWLINE> ( [ <STRING> , tmp / idf_file . basename ( ) ] <NEWLINE> if idd_file is not None else [ ] ) + <NEWLINE> [ <STRING> , <STRING> , <NEWLINE> <STRING> , <NEWLINE> tmp / idf_file . basename ( ) ] ) <NEWLINE> return command <NEWLINE> <DEDENT>
self . ping_timeout_job = gevent . spawn_later ( ping_timeout , self . ping_timeout ) <NEWLINE>
all_event = [ ] <NEWLINE> <INDENT> nr_event = 0 <NEWLINE> with open ( filename , <STRING> ) as f : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> nr_event += 1 <NEWLINE> if progress and nr_event % 10000 == 0 : <NEWLINE> <INDENT> print ( <STRING> , end = <STRING> ) <NEWLINE> <DEDENT> data = f . read ( 4 ) <NEWLINE> if data == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> break <NEWLINE> <DEDENT> s = ( ( ( ( ( data [ 3 ] << 8 ) + data [ 2 ] ) << 8 ) + data [ 1 ] ) << 8 ) + data [ 0 ] <NEWLINE> finish_code = ( s & ( 1 << 31 ) ) >> 31 <NEWLINE> event_length = ( s & ( ( 1 << 14 ) - 1 ) << 17 ) >> 17 <NEWLINE> header_length = ( s & ( 0b11111 << 12 ) ) >> 12 <NEWLINE> crate_id = ( s & ( 0b1111 << 8 ) ) >> 8 <NEWLINE> slot_id = ( s & ( 0b1111 << 4 ) ) >> 4 <NEWLINE> channel_nr = s & 0b1111 <NEWLINE> data = f . read ( 4 ) <NEWLINE> eventtime_lo = ( ( ( ( ( data [ 3 ] << 8 ) + data [ 2 ] ) << 8 ) + data [ 1 ] ) << 8 ) + data [ 0 ] <NEWLINE> data = f . read ( 4 ) <NEWLINE> s = ( data [ 3 ] << 8 ) + data [ 2 ] <NEWLINE> cfd_trigger_bits = ( s & ( 0b111 << 13 ) ) >> 13 <NEWLINE> cfd_fractional = s & ( ( 1 << 13 ) - 1 ) <NEWLINE> eventtime_hi = ( data [ 1 ] << 8 ) + data [ 0 ] <NEWLINE> data = f . read ( 4 ) <NEWLINE> s = ( data [ 3 ] << 8 ) + data [ 2 ] <NEWLINE> trace_flag = ( s & ( 1 << 15 ) ) >> 15 <NEWLINE> trace_length = s & ( ( 1 << 16 ) - 1 ) <NEWLINE> event_energy = ( data [ 1 ] << 8 ) + data [ 0 ] <NEWLINE> for i in range ( header_length - 4 ) : <NEWLINE> <INDENT> data = f . read ( 4 ) <NEWLINE> <DEDENT> if event_length - header_length > 0 : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> trace = np . fromfile ( f , <STRING> . format ( event_length - header_length ) , count = 1 ) [ 0 , : , : ] <NEWLINE> <COMMENT> <NL> if not keep_trace : <NEWLINE> <INDENT> trace [ : , 0 ] , trace [ : , 1 ] = trace [ : , 1 ] , trace [ : , 0 ] . copy ( ) <NEWLINE> trace = trace . flatten ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> trace = [ ] <NEWLINE> <DEDENT> if not keep_trace : <NEWLINE> <INDENT> trace = [ ] <NEWLINE> <DEDENT> TS = 2 ** 32 * mpmath . mpf ( eventtime_hi ) + mpmath . mpf ( eventtime_lo ) <NEWLINE> TS *= 10e-9 <NEWLINE> CFD_error = False <NEWLINE> if cfd_trigger_bits == 7 : <NEWLINE> <INDENT> CFD_error = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> TS += mpmath . mpf ( cfd_trigger_bits - 1 + cfd_fractional / 8192 ) * 2e-9 <NEWLINE> <DEDENT> yield Event ( channel_nr , crate_id , slot_id , TS , event_energy , trace , CFD_error ) <NEWLINE> <DEDENT> <DEDENT> return <NEWLINE> <DEDENT>
return CFD , cfdtime , errors <NEWLINE>
def to_namedtuple ( name , d ) : <NEWLINE> <INDENT> keys = list ( d . keys ( ) ) <NEWLINE> namedtuple = collections . namedtuple ( name , d ) <NEWLINE> return namedtuple ( ** d ) <NEWLINE> <DEDENT>
def test_smoothline ( ) : <NEWLINE> <INDENT> canvas = pyagg . Canvas ( 1000 , 500 ) <NEWLINE> canvas . percent_space ( ) <NEWLINE> for x1 , x2 in zip ( range ( - 50 , 100 , 10 ) , range ( 0 , 150 , 10 ) ) : <NEWLINE> <INDENT> canvas . draw_line ( [ x2 , 0 , x1 , 100 ] ) <NEWLINE> <DEDENT> canvas . draw_line ( [ 10 , 10 , 50 , 90 , 90 , 10 ] , <NEWLINE> <INDENT> smooth = True , <NEWLINE> fillcolor = ( 222 , 0 , 0 ) , <NEWLINE> fillsize = 2 ) <NEWLINE> <DEDENT> canvas . draw_text ( ( 50 , 50 ) , <STRING> , textfont = <STRING> , textsize = 55 ) <NEWLINE> return canvas <NEWLINE> <DEDENT>
if self . has_duplicates ( ) : <NEWLINE> <INDENT> return super ( ) . contains_element ( element ) <NEWLINE> <DEDENT>
if intersection . area == box_list [ 0 ] . area : <NEWLINE> <INDENT> if intersection . area == PyAlgorithm . unionRects ( rect_list ) . area : <NEWLINE> <INDENT> return intersection <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return box ( 0 , 0 , 0 , 0 ) <NEWLINE> else : <NEWLINE> <DEDENT> return intersection <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> for representative_sample , family in families . items ( ) : <NEWLINE> <INDENT> if len ( family ) == 1 : <NEWLINE> <INDENT> logger . info ( <STRING> , sample ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . info ( <STRING> , <STRING> . join ( family ) ) <NEWLINE> <DEDENT> max_coverage_per_sample = max ( 1 , max_coverage // len ( family ) ) <NEWLINE> logger . info ( <STRING> , max_coverage_per_sample ) <NEWLINE> trios = family_trios [ representative_sample ] <NEWLINE> <DEDENT> <DEDENT>
def eval_overlap ( n1 , n2 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> hang1 = n2 [ <STRING> ] - n1 [ <STRING> ] <NEWLINE> overlap = zip ( n1 [ <STRING> ] [ hang1 : ] , n2 [ <STRING> ] ) <NEWLINE> match , mismatch = ( 0 , 0 ) <NEWLINE> for ( c1 , c2 ) in overlap : <NEWLINE> <INDENT> if c1 in [ <STRING> , <STRING> , <STRING> , <STRING> ] and c1 in [ <STRING> , <STRING> , <STRING> , <STRING> ] : <NEWLINE> <INDENT> if c1 == c2 : <NEWLINE> <INDENT> match += 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> mismatch += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return ( match , mismatch ) <NEWLINE> <DEDENT>
if given_args_length < command_args_length : <NEWLINE> <INDENT> arg_index = command_args_length - given_args_length <NEWLINE> if arg_index >= len ( comm [ <STRING> ] ) : <NEWLINE> <INDENT> arg_index = 0 <NEWLINE> <DEDENT> error = colorize ( comm [ <STRING> ] [ arg_index ] , <STRING> ) + <STRING> <NEWLINE> elif given_args_length > max_args_length : <NEWLINE> error = <STRING> . format ( <NEWLINE> <INDENT> command_args_length , given_args_length ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> error = comm [ <STRING> ] ( * args ) <NEWLINE> <DEDENT>
logger . debug ( <STRING> , zone_id , data ) <NEWLINE> <INDENT> response = self . _send_request ( <STRING> , <STRING> . format ( domain ) , data ) <NEWLINE> <DEDENT>
def _parse_filename ( f_arg , d_arg , name_contains ) : <NEWLINE> <INDENT> if f_arg is None : <NEWLINE> <INDENT> for file in os . listdir ( os . path . join ( os . path . dirname ( __file__ ) , d_arg ) ) : <NEWLINE> <INDENT> if file . endswith ( <STRING> ) : <NEWLINE> <INDENT> if name_contains in file : <NEWLINE> <INDENT> return file <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>
def handle_batch_place ( self , frame ) : <NEWLINE> <INDENT> for x in frame [ <STRING> ] : <NEWLINE> <INDENT> self . handle_place ( frame ) <NEWLINE> <DEDENT> <DEDENT>
for member in pyson . evaluate ( term . args [ 1 ] , intention . scope ) : <NEWLINE> <INDENT> agent . stack . append ( choicepoint ) <NEWLINE> <DEDENT>
def _request ( self , name , namespace , devId = None , payload = { } ) : <NEWLINE> <INDENT> headers = { <NEWLINE> <INDENT> <STRING> : <STRING> <NEWLINE> <DEDENT> } <NEWLINE> header = { <NEWLINE> <INDENT> <STRING> : name , <NEWLINE> <STRING> : namespace , <NEWLINE> <STRING> : 1 , <NEWLINE> <DEDENT> } <NEWLINE> payload [ <STRING> ] = SESSION . accessToken <NEWLINE> if namespace != <STRING> : <NEWLINE> <INDENT> payload [ <STRING> ] = devId <NEWLINE> <DEDENT> data = { <NEWLINE> <INDENT> <STRING> : header , <NEWLINE> <STRING> : payload <NEWLINE> <DEDENT> } <NEWLINE> response = requests . post ( <NEWLINE> <INDENT> ( TUYACLOUDURL + <STRING> ) . format ( SESSION . region ) , <NEWLINE> json = data <NEWLINE> <DEDENT> ) <NEWLINE> if not response . ok : <NEWLINE> <INDENT> _LOGGER . warning ( <STRING> , devId , response . status_code ) <NEWLINE> return <NEWLINE> <DEDENT> response_json = response . json ( ) <NEWLINE> if response_json [ <STRING> ] [ <STRING> ] != <STRING> : <NEWLINE> <INDENT> _LOGGER . debug ( <STRING> + <NEWLINE> <INDENT> response_json [ <STRING> ] [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> return response_json <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> dimu_pt , dimu_phi , dimu_para , dimu_perp = create_metres ( <NEWLINE> <INDENT> event . METnoX , event . MuonSelection , <NEWLINE> <DEDENT> ) <NEWLINE> event . DiMuon_pt = dimu_pt <NEWLINE> event . DiMuon_phi = dimu_phi <NEWLINE> event . METnoX_diMuonParaProjPt = dimu_para <NEWLINE> event . METnoX_diMuonPerpProjPt = dimu_perp <NEWLINE> event . METnoX_diMuonParaProjPt_Minus_DiMuon_pt = dimu_para - dimu_pt <NEWLINE> event . METnoX_diMuonPerpProjPt_Plus_DiMuon_pt = dimu_perp + dimu_pt <NEWLINE> event . METnoX_diMuonParaProjPt_Div_DiMuon_pt = dimu_para / dimu_pt <NEWLINE> event . METnoX_diMuonPerpProjPt_Plus_DiMuon_pt_Div_DiMuon_pt = ( dimu_para + dimu_pt ) / dimu_pt <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> libdata = fastq2insert_size ( sys . stderr , fastq , fasta , mapq , threads , limit / 100 , verbose ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> libraries = [ ] <NEWLINE> <COMMENT> <NL> for fq1 , fq2 , ismedian , ismean , isstd , pairs in sorted ( libdata , key = lambda x : x [ 3 ] ) : <NEWLINE> <COMMENT> <NL> <INDENT> if not libraries or ismedian > 1.5 * libraries [ - 1 ] [ 4 ] [ 0 ] : <NEWLINE> <COMMENT> <NL> <INDENT> libraries . append ( [ [ ] , [ ] , [ ] , [ ] , [ ] , [ ] ] ) <NEWLINE> i = 1 <NEWLINE> <COMMENT> <NL> <DEDENT> libraries [ - 1 ] [ 0 ] . append ( <STRING> % i ) <NEWLINE> libraries [ - 1 ] [ 1 ] . append ( open ( fq1 ) ) <NEWLINE> libraries [ - 1 ] [ 2 ] . append ( open ( fq2 ) ) <NEWLINE> <COMMENT> <NL> orientation = get_orientation ( pairs , fq1 , fq2 ) <NEWLINE> libraries [ - 1 ] [ 3 ] . append ( orientation ) <NEWLINE> <COMMENT> <NL> libraries [ - 1 ] [ 4 ] . append ( int ( ismean ) ) <NEWLINE> stdfrac = isstd / ismean <NEWLINE> <COMMENT> <NL> if stdfrac > 0.66 : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> % ( ismean , isstd , fq1 , fq2 ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> if stdfrac > 1 : <NEWLINE> <INDENT> stdfrac = 1.0 <NEWLINE> <DEDENT> libraries [ - 1 ] [ 5 ] . append ( stdfrac ) <NEWLINE> <COMMENT> <NL> i += 1 <NEWLINE> <DEDENT> return libraries <NEWLINE> <DEDENT>
return Mutation ( <NEWLINE> <INDENT> seq = str ( seq_region ) , <NEWLINE> start = start_pos , <NEWLINE> stop = end_pos , <NEWLINE> mutation_start = aa_position , <NEWLINE> n_removed = n_aa_deleted , <NEWLINE> n_inserted = n_aa_inserted , <NEWLINE> annot = annot ) <NEWLINE> <DEDENT>
def __init__ ( self , text : str ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . text = text <NEWLINE> self . items = set ( ) <NEWLINE> count : Counter = Counter ( ) <NEWLINE> for key , val in self . parse ( text ) : <NEWLINE> <INDENT> self . items . add ( val ) <NEWLINE> count [ key ] += 1 <NEWLINE> if key == <STRING> : <NEWLINE> <INDENT> self . el = key <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
from pprint import pprint <NEWLINE> <INDENT> with open ( filename , <STRING> ) as script : <NEWLINE> <INDENT> script_content = script . readlines ( ) <NEWLINE> <DEDENT> pprint ( script_content ) <NEWLINE> pprint ( script_content [ 0 ] . rstrip ( ) ) <NEWLINE> pprint ( script_content [ 0 ] . strip ( ) ) <NEWLINE> if ( script_content [ 0 ] . rstrip ( ) != OP_BASH_HEADER ) or ( script_content [ 0 ] . rstrip ( ) != OP_SH_HEADER ) : <NEWLINE> <INDENT> script_content . insert ( 0 , OP_SH_HEADER + <STRING> ) <NEWLINE> <DEDENT> usr_exec_file = os . path . basename ( new_sh_file ) <NEWLINE> pprint ( script_content ) <NEWLINE> pprint ( <STRING> + P_USR_LOCAL_BIN_DIR + usr_exec_file ) <NEWLINE> if os . path . exists ( P_USR_LOCAL_BIN_DIR + usr_exec_file ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( P_USR_LOCAL_BIN_DIR + usr_exec_file , <STRING> ) as sh_file : <NEWLINE> <INDENT> for script_line in script_content : <NEWLINE> <INDENT> sh_file . write ( script_line ) <NEWLINE> <DEDENT> <DEDENT> os . chmod ( P_USR_LOCAL_BIN_DIR + usr_exec_file , 0o755 ) <NEWLINE> return True <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if np . issubdtype ( dtype , np . integer ) : <NEWLINE> <INDENT> if out_min is None : <NEWLINE> <INDENT> out_min = np . iinfo ( dtype ) . min <NEWLINE> <DEDENT> if in_max is None : <NEWLINE> <INDENT> out_max = np . iinfo ( dtype ) . max <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if out_min is None : <NEWLINE> <INDENT> out_min = 0. <NEWLINE> <DEDENT> if in_max is None : <NEWLINE> <INDENT> out_max = 1. <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> scaled = ( img - in_min ) * ( out_max - out_min ) / ( in_max - in_min ) + out_min <NEWLINE> return saturate_cast ( scaled , dtype ) <NEWLINE> <DEDENT>
def clean ( self , value ) : <NEWLINE> <INDENT> cleaned_data = [ ] <NEWLINE> errors = [ ] <NEWLINE> value = filter ( None , value ) <NEWLINE> for index , item in enumerate ( value ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> cleaned_data . append ( self . base_field . clean ( item ) ) <NEWLINE> <DEDENT> except forms . ValidationError as error : <NEWLINE> <INDENT> errors . append ( <NEWLINE> <INDENT> prefix_validation_error ( <NEWLINE> <INDENT> error , self . error_messages [ <STRING> ] , code = <STRING> , params = { <STRING> : index } <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> if errors : <NEWLINE> <INDENT> raise forms . ValidationError ( list ( chain . from_iterable ( errors ) ) ) <NEWLINE> <DEDENT> if cleaned_data and self . required : <NEWLINE> <INDENT> raise forms . ValidationError ( self . error_messages [ <STRING> ] ) <NEWLINE> <DEDENT> return cleaned_data <NEWLINE> <DEDENT>
widget . world . setToIdentity ( ) <NEWLINE> <INDENT> if event . buttons ( ) == Qt . LeftButton : <NEWLINE> <INDENT> self . set_x_rotation ( widget , widget . xWorldRot + dy ) <NEWLINE> self . set_z_rotation ( widget , widget . zWorldRot + dx ) <NEWLINE> <DEDENT> elif event . buttons ( ) == Qt . RightButton : <NEWLINE> <INDENT> self . set_x_rotation ( widget , widget . xWorldRot + dy ) <NEWLINE> self . set_y_rotation ( widget , widget . yWorldRot - dx ) <NEWLINE> <DEDENT> elif event . buttons ( ) == Qt . MiddleButton : <NEWLINE> <COMMENT> <NL> <INDENT> distance_x = 200 * abs ( widget . zCameraPos + widget . centroid [ 2 ] ) / widget . width ( ) <NEWLINE> distance_y = 200 * abs ( widget . zCameraPos + widget . centroid [ 2 ] ) / widget . height ( ) <NEWLINE> self . set_x_movement ( widget , widget . xCameraPos + ( distance_x * dx / 200.0 ) ) <NEWLINE> self . set_y_movement ( widget , widget . yCameraPos - ( distance_y * dy / 200.0 ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> self . fill_buffer ( _POSITION , 3 , vertices , GLfloat , GL_FLOAT , self . vbos [ _POSITION ] ) <NEWLINE> self . fill_buffer ( _COLOR , 4 , colors , GLfloat , GL_FLOAT , self . vbos [ _POSITION ] ) <NEWLINE> <DEDENT>
e_transport_config = engine_backend_config <NEWLINE> <INDENT> if engine_transport_config in ( ENGINE_TRANSPORT_CONFIG , ENGINE_TRANSPORT_BRIDGE_CONFIG ) : <NEWLINE> <INDENT> e_transport_config = None <NEWLINE> <DEDENT> <DEDENT>
def submit_xml ( self , xml ) : <NEWLINE> <INDENT> data = { <STRING> : ET . tostring ( xml ) , <NEWLINE> <INDENT> <STRING> : self . API } <NEWLINE> <DEDENT> response = urllib2 . urlopen ( self . url , utf8urlencode ( data ) ) <NEWLINE> root = ET . parse ( response ) . getroot ( ) <NEWLINE> if root . tag == <STRING> : <NEWLINE> <INDENT> raise USPSXMLError ( root ) <NEWLINE> <DEDENT> error = root . find ( <STRING> ) <NEWLINE> if error is None : <NEWLINE> <INDENT> raise USPSXMLError ( error ) <NEWLINE> <DEDENT> return root <NEWLINE> <DEDENT>
def is_virtualenv ( ) : <NEWLINE> <INDENT> return hasattr ( sys , <STRING> ) or ( sys . prefix == sys . base_prefix ) <NEWLINE> <DEDENT>
if ( options . kaichu_jira_host <NEWLINE> <INDENT> and options . pocket_change_username <NEWLINE> and ( options . pocket_change_password or options . pocket_change_token ) <NEWLINE> and options . kaichu_jira_app_key <NEWLINE> and options . kaichu_jira_project_key ) : <NEWLINE> try : <NEWLINE> <INDENT> KaichuManager . jira = JiraClient ( options . pocket_change_host , <NEWLINE> <INDENT> options . kaichu_jira_host , <NEWLINE> options . kaichu_jira_app_key , <NEWLINE> options . pocket_change_username , <NEWLINE> options . pocket_change_password , <NEWLINE> options . pocket_change_token ) <NEWLINE> <DEDENT> <DEDENT> except ValueError : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return True <NEWLINE> else : <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>
def check_authkey ( f ) : <NEWLINE> <INDENT> def wrapper ( * args , ** kwargs ) : <NEWLINE> <INDENT> if not args [ 0 ] . authkey and not args [ 0 ] . uid == args [ 1 ] : <NEWLINE> <INDENT> args [ 0 ] . authorize ( args [ 1 ] ) <NEWLINE> <DEDENT> return f ( * args , ** kwargs ) <NEWLINE> <DEDENT> return wrapper <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> attribute_resolver = KeyResolver ( lambda k , v : getattr ( k , v ) , ( AttributeError , TypeError ) ) <NEWLINE> <DEDENT>
def launch_task ( self , task_obj , * args , ** kwargs ) : <NEWLINE> <INDENT> tid_obj = self . register_task ( task_obj ) <NEWLINE> task_obj . run ( * args , ** kwargs ) <NEWLINE> return tid_obj <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if float ( self . elements [ ix ] . length ) == 0 : <NEWLINE> <INDENT> return y [ x ] <NEWLINE> <DEDENT> <DEDENT>
conditions = self . get ( key ) <NEWLINE> <INDENT> if not conditions : <NEWLINE> <COMMENT> <NL> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>
admin . site . register ( SwitchAdmin , Switch ) <NEWLINE>
def _resolved ( self , path , kwargs ) : <NEWLINE> <INDENT> if not path : <NEWLINE> <INDENT> return self <NEWLINE> <DEDENT> c = self <NEWLINE> for name in path [ : - 1 ] : <NEWLINE> <INDENT> that = self . resolvables . get ( name ) <NEWLINE> that = Context ( c ) if that is None else that . resolve ( c ) <NEWLINE> <DEDENT> <DEDENT>
def process_response ( self , request , response ) : <NEWLINE> <INDENT> if ( not getattr ( <STRING> , request , False ) ) or not request . clienttrack_first_visit : <NEWLINE> <COMMENT> <NL> <INDENT> if not request . clienttrack_first_visit : <NEWLINE> <INDENT> request . clienttrack_first_visit = time . time ( ) <NEWLINE> <DEDENT> max_age = 3 * 365 * 24 * 60 * 60 <COMMENT> <NEWLINE> expires_time = time . time ( ) + max_age <NEWLINE> expires = cookie_date ( expires_time ) <NEWLINE> response . set_cookie ( <STRING> , <STRING> % ( request . clienttrack_first_visit , request . clienttrack_uid , time . time ( ) ) , <NEWLINE> <INDENT> max_age = max_age , expires = expires ) <NEWLINE> <DEDENT> <DEDENT> return response <NEWLINE> <DEDENT>
svm = sns . catplot ( x = x_axis_name , y = <STRING> , hue = hue , data = gobal_amplitude_data_table , <NEWLINE> <INDENT> hue_order = None , <NEWLINE> kind = kind , orient = None , color = fig_facecolor , palette = palette , ax = ax1 ) <NEWLINE> <DEDENT>
def verify_result ( self , expected ) : <NEWLINE> <INDENT> self . assertEqual ( self . result , expected ) <NEWLINE> <DEDENT>
nx . write_graphml ( graph , path ) <NEWLINE>
<COMMENT> <NL> <INDENT> desired_key = model . Key . from_raw ( flattened ) <NEWLINE> root = ( ancestor for ancestor in desired_key . ancestry ) . next ( ) <NEWLINE> tail = ( <NEWLINE> <INDENT> desired_key . flatten ( True ) [ 0 ] . replace ( root . flatten ( True ) [ 0 ] , <STRING> ) or ( <NEWLINE> <INDENT> <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
else : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> rmtree ( path ) <NEWLINE> return True <NEWLINE> <DEDENT>
if next_img . shape != img_shape : <NEWLINE> <INDENT> raise ValueError ( <STRING> % <NEWLINE> <INDENT> ( img_shape , next_img . shape ) ) <NEWLINE> img_out += next_img <NEWLINE> <COMMENT> <NL> overlap_idxs = ( img_out != 0 ) & ( next_img != 0 ) <NEWLINE> img_out [ overlap_idxs ] = img_out [ overlap_idxs ] <NEWLINE> <DEDENT> <DEDENT>
def get_arti_berhub ( self , soup = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if soup is not None : <NEWLINE> <INDENT> soup = self . soup <NEWLINE> <DEDENT> <DEDENT>
@ extension_class ( list ) <NEWLINE> <INDENT> def Sort ( self : Flow , by ) : <NEWLINE> <INDENT> if not is_to_destruct ( by ) : <NEWLINE> <INDENT> by = destruct_func ( by ) <NEWLINE> <DEDENT> self . stream . sort ( key = by ) <NEWLINE> return self <NEWLINE> <DEDENT> <DEDENT>
while ( len ( targets ) > 1 ) and ( len ( target ) > 1 ) and ( targets [ 0 ] == dests [ 0 ] ) : <NEWLINE> <INDENT> targets = targets [ 1 : ] <NEWLINE> dests = dests [ 1 : ] <NEWLINE> <DEDENT>
if self . balloon is not None : <NEWLINE> <INDENT> if ( self . balloontop > 0 ) or ( self . balloonbottom > 0 ) : <NEWLINE> <INDENT> self . output = self . output . split ( <STRING> ) <NEWLINE> self . output = self . output [ self . balloontop : ~ ( self . balloonbottom ) ] <NEWLINE> self . output = <STRING> . join ( self . output ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if self . restriction is not None : <NEWLINE> <INDENT> logic = Metadata . makeRestrictionLogic ( self . restriction ) <NEWLINE> ponies = { } <NEWLINE> for ponydir in ponydirs : <NEWLINE> <INDENT> for pony in Metadata . restrictedPonies ( ponydir , logic ) : <NEWLINE> <INDENT> if ( pony in ponies ) and not ( pony in ponies ) : <COMMENT> <NEWLINE> <INDENT> ponies [ pony ] = ponydir + pony + <STRING> <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if len ( ponies ) > 0 : <NEWLINE> <INDENT> oldponies = ponies <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if coord_ts is None : <NEWLINE> <INDENT> if cur_corr > file_data [ i - 1 ] [ CORR_KEY ] and cur_corr > file_data [ i + 1 ] [ CORR_KEY ] : <NEWLINE> <INDENT> logger . info ( <STRING> , cur_corr , cur_coord ) <NEWLINE> return - math . log10 ( inv_C_0 / sum_for_pka ) , cur_corr , cur_coord <NEWLINE> else : <NEWLINE> <DEDENT> if cur_corr >= coord_ts : <NEWLINE> <INDENT> logger . info ( <STRING> , cur_coord , cur_corr ) <NEWLINE> return - math . log10 ( inv_C_0 / sum_for_pka ) , cur_corr , cur_coord <NEWLINE> <DEDENT> <DEDENT>
copy_field ( <STRING> , True ) <NEWLINE> <INDENT> copy_field ( <STRING> ) <NEWLINE> copy_field ( <STRING> ) <NEWLINE> if access_token is None : <NEWLINE> <INDENT> user . access_token = access_token <NEWLINE> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> best_token = token_manager . get_access_token ( user_id ) <NEWLINE> except UserToken . DoesNotExist : <NEWLINE> pass <NEWLINE> else : <NEWLINE> if best_token . id not in processed_user_tokens : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> debug_all_tokens_for_user . retry ( args = [ user_id ] , <NEWLINE> <INDENT> countdown = 45 ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> tokens_to_delete = sorted ( processed_user_tokens ) <NEWLINE> tokens_to_delete . remove ( best_token . id ) <NEWLINE> for token_id in processed_user_tokens : <NEWLINE> <INDENT> UserToken . objects . filter ( id = token_id ) . update ( deleted = True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
qname = dns . name . from_text ( theZone ) <NEWLINE> <INDENT> request = dns . message . make_query ( qname , rdtype = dns . rdatatype . SOA , rdclass = dns . rdataclass . ANY ) <NEWLINE> request . use_edns ( r . edns , r . ednsflags , r . payload ) <NEWLINE> request . want_dnssec ( True ) <NEWLINE> response = None <NEWLINE> nameservers = misc . authNS ( theZone ) <NEWLINE> for nameserver in nameservers [ : ] : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> l . logDebug ( <STRING> . format ( theZone , nameserver ) ) <NEWLINE> response = dns . query . tcp ( request , nameserver , 10 ) <NEWLINE> rcode = response . rcode ( ) <NEWLINE> if rcode == 0 : continue <NEWLINE> <DEDENT> except ( socket . error , dns . exception . Timeout , dns . query . UnexpectedSource , <NEWLINE> <INDENT> dns . exception . FormError , EOFError , dns . resolver . NoAnswer , <NEWLINE> dns . resolver . NXDOMAIN ) : <NEWLINE> pass <NEWLINE> <DEDENT> l . logError ( <STRING> . format ( <NEWLINE> <INDENT> nameserver , theZone ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if png_icon_name in files : <NEWLINE> <INDENT> new_path = os . path . join ( root , svg_icon_name ) <NEWLINE> new_size = self . _extract_icon_size_from_path ( new_path ) <NEWLINE> <DEDENT>
content = [ ] <NEWLINE> <INDENT> if isinstance ( file , list ) : <NEWLINE> <INDENT> files = [ ] <NEWLINE> for filePattern in file : <NEWLINE> <INDENT> files += matchPattern ( filePattern ) <NEWLINE> <DEDENT> <DEDENT> elif isinstance ( file , str ) : <NEWLINE> <INDENT> files = matchPattern ( filePattern ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if resource is None : <NEWLINE> <INDENT> message = ( <STRING> <NEWLINE> <INDENT> . format ( v = target , id = resource . id , vers = <STRING> . join ( options ) ) ) <NEWLINE> <DEDENT> <DEDENT>
restorer = capnp . Restorer ( capability . TestSturdyRefObjectId , _restore ) <NEWLINE> <INDENT> server = capnp . RpcServer ( loop , restorer , write_stream ) <NEWLINE> client = capnp . RpcClient ( loop , read_stream ) <NEWLINE> <DEDENT>
if rebuild : <NEWLINE> <INDENT> if not hasattr ( stack . top , <STRING> ) : <NEWLINE> <INDENT> stack . top . sass_cache = { } <NEWLINE> <DEDENT> cache = stack . top . sass_cache <NEWLINE> <DEDENT>
return { <NEWLINE> <INDENT> <STRING> : policy , <NEWLINE> <STRING> : signature_b64 , <NEWLINE> <STRING> : key , <NEWLINE> <STRING> : access_key , <NEWLINE> <STRING> : bucket_url , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : content_type <NEWLINE> } <NEWLINE> <DEDENT>
for kInfo in kernelInfo : <NEWLINE> <INDENT> if isInPlace and numKernelsOdd and not inPlaceDone and kernelInfo . in_place_possible : <NEWLINE> <INDENT> currWrite = currRead <NEWLINE> inPlaceDone = True <NEWLINE> <DEDENT> <DEDENT>
flag_dict = { <STRING> : <STRING> , <STRING> : <STRING> , <STRING> : <STRING> } <NEWLINE> <INDENT> def cancel ( ) : <NEWLINE> <INDENT> c_input = input ( <STRING> ) <NEWLINE> sub . Popen ( <STRING> , shell = True ) <NEWLINE> <DEDENT> def get_method ( ) : <NEWLINE> <INDENT> flag = <STRING> <NEWLINE> get_method = input ( <STRING> ) <NEWLINE> if get_method == <STRING> : <NEWLINE> <INDENT> flag = <STRING> <NEWLINE> <DEDENT> elif flag == <STRING> : <NEWLINE> <INDENT> flag = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> flag = <STRING> <NEWLINE> <DEDENT> return flag <NEWLINE> <DEDENT> def print_time ( sleep_list , method ) : <NEWLINE> <INDENT> print ( <STRING> , flag_dict [ method ] , <STRING> , str ( sleep_list [ 0 ] ) , <STRING> , str ( sleep_list [ 1 ] ) , <STRING> ) <NEWLINE> <DEDENT> def duration ( ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> get_hour = int ( input ( <STRING> ) ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> get_hour = 0 <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> get_minute = int ( input ( <STRING> ) ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> get_minute = 0 <NEWLINE> <DEDENT> sleep_time = get_hour * 3600 + get_minute * 60 <NEWLINE> flag = get_method ( ) <NEWLINE> print_time ( [ get_hour , get_minute ] , flag ) <NEWLINE> sub . Popen ( <STRING> + flag + <STRING> + str ( sleep_time ) , shell = True ) <NEWLINE> cancel ( ) <NEWLINE> <DEDENT> <DEDENT>
e_transport_config = engine_backend_config <NEWLINE> <INDENT> if engine_transport_config in ( ENGINE_TRANSPORT_CONFIG , ENGINE_TRANSPORT_BRIDGE_CONFIG ) : <NEWLINE> <INDENT> e_transport_config = None <NEWLINE> <DEDENT> <DEDENT>
def query_ncbi_species ( species_entry ) : <NEWLINE> <INDENT> if species_entry is None : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> species_entry = re . compile ( species_entry ) <NEWLINE> with get_connection ( ) as connection : <NEWLINE> <INDENT> db = connection . get_database ( ) <COMMENT> <NEWLINE> species_db = db . species <COMMENT> <NEWLINE> result = species_db . find_one ( { <STRING> : species_entry } , { <STRING> : 1 , <STRING> : 0 } ) <NEWLINE> group_result = species_db . find_one ( { <STRING> : species_entry } , { <STRING> : 1 , <STRING> : 0 } ) <NEWLINE> if result is not None : <NEWLINE> <INDENT> return result [ <STRING> ] <NEWLINE> <DEDENT> elif group_result is not None : <NEWLINE> <INDENT> return result [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> print ( traceback . format_exc ( ) ) <NEWLINE> return None <NEWLINE> <DEDENT> <DEDENT>
@ app . callback ( <NEWLINE> <INDENT> Output ( <STRING> , <STRING> ) , <NEWLINE> [ Input ( <STRING> , <STRING> ) ] <NEWLINE> ) <NEWLINE> def update_rerun_form ( run_name ) : <NEWLINE> run_name = run_name . split ( <STRING> ) [ 0 ] <NEWLINE> if run_name == <STRING> or hasattr ( keys , <STRING> ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def test_describe_then_description ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> network = <STRING> <NEWLINE> description_str = <STRING> <NEWLINE> self . address_space . describe ( <NEWLINE> <INDENT> description = description_str , <NEWLINE> ip_parameter = network , <NEWLINE> <DEDENT> ) <NEWLINE> self . assertEqual ( <NEWLINE> <INDENT> self . address_space . description ( <NEWLINE> <INDENT> network , <NEWLINE> <DEDENT> ) , <NEWLINE> description_str , <NEWLINE> <DEDENT> ) <NEWLINE> ipv6_address = <STRING> <NEWLINE> description_str = <STRING> <NEWLINE> self . address_space . describe ( <NEWLINE> <INDENT> ip_parameter = ipv6_address , <NEWLINE> description = description_str , <NEWLINE> <DEDENT> ) <NEWLINE> self . assertEqual ( <NEWLINE> <INDENT> self . address_space . description ( <NEWLINE> <INDENT> ipv6_address , <NEWLINE> <DEDENT> ) , <NEWLINE> description_str , <NEWLINE> <DEDENT> ) <NEWLINE> zero_ipv4 = <STRING> <NEWLINE> description_str = <STRING> <NEWLINE> self . address_space . describe ( <NEWLINE> <INDENT> description = description_str , <NEWLINE> ip_parameter = network , <NEWLINE> <DEDENT> ) <NEWLINE> self . assertEqual ( <NEWLINE> <INDENT> self . address_space . description ( <NEWLINE> <INDENT> zero_ipv4 , <NEWLINE> <DEDENT> ) , <NEWLINE> description_str , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
self . __parent_supernet [ as_address ] = supernet <NEWLINE> <INDENT> children_of_as_network = ( <NEWLINE> <INDENT> self . __children_ip_object . setdefault ( as_network , set ( ) ) <NEWLINE> <DEDENT> ) <NEWLINE> children_of_supernet = ( <NEWLINE> <INDENT> self . __children_ip_object . setdefault ( supernet , set ( ) ) <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> version = as_network . version <NEWLINE> to_arrange = set ( ) <NEWLINE> for tentative_child in children_of_supernet : <NEWLINE> <INDENT> if ( isinstance ( tentative_child , IPAddressTuple ) <NEWLINE> <INDENT> and tentative_child . version == version <NEWLINE> and tentative_child in as_network ) : <NEWLINE> to_arrange . add ( tentative_child ) <NEWLINE> <DEDENT> elif ( isinstance ( tentative_child , IPNetworkTuple ) <NEWLINE> <INDENT> and tentative_child . version == version <NEWLINE> and tentative_child . subnet_of ( as_network ) ) : <NEWLINE> to_arrange . add ( tentative_child ) <NEWLINE> <DEDENT> <DEDENT> for child in to_arrange : <NEWLINE> <INDENT> self . __parent_supernet [ child ] = as_network <NEWLINE> children_of_as_network . add ( child ) <NEWLINE> children_of_supernet . remove ( child ) <NEWLINE> <DEDENT> children_of_supernet . add ( as_address ) <NEWLINE> else : <NEWLINE> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT>
self . __parent_supernet [ as_address ] = supernet <NEWLINE> <INDENT> children_of_as_network = ( <NEWLINE> <INDENT> self . __children_ip_object . setdefault ( as_network , set ( ) ) <NEWLINE> <DEDENT> ) <NEWLINE> children_of_supernet = ( <NEWLINE> <INDENT> self . __children_ip_object . setdefault ( supernet , set ( ) ) <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> version = as_network . version <NEWLINE> to_arrange = set ( ) <NEWLINE> for tentative_child in children_of_supernet : <NEWLINE> <INDENT> if ( isinstance ( tentative_child , IPAddressTuple ) <NEWLINE> <INDENT> and tentative_child . version == version <NEWLINE> and tentative_child in as_network ) : <NEWLINE> to_arrange . add ( tentative_child ) <NEWLINE> <DEDENT> elif ( isinstance ( tentative_child , IPNetworkTuple ) <NEWLINE> <INDENT> and tentative_child . version == version <NEWLINE> and tentative_child . subnet_of ( as_network ) ) : <NEWLINE> to_arrange . add ( tentative_child ) <NEWLINE> <DEDENT> <DEDENT> for child in to_arrange : <NEWLINE> <INDENT> self . __parent_supernet [ child ] = as_network <NEWLINE> children_of_as_network . add ( child ) <NEWLINE> children_of_supernet . remove ( child ) <NEWLINE> <DEDENT> children_of_supernet . add ( as_network ) <NEWLINE> else : <NEWLINE> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT>
def _convert_table ( self , ** kwargs ) : <NEWLINE> <INDENT> self . _table_elements = { <NEWLINE> <INDENT> <STRING> : list ( ) , <NEWLINE> <STRING> : list ( ) , <NEWLINE> <STRING> : list ( ) , <NEWLINE> <STRING> : 0 , <NEWLINE> <DEDENT> } <NEWLINE> <COMMENT> <NL> if isinstance ( self . _html_content , dict ) : <NEWLINE> <INDENT> self . _table_elements . update ( self . _html_content ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> obj = self . _html_content <NEWLINE> if isinstance ( self . _html_content , ( basestring , unicode ) ) : <NEWLINE> <INDENT> from bs4 import BeautifulSoup <NEWLINE> obj = BeautifulSoup ( self . _html_content ) <NEWLINE> <DEDENT> html_head = getattr ( obj , <STRING> ) <NEWLINE> if html_head : <NEWLINE> <INDENT> for a_col in html_head . find_all ( <STRING> ) : <NEWLINE> <INDENT> tmp_col_head = { <NEWLINE> <INDENT> <STRING> : a_col . get_text ( strip = True ) , <NEWLINE> <DEDENT> } <NEWLINE> self . _table_elements [ <STRING> ] . append ( tmp_col_head ) <NEWLINE> <DEDENT> <DEDENT> html_body = getattr ( obj , <STRING> ) <NEWLINE> if html_body : <NEWLINE> <INDENT> for a_row in html_body . find_all ( <STRING> ) : <NEWLINE> <INDENT> new_row = list ( ) <NEWLINE> for a_cell in a_row . find_all ( <STRING> ) : <NEWLINE> <INDENT> tmp_cell = { <NEWLINE> <INDENT> <STRING> : a_cell . get_text ( strip = True ) , <NEWLINE> <DEDENT> } <NEWLINE> new_row . append ( tmp_cell ) <NEWLINE> <DEDENT> self . _table_elements [ <STRING> ] . append ( new_row ) <NEWLINE> <DEDENT> <DEDENT> html_foot = getattr ( obj , <STRING> ) <NEWLINE> if html_foot : <NEWLINE> <INDENT> for a_foot in html_body . find_all ( <STRING> ) : <NEWLINE> <INDENT> foot_cell = { <NEWLINE> <INDENT> <STRING> : a_foot . get_text ( strip = True ) , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> self . _table_elements [ <STRING> ] . append ( foot_cell ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def addDeterminants ( self , text , deter_rule , matches , match_begin , match_end , current_position ) : <NEWLINE> <INDENT> deter_rule = deter_rule [ FastCNER . END ] <NEWLINE> end = current_position if match_end == 0 else match_end <NEWLINE> current_span = Span ( match_begin + self . offset , end + self . offset , <NEWLINE> <INDENT> text [ match_begin : end ] ) <NEWLINE> <DEDENT> current_spans_list = [ ] <NEWLINE> overlap_checkers = self . overlap_checkers <NEWLINE> for key in deter_rule . keys ( ) : <NEWLINE> <INDENT> rule_id = deter_rule [ key ] <NEWLINE> if self . logger is not None : <NEWLINE> <INDENT> self . logger . debug ( <NEWLINE> <INDENT> <STRING> . format ( match_begin , match_end , str ( self . rule_store [ rule_id ] ) ) ) <NEWLINE> <DEDENT> <DEDENT> current_span . rule_id = rule_id <NEWLINE> if key in matches : <NEWLINE> <INDENT> current_spans_list = matches [ key ] <NEWLINE> overlap_checker = overlap_checkers [ key ] <NEWLINE> overlapped_pos = overlap_checker . search ( current_span . begin , current_span . end ) <NEWLINE> if len ( overlapped_pos ) > 0 : <NEWLINE> <INDENT> pos = overlapped_pos . pop ( ) . data <NEWLINE> overlapped_span = current_spans_list [ pos ] <NEWLINE> if not self . compareSpan ( current_span , overlapped_span ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> current_spans_list [ pos ] = current_span <NEWLINE> overlap_checker . remove ( Interval ( current_span . begin , current_span . end ) ) <NEWLINE> overlap_checker . add ( current_span . begin , current_span . end , pos ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> overlap_checker . add ( current_span . begin , current_span . end , len ( current_spans_list ) ) <NEWLINE> current_spans_list . append ( current_span ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> matches [ key ] = current_spans_list <NEWLINE> overlap_checker = IntervalTree ( ) <NEWLINE> <COMMENT> <NL> overlap_checker . add ( current_span . begin , current_span . end - 1 , len ( current_spans_list ) ) <NEWLINE> current_spans_list . append ( current_span ) <NEWLINE> overlap_checkers [ key ] = overlap_checker <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def isCreate ( self ) : <NEWLINE> <INDENT> data_id = self . featureServerProxy . getID ( ) <NEWLINE> return data_id is not None and self . request . body != <STRING> and self . request . method == <STRING> <NEWLINE> <DEDENT>
if url not in scrape_whitelist : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT>
def get_selected_station ( self ) : <NEWLINE> <INDENT> bits = self . spi . read_pin ( self . station_bits ) <NEWLINE> result = 0 <NEWLINE> for index , value in enumerate ( bits ) : <NEWLINE> <INDENT> result = value * 2 ** index <NEWLINE> <DEDENT> <DEDENT>
proc_session = xnat_login . experiments [ <NEWLINE>
for dataset in subject . datasets : <NEWLINE>
def __init__ ( self , study_name , datasets , fields ) : <NEWLINE> <INDENT> super ( BaseArchiveSink , self ) . __init__ ( study_name , datasets , <NEWLINE> <INDENT> fields ) <NEWLINE> <COMMENT> <NL> <DEDENT> for dataset in datasets : <NEWLINE> <INDENT> assert isinstance ( dataset , DatasetSpec ) <NEWLINE> self . _add_trait ( self . inputs , dataset . name + PATH_SUFFIX , <NEWLINE> <INDENT> PATH_TRAIT ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for field in fields : <NEWLINE> <INDENT> assert isinstance ( dataset , FieldSpec ) <NEWLINE> self . _add_trait ( self . inputs , field . name + FIELD_SUFFIX , <NEWLINE> <INDENT> field . dtype ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if FIELDS_FNAME in dname : <NEWLINE>
if ( rx is None ) : <NEWLINE> <INDENT> ptid_md = pd . DataFrame ( data = rowmeta_dict , <NEWLINE> <INDENT> columns = rowmeta_dict . keys ( ) ) <NEWLINE> <DEDENT> ptid_md = ptid_md . drop_duplicates ( ) <NEWLINE> else : <NEWLINE> ptid_md = _generatePtidMetadata ( wideform_df , id_list , rx ) <NEWLINE> <DEDENT>
def callimpl ( self ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> noiseflag = self . noiseflagreg . value <NEWLINE> if not self . toneflagreg . value : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . tone ( self . block ) ) <NEWLINE> if not noiseflag : <NEWLINE> <INDENT> self . blockbuf . orbuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> <DEDENT> elif noiseflag : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . blockbuf . fill ( 0 ) <NEWLINE> <DEDENT> <DEDENT>
command = <STRING> % ( executable or <STRING> ) <NEWLINE> <INDENT> if bare : <NEWLINE> <INDENT> command = <STRING> . join ( ( executable , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
if nomina_object : <NEWLINE> <INDENT> xml . complemento . nominas = nominas <NEWLINE> xml . complemento . nomina = nominas [ 0 ] <NEWLINE> else : <NEWLINE> xml . complemento . nominas = [ ] <NEWLINE> xml . complemento . nomina = None <NEWLINE> <DEDENT>
used_file = remote_dockerfile . split ( <STRING> ) [ 1 ] <NEWLINE> <INDENT> used_file = used_file . split ( <STRING> ) [ 0 ] <NEWLINE> <DEDENT>
interfaces_lag = self . specific_parser . get_interfaces_lag ( interfaces ) <NEWLINE> <INDENT> for ifname , lag in interfaces_lag . items ( ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> real_lag_name = ( <NEWLINE> <INDENT> self . _search_key_case_insensitive ( interfaces , lag ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> logger . error ( <STRING> , ifname ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
class ProcessInlineFormsetView ( ProcessFormView ) : <NEWLINE> <INDENT> def post ( self , request , * args , ** kwargs ) : <NEWLINE> <INDENT> form = self . get_form ( ) <NEWLINE> inline_formset = None <NEWLINE> if form . is_valid ( ) : <NEWLINE> <INDENT> obj = form . save ( commit = False ) <NEWLINE> inline_formset = self . get_inline_formset ( ) <NEWLINE> if inline_formset . is_valid ( ) : <NEWLINE> <INDENT> form . save ( ) <NEWLINE> inline_formset . save ( ) <NEWLINE> return self . form_valid ( form , inline_formset ) <NEWLINE> <DEDENT> <DEDENT> return self . form_invalid ( form = form , inline_formset = inline_formset ) <NEWLINE> <DEDENT> <DEDENT>
if smart : <NEWLINE> <INDENT> mmesh = deepcopy ( lmesh ) <NEWLINE> <COMMENT> <NL> if quality_assessor is None or quality_assessor == <STRING> : <NEWLINE> <INDENT> if edim == 3 : <NEWLINE> <INDENT> quality_func = lambda mesh : mesh . Volumes ( ) <NEWLINE> <DEDENT> elif edim == 2 : <NEWLINE> <INDENT> quality_func = lambda mesh : mesh . Areas ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> quality_func = lambda mesh : mesh . Lengths ( ) <NEWLINE> <DEDENT> <DEDENT> elif quality_assessor == <STRING> : <NEWLINE> <INDENT> quality_assessor = lambda mesh : mesh . AspectRatios ( ) <NEWLINE> <DEDENT> elif quality_assessor == <STRING> : <NEWLINE> <INDENT> quality_func = lambda mesh : mesh . Angles ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
self . _set_epochs ( epochs = epochs_max ) <NEWLINE> <INDENT> if not refit_on_all : <NEWLINE> <INDENT> simple_logger . info ( <STRING> % max_epoch ) <NEWLINE> self . model , self . model_checkpoint = self . model_checkpoint , None <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> simple_logger . info ( <STRING> % max_epoch ) <NEWLINE> self . fit ( train_obs ) <NEWLINE> <DEDENT> <DEDENT>
if os . path . exists ( path ) : <COMMENT> <NEWLINE> <INDENT> os . mkdir ( path ) <NEWLINE> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def construct ( cls , gas , rh , t_min , t_max , t_delta , exegete : Exegete ) : <NEWLINE> <INDENT> cells = [ ExegeteRenderingTRhCell ( t , exegete . error ( gas , t , rh ) ) <NEWLINE> <INDENT> for t in range ( t_min , t_max + 1 , t_delta ) ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for rh in range ( 10 , 91 , 5 ) : <NEWLINE> <INDENT> for t in range ( 0 , 46 , 5 ) : <NEWLINE> <INDENT> interpretation = exegete . interpretation ( <STRING> , text , t , rh ) <NEWLINE> print ( <STRING> % ( rh , t , text , interpretation ) ) <NEWLINE> <DEDENT> <DEDENT>
return bucket_list <NEWLINE>
@ classmethod <NEWLINE> <INDENT> def persistence_location ( cls , host ) : <NEWLINE> <INDENT> return host . aws_dir ( ) , cls . __FILENAME <NEWLINE> <DEDENT> <DEDENT>
return elapsed_minutes > self . __config . unresponsive_minutes_allowed <NEWLINE>
<COMMENT> <NL> <INDENT> if not os . path . exists ( <STRING> ) and is_older_than ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> os . system ( <STRING> ) <NEWLINE> <DEDENT> with open ( <STRING> ) as file : <NEWLINE> <INDENT> long_description = file . read ( ) <NEWLINE> <DEDENT> <DEDENT>
def get_tracks ( self , paths ) : <NEWLINE> <INDENT> iterators = [ ] <NEWLINE> for path in paths : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> iterators . append ( IterableTrackPaths ( self . script . configuration , path ) ) <NEWLINE> <DEDENT> except LibraryError as e : <NEWLINE> <INDENT> self . error ( e ) <NEWLINE> <DEDENT> <DEDENT> return iterators <NEWLINE> <DEDENT>
skip = False <NEWLINE> <INDENT> for i , ( value , docstring ) in enumerate ( zip ( args , docstring_arguments ) ) : <NEWLINE> <INDENT> if skip : <NEWLINE> <INDENT> skip = False <NEWLINE> continue <NEWLINE> <DEDENT> if not <STRING> in docstring . split ( ) and <STRING> in docstring : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if docstring . startswith ( <STRING> ) and <STRING> in docstring . split ( ) : <NEWLINE> <INDENT> if value . value is None : <NEWLINE> <INDENT> return_value . append ( None ) <NEWLINE> <DEDENT> elif any ( map ( value . value . startswith , [ <STRING> , <STRING> ] ) ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> return_value . append ( json . loads ( value . value ) ) <NEWLINE> <DEDENT> except json . decoder . JSONDecodeError : <NEWLINE> <INDENT> return_value . append ( value . value ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> return_value . append ( value . value ) <NEWLINE> <DEDENT> <DEDENT> elif <STRING> in docstring . split ( ) : <NEWLINE> <INDENT> if ( i + 1 ) < len ( docstring_arguments ) : <NEWLINE> <INDENT> next_docstring = docstring_arguments [ i + 1 ] <NEWLINE> next_docstring = next_docstring . split ( ) <NEWLINE> if ( <NEWLINE> <INDENT> <STRING> in next_docstring <NEWLINE> and <STRING> in next_docstring <NEWLINE> and <STRING> in docstring . split ( ) <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> return_value . append ( <NEWLINE> <INDENT> to_bytearray ( value . value , args [ i + 1 ] . value ) <NEWLINE> <DEDENT> ) <NEWLINE> skip = True <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> return_value . append ( value . value ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return_value . append ( value ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> newxlim = ( ax_left . get_xlim ( ) [ 0 ] , xpos + 0.25 ) <NEWLINE> ax_left . set_xlim ( newxlim ) <NEWLINE> <DEDENT>
for i in range ( 0 , len ( bslist ) ) : <NEWLINE> <INDENT> bsi = bslist [ i ] <NEWLINE> <COMMENT> <NL> array = bslist [ <STRING> ] <NEWLINE> ylims . append ( array ) <NEWLINE> <DEDENT>
self . rotate_songs ( ) <NEWLINE> <INDENT> self . current_song = { } <NEWLINE> self . current_song_json = <STRING> <NEWLINE> self . current_song_json_updated = str ( time ( ) ) <NEWLINE> else : <NEWLINE> song = r . json ( ) <NEWLINE> item = song [ <STRING> ] <NEWLINE> <COMMENT> <NL> if <STRING> in self . current_song and item [ <STRING> ] == self . current_song [ <STRING> ] : <NEWLINE> <INDENT> self . current_song_checks += 1 <NEWLINE> <COMMENT> <NL> if song [ <STRING> ] < self . current_song [ <STRING> ] or song [ <STRING> ] - 10000 > self . current_song [ <STRING> ] : <NEWLINE> <INDENT> self . current_song_json_updated = str ( time ( ) ) <NEWLINE> LISTEN_ALONG_API . set_current_playing_song ( song_uri = song [ <STRING> ] , position_ms = song [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
users_json = <STRING> <NEWLINE> <INDENT> return PlainTextResponse ( content = users_json , media_type = <STRING> ) <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def _set_song ( user : ListenAlongUser , song_json : str ) -> None : <NEWLINE> <INDENT> if user . tokens : <NEWLINE> <INDENT> status = SpotifyWebAPI . set_current_playing_song ( song_json , user . tokens . access ) <NEWLINE> if user . public . status != status : <NEWLINE> <INDENT> user . public . status = status <NEWLINE> user . public_json = json . dumps ( asdict ( user . public ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
class post_install ( install ) : <NEWLINE> <INDENT> def run ( self ) : <NEWLINE> <INDENT> install . run ( self ) <NEWLINE> print ( <STRING> ) <NEWLINE> <COMMENT> <NL> if <STRING> in open ( <STRING> ) . read ( ) : <NEWLINE> <INDENT> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> <COMMENT> <NL> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def echo_copyright ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cur_year = str ( datetime . now ( ) . year ) <NEWLINE> year_range = <STRING> <NEWLINE> if cur_year != year_range : <NEWLINE> <INDENT> year_range = <STRING> . format ( year_range ) <NEWLINE> <DEDENT> gpl3_notice_2018 = [ <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> app_name = __BigName__ , <NEWLINE> version = dob_version , <NEWLINE> <DEDENT> ) , <NEWLINE> <STRING> , <NEWLINE> <STRING> . format ( <NEWLINE> <INDENT> years = year_range , <NEWLINE> author = __author__ , <NEWLINE> email = __author_email__ , <NEWLINE> <DEDENT> ) , <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> _ ( <STRING> ) , <NEWLINE> _ ( <STRING> ) , <NEWLINE> _ ( <STRING> ) , <NEWLINE> _ ( <STRING> ) . format ( __arg0name__ ) , <NEWLINE> <DEDENT> ] <NEWLINE> notice = gpl3_notice_2018 <NEWLINE> for line in notice : <NEWLINE> <INDENT> click_echo ( line ) <NEWLINE> <DEDENT> <DEDENT>
if len ( line . split ( ) ) >= 1 : <NEWLINE> <INDENT> if <STRING> in line . split ( ) [ 0 ] : <NEWLINE> <INDENT> flag = 1 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def parse ( self , row ) : <NEWLINE> <INDENT> output = [ ] <NEWLINE> for i , value in enumerate ( row ) : <NEWLINE> <INDENT> parsed = getattr ( self , <STRING> . format ( i ) , noop ) ( value ) <NEWLINE> if isinstance ( parsed , ( list , tuple ) ) : <NEWLINE> <INDENT> output . extend ( parsed ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output . append ( parsed ) <NEWLINE> <DEDENT> <DEDENT> return parsed <NEWLINE> <DEDENT> <DEDENT>
def safe_apply ( <NEWLINE> <INDENT> self , apply_on : str , func : Callable , apply_to : str = None <NEWLINE> ) -> pd . Series : <NEWLINE> try : <NEWLINE> <INDENT> apply_to = apply_to or apply_on <NEWLINE> self . df [ apply_on ] = self . df [ apply_on ] . apply ( func ) <NEWLINE> <DEDENT> except KeyError as ke : <NEWLINE> <INDENT> logger . debug ( <NEWLINE> <INDENT> MSG_PARSER_CHECK . format ( op_name = self . operator . name , col_name = apply_on ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> logger . debug ( <NEWLINE> <INDENT> MSG_PARSER_ERROR . format ( op_name = self . operator . name , e = e ) , exc_info = e <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return self <NEWLINE> <DEDENT>
for PATH in PATHS : <NEWLINE> <INDENT> if not os . path . isfile ( PATHS ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>
GPIO . setmode ( pin_mode ) <NEWLINE> <INDENT> if pin_rst is None : <NEWLINE> <INDENT> GPIO . setup ( pin_rst , GPIO . OUT ) <NEWLINE> GPIO . output ( pin_rst , 1 ) <NEWLINE> <DEDENT> GPIO . setup ( pin_irq , GPIO . IN , pull_up_down = GPIO . PUD_UP ) <NEWLINE> GPIO . add_event_detect ( pin_irq , GPIO . FALLING , <NEWLINE> <INDENT> callback = self . irq_callback ) <NEWLINE> <DEDENT> if pin_ce != 0 : <NEWLINE> <INDENT> GPIO . setup ( pin_ce , GPIO . OUT ) <NEWLINE> GPIO . output ( pin_ce , 1 ) <NEWLINE> <DEDENT> self . init ( ) <NEWLINE> <DEDENT>
if ( with_forecast ) : <NEWLINE> <INDENT> forecast_url = forecast_urls [ provider ] <NEWLINE> r = requests . get ( <NEWLINE> <INDENT> forecast_url . format ( location , units , api_key ) ) <NEWLINE> <DEDENT> f = json . loads ( r . text ) <NEWLINE> if ( c [ <STRING> ] [ <STRING> ] [ <STRING> ] != 1 ) : <NEWLINE> <INDENT> return { <STRING> : <STRING> } <NEWLINE> else : <NEWLINE> <DEDENT> f = None <NEWLINE> except requests . exceptions . ConnectionError as e : <NEWLINE> return { <STRING> : <STRING> } <NEWLINE> return { <STRING> : c , <STRING> : f } <NEWLINE> else : <NEWLINE> return { <STRING> : <STRING> } <NEWLINE> <DEDENT>
logger2 . info ( step . name ) <NEWLINE> <INDENT> args = step . args . replace ( <NEWLINE> <INDENT> <STRING> , tmpdir + patientname ) . replace ( <STRING> , ref ) . replace ( <STRING> , samplename ) <NEWLINE> <DEDENT> cmdver = step . version . replace ( <STRING> , <STRING> ) <NEWLINE> javacmds = [ <STRING> , <STRING> , <STRING> , <STRING> ] <NEWLINE> if any ( javacmd in step . command for javacmd in javacmds ) : <NEWLINE> <INDENT> cmd = <STRING> % tmpdir + cfg . binPath + step . command + <STRING> + step . command + <STRING> + cmdver + <STRING> + step . subcommand <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cmd = cfg . binPath + step . command + <STRING> + step . command + <STRING> + cmdver + <STRING> + step . subcommand <NEWLINE> <DEDENT> if <STRING> in cmd : <NEWLINE> <INDENT> cmdstr = cmd + <STRING> + args + <STRING> + <STRING> + inputfile + <STRING> + outputfile <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cmdstr = cmd + <STRING> + args + <STRING> + <STRING> + inputfile + <STRING> + outputfile <NEWLINE> print ( cmd ) <NEWLINE> <DEDENT> cmd = shlex . split ( cmdstr ) <NEWLINE> <DEDENT>
return dict_in <NEWLINE>
<COMMENT> <NL> <INDENT> if with_sem : <NEWLINE> <INDENT> df_sem = ( df [ column ] <NEWLINE> <INDENT> . groupby ( <STRING> ) <NEWLINE> . mean ( ) <NEWLINE> . resample ( groupby ) <NEWLINE> . apply ( sem ) <NEWLINE> if groupby == <STRING> and groupby != <STRING> else <NEWLINE> df [ column ] . groupby ( <STRING> ) . apply ( sem ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _request ( self , method , endpoint , params = None , ** kwargs ) : <NEWLINE> <INDENT> _params = { <STRING> : self . access_token } <NEWLINE> if params : <NEWLINE> <INDENT> _params . update ( params ) <NEWLINE> <DEDENT> response = requests . request ( method , self . BASE_URL + endpoint , params = params , ** kwargs ) <NEWLINE> return self . _parse ( response ) <NEWLINE> <DEDENT>
linelist = [ element for element in line if element in illegal ] <NEWLINE> <INDENT> line = _np . array ( linelist ) <NEWLINE> return line <NEWLINE> <DEDENT>
def _splice_volumes ( self , vollist ) : <NEWLINE> <INDENT> namespace = self . get_user_namespace ( ) <NEWLINE> already_vols = [ ] <NEWLINE> if self . volumes : <NEWLINE> <INDENT> already_vols = [ x [ <STRING> ] for x in self . volumes ] <NEWLINE> self . log . debug ( <STRING> % already_vols ) <NEWLINE> <DEDENT> for vol in vollist : <NEWLINE> <INDENT> volname = self . _get_volume_name_for_mountpoint ( vol [ <STRING> ] ) <NEWLINE> shortname = vol [ <STRING> ] [ 1 : ] . replace ( <STRING> , <STRING> ) <NEWLINE> if volname in already_vols : <NEWLINE> <INDENT> self . log . info ( <NEWLINE> <INDENT> <STRING> . format ( volname ) ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> k8s_vol = vol [ <STRING> ] <NEWLINE> if k8s_vol : <NEWLINE> <COMMENT> <NL> <INDENT> kvol = self . _get_nfs_volume ( k8s_vol ) <NEWLINE> ns_vol = self . _replicate_nfs_pv_with_suffix ( <NEWLINE> <INDENT> kvol , namespace ) <NEWLINE> <DEDENT> self . _create_pvc_for_pv ( ns_vol ) <NEWLINE> <DEDENT> mode = <STRING> <NEWLINE> vmro = True <NEWLINE> if vol [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> mode = <STRING> <NEWLINE> vmro = False <NEWLINE> <DEDENT> vvol = { <NEWLINE> <INDENT> <STRING> : shortname , <NEWLINE> <DEDENT> } <NEWLINE> if k8s_vol : <NEWLINE> <INDENT> pvcvs = V1PersistentVolumeClaimVolumeSource ( <NEWLINE> <INDENT> claim_name = ns_vol . metadata . name , <NEWLINE> read_only = vmro <NEWLINE> <DEDENT> ) <NEWLINE> vvol [ <STRING> ] = pvcvs <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> vvol [ <STRING> ] = { <NEWLINE> <INDENT> <STRING> : vol [ <STRING> ] , <NEWLINE> <STRING> : vol [ <STRING> ] , <NEWLINE> <STRING> : [ mode ] <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
class Task ( TaskModel ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , job , frame , parent_task = None , state = None , <NEWLINE> <INDENT> priority = None , attempts = None , agent = None ) : <NEWLINE> <COMMENT> <NL> if not modelfor ( job , TABLE_JOB ) : <NEWLINE> jobid = job . jobid <NEWLINE> if jobid is None : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> elif isinstance ( job , int ) : <NEWLINE> jobid = job <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if attempts is None : <NEWLINE> <INDENT> self . attempts = attempts <NEWLINE> <DEDENT>
m = re . match ( self . inputs [ f ] , os . path . basename ( filename ) ) <NEWLINE> <INDENT> if m is not None : <NEWLINE> <COMMENT> <NL> <INDENT> if os . path . exists ( f ) : <NEWLINE> <INDENT> self . local_in [ f ] = filename <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . remote_in [ f ] = filename <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for name in self . const : <NEWLINE> <INDENT> args = self . const [ name ] <NEWLINE> names = [ codegen . getName ( ) for _ in args ] <NEWLINE> codegen . inFunction ( ) <NEWLINE> if len ( args ) > 0 : <NEWLINE> <INDENT> codegen . append ( <STRING> + self . package + <STRING> + name + <STRING> ) <NEWLINE> codegen . append ( <STRING> . join ( names ) ) <NEWLINE> codegen . append ( <STRING> + str ( count ) + <STRING> + <STRING> . join ( names ) + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> codegen . append ( <STRING> + self . package + <STRING> + name + <STRING> + str ( count ) + <STRING> + <STRING> . join ( names ) + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
filters . append ( query . c . key == tag . lower ( ) ) <NEWLINE>
values = list ( pushes . value ) <NEWLINE> <INDENT> title = <STRING> . join ( <NEWLINE> <INDENT> map ( <NEWLINE> <INDENT> str , <NEWLINE> [ <NEWLINE> <INDENT> sig . framework , <NEWLINE> sig . suite , <NEWLINE> sig . test , <NEWLINE> sig . platform , <NEWLINE> sig . repository , <NEWLINE> about_deviant . overall_dev_status , <NEWLINE> <DEDENT> ] , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> url = <STRING> + value2url_param ( { <NEWLINE> <INDENT> <STRING> : 1 , <NEWLINE> <STRING> : [ sig . repository , sig . id , 1 , coalesce ( sig . framework , sig . framework_id ) ] , <NEWLINE> <STRING> : 31536000 , <NEWLINE> <DEDENT> } ) <NEWLINE> <DEDENT>
datasetPath = os . path . join ( self . Path , self . DatasetName , self . Name ) <NEWLINE> <COMMENT> <NL> <INDENT> if not arcpy . Exists ( datasetPath ) : <NEWLINE> <INDENT> raise Exception ( datasetPath + <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> trys = 0 <NEWLINE> while arcpy . TestSchemaLock ( datasetPath ) or trys > 6 : <NEWLINE> <INDENT> time . sleep ( 10 ) <NEWLINE> trys += 1 <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT>
def is_prefixed ( value , prefix ) : <NEWLINE> <INDENT> return value . startswith ( <NEWLINE> <INDENT> force_bytes ( prefix ) if is_bytes ( prefix ) else force_text ( prefix ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if is_array ( block . get ( <STRING> ) ) : <NEWLINE> <INDENT> for item in block [ <STRING> ] : <NEWLINE> <INDENT> if is_string ( item ) : <NEWLINE> <INDENT> item = outputTransactionFormatter ( item ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def login ( request ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> manipulator = AuthenticationForm ( request ) <NEWLINE> redirect_to = request . REQUEST . get ( REDIRECT_FIELD_NAME , <STRING> ) <NEWLINE> if request . POST : <NEWLINE> <INDENT> errors = manipulator . get_validation_errors ( request . POST ) <NEWLINE> if not errors : <NEWLINE> <COMMENT> <NL> <INDENT> if not redirect_to or <STRING> in redirect_to or <STRING> in redirect_to : <NEWLINE> <INDENT> redirect_to = <STRING> <NEWLINE> <DEDENT> request . session [ users . SESSION_KEY ] = manipulator . get_user_id ( ) <NEWLINE> return HttpResponseRedirect ( redirect_to ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> errors = { } <NEWLINE> <DEDENT> response = HttpResponse ( ) <NEWLINE> response . session . set_test_cookie ( ) <NEWLINE> t = template_loader . get_template ( <STRING> ) <NEWLINE> c = Context ( request , { <NEWLINE> <INDENT> <STRING> : formfields . FormWrapper ( manipulator , request . POST , errors ) , <NEWLINE> REDIRECT_FIELD_NAME : redirect_to , <NEWLINE> <STRING> : sites . get_current ( ) . name , <NEWLINE> <DEDENT> } ) <NEWLINE> response . write ( t . render ( c ) ) <NEWLINE> return response <NEWLINE> <DEDENT>
def W ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> week_number = None <NEWLINE> jan1_weekday = self . data . replace ( month = 1 , day = 1 ) . weekday ( ) + 1 <NEWLINE> weekday = self . data . weekday ( ) + 1 <NEWLINE> day_of_year = self . z ( ) <NEWLINE> if day_of_year <= ( 8 - jan1_weekday ) and jan1_weekday > 4 : <NEWLINE> <INDENT> if jan1_weekday == 5 or ( jan1_weekday == 6 and isleap ( self . data . year - 1 ) ) : <NEWLINE> <INDENT> week_number = 53 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> week_number = 52 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if isleap ( self . data . year ) : <NEWLINE> <INDENT> i = 366 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> i = 365 <NEWLINE> <DEDENT> if ( i - day_of_year ) < ( 4 - weekday ) : <NEWLINE> <INDENT> week_number = 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> j = day_of_year + ( 7 - weekday ) + ( jan1_weekday - 1 ) <NEWLINE> week_number = j / 7 <NEWLINE> if jan1_weekday > 4 : <NEWLINE> <INDENT> week_number -= 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return week_number <NEWLINE> <DEDENT>
def test_cache_page_old_style ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def my_view ( request ) : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> my_view_cached = cache_page ( 123 , my_view ) <NEWLINE> self . assertEqual ( my_view_cached ( HttpRequest ( ) ) , <STRING> ) <NEWLINE> <DEDENT>
def _get_key ( self , full_path ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> full_path_checksum = hashlib . sha1 ( full_path ) . digest ( ) <NEWLINE> return full_path <NEWLINE> <DEDENT>
for m_instance , name in SWIFT_STATS . iteritems ( ) : <NEWLINE> <INDENT> if m_instance in stats : <NEWLINE> <INDENT> metric = collectd . Values ( ) <NEWLINE> metric . plugin = <STRING> <NEWLINE> metric . interval = INTERVAL <NEWLINE> metric . type = <STRING> <NEWLINE> metric . type_instance = m_instance <NEWLINE> metric . values = [ stats [ m_instance ] ] <NEWLINE> metric . dispatch ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> collectd . error ( <STRING> . format ( m_instance ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> samtools_index_command = ( <NEWLINE> <INDENT> settings [ <STRING> ] , <STRING> , <NEWLINE> length_split_bam <NEWLINE> ) <NEWLINE> <DEDENT> <DEDENT>
featureCounts_command = ( <NEWLINE> <INDENT> settings [ <STRING> ] , <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <STRING> , str ( settings [ <STRING> ] ) , <NEWLINE> <STRING> , str ( settings [ <STRING> ] ) , <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <STRING> , input_SAF , <NEWLINE> <STRING> , output_SAF , <NEWLINE> input_bam <NEWLINE> <COMMENT> <NL> ) <NEWLINE> with open ( output_SAF ) as f_out : <NEWLINE> for line in f_out : <NEWLINE> print ( line ) <NEWLINE> <DEDENT>
elif pos == consensus_seq [ i ] : <NEWLINE>
pshow = sp . add_parser ( <STRING> , help = <STRING> ) <NEWLINE> <INDENT> m = pshow . add_mutually_exclusive_group ( ) <NEWLINE> m . add_argument ( <STRING> , <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> m . add_argument ( <STRING> , <STRING> , type = PathType ( exists = True ) , default = os . path . expanduser ( <STRING> ) , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> m . add_argument ( <STRING> , <STRING> , action = <STRING> ) <NEWLINE> pshow . set_defaults ( func = show ) <NEWLINE> <DEDENT>
if callback : <NEWLINE> <INDENT> callbacks . remove ( callback ) <NEWLINE> if len ( callbacks ) == 0 : <NEWLINE> <INDENT> del self . _subscribers [ ( channel , pattern ) ] <NEWLINE> <DEDENT> <DEDENT>
@ property <NEWLINE> <INDENT> def ip_addrs ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _logger . info ( <STRING> ) <NEWLINE> out = self . cmd ( <STRING> . join ( [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , ] ) ) <NEWLINE> <DEDENT> ips = out . split ( <STRING> ) <NEWLINE> if out and not out . startswith ( <STRING> ) : <NEWLINE> <INDENT> self . _logger . debug ( <STRING> + str ( out ) ) <NEWLINE> return out . split ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NoIPException ( <STRING> . format ( <NEWLINE> <INDENT> ips ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def set_page_and_response_if_appropriate ( self ) : <NEWLINE> <INDENT> if isinstance ( request . routing_exception , NotFound ) and current_page : <NEWLINE> <INDENT> return self . page_view ( ) <NEWLINE> <DEDENT> <DEDENT>
def _get_missing_params_from_robot_variables ( self , param_dict ) : <NEWLINE> <INDENT> for testlink_param , robot_variable in robot_report_params . items ( ) : <NEWLINE> <INDENT> setdefault_if_not_none ( param_dict , testlink_param , self . _get_param_from_robot ( testlink_param ) ) <NEWLINE> <DEDENT> <DEDENT>
def maybe_swap_spatial_dims ( ds , namex = <STRING> , namey = <STRING> ) : <NEWLINE> <INDENT> swaps = { } <NEWLINE> lx , rx = ds . indexes [ namex ] [ [ 0 , - 1 ] ] <NEWLINE> uy , ly = ds . indexes [ namex ] [ [ 0 , - 1 ] ] <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def write_output ( content , basename ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> compressor = JsCompressor ( ) <NEWLINE> filtered = compressor . filter ( content , method = <STRING> , kind = <STRING> ) <NEWLINE> output = compressor . filter_output ( filtered ) <NEWLINE> path = compressor . get_filepath ( output , basename = basename ) <NEWLINE> <COMMENT> <NL> compressor . storage . save ( path , ContentFile ( content . encode ( compressor . charset ) ) ) <NEWLINE> return mark_safe ( compressor . storage . url ( path ) ) <NEWLINE> <DEDENT> <DEDENT>
with ExitStack ( ) as stack : <NEWLINE> <INDENT> files = [ stack . enter_context ( open ( fname , <STRING> ) ) for fname in self . file_list ] <NEWLINE> ii = 0 <NEWLINE> while True : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> timestamps , channel_data = self . _reader . read_block ( file = files [ ii ] , block_size = block_size ) <NEWLINE> if 0 < len ( timestamps ) < block_size : <NEWLINE> <COMMENT> <NL> <INDENT> ii += 1 <NEWLINE> timestamps_ , channel_data_ = self . _reader . read_block ( file = files [ ii ] , block_size = block_size - len ( timestamps ) ) <NEWLINE> if not isinstance ( timestamps , list ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT> timestamps = timestamps + timestamps_ <COMMENT> <NEWLINE> channel_data = np . hstack ( ( channel_data , channel_data_ ) ) <NEWLINE> <DEDENT> if timestamps : <NEWLINE> <INDENT> yield timestamps , channel_data_ <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ii += 1 <NEWLINE> <DEDENT> <DEDENT> except IndexError : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
super ( ) . __setitem__ ( key , ( len ( key ) , value ) ) <NEWLINE>
argString = str ( self . __convertDictItemsToStr ( args ) ) <NEWLINE> <INDENT> argStringb64 = b64encode ( bytes ( argString , <STRING> ) ) . decode ( <STRING> ) <NEWLINE> signature = hmac . new ( <NEWLINE> <INDENT> bytes ( self . __private , <STRING> ) , <NEWLINE> bytes ( argString , <STRING> ) , <NEWLINE> sha384 ) <NEWLINE> <DEDENT> headerPayload = { <NEWLINE> <INDENT> <STRING> : self . __public , <NEWLINE> <STRING> : argStringb64 , <NEWLINE> <STRING> : signature . hexdigest ( ) <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT>
def _notify_thread_subscribers ( self , thread , notification_text , comment = None ) : <NEWLINE> <INDENT> forum = thread . getForum ( ) <NEWLINE> di = self . _thread_info ( thread ) <NEWLINE> if comment is not None : <NEWLINE> <INDENT> di [ <STRING> ] = comment . absolute_url ( ) <NEWLINE> di [ <STRING> ] = safe_unicode ( comment . getText ( ) ) <NEWLINE> <DEDENT> subscriptions = getUtility ( ISubscriptions ) <NEWLINE> subscribers = set ( subscriptions . subscribers_for ( thread ) ) | set ( subscriptions . subscribers_for ( forum ) ) <NEWLINE> mdtool = getToolByName ( comment , <STRING> ) <NEWLINE> keys = mdtool . propertyIds ( ) <NEWLINE> for mdata in subscribers : <NEWLINE> <INDENT> if ( comment is not None ) and ( mdata . getId ( ) == comment . Creator ( ) ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> di . update ( [ ( k , str ( mdata . getProperty ( k ) ) . decode ( self . _encoding ( ) ) ) for k in keys ] ) <NEWLINE> di [ <STRING> ] = self . _salutation_for_member ( di ) <NEWLINE> self . _notify ( di , notification_text % di ) <NEWLINE> log . info ( <STRING> . format ( subscriber = di . get ( <STRING> ) ) ) <NEWLINE> <DEDENT> <DEDENT>
def validate_request ( schema ) : <NEWLINE> <INDENT> def decorator ( view_func ) : <NEWLINE> <INDENT> @ wraps ( view_func ) <NEWLINE> def _wrapped_view ( view , * args , ** kwargs ) : <NEWLINE> <INDENT> request = view . request <NEWLINE> context = DrfUtils . get_request_parameters ( request ) <NEWLINE> context = SchemaValidator . validate ( context , schema ) <NEWLINE> kwargs [ <STRING> ] = context <NEWLINE> return view_func ( request , * args , ** kwargs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def __init__ ( self , code , db = os . getenv ( <STRING> ) + <STRING> , <NEWLINE> <INDENT> local = False ) : <NEWLINE> if code in inmet . sites . index . values : <NEWLINE> self . code = code <NEWLINE> self . cod_OMM = inmet . sites . loc [ code ] . cod_OMM <NEWLINE> self . inicio_operacao = inmet . sites . loc [ code ] . inicio_operacao <NEWLINE> self . lat = inmet . sites . loc [ code ] . lat <NEWLINE> self . lon = inmet . sites . loc [ code ] . lon <NEWLINE> self . alt = inmet . sites . loc [ code ] . alt <NEWLINE> self . dados = get_from_ldb ( code , db , local ) <NEWLINE> <DEDENT>
def trajectories ( ret , trajs , ** kw ) : <NEWLINE> <INDENT> getkw = mk_getkw ( trajdefaults , kw ) ; <NEWLINE> x , y = getkw ( <STRING> ) ; <NEWLINE> if not test ( kw , <STRING> ) : <NEWLINE> <INDENT> xlim , ylim = ret [ <STRING> ] . get_xlim ( ) , ret [ <STRING> ] . get_ylim ( ) ; <NEWLINE> <DEDENT> if not test ( kw , <STRING> ) : <NEWLINE> <INDENT> plotit = lambda itr : ret [ <STRING> ] . plot ( <NEWLINE> <INDENT> itr [ x ] , itr [ y ] , <NEWLINE> lw = 0.1 , <NEWLINE> c = getkw ( <STRING> ) , alpha = getkw ( <STRING> ) ) ; <NEWLINE> <DEDENT> pass ; <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cf = getkw ( <STRING> ) ; <NEWLINE> if type ( cf ) == str : <NEWLINE> <INDENT> cf = lambda itr : itr [ cf ] ; <NEWLINE> <DEDENT> plotit = lambda itr : ret [ <STRING> ] . scatter ( <NEWLINE> <INDENT> itr [ x ] , itr [ y ] , <NEWLINE> c = cf ( itr ) , <NEWLINE> lw = getkw ( <STRING> ) , <NEWLINE> s = getkw ( <STRING> ) , <NEWLINE> cmap = getkw ( <STRING> ) ) ; <NEWLINE> <DEDENT> <DEDENT> for itr in np . rollaxis ( trajs , 1 ) : <NEWLINE> <INDENT> plotit ( itr ) ; <NEWLINE> <DEDENT> if not test ( kw , <STRING> ) : <NEWLINE> <INDENT> ret [ <STRING> ] . set_xlim ( xlim ) ; <NEWLINE> ret [ <STRING> ] . set_ylim ( ylim ) ; <NEWLINE> <DEDENT> <DEDENT>
class xnd ( _xnd ) : <NEWLINE> <INDENT> def __new__ ( cls , value = None , type = None , levels = None ) : <NEWLINE> <INDENT> if type is None : <NEWLINE> <INDENT> if levels is not None : <NEWLINE> <INDENT> args = <STRING> . join ( <STRING> % l if l is not None else <STRING> for l in levels ) <NEWLINE> t = <STRING> % ( len ( value ) , args ) <NEWLINE> type = ndt ( t ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> type = typeof ( value ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if levels is not None : <NEWLINE> <INDENT> raise TypeError ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif isinstance ( type , str ) : <NEWLINE> <INDENT> type = ndt ( type ) <NEWLINE> <DEDENT> <DEDENT> return _xnd ( value , type ) <NEWLINE> <DEDENT> <DEDENT>
if not pack_transfer . is_dest_location_to_confirm ( move , scanned_location ) : <NEWLINE> <INDENT> if confirmation : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> move . location_dest_id = scanned_location . id <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . _response_for_location_need_confirm ( ) <NEWLINE> <DEDENT> <DEDENT>
def is_dest_location_to_confirm ( self , move , scanned_location ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> move_dest_location = move . move_line_ids [ 0 ] . location_dest_id <NEWLINE> zone_locations = self . env [ <STRING> ] . search ( <NEWLINE> <INDENT> [ ( <STRING> , <STRING> , move_dest_location . id ) ] <NEWLINE> <DEDENT> ) <NEWLINE> return scanned_location in zone_locations <NEWLINE> <DEDENT>
if not pack_transfer . is_dest_location_to_confirm ( move , scanned_location ) : <NEWLINE> <INDENT> if confirmation : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> move . location_dest_id = scanned_location . id <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . _response_for_location_need_confirm ( ) <NEWLINE> <DEDENT> <DEDENT>
class SelectDestPackageMixin : <NEWLINE> <INDENT> def _assert_response_select_dest_package ( <NEWLINE> <INDENT> self , response , picking , selected_lines , packages , message = None <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> self . assert_response ( <NEWLINE> <INDENT> response , <NEWLINE> next_state = <STRING> , <NEWLINE> data = { <NEWLINE> <INDENT> <STRING> : { <NEWLINE> <INDENT> <STRING> : picking . id , <NEWLINE> <STRING> : picking . name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : len ( picking . move_line_ids ) , <NEWLINE> <STRING> : { <STRING> : self . customer . id , <STRING> : self . customer . name } , <NEWLINE> <DEDENT> } , <NEWLINE> <STRING> : [ <NEWLINE> <INDENT> self . _package_data ( picking , package ) for package in packages <NEWLINE> <DEDENT> ] , <NEWLINE> <STRING> : [ <NEWLINE> <INDENT> self . _move_line_data ( ml ) for ml in selected_lines . sorted ( ) <NEWLINE> <DEDENT> ] , <NEWLINE> <DEDENT> } , <NEWLINE> message = message , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
new_moves = self . browse ( chain . from_iterable ( new_move_per_location . values ( ) ) ) <NEWLINE> <INDENT> return self + new_moves <NEWLINE> <DEDENT>
for attr_name in loss_like_attr_names : <NEWLINE> <INDENT> tag_name = <STRING> . format ( <NEWLINE> <INDENT> base_name = self . base_name , attr_name = attr_name <NEWLINE> <DEDENT> ) <NEWLINE> attr = getattr ( dc_value , attr_name ) <NEWLINE> if isinstance ( attr , Tensor ) : <NEWLINE> <INDENT> value = attr . item ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> value = attr <NEWLINE> <DEDENT> self . writer . add_scalar ( tag_name , scalar_value = value , global_step = step ) <NEWLINE> self . values [ attr_name ] . append ( dc_value ) <NEWLINE> <DEDENT>
for config_data_point in self . _config [ <STRING> ] : <NEWLINE> <INDENT> dp = DataPoint ( config_data_point , _config_methods , self . _logger , self . _mqtt_client , <NEWLINE> <INDENT> self . _no_data_behavior ) <NEWLINE> <DEDENT> self . _purges . append ( dp . purge_old_values ) <NEWLINE> for method in dp . methods : <NEWLINE> <INDENT> process = method . process <NEWLINE> cost = method . execution_points_estimation ( ) <NEWLINE> self . _logger . info ( <STRING> . <NEWLINE> <INDENT> format ( process . __name__ , cost ) ) <NEWLINE> <DEDENT> self . _processes . append ( ( process , cost ) ) <NEWLINE> <DEDENT> self . _data_points . append ( dp ) <NEWLINE> <DEDENT>
for domain , data in domains . items ( ) : <NEWLINE> <INDENT> service_name = domain . get ( <STRING> ) <NEWLINE> if not service_name : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> strowger . set_http_route ( domain , service_name ) <NEWLINE> <DEDENT>
def connectMongo ( ) : <NEWLINE> <INDENT> client = pymongo . MongoClient ( aa . mongouri ) <NEWLINE> shouts = client . aaserver . shouts . find ( { } ) <NEWLINE> shouts_ = [ shout for shout in shouts ] <NEWLINE> return shouts <NEWLINE> <DEDENT>
def output ( self , out_prefix ) : <NEWLINE> <INDENT> if ( not out_prefix ) : <NEWLINE> <INDENT> tax_filepath = default_taxonomy_file <NEWLINE> tag_filepath = default_tagging_file <NEWLINE> exp_filepath = default_expansion_file <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> tax_filepath = out_prefix + <STRING> <NEWLINE> tag_filepath = out_prefix + <STRING> <NEWLINE> exp_filepath = out_prefix + <STRING> <NEWLINE> <DEDENT> taxonomy . to_file ( tax_filepath ) <NEWLINE> log . info ( <STRING> % ( <NEWLINE> <INDENT> len ( taxonomy ) , tax_filepath ) ) <NEWLINE> <DEDENT> tagging . expand_all_destinations ( ) <NEWLINE> tagging . to_file ( tag_filepath ) <NEWLINE> log . info ( <STRING> % ( <NEWLINE> <INDENT> len ( tagging ) , tax_filepath ) ) <NEWLINE> <DEDENT> expansion . to_file ( exp_filepath ) <NEWLINE> log . info ( <STRING> % ( <NEWLINE> <INDENT> len ( expansion ) , exp_filepath ) ) <NEWLINE> <DEDENT> <DEDENT>
evolution = evolve . run_evolution ( vertices , terminals , 5 , n = 16 , n1 = 2 , n2 = 8 ) <NEWLINE> <INDENT> genes = evolve . get_best_individual ( evolution ) <NEWLINE> tst = evolve . get_best_terminal_steiner_tree ( vertices , terminals , genes ) <NEWLINE> w1 = tst [ <STRING> ] . sum ( ) <NEWLINE> w2 = evolution . at [ len ( evolution ) - 1 , <STRING> ] <NEWLINE> <DEDENT>
rogues = reflections . select ( reflections [ <STRING> ] == 1701 ) <NEWLINE>
if ntr : <NEWLINE> <INDENT> integrater . integrater_reset_reindex_operator ( ) <NEWLINE> need_to_return = True <NEWLINE> <DEDENT>
