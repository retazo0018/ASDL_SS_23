{
    "2dfc4a8d44ed470b879313e36f78a739": {
        "code_string": "def visualize_attention(self, img_data, attentions, output, label, flag_incorrect):\n         if flag_incorrect:\n             output_dir = os.path.join(self.output_dir, 'incorrect')\n         else:\n             output_dir = os.path.join(self.output_dir, 'correct')\n         filename = label.replace('/', '_').replace(' ', '_')\n         if len(filename) == 0:\n             filename = 'empty_label'\n         output_dir = os.path.join(output_dir, filename)\n         if not os.path.exists(output_dir):\n             os.makedirs(output_dir)\n         with open(os.path.join(output_dir, 'word.txt'), 'w') as fword:\n             fword.write(output+'\\n')\n             fword.write(label)\n             file_img_data = BytesIO(img_data)\n             img = Image.open(file_img_data)\n             w, h = img.size\n             mh = 32\n             mw = math.floor(1. * w / h * mh)\n             img = img.resize(\n                     (mw, h),\n                     Image.ANTIALIAS)\n             img_data = np.asarray(img, dtype=np.uint8)\n             for idx in xrange(len(output)):\n                 output_filename = os.path.join(output_dir, 'image_%d.jpg' % (idx))\n                 attention = attentions[idx][:(int(mw/4)-1)]\n                 attention_orig = np.zeros(mw)\n                 for i in xrange(mw):\n                     if i/4-1 > 0 and i/4-1 < len(attention):\n                         attention_orig[i] = attention[int(i/4)-1]\n                 attention_orig = np.convolve(attention_orig, [0.199547, 0.200226, 0.200454, 0.200226, 0.199547], mode='same')\n                 attention_orig = np.maximum(attention_orig, 0.3)\n                 attention_out = np.zeros((h, mw))\n                 for i in xrange(mw):\n                     attention_out[:, i] = attention_orig[i]\n                 if len(img_data.shape) == 3:\n                     attention_out = attention_out[:, :, np.newaxis]\n                 img_out_data = img_data * attention_out\n                 img_out = Image.fromarray(img_out_data.astype(np.uint8))\n                 img_out.save(output_filename)\n",
        "code_toks_joined": "def visualize_attention ( self , img_data , attentions , output , label , flag_incorrect ) : <NEWLINE> <INDENT> if flag_incorrect : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> filename = label . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) <NEWLINE> if len ( filename ) == 0 : <NEWLINE> <INDENT> filename = <STRING> <NEWLINE> <DEDENT> output_dir = os . path . join ( output_dir , filename ) <NEWLINE> if not os . path . exists ( output_dir ) : <NEWLINE> <INDENT> os . makedirs ( output_dir ) <NEWLINE> <DEDENT> with open ( os . path . join ( output_dir , <STRING> ) , <STRING> ) as fword : <NEWLINE> <INDENT> fword . write ( output + <STRING> ) <NEWLINE> fword . write ( label ) <NEWLINE> file_img_data = BytesIO ( img_data ) <NEWLINE> img = Image . open ( file_img_data ) <NEWLINE> w , h = img . size <NEWLINE> mh = 32 <NEWLINE> mw = math . floor ( 1. * w / h * mh ) <NEWLINE> img = img . resize ( <NEWLINE> <INDENT> ( mw , h ) , <NEWLINE> Image . ANTIALIAS ) <NEWLINE> <DEDENT> img_data = np . asarray ( img , dtype = np . uint8 ) <NEWLINE> for idx in xrange ( len ( output ) ) : <NEWLINE> <INDENT> output_filename = os . path . join ( output_dir , <STRING> % ( idx ) ) <NEWLINE> attention = attentions [ idx ] [ : ( int ( mw / 4 ) - 1 ) ] <NEWLINE> attention_orig = np . zeros ( mw ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> if i / 4 - 1 > 0 and i / 4 - 1 < len ( attention ) : <NEWLINE> <INDENT> attention_orig [ i ] = attention [ int ( i / 4 ) - 1 ] <NEWLINE> <DEDENT> <DEDENT> attention_orig = np . convolve ( attention_orig , [ 0.199547 , 0.200226 , 0.200454 , 0.200226 , 0.199547 ] , mode = <STRING> ) <NEWLINE> attention_orig = np . maximum ( attention_orig , 0.3 ) <NEWLINE> attention_out = np . zeros ( ( h , mw ) ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> attention_out [ : , i ] = attention_orig [ i ] <NEWLINE> <DEDENT> if len ( img_data . shape ) == 3 : <NEWLINE> <INDENT> attention_out = attention_out [ : , : , np . newaxis ] <NEWLINE> <DEDENT> img_out_data = img_data * attention_out <NEWLINE> img_out = Image . fromarray ( img_out_data . astype ( np . uint8 ) ) <NEWLINE> img_out . save ( output_filename ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'incorrect'",
                "'correct'",
                "'/'",
                "'_'",
                "' '",
                "'_'",
                "'empty_label'",
                "'word.txt'",
                "'w'",
                "'\\n'",
                "'image_%d.jpg'",
                "'same'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9fa9246892ff457f90a0b8eaa213f429": {
        "code_string": "def visualize_attention(self, img_data, attentions, output, label, flag_incorrect):\n         if flag_incorrect:\n             output_dir = os.path.join(self.output_dir, 'incorrect')\n         else:\n             output_dir = os.path.join(self.output_dir, 'correct')\n         filename = label.replace('/', '_').replace(' ', '_')\n         if len(filename) == 0:\n             filename = 'empty_label'\n         output_dir = os.path.join(output_dir, filename)\n         if not os.path.exists(output_dir):\n             os.makedirs(output_dir)\n         with open(os.path.join(output_dir, 'word.txt'), 'w') as fword:\n             fword.write(output+'\\n')\n             fword.write(label)\n             file_img_data = BytesIO(img_data)\n             img = Image.open(file_img_data)\n             w, h = img.size\n             mh = 32\n             mw = math.floor(1. * w / h * mh)\n             img = img.resize(\n                     (mw, mh),\n                     Image.ANTIALIAS)\n             img_data = np.asarray(img, dtype=np.uint8)\n             for idx in xrange(len(output)):\n                 output_filename = os.path.join(output_dir, 'image_%d.jpg' % (idx))\n                 attention = attentions[idx][:(int(mw/4)-1)]\n                 attention_orig = np.zeros(mw)\n                 for i in xrange(mw):\n                     if i/4-1 > 0 and i/4-1 < len(attention):\n                         attention_orig[i] = attention[int(i/4)-1]\n                 attention_orig = np.convolve(attention_orig, [0.199547, 0.200226, 0.200454, 0.200226, 0.199547], mode='same')\n                 attention_orig = np.maximum(attention_orig, 0.3)\n                 attention_out = np.zeros((h, mw))\n                 for i in xrange(mw):\n                     attention_out[:, i] = attention_orig[i]\n                 if len(img_data.shape) == 3:\n                     attention_out = attention_out[:, :, np.newaxis]\n                 img_out_data = img_data * attention_out\n                 img_out = Image.fromarray(img_out_data.astype(np.uint8))\n                 img_out.save(output_filename)\n",
        "code_toks_joined": "def visualize_attention ( self , img_data , attentions , output , label , flag_incorrect ) : <NEWLINE> <INDENT> if flag_incorrect : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> filename = label . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) <NEWLINE> if len ( filename ) == 0 : <NEWLINE> <INDENT> filename = <STRING> <NEWLINE> <DEDENT> output_dir = os . path . join ( output_dir , filename ) <NEWLINE> if not os . path . exists ( output_dir ) : <NEWLINE> <INDENT> os . makedirs ( output_dir ) <NEWLINE> <DEDENT> with open ( os . path . join ( output_dir , <STRING> ) , <STRING> ) as fword : <NEWLINE> <INDENT> fword . write ( output + <STRING> ) <NEWLINE> fword . write ( label ) <NEWLINE> file_img_data = BytesIO ( img_data ) <NEWLINE> img = Image . open ( file_img_data ) <NEWLINE> w , h = img . size <NEWLINE> mh = 32 <NEWLINE> mw = math . floor ( 1. * w / h * mh ) <NEWLINE> img = img . resize ( <NEWLINE> <INDENT> ( mw , mh ) , <NEWLINE> Image . ANTIALIAS ) <NEWLINE> <DEDENT> img_data = np . asarray ( img , dtype = np . uint8 ) <NEWLINE> for idx in xrange ( len ( output ) ) : <NEWLINE> <INDENT> output_filename = os . path . join ( output_dir , <STRING> % ( idx ) ) <NEWLINE> attention = attentions [ idx ] [ : ( int ( mw / 4 ) - 1 ) ] <NEWLINE> attention_orig = np . zeros ( mw ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> if i / 4 - 1 > 0 and i / 4 - 1 < len ( attention ) : <NEWLINE> <INDENT> attention_orig [ i ] = attention [ int ( i / 4 ) - 1 ] <NEWLINE> <DEDENT> <DEDENT> attention_orig = np . convolve ( attention_orig , [ 0.199547 , 0.200226 , 0.200454 , 0.200226 , 0.199547 ] , mode = <STRING> ) <NEWLINE> attention_orig = np . maximum ( attention_orig , 0.3 ) <NEWLINE> attention_out = np . zeros ( ( h , mw ) ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> attention_out [ : , i ] = attention_orig [ i ] <NEWLINE> <DEDENT> if len ( img_data . shape ) == 3 : <NEWLINE> <INDENT> attention_out = attention_out [ : , : , np . newaxis ] <NEWLINE> <DEDENT> img_out_data = img_data * attention_out <NEWLINE> img_out = Image . fromarray ( img_out_data . astype ( np . uint8 ) ) <NEWLINE> img_out . save ( output_filename ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'incorrect'",
                "'correct'",
                "'/'",
                "'_'",
                "' '",
                "'_'",
                "'empty_label'",
                "'word.txt'",
                "'w'",
                "'\\n'",
                "'image_%d.jpg'",
                "'same'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ea4fc072072f471e984f0746ff165706": {
        "code_string": "self.decoder_inputs = []\n             self.target_weights = []\n             for i in xrange(self.decoder_size + 1):\n                 self.decoder_inputs.append(\n                     tf.tile([0], [num_images])\n                 )\n                 if i < self.decoder_size:\n                     self.target_weights.append(tf.tile([1.], [num_images]))\n                 else:\n                     self.target_weights.append(tf.tile([0.], [num_images]))\n",
        "code_toks_joined": "self . decoder_inputs = [ ] <NEWLINE> <INDENT> self . target_weights = [ ] <NEWLINE> for i in xrange ( self . decoder_size + 1 ) : <NEWLINE> <INDENT> self . decoder_inputs . append ( <NEWLINE> <INDENT> tf . tile ( [ 0 ] , [ num_images ] ) <NEWLINE> <DEDENT> ) <NEWLINE> if i < self . decoder_size : <NEWLINE> <INDENT> self . target_weights . append ( tf . tile ( [ 1. ] , [ num_images ] ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . target_weights . append ( tf . tile ( [ 0. ] , [ num_images ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8bd22316d69b4ee8a97abbd389b41a9b": {
        "code_string": "timestruct = datetime(year, month, day, hour - hourdiff).isoformat() + '.000Z'\n         data['time_entry']['start'] = timestruct\n         data['time_entry']['duration'] = hourduration * 3600\n         data['time_entry']['pid'] = projectid\n         data['time_entry']['created_with'] = 'NAME'\n         data['time_entry']['billable'] = billable\n",
        "code_toks_joined": "timestruct = datetime ( year , month , day , hour - hourdiff ) . isoformat ( ) + <STRING> <NEWLINE> <INDENT> data [ <STRING> ] [ <STRING> ] = timestruct <NEWLINE> data [ <STRING> ] [ <STRING> ] = hourduration * 3600 <NEWLINE> data [ <STRING> ] [ <STRING> ] = projectid <NEWLINE> data [ <STRING> ] [ <STRING> ] = <STRING> <NEWLINE> data [ <STRING> ] [ <STRING> ] = billable <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.000Z'",
                "'time_entry'",
                "'start'",
                "'time_entry'",
                "'duration'",
                "'time_entry'",
                "'pid'",
                "'time_entry'",
                "'created_with'",
                "'NAME'",
                "'time_entry'",
                "'billable'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2f30fe198a874de9b10b6996cbaa67a9": {
        "code_string": "def extract(self, sourcepc, neighborhood, targetpc, targetindex, volume):\n         t2a, t2c = utils.get_features(targetpc, targetindex, self.requires())\n         x, y, z = utils.get_point(targetpc, targetindex)\n         return t2c - t2a - z  # z\n",
        "code_toks_joined": "def extract ( self , sourcepc , neighborhood , targetpc , targetindex , volume ) : <NEWLINE> <INDENT> t2a , t2c = utils . get_features ( targetpc , targetindex , self . requires ( ) ) <NEWLINE> x , y , z = utils . get_point ( targetpc , targetindex ) <NEWLINE> return t2c - t2a - z <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# z"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "83ca6075ecb24f48ab49459760ed6b28": {
        "code_string": "def test_GetPointCloudPointFeatures(self):\n         \"\"\" Should not raise exception. \"\"\"\n         pc = test_tools.generate_test_point_cloud()\n         cols = 0.5 * (pc[keys.point][\"x\"][\"data\"] + pc[keys.point][\"y\"][\"data\"])\n         flavs = 0.5 * (pc[keys.point][\"x\"][\"data\"] - pc[keys.point][\"y\"][\"data\"])\n         pc[keys.point][\"color\"] = {\"type\": \"double\", \"data\": cols}\n         pc[keys.point][\"flavor\"] = {\"type\": \"double\", \"data\": flavs}\n         x, y, z = utils.get_point(pc, 2)\n         c, f = utils.get_features(pc, 2, (\"color\", \"flavor\"))\n         self.assertEqual(c, 0.5 * (x + y))\n         self.assertEqual(f, 0.5 * (x - y))\n",
        "code_toks_joined": "def test_GetPointCloudPointFeatures ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> pc = test_tools . generate_test_point_cloud ( ) <NEWLINE> cols = 0.5 * ( pc [ keys . point ] [ <STRING> ] [ <STRING> ] + pc [ keys . point ] [ <STRING> ] [ <STRING> ] ) <NEWLINE> flavs = 0.5 * ( pc [ keys . point ] [ <STRING> ] [ <STRING> ] - pc [ keys . point ] [ <STRING> ] [ <STRING> ] ) <NEWLINE> pc [ keys . point ] [ <STRING> ] = { <STRING> : <STRING> , <STRING> : cols } <NEWLINE> pc [ keys . point ] [ <STRING> ] = { <STRING> : <STRING> , <STRING> : flavs } <NEWLINE> x , y , z = utils . get_point ( pc , 2 ) <NEWLINE> c , f = utils . get_features ( pc , 2 , ( <STRING> , <STRING> ) ) <NEWLINE> self . assertEqual ( c , 0.5 * ( x + y ) ) <NEWLINE> self . assertEqual ( f , 0.5 * ( x - y ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Should not raise exception. \"\"\"",
                "\"x\"",
                "\"data\"",
                "\"y\"",
                "\"data\"",
                "\"x\"",
                "\"data\"",
                "\"y\"",
                "\"data\"",
                "\"color\"",
                "\"type\"",
                "\"double\"",
                "\"data\"",
                "\"flavor\"",
                "\"type\"",
                "\"double\"",
                "\"data\"",
                "\"color\"",
                "\"flavor\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6fb45f7a8dc84b1da4f63d1e2fb11bd1": {
        "code_string": "self.assertEqual(\"laserchicken.feature_extractor.eigenvals_feature_extractor\",\n                          target_point_cloud[keys.provenance][0][\"module\"])\n",
        "code_toks_joined": "self . assertEqual ( <STRING> , <NEWLINE> <INDENT> target_point_cloud [ keys . provenance ] [ 0 ] [ <STRING> ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"laserchicken.feature_extractor.eigenvals_feature_extractor\"",
                "\"module\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "32afaceabe35455ca31e821425ca3812": {
        "code_string": "self.assertEqual(\"laserchicken.feature_extractor.eigenvals_feature_extractor\",\n                          target_point_cloud[keys.provenance][1][\"module\"])\n",
        "code_toks_joined": "self . assertEqual ( <STRING> , <NEWLINE> <INDENT> target_point_cloud [ keys . provenance ] [ 1 ] [ <STRING> ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"laserchicken.feature_extractor.eigenvals_feature_extractor\"",
                "\"module\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "118d2ff787f84e9a87298b841135f501": {
        "code_string": "if end == sys.maxsize:\n             return self._data[self._lb+start:self._ub]\n         elif self._lb + end >= self._ub:\n                 raise IndexError()\n         else:\n             return self._data[self._lb+start:self._lb+end]\n",
        "code_toks_joined": "if end == sys . maxsize : <NEWLINE> <INDENT> return self . _data [ self . _lb + start : self . _ub ] <NEWLINE> elif self . _lb + end >= self . _ub : <NEWLINE> <INDENT> raise IndexError ( ) <NEWLINE> else : <NEWLINE> <DEDENT> return self . _data [ self . _lb + start : self . _lb + end ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ebfae3aaa1c94237adcf698a0c454892": {
        "code_string": "if (Energy_needed_from_battery>0):\n         # Lack of energy\n             if (self._lastPonctualObservation[0]*self.battery_size>Energy_needed_from_battery):\n             # If enough energy in the battery, use it\n                 self._lastPonctualObservation[0]=self._lastPonctualObservation[0]-Energy_needed_from_battery/self.battery_size*self.battery_eta\n             else:\n             # Otherwise: use what is left and then penalty                \n                 reward-=(Energy_needed_from_battery-self._lastPonctualObservation[0]*self.battery_size)*2 #2euro/kWh\n                 self._lastPonctualObservation[0]=0\n         elif (Energy_needed_from_battery<0):\n         # Surplus of energy --> load the battery\n             self._lastPonctualObservation[0]=min(1.,self._lastPonctualObservation[0]-Energy_needed_from_battery/self.battery_size*self.battery_eta)\n",
        "code_toks_joined": "if ( Energy_needed_from_battery > 0 ) : <NEWLINE> <COMMENT> <NL> <INDENT> if ( self . _lastPonctualObservation [ 0 ] * self . battery_size > Energy_needed_from_battery ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . _lastPonctualObservation [ 0 ] = self . _lastPonctualObservation [ 0 ] - Energy_needed_from_battery / self . battery_size * self . battery_eta <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> reward -= ( Energy_needed_from_battery - self . _lastPonctualObservation [ 0 ] * self . battery_size ) * 2 <COMMENT> <NEWLINE> self . _lastPonctualObservation [ 0 ] = 0 <NEWLINE> elif ( Energy_needed_from_battery < 0 ) : <NEWLINE> <COMMENT> <NL> <DEDENT> self . _lastPonctualObservation [ 0 ] = min ( 1. , self . _lastPonctualObservation [ 0 ] - Energy_needed_from_battery / self . battery_size * self . battery_eta ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Lack of energy",
                "# If enough energy in the battery, use it",
                "# Otherwise: use what is left and then penalty                ",
                "#2euro/kWh",
                "# Surplus of energy --> load the battery"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b948a1ef2a0c4b798579d764f32f7a53": {
        "code_string": "@viz_reg_test\n def test_boxplot_melted():\n     return ar.boxplot(data.iris(), \"species\", \"petalLength\")\n",
        "code_toks_joined": "@ viz_reg_test <NEWLINE> <INDENT> def test_boxplot_melted ( ) : <NEWLINE> <INDENT> return ar . boxplot ( data . iris ( ) , <STRING> , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"species\"",
                "\"petalLength\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d53241df90b246f7befc40e5a8548248": {
        "code_string": "def readrequest():\n             while True:\n                 idx = buf.find(b'\\r\\n\\r\\n', pos[0])\n                 if idx >= 0:\n                     break\n                 readmore()\n             head = buf[pos[0]:idx]\n             pos[0] = idx + 4\n             lines = iter(head.decode('ascii').split('\\r\\n'))\n             status = next(lines)\n             headers = {}\n             last_header = None\n             for line in lines:\n                 if line.startswith((' ', '\\t')):\n                     if last_header is not None:\n                         headers[last_header] += line\n                     else:\n                         raise EOFError(\"Wrong http headers\")\n                 elif ':' in line:\n                     k, v = line.split(':', 1)\n                     k = k.strip()\n                     if k in headers:\n                         headers[k] += ', ' + v.strip()\n                     else:\n                         headers[k] = v.strip()\n                 else:\n                     raise EOFError(\"Wrong http headers\")\n             clen = int(headers.get('Content-Length', '0'))\n             if clen < 0:\n                 raise EOFError(\"Wrong content length\")\n             while pos[0] + clen < len(buf):\n                 readmore()\n             return status, headers, buf[pos[0]:pos[0]+clen]\n",
        "code_toks_joined": "def readrequest ( ) : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> idx = buf . find ( <STRING> , pos [ 0 ] ) <NEWLINE> if idx >= 0 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> readmore ( ) <NEWLINE> <DEDENT> head = buf [ pos [ 0 ] : idx ] <NEWLINE> pos [ 0 ] = idx + 4 <NEWLINE> lines = iter ( head . decode ( <STRING> ) . split ( <STRING> ) ) <NEWLINE> status = next ( lines ) <NEWLINE> headers = { } <NEWLINE> last_header = None <NEWLINE> for line in lines : <NEWLINE> <INDENT> if line . startswith ( ( <STRING> , <STRING> ) ) : <NEWLINE> <INDENT> if last_header is not None : <NEWLINE> <INDENT> headers [ last_header ] += line <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise EOFError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif <STRING> in line : <NEWLINE> <INDENT> k , v = line . split ( <STRING> , 1 ) <NEWLINE> k = k . strip ( ) <NEWLINE> if k in headers : <NEWLINE> <INDENT> headers [ k ] += <STRING> + v . strip ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> headers [ k ] = v . strip ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise EOFError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> clen = int ( headers . get ( <STRING> , <STRING> ) ) <NEWLINE> if clen < 0 : <NEWLINE> <INDENT> raise EOFError ( <STRING> ) <NEWLINE> <DEDENT> while pos [ 0 ] + clen < len ( buf ) : <NEWLINE> <INDENT> readmore ( ) <NEWLINE> <DEDENT> return status , headers , buf [ pos [ 0 ] : pos [ 0 ] + clen ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "b'\\r\\n\\r\\n'",
                "'ascii'",
                "'\\r\\n'",
                "' '",
                "'\\t'",
                "\"Wrong http headers\"",
                "':'",
                "':'",
                "', '",
                "\"Wrong http headers\"",
                "'Content-Length'",
                "'0'",
                "\"Wrong content length\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a31a072732744a95ab7999e44f31c432": {
        "code_string": "with pytest.raises(ValueError) as errinfo:\n         bb.io.save(parser, df_anno_simple, 'path.txt')\n     assert 'single-class problems' in str(errinfo.value)\n",
        "code_toks_joined": "with pytest . raises ( ValueError ) as errinfo : <NEWLINE> <INDENT> bb . io . save ( parser , df_anno_simple , <STRING> ) <NEWLINE> assert <STRING> in str ( errinfo . value ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'path.txt'",
                "'single-class problems'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "55ebe3afea3f4c49b3e9f9c58c305b98": {
        "code_string": "def hex_ranges(h3_address_list, ring_size):\n     \"\"\"\n     Get K-Rings for all hexagons properly split by ring,\n     aborting if a pentagon is reached\n     \"\"\"\n     num_hexagons = len(h3_address_list)\n     array_len = num_hexagons * libh3.maxKringSize(ring_size)\n     HexArray = c_long * num_hexagons\n     KringArray = c_long * array_len\n     # Initializes to zeroes by default, don't need to force\n     hex_array = HexArray(\n         *[string_to_h3(h3_address) for h3_address in h3_address_list])\n     krings = KringArray()\n     success = libh3.hexRanges(\n         hex_array,\n         num_hexagons,\n         ring_size,\n         krings,\n     )\n     if success != 0:\n         raise ValueError(\n             'One of the specified hexagon ranges contains a pentagon')\n     out = {}\n     for i in range(0, num_hexagons):\n         h3_address = h3_address_list[i]\n         hex_range_list = []\n         out[h3_address] = hex_range_list\n         for j in range(0, ring_size + 1):\n             hex_range_list.append(set([]))\n         for j in range(0, int(array_len / num_hexagons)):\n             # Mostly from\n             # https://math.stackexchange.com/questions/455511/formula-for-the-nth-term-of-1-2-2-3-3-3-4-4-4-4-5\n             # After figuring out the k-ring sequence matches that term once\n             # divided by 6 ceiled.\n             ring_index = int(\n                 math.floor(\n                     (1 + math.sqrt(1 + 8 * math.ceil(j / 6.0))) / 2)) - 1\n             # hexRanges doesn't return distance array\n             hex_range_list[ring_index].add(\n                 h3_to_string(krings[i * num_hexagons + j]))\n     return out\n",
        "code_toks_joined": "def hex_ranges ( h3_address_list , ring_size ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> num_hexagons = len ( h3_address_list ) <NEWLINE> array_len = num_hexagons * libh3 . maxKringSize ( ring_size ) <NEWLINE> HexArray = c_long * num_hexagons <NEWLINE> KringArray = c_long * array_len <NEWLINE> <COMMENT> <NL> hex_array = HexArray ( <NEWLINE> <INDENT> * [ string_to_h3 ( h3_address ) for h3_address in h3_address_list ] ) <NEWLINE> <DEDENT> krings = KringArray ( ) <NEWLINE> success = libh3 . hexRanges ( <NEWLINE> <INDENT> hex_array , <NEWLINE> num_hexagons , <NEWLINE> ring_size , <NEWLINE> krings , <NEWLINE> <DEDENT> ) <NEWLINE> if success != 0 : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> out = { } <NEWLINE> for i in range ( 0 , num_hexagons ) : <NEWLINE> <INDENT> h3_address = h3_address_list [ i ] <NEWLINE> hex_range_list = [ ] <NEWLINE> out [ h3_address ] = hex_range_list <NEWLINE> for j in range ( 0 , ring_size + 1 ) : <NEWLINE> <INDENT> hex_range_list . append ( set ( [ ] ) ) <NEWLINE> <DEDENT> for j in range ( 0 , int ( array_len / num_hexagons ) ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> ring_index = int ( <NEWLINE> <INDENT> math . floor ( <NEWLINE> <INDENT> ( 1 + math . sqrt ( 1 + 8 * math . ceil ( j / 6.0 ) ) ) / 2 ) ) - 1 <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> hex_range_list [ ring_index ] . add ( <NEWLINE> <INDENT> h3_to_string ( krings [ i * num_hexagons + j ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return out <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Get K-Rings for all hexagons properly split by ring,\n     aborting if a pentagon is reached\n     \"\"\"",
                "'One of the specified hexagon ranges contains a pentagon'"
            ],
            "<COMMENT>": [
                "# Initializes to zeroes by default, don't need to force",
                "# Mostly from",
                "# https://math.stackexchange.com/questions/455511/formula-for-the-nth-term-of-1-2-2-3-3-3-4-4-4-4-5",
                "# After figuring out the k-ring sequence matches that term once",
                "# divided by 6 ceiled.",
                "# hexRanges doesn't return distance array"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a5f2dd93dffd40198199f3f15078d11b": {
        "code_string": "with closing(os.fdopen(os.open(log_technical_terms_to_path,\n                                        os.O_RDWR | os.O_CREAT),\n                                \"r+\")) as terms_file:\n             # pychecker can't see through the handle returned by closing\n             # so we need to suppress these warnings.\n             terms = set(terms_file.read().splitlines())  # suppress(PYC70)\n             terms_file.seek(0)  # suppress(PYC70)\n             terms_file.truncate(0)  # suppress(PYC70)\n             tech_terms = freduce(lambda x, y: x + y,\n                                  _drain(log_technical_terms_to_queue))\n             terms_file.write(\"\\n\".join(list(terms |  # suppress(PYC70)\n                                             set(tech_terms))))\n",
        "code_toks_joined": "with closing ( os . fdopen ( os . open ( log_technical_terms_to_path , <NEWLINE> <INDENT> os . O_RDWR | os . O_CREAT ) , <NEWLINE> <STRING> ) ) as terms_file : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> terms = set ( terms_file . read ( ) . splitlines ( ) ) <COMMENT> <NEWLINE> terms_file . seek ( 0 ) <COMMENT> <NEWLINE> terms_file . truncate ( 0 ) <COMMENT> <NEWLINE> tech_terms = freduce ( lambda x , y : x + y , <NEWLINE> _drain ( log_technical_terms_to_queue ) ) <NEWLINE> terms_file . write ( <STRING> . join ( list ( terms | <COMMENT> <NEWLINE> <INDENT> set ( tech_terms ) ) ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"r+\"",
                "\"\\n\""
            ],
            "<COMMENT>": [
                "# pychecker can't see through the handle returned by closing",
                "# so we need to suppress these warnings.",
                "# suppress(PYC70)",
                "# suppress(PYC70)",
                "# suppress(PYC70)",
                "# suppress(PYC70)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "03ba7f1aa6934b22b4f224bf3f8d2be1": {
        "code_string": "def intersect(obj1, obj2):\n     if not (isinstance(obj1, Vector) or isinstance(obj2, Vector)):\n         raise IOError('object must be of type Vector')\n     obj1.reproject(obj2.srs)\n",
        "code_toks_joined": "def intersect ( obj1 , obj2 ) : <NEWLINE> <INDENT> if not ( isinstance ( obj1 , Vector ) or isinstance ( obj2 , Vector ) ) : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> obj1 . reproject ( obj2 . srs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'object must be of type Vector'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f0f7082dc634e85b3c9f67d1541db4d": {
        "code_string": "if pattern:\n         tests = [f for f in docfiles if f.find(pattern) >= 0]\n     else:\n         tests = docfiles\n",
        "code_toks_joined": "if pattern : <NEWLINE> <INDENT> tests = [ f for f in docfiles if f . find ( pattern ) >= 0 ] <NEWLINE> else : <NEWLINE> tests = docfiles <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6c679e1580174423957fb0c0a684478e": {
        "code_string": "def halton(base):\n     \"\"\"Returns an iterator over an infinite Halton sequence\"\"\"\n     def value(index):\n         result = 0.0\n         f = 1.0/base\n         i = index\n         while i > 0:\n             result += f * (i % base)\n             i = i/base\n             f = f/base\n         return result\n     i = 1\n     while i > 0:\n         yield value(i)\n         i += 1\n",
        "code_toks_joined": "def halton ( base ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def value ( index ) : <NEWLINE> <INDENT> result = 0.0 <NEWLINE> f = 1.0 / base <NEWLINE> i = index <NEWLINE> while i > 0 : <NEWLINE> <INDENT> result += f * ( i % base ) <NEWLINE> i = i / base <NEWLINE> f = f / base <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT> i = 1 <NEWLINE> while i > 0 : <NEWLINE> <INDENT> yield value ( i ) <NEWLINE> i += 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Returns an iterator over an infinite Halton sequence\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3fabc5beed1e477fa7f7aa9890b1c554": {
        "code_string": "# Handle UTF-8 encoding of certain text files.\n open_kwds = {}\n if sys.version_info > (3,):\n     open_kwds['encoding'] = 'utf-8'\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> open_kwds = { } <NEWLINE> if sys . version_info > ( 3 , ) : <NEWLINE> <INDENT> open_kwds [ <STRING> ] = <STRING> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Handle UTF-8 encoding of certain text files."
            ],
            "<STRING>": [
                "'encoding'",
                "'utf-8'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f897509403d94479b34c27c23c33ef54": {
        "code_string": "class TaggedItemManager(models.Manager):\n     def get_by_model(self, Model, tags):\n         \"\"\"\n         Create a queryset matching instances of the given Model\n         associated with a given Tag or list of Tags.\n         \"\"\"\n         tags = get_tag_list(tags)\n         if len(tags) == 0:\n             tag = tags[0] # Optimisation for single tag\n         else:\n             return self.get_intersection_by_model(Model, tags)\n         ctype = ContentType.objects.get_for_model(Model)\n         rel_table = backend.quote_name(self.model._meta.db_table)\n         return Model.objects.extra(\n             tables=[self.model._meta.db_table], # Use a non-explicit join\n             where=[\n                 '%s.content_type_id = %%s' % rel_table,\n                 '%s.tag_id = %%s' % rel_table,\n                 '%s.%s = %s.object_id' % (backend.quote_name(Model._meta.db_table),\n                                           backend.quote_name(Model._meta.pk.column),\n                                           rel_table)\n             ],\n             params=[ctype.id, tag.id],\n         )\n",
        "code_toks_joined": "class TaggedItemManager ( models . Manager ) : <NEWLINE> <INDENT> def get_by_model ( self , Model , tags ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> tags = get_tag_list ( tags ) <NEWLINE> if len ( tags ) == 0 : <NEWLINE> <INDENT> tag = tags [ 0 ] <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . get_intersection_by_model ( Model , tags ) <NEWLINE> <DEDENT> ctype = ContentType . objects . get_for_model ( Model ) <NEWLINE> rel_table = backend . quote_name ( self . model . _meta . db_table ) <NEWLINE> return Model . objects . extra ( <NEWLINE> <INDENT> tables = [ self . model . _meta . db_table ] , <COMMENT> <NEWLINE> where = [ <NEWLINE> <INDENT> <STRING> % rel_table , <NEWLINE> <STRING> % rel_table , <NEWLINE> <STRING> % ( backend . quote_name ( Model . _meta . db_table ) , <NEWLINE> <INDENT> backend . quote_name ( Model . _meta . pk . column ) , <NEWLINE> rel_table ) <NEWLINE> <DEDENT> <DEDENT> ] , <NEWLINE> params = [ ctype . id , tag . id ] , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Create a queryset matching instances of the given Model\n         associated with a given Tag or list of Tags.\n         \"\"\"",
                "'%s.content_type_id = %%s'",
                "'%s.tag_id = %%s'",
                "'%s.%s = %s.object_id'"
            ],
            "<COMMENT>": [
                "# Optimisation for single tag",
                "# Use a non-explicit join"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3abc412345634d369262a66e35e26622": {
        "code_string": "class TaggedItemManager(models.Manager):\n     def get_by_model(self, Model, tags):\n         \"\"\"\n         Create a queryset matching instances of the given Model\n         associated with a given Tag or list of Tags.\n         \"\"\"\n         tags = get_tag_list(tags)\n         if len(tags) == 0:\n             tag = tags[0] # Optimisation for single tag\n         else:\n             return self.get_intersection_by_model(Model, tags)\n         ctype = ContentType.objects.get_for_model(Model)\n         rel_table = backend.quote_name(self.model._meta.db_table)\n         return Model.objects.extra(\n             tables=[self.model._meta.db_table], # Use a non-explicit join\n             where=[\n                 '%s.content_type_id = %%s' % rel_table,\n                 '%s.tag_id = %%s' % rel_table,\n                 '%s.%s = %s.object_id' % (backend.quote_name(Model._meta.db_table),\n                                           backend.quote_name(Model._meta.pk.column),\n                                           rel_table)\n             ],\n             params=[ctype.id, tag.id],\n         )\n",
        "code_toks_joined": "class TaggedItemManager ( models . Manager ) : <NEWLINE> <INDENT> def get_by_model ( self , Model , tags ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> tags = get_tag_list ( tags ) <NEWLINE> if len ( tags ) == 0 : <NEWLINE> <INDENT> tag = tags [ 0 ] <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . get_intersection_by_model ( Model , tags ) <NEWLINE> <DEDENT> ctype = ContentType . objects . get_for_model ( Model ) <NEWLINE> rel_table = backend . quote_name ( self . model . _meta . db_table ) <NEWLINE> return Model . objects . extra ( <NEWLINE> <INDENT> tables = [ self . model . _meta . db_table ] , <COMMENT> <NEWLINE> where = [ <NEWLINE> <INDENT> <STRING> % rel_table , <NEWLINE> <STRING> % rel_table , <NEWLINE> <STRING> % ( backend . quote_name ( Model . _meta . db_table ) , <NEWLINE> <INDENT> backend . quote_name ( Model . _meta . pk . column ) , <NEWLINE> rel_table ) <NEWLINE> <DEDENT> <DEDENT> ] , <NEWLINE> params = [ ctype . id , tag . id ] , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Create a queryset matching instances of the given Model\n         associated with a given Tag or list of Tags.\n         \"\"\"",
                "'%s.content_type_id = %%s'",
                "'%s.tag_id = %%s'",
                "'%s.%s = %s.object_id'"
            ],
            "<COMMENT>": [
                "# Optimisation for single tag",
                "# Use a non-explicit join"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3d6b6f4dd0254d709f338470f07d8dcd": {
        "code_string": "if data is None:\n             if exc is not None:\n                 raise exc(criteria)\n             else:\n                 return None\n",
        "code_toks_joined": "if data is None : <NEWLINE> <INDENT> if exc is not None : <NEWLINE> <INDENT> raise exc ( criteria ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a3dd896c65a949eb91ba6f28f2af2e7a": {
        "code_string": "def upload(self):\n         self.fitting = 0\n         self.save()\n         lore.io.upload(self.remote_model_path(), self.model_path())\n",
        "code_toks_joined": "def upload ( self ) : <NEWLINE> <INDENT> self . fitting = 0 <NEWLINE> self . save ( ) <NEWLINE> lore . io . upload ( self . remote_model_path ( ) , self . model_path ( ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6bedae485f964963b882ba17efaa0360": {
        "code_string": "def upload(self):\n         super(Base, self).upload()\n         lore.io.upload(self.remote_weights_path(), self.weights_path())\n",
        "code_toks_joined": "def upload ( self ) : <NEWLINE> <INDENT> super ( Base , self ) . upload ( ) <NEWLINE> lore . io . upload ( self . remote_weights_path ( ) , self . weights_path ( ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a598d433c40642f68376de889671fdb2": {
        "code_string": "if self.towers > 1:\n             result = numpy.mean(result, axis=0).squeeze(axis=0)\n",
        "code_toks_joined": "if self . towers > 1 : <NEWLINE> <INDENT> result = numpy . mean ( result , axis = 0 ) . squeeze ( axis = 0 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b144f154080b4a1abbf03b55b05fd186": {
        "code_string": "if cluster not in clusters:\n             #logging.info(\"COL %s -> CLUSTER %d\", column, cluster)\n             clusters.append(cluster)\n         else:\n             #logging.warn(\"cluster %s already associated with %s\",\n             #             str(cluster), str(column))\n             pass\n         if columns not in columns:\n             columns.append(column)\n",
        "code_toks_joined": "if cluster not in clusters : <NEWLINE> <COMMENT> <NL> <INDENT> clusters . append ( cluster ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if columns not in columns : <NEWLINE> columns . append ( column ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#logging.info(\"COL %s -> CLUSTER %d\", column, cluster)",
                "#logging.warn(\"cluster %s already associated with %s\",",
                "#             str(cluster), str(column))"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f55de02d6d614277b2d56d16dcf2d2a8": {
        "code_string": "def adjust_cluster(self, cluster, rowscores, cutoff, limit):\n         \"\"\"adjust a single cluster\"\"\"\n         def max_row_in_column(matrix, column):\n             \"\"\"returns a pair of the maximum row index and score in the given matrix and column\"\"\"\n             sm = matrix.submatrix_by_name(wh, [matrix.column_names[column]])\n             sm_values = sm.values\n             max_row = 0\n             max_score = sys.float_info.min\n             for row in range(sm.num_rows()):\n                 if sm_values[row][0] > max_score:\n                     max_score = sm[row][0]\n                     max_row = row\n             return sm.row_names[max_row]\n",
        "code_toks_joined": "def adjust_cluster ( self , cluster , rowscores , cutoff , limit ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def max_row_in_column ( matrix , column ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sm = matrix . submatrix_by_name ( wh , [ matrix . column_names [ column ] ] ) <NEWLINE> sm_values = sm . values <NEWLINE> max_row = 0 <NEWLINE> max_score = sys . float_info . min <NEWLINE> for row in range ( sm . num_rows ( ) ) : <NEWLINE> <INDENT> if sm_values [ row ] [ 0 ] > max_score : <NEWLINE> <INDENT> max_score = sm [ row ] [ 0 ] <NEWLINE> max_row = row <NEWLINE> <DEDENT> <DEDENT> return sm . row_names [ max_row ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"adjust a single cluster\"\"\"",
                "\"\"\"returns a pair of the maximum row index and score in the given matrix and column\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "85adf67914d14a5d92f03fc99d645391": {
        "code_string": "def setUp(self):  # pylint: disable-msg=C0103\n         \"\"\"test fixture\"\"\"\n         if not os.path.exists('testcache'):\n             os.mkdir('testcache')\n         self.service = mo.MicrobesOnline(mo.MICROBES_ONLINE_BASE_URL, 'testcache')\n",
        "code_toks_joined": "def setUp ( self ) : <COMMENT> <NEWLINE> <INDENT> <STRING> <NEWLINE> if not os . path . exists ( <STRING> ) : <NEWLINE> <INDENT> os . mkdir ( <STRING> ) <NEWLINE> <DEDENT> self . service = mo . MicrobesOnline ( mo . MICROBES_ONLINE_BASE_URL , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# pylint: disable-msg=C0103"
            ],
            "<STRING>": [
                "\"\"\"test fixture\"\"\"",
                "'testcache'",
                "'testcache'",
                "'testcache'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3bc8d0e897fd41a78ffd7091c00227c9": {
        "code_string": "h, m, s = convert_elapsed_time(training_time)\n     logger.info('Training finished in {} hours {} minutes {:.2f} seconds.'\n                 .format(h, m, s))\n     logger.info('outputs')\n     logger.info(outputs)\n     logger.info('targets')\n     logger.info(targets)\n",
        "code_toks_joined": "h , m , s = convert_elapsed_time ( training_time ) <NEWLINE> <INDENT> logger . info ( <STRING> <NEWLINE> <INDENT> . format ( h , m , s ) ) <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> logger . info ( outputs ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> logger . info ( targets ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Training finished in {} hours {} minutes {:.2f} seconds.'",
                "'outputs'",
                "'targets'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "91e1cafbf845400bb550feeab297aca7": {
        "code_string": "if point_xy:\n                 x_point = self.workspace.get_content_point(x_uuid, point_xy)\n                 format_str, unit_str, x_point = self.document.convert_units(x_uuid, x_point)\n                 y_point = self.workspace.get_content_point(y_uuid, point_xy)\n                 format_str, unit_str, y_point = self.document.convert_units(x_uuid, y_point)\n             else:\n                 x_point = None\n                 y_point = None\n",
        "code_toks_joined": "if point_xy : <NEWLINE> <INDENT> x_point = self . workspace . get_content_point ( x_uuid , point_xy ) <NEWLINE> format_str , unit_str , x_point = self . document . convert_units ( x_uuid , x_point ) <NEWLINE> y_point = self . workspace . get_content_point ( y_uuid , point_xy ) <NEWLINE> format_str , unit_str , y_point = self . document . convert_units ( x_uuid , y_point ) <NEWLINE> else : <NEWLINE> x_point = None <NEWLINE> y_point = None <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aacfed6d45924753ac6d237e64ec6072": {
        "code_string": "asset_map['id'] = host\n",
        "code_toks_joined": "asset_map [ <STRING> ] = host <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'id'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8624cc82101d463cb346f8f02922d43b": {
        "code_string": "rec = params.record_cache[record.record_uid]\n         data.update(json.loads(rec['data'].decode()))\n         if 'extra' in rec:\n             extra.update(json.loads(rec['extra'].decode()))\n         if 'udata' in rec:\n             udata.update(rec['udata'])\n         unencrypted_key = rec['record_key_unencrypted']\n         record_object['revision'] = rec['revision']\n         if rec.get('is_converted_record_type'):\n             if params.debug: print('Converted record sends record key')\n             record_object['record_key'] = encrypt_aes(params.data_key, unencrypted_key)\n     else:\n         if params.debug: print('Generated record key')\n         unencrypted_key = os.urandom(32)\n         record_object['record_key'] = encrypt_aes(params.data_key, unencrypted_key)\n         record_object['revision'] = 0\n",
        "code_toks_joined": "rec = params . record_cache [ record . record_uid ] <NEWLINE> <INDENT> data . update ( json . loads ( rec [ <STRING> ] . decode ( ) ) ) <NEWLINE> if <STRING> in rec : <NEWLINE> <INDENT> extra . update ( json . loads ( rec [ <STRING> ] . decode ( ) ) ) <NEWLINE> <DEDENT> if <STRING> in rec : <NEWLINE> <INDENT> udata . update ( rec [ <STRING> ] ) <NEWLINE> <DEDENT> unencrypted_key = rec [ <STRING> ] <NEWLINE> record_object [ <STRING> ] = rec [ <STRING> ] <NEWLINE> if rec . get ( <STRING> ) : <NEWLINE> <INDENT> if params . debug : print ( <STRING> ) <NEWLINE> record_object [ <STRING> ] = encrypt_aes ( params . data_key , unencrypted_key ) <NEWLINE> else : <NEWLINE> <DEDENT> if params . debug : print ( <STRING> ) <NEWLINE> unencrypted_key = os . urandom ( 32 ) <NEWLINE> record_object [ <STRING> ] = encrypt_aes ( params . data_key , unencrypted_key ) <NEWLINE> record_object [ <STRING> ] = 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'data'",
                "'extra'",
                "'extra'",
                "'udata'",
                "'udata'",
                "'record_key_unencrypted'",
                "'revision'",
                "'revision'",
                "'is_converted_record_type'",
                "'Converted record sends record key'",
                "'record_key'",
                "'Generated record key'",
                "'record_key'",
                "'revision'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4eae5ef7ffe34f95bc252eb8a5020ac0": {
        "code_string": "svg_text = game.get_svg_str()\n     html_text = HTML_WRAPPER.format(title=game_id, filename=html_filename)\n",
        "code_toks_joined": "svg_text = game . get_svg_str ( ) <NEWLINE> <INDENT> html_text = HTML_WRAPPER . format ( title = game_id , filename = html_filename ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a8bad9a8058f483389bc24849cab9e20": {
        "code_string": "def test_get_fee():\n     assert get_fee(fast=True) != get_fee(fast=False)\n",
        "code_toks_joined": "def test_get_fee ( ) : <NEWLINE> <INDENT> assert get_fee ( fast = True ) != get_fee ( fast = False ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e832672cf634403b8616af2dd963daf0": {
        "code_string": "org_info = wa.organisms.show_organism(org_id)\n         if 'directory' in org_info:\n             time.sleep(1)\n             org_info = wa.organisms.show_organism(org_id)\n",
        "code_toks_joined": "org_info = wa . organisms . show_organism ( org_id ) <NEWLINE> <INDENT> if <STRING> in org_info : <NEWLINE> <INDENT> time . sleep ( 1 ) <NEWLINE> org_info = wa . organisms . show_organism ( org_id ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'directory'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0049e4db85e543bf8e4beeb02dc878c0": {
        "code_string": "class TestFK(unittest.TestCase):\n     def setUp(self):\n         self.creature = creature.creature(creature_type=creature_type)\n         all_zeros = [0 for i in range(0, self.creature.config.joints_number)]\n         one_move = [0 for i in range(0, self.creature.config.joints_number)]\n         one_move[5] = np.pi / 4\n         one_move[6] = -np.pi / 2\n         one_move[4] = -np.pi / 2\n         self.test_pos = one_move\n",
        "code_toks_joined": "class TestFK ( unittest . TestCase ) : <NEWLINE> <INDENT> def setUp ( self ) : <NEWLINE> <INDENT> self . creature = creature . creature ( creature_type = creature_type ) <NEWLINE> all_zeros = [ 0 for i in range ( 0 , self . creature . config . joints_number ) ] <NEWLINE> one_move = [ 0 for i in range ( 0 , self . creature . config . joints_number ) ] <NEWLINE> one_move [ 5 ] = np . pi / 4 <NEWLINE> one_move [ 6 ] = - np . pi / 2 <NEWLINE> one_move [ 4 ] = - np . pi / 2 <NEWLINE> self . test_pos = one_move <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ad5e6e7739324aa592d0e8712f808aa5": {
        "code_string": "def run(self):\n         self.running = True\n         while self.running:\n             if len(self.timers) > 0:\n                 try:\n                     self._wait(self.timers[0].next_fire_time)\n                 except Exception as e:\n                     self._error(e)\n                     continue\n",
        "code_toks_joined": "def run ( self ) : <NEWLINE> <INDENT> self . running = True <NEWLINE> while self . running : <NEWLINE> <INDENT> if len ( self . timers ) > 0 : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . _wait ( self . timers [ 0 ] . next_fire_time ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> self . _error ( e ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "63dc3070e9ee41f8af1bc618ca966ab8": {
        "code_string": "##3\n         self.assertEqual(pmml_obj.steps[-1][-1].n_neighbors, pmml_obj.NearestNeighborModel[0].numberOfNeighbors)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> self . assertEqual ( pmml_obj . steps [ - 1 ] [ - 1 ] . n_neighbors , pmml_obj . NearestNeighborModel [ 0 ] . numberOfNeighbors ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "##3"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "452808ac569c4b66bfea5ac14e469fcf": {
        "code_string": "self.start_time = self.distances.arrival + self.config.offsets[0]\n         self.end_time = self.distances.arrival + self.config.offsets[1]\n",
        "code_toks_joined": "self . start_time = self . distances . arrival + self . config . offsets [ 0 ] <NEWLINE> <INDENT> self . end_time = self . distances . arrival + self . config . offsets [ 1 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "51e9343fb06948a99fbbbfbf6a9b4f78": {
        "code_string": "if preview_path:\n     mimetype = magic.from_file(src_path, mime=True).lower()\n     if mimetype in [ExportMimeType.PNG, ExportMimeType.PDF]:\n       return preview_path\n",
        "code_toks_joined": "if preview_path : <NEWLINE> <INDENT> mimetype = magic . from_file ( src_path , mime = True ) . lower ( ) <NEWLINE> if mimetype in [ ExportMimeType . PNG , ExportMimeType . PDF ] : <NEWLINE> <INDENT> return preview_path <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c05437cbd4c64de484376d4d3ee27598": {
        "code_string": "if RuleMods.EXPAND in mods or RuleMods.EXPAND1 in mods:\n             # EXPAND is here to keep tree-depth minimal, it won't expand all EXPAND rules, just the recursive ones\n             # EXPAND1: perform necessary expansions on children first to ensure we don't end up expanding inside our\n             #          parents if (after expansion) we have more than one child.\n             def p_rule(self, p):\n                 subtree = []\n                 for child in p[1:]:\n                     if isinstance(child, self.tree_class) and child.head in self.rules_to_expand:\n                         subtree.extend(child.tail)\n                     else:\n                         subtree.append(child)\n                 p[0] = self.tree_class(rule_name, subtree, skip_adjustments=True) if len(subtree) > 1 else subtree[0]\n         else:\n             def p_rule(self, p):\n                 p[0] = self.tree_class(rule_name, p[1:], skip_adjustments=True)\n         p_rule.__doc__ = rule_def\n         setattr(self, 'p_%s' % (rule_name,), types.MethodType(p_rule, self))\n",
        "code_toks_joined": "if RuleMods . EXPAND in mods or RuleMods . EXPAND1 in mods : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> def p_rule ( self , p ) : <NEWLINE> <INDENT> subtree = [ ] <NEWLINE> for child in p [ 1 : ] : <NEWLINE> <INDENT> if isinstance ( child , self . tree_class ) and child . head in self . rules_to_expand : <NEWLINE> <INDENT> subtree . extend ( child . tail ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> subtree . append ( child ) <NEWLINE> <DEDENT> <DEDENT> p [ 0 ] = self . tree_class ( rule_name , subtree , skip_adjustments = True ) if len ( subtree ) > 1 else subtree [ 0 ] <NEWLINE> else : <NEWLINE> <DEDENT> def p_rule ( self , p ) : <NEWLINE> <INDENT> p [ 0 ] = self . tree_class ( rule_name , p [ 1 : ] , skip_adjustments = True ) <NEWLINE> p_rule . __doc__ = rule_def <NEWLINE> setattr ( self , <STRING> % ( rule_name , ) , types . MethodType ( p_rule , self ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# EXPAND is here to keep tree-depth minimal, it won't expand all EXPAND rules, just the recursive ones",
                "# EXPAND1: perform necessary expansions on children first to ensure we don't end up expanding inside our",
                "#          parents if (after expansion) we have more than one child."
            ],
            "<STRING>": [
                "'p_%s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6c6b6ecffddb4e8fa5ba20cd54d69224": {
        "code_string": "def colorStr(text, color=WHITE):\n \tif has_colors:\n \t\tseq = '\\x1b[1;%dm' % (30+color) + text + '\\x1b[0m'\n \t\treturn seq\n \t\tsys.stdout.write(seq + '\\n')\n \telse:\n \t\treturn seq\n \t\tsys.stdout.write(text + '\\n')\n",
        "code_toks_joined": "def colorStr ( text , color = WHITE ) : <NEWLINE> <INDENT> if has_colors : <NEWLINE> <INDENT> seq = <STRING> % ( 30 + color ) + text + <STRING> <NEWLINE> return seq <NEWLINE> sys . stdout . write ( seq + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return seq <NEWLINE> sys . stdout . write ( text + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'\\x1b[1;%dm'",
                "'\\x1b[0m'",
                "'\\n'",
                "'\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "302651706bd34de9b70ed41c1271a476": {
        "code_string": "(type,len) = ofp_action_header(message, offset)\n \tfield = message[cursor.offset:offset+len]\n \tcursor.offset = offset+len\n \treturn namedtuple(\"ofp_action_set_field\",\n \t\t\"type,len,field\")(type,len,field)\n",
        "code_toks_joined": "( type , len ) = ofp_action_header ( message , offset ) <NEWLINE> <INDENT> field = message [ cursor . offset : offset + len ] <NEWLINE> cursor . offset = offset + len <NEWLINE> return namedtuple ( <STRING> , <NEWLINE> <INDENT> <STRING> ) ( type , len , field ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"ofp_action_set_field\"",
                "\"type,len,field\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e63e0c9a637e4eda902d68882ff0f207": {
        "code_string": "def _align(length):\n \treturn (length+7)/8*8\n",
        "code_toks_joined": "def _align ( length ) : <NEWLINE> <INDENT> return ( length + 7 ) / 8 * 8 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3daa38ec7bba4b97bcae7467102ea1e8": {
        "code_string": "def _find_completion(fuser, relation):\n     \"\"\"Returns `fuser`-completed relation that matches `relation`, or None\"\"\"\n     for fuser_relation in fuser.fusion_graph.get_relations(relation.row_type,\n                                                            relation.col_type):\n         if fuser_relation._id == relation._id:\n             return fuser.complete(fuser_relation)\n     return None\n",
        "code_toks_joined": "def _find_completion ( fuser , relation ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for fuser_relation in fuser . fusion_graph . get_relations ( relation . row_type , <NEWLINE> <INDENT> relation . col_type ) : <NEWLINE> if fuser_relation . _id == relation . _id : <NEWLINE> return fuser . complete ( fuser_relation ) <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Returns `fuser`-completed relation that matches `relation`, or None\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ba7e845c4f7b4b5b9994079c2e001089": {
        "code_string": "assert isinstance(ams_netid, SAmsNetId)\n",
        "code_toks_joined": "assert isinstance ( ams_netid , SAmsNetId ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7c3b83c6ffcb4be5abeec94da95e7e71": {
        "code_string": "in_channels = convolutions[0][0]\n         self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n         self.projections = nn.ModuleList()\n         self.convolutions = nn.ModuleList()\n         for (out_channels, kernel_size) in convolutions:\n             pad = (kernel_size - 1) // 2\n             self.projections.append(Linear(in_channels, out_channels)\n                                     if in_channels != out_channels else None)\n             self.convolutions.append(\n                 ConvTBC(in_channels, out_channels * 2, kernel_size, padding=pad,\n                         dropout=dropout))\n             in_channels = out_channels\n         self.fc2 = Linear(in_channels, embed_dim)\n",
        "code_toks_joined": "in_channels = convolutions [ 0 ] [ 0 ] <NEWLINE> <INDENT> self . fc1 = Linear ( embed_dim , in_channels , dropout = dropout ) <NEWLINE> self . projections = nn . ModuleList ( ) <NEWLINE> self . convolutions = nn . ModuleList ( ) <NEWLINE> for ( out_channels , kernel_size ) in convolutions : <NEWLINE> <INDENT> pad = ( kernel_size - 1 ) // 2 <NEWLINE> self . projections . append ( Linear ( in_channels , out_channels ) <NEWLINE> <INDENT> if in_channels != out_channels else None ) <NEWLINE> <DEDENT> self . convolutions . append ( <NEWLINE> <INDENT> ConvTBC ( in_channels , out_channels * 2 , kernel_size , padding = pad , <NEWLINE> <INDENT> dropout = dropout ) ) <NEWLINE> <DEDENT> <DEDENT> in_channels = out_channels <NEWLINE> <DEDENT> self . fc2 = Linear ( in_channels , embed_dim ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f4be082f82dd46c6bc764cc6698acd0a": {
        "code_string": "def forward(self, input, incremental_state=None):\n         \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n         # recompute/expand embeddings if needed\n         bsz, seq_len = input.size()\n         max_pos = self.padding_idx + 1 + seq_len\n         if seq_len > self.weights.size(0):\n             self.weights = SinusoidalPositionalEmbedding.get_embedding(\n                 max_pos,\n                 self.embedding_dim,\n                 self.padding_idx,\n             ).type_as(self.weights)\n         weights = Variable(self.weights)\n",
        "code_toks_joined": "def forward ( self , input , incremental_state = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> bsz , seq_len = input . size ( ) <NEWLINE> max_pos = self . padding_idx + 1 + seq_len <NEWLINE> if seq_len > self . weights . size ( 0 ) : <NEWLINE> <INDENT> self . weights = SinusoidalPositionalEmbedding . get_embedding ( <NEWLINE> <INDENT> max_pos , <NEWLINE> self . embedding_dim , <NEWLINE> self . padding_idx , <NEWLINE> <DEDENT> ) . type_as ( self . weights ) <NEWLINE> <DEDENT> weights = Variable ( self . weights ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Input is expected to be of size [bsz x seqlen].\"\"\""
            ],
            "<COMMENT>": [
                "# recompute/expand embeddings if needed"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f1bdbea6d07549dbae857c9f95597b53": {
        "code_string": "# Train until the learning rate gets too small\n     max_epoch = args.max_epoch or math.inf\n     max_update = args.max_update or math.inf\n     lr = trainer.get_lr()\n     train_meter = StopwatchMeter()\n     train_meter.start()\n     valid_losses = [None]\n     valid_subsets = args.valid_subset.split(',')\n     while lr > args.min_lr and epoch_itr.epoch <= max_epoch and trainer.get_num_updates() < max_update:\n         # train for one epoch\n         train(args, trainer, task, epoch_itr)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> max_epoch = args . max_epoch or math . inf <NEWLINE> max_update = args . max_update or math . inf <NEWLINE> lr = trainer . get_lr ( ) <NEWLINE> train_meter = StopwatchMeter ( ) <NEWLINE> train_meter . start ( ) <NEWLINE> valid_losses = [ None ] <NEWLINE> valid_subsets = args . valid_subset . split ( <STRING> ) <NEWLINE> while lr > args . min_lr and epoch_itr . epoch <= max_epoch and trainer . get_num_updates ( ) < max_update : <NEWLINE> <COMMENT> <NL> <INDENT> train ( args , trainer , task , epoch_itr ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Train until the learning rate gets too small",
                "# train for one epoch"
            ],
            "<STRING>": [
                "','"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "03b78036b394405abf2346f1102b1937": {
        "code_string": "# Train until the learning rate gets too small\n     max_epoch = args.max_epoch or math.inf\n     max_update = args.max_update or math.inf\n     lr = trainer.get_lr()\n     train_meter = StopwatchMeter()\n     train_meter.start()\n     valid_losses = [None]\n     valid_subsets = args.valid_subset.split(',')\n     while lr > args.min_lr and epoch_itr.epoch <= max_epoch and trainer.get_num_updates() < max_update:\n         # train for one epoch\n         train(args, trainer, task, epoch_itr)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> max_epoch = args . max_epoch or math . inf <NEWLINE> max_update = args . max_update or math . inf <NEWLINE> lr = trainer . get_lr ( ) <NEWLINE> train_meter = StopwatchMeter ( ) <NEWLINE> train_meter . start ( ) <NEWLINE> valid_losses = [ None ] <NEWLINE> valid_subsets = args . valid_subset . split ( <STRING> ) <NEWLINE> while lr > args . min_lr and epoch_itr . epoch <= max_epoch and trainer . get_num_updates ( ) < max_update : <NEWLINE> <COMMENT> <NL> <INDENT> train ( args , trainer , task , epoch_itr ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Train until the learning rate gets too small",
                "# train for one epoch"
            ],
            "<STRING>": [
                "','"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3400ed1263a743b19774f87e9bb7c2d1": {
        "code_string": "sample_size = sample['target'].size(0) if self.args.sentence_avg else sample['ntokens']\n         logging_output = {\n             'loss': utils.item(loss.data) if reduce else loss.data,\n             'nll_loss': utils.item(nll_loss.data) if reduce else loss.data,\n             'ntokens': sample['ntokens'],\n             'sample_size': sample_size,\n         }\n         return loss, sample_size, logging_output\n",
        "code_toks_joined": "sample_size = sample [ <STRING> ] . size ( 0 ) if self . args . sentence_avg else sample [ <STRING> ] <NEWLINE> <INDENT> logging_output = { <NEWLINE> <INDENT> <STRING> : utils . item ( loss . data ) if reduce else loss . data , <NEWLINE> <STRING> : utils . item ( nll_loss . data ) if reduce else loss . data , <NEWLINE> <STRING> : sample [ <STRING> ] , <NEWLINE> <STRING> : sample_size , <NEWLINE> <DEDENT> } <NEWLINE> return loss , sample_size , logging_output <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'target'",
                "'ntokens'",
                "'loss'",
                "'nll_loss'",
                "'ntokens'",
                "'ntokens'",
                "'sample_size'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a6cf8c8a8d5a4ff38b1af4c958ed2fc8": {
        "code_string": "# random attention\n         attn = torch.rand(bbsz, src_len, tgt_len)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> attn = torch . rand ( bbsz , src_len , tgt_len ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# random attention"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "09ae52c1a6ff4d21a9e4228fbca87ab5": {
        "code_string": "def step_update(self, num_updates):\n         \"\"\"Update the learning rate after each update.\"\"\"\n         if num_updates < self.args.warmup_updates:\n             self.lr = self.args.warmup_init_lr + num_updates * self.lr_step\n         else:\n             curr_updates = num_updates - self.args.warmup_updates\n             if self.t_mult != 1:\n                 i = math.floor(math.log(1 - curr_updates / self.period * (1 - self.t_mult), self.t_mult))\n                 t_i = self.t_mult ** i * self.period\n                 t_curr = curr_updates - (1 - self.t_mult ** i) / (1 - self.t_mult) * self.period\n             else:\n                 i = math.floor(curr_updates / self.period)\n                 t_i = self.period\n                 t_curr = num_updates - (self.period * i)\n",
        "code_toks_joined": "def step_update ( self , num_updates ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if num_updates < self . args . warmup_updates : <NEWLINE> <INDENT> self . lr = self . args . warmup_init_lr + num_updates * self . lr_step <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> curr_updates = num_updates - self . args . warmup_updates <NEWLINE> if self . t_mult != 1 : <NEWLINE> <INDENT> i = math . floor ( math . log ( 1 - curr_updates / self . period * ( 1 - self . t_mult ) , self . t_mult ) ) <NEWLINE> t_i = self . t_mult ** i * self . period <NEWLINE> t_curr = curr_updates - ( 1 - self . t_mult ** i ) / ( 1 - self . t_mult ) * self . period <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> i = math . floor ( curr_updates / self . period ) <NEWLINE> t_i = self . period <NEWLINE> t_curr = num_updates - ( self . period * i ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Update the learning rate after each update.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "39203eeb4a374b7f8e938aa1d43527ec": {
        "code_string": "def __get_safe_conn(self, retry_count):\n         self.current_size += 1\n         c = self.unuse_list.pop()\n         if self.ping_check:\n             now = int(time())\n             timeout = now\n             if isinstance(int, self.ping_check):\n                 timeout = timeout - self.ping_check\n             if not hasattr(c, '__ping_check_timestamp'):\n                 c.__ping_check_timestamp = now\n             try:\n                 if c.__ping_check_timestamp < timeout:\n                     c.__ping_check_timestamp = now\n                     c.ping()\n             except:\n                 self.current_size -= 1\n                 if retry_count < 10: c = self.__get_conn(retry_count+1)\n         if c: self.inuse_list.add(c)\n         return c\n",
        "code_toks_joined": "def __get_safe_conn ( self , retry_count ) : <NEWLINE> <INDENT> self . current_size += 1 <NEWLINE> c = self . unuse_list . pop ( ) <NEWLINE> if self . ping_check : <NEWLINE> <INDENT> now = int ( time ( ) ) <NEWLINE> timeout = now <NEWLINE> if isinstance ( int , self . ping_check ) : <NEWLINE> <INDENT> timeout = timeout - self . ping_check <NEWLINE> <DEDENT> if not hasattr ( c , <STRING> ) : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> if c . __ping_check_timestamp < timeout : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> c . ping ( ) <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> self . current_size -= 1 <NEWLINE> if retry_count < 10 : c = self . __get_conn ( retry_count + 1 ) <NEWLINE> <DEDENT> <DEDENT> if c : self . inuse_list . add ( c ) <NEWLINE> return c <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'__ping_check_timestamp'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "853009ee33384fd1b929d90eb4e7f55b": {
        "code_string": "def copy(self, new_path, replace=False):\n         \"\"\"Uses boto to copy the file to the new path instead of uploading another file to the new key\"\"\"\n         if replace or get_file(new_path).exists():\n             self.key.copy(self.key.bucket, new_path)\n             return True\n         return False\n",
        "code_toks_joined": "def copy ( self , new_path , replace = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if replace or get_file ( new_path ) . exists ( ) : <NEWLINE> <INDENT> self . key . copy ( self . key . bucket , new_path ) <NEWLINE> return True <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Uses boto to copy the file to the new path instead of uploading another file to the new key\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d10886f4b2a747c1b4dc322a7ee2900a": {
        "code_string": "if 'JUJU_TEST_CHARM' in os.environ:\n             pass  # Copy the current parent directory to temp and deploy that\n         elif self.charm_name:\n             if charm_name == self.charm_name:\n                 charm = os.getcwd()\n",
        "code_toks_joined": "if <STRING> in os . environ : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> elif self . charm_name : <NEWLINE> if charm_name == self . charm_name : <NEWLINE> <INDENT> charm = os . getcwd ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'JUJU_TEST_CHARM'"
            ],
            "<COMMENT>": [
                "# Copy the current parent directory to temp and deploy that"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "80d6d11eb26040beb3d4edd623f1f5a1": {
        "code_string": "#{ Configuration\n \tif not has_perf_mod:\n \t\t_set_cache_ = _set_cache_brute_\n \telse:\n \t\t_set_cache_ = _set_cache_too_slow_without_c\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if not has_perf_mod : <NEWLINE> <INDENT> _set_cache_ = _set_cache_brute_ <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> _set_cache_ = _set_cache_too_slow_without_c <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#{ Configuration"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "23a27a8fd151479c8aff8020e9ea3113": {
        "code_string": "step = peda.intsize()\n         if not peda.is_address(address): # cannot determine address\n             msg(\"Invalid $SP address: 0x%x\" % sp, \"red\")\n             return\n             for i in range(count):\n                 if not peda.execute(\"x/%sx 0x%x\" % (\"g\" if step == 8 else \"w\", address + i*step)):\n                     break\n             return\n",
        "code_toks_joined": "step = peda . intsize ( ) <NEWLINE> <INDENT> if not peda . is_address ( address ) : <COMMENT> <NEWLINE> <INDENT> msg ( <STRING> % sp , <STRING> ) <NEWLINE> return <NEWLINE> for i in range ( count ) : <NEWLINE> <INDENT> if not peda . execute ( <STRING> % ( <STRING> if step == 8 else <STRING> , address + i * step ) ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> return <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# cannot determine address"
            ],
            "<STRING>": [
                "\"Invalid $SP address: 0x%x\"",
                "\"red\"",
                "\"x/%sx 0x%x\"",
                "\"g\"",
                "\"w\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d2fdeb0a84964cac8da1b8063980fd91": {
        "code_string": "def set_statics(self):\n         if os.path.exists(self.results_dir):\n             return\n         try:\n             shutil.copytree(os.path.join(self.templates_dir, 'css'), os.path.join(self.results_dir, 'css'))\n             shutil.copytree(os.path.join(self.templates_dir, 'img'), os.path.join(self.results_dir, 'img'))\n             shutil.copytree(os.path.join(self.templates_dir, 'scripts'), os.path.join(self.results_dir, 'scripts'))\n         except OSError:\n             sys.stderr.write('\\nERROR : can not create directory for results\\n\\n')\n             sys.exit(1)\n",
        "code_toks_joined": "def set_statics ( self ) : <NEWLINE> <INDENT> if os . path . exists ( self . results_dir ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> shutil . copytree ( os . path . join ( self . templates_dir , <STRING> ) , os . path . join ( self . results_dir , <STRING> ) ) <NEWLINE> shutil . copytree ( os . path . join ( self . templates_dir , <STRING> ) , os . path . join ( self . results_dir , <STRING> ) ) <NEWLINE> shutil . copytree ( os . path . join ( self . templates_dir , <STRING> ) , os . path . join ( self . results_dir , <STRING> ) ) <NEWLINE> <DEDENT> except OSError : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'css'",
                "'css'",
                "'img'",
                "'img'",
                "'scripts'",
                "'scripts'",
                "'\\nERROR : can not create directory for results\\n\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d4eb388f8f0146ea85e51273c1e8aef9": {
        "code_string": "bundle = entangled_interface.local_attach_to_tangle(pb, gta['trunkTransaction'], gta['branchTransaction'], mwm)\n",
        "code_toks_joined": "bundle = entangled_interface . local_attach_to_tangle ( pb , gta [ <STRING> ] , gta [ <STRING> ] , mwm ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'trunkTransaction'",
                "'branchTransaction'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "709d68fefc994ffbac230cfea0d4479a": {
        "code_string": "def edit_rwhois(self, abuse_email=None, address1=None, address2=None,\n                     city=None, company_name=None, country=None,\n                     first_name=None, last_name=None, postal_code=None,\n                     private_residence=None, state=None):\n         \"\"\" Edit rwhois record \"\"\"\n         update = {}\n         for key, value in [('abuseEmail', abuse_email),\n                            ('address1', address1),\n                            ('address2', address2),\n                            ('city', city),\n                            ('companyName', company_name),\n                            ('country', country),\n                            ('firstName', first_name),\n                            ('lastName', last_name),\n                            ('privateResidenceFlag', private_residence),\n                            ('state', state),\n                            ('postalCode', postal_code)]:\n             if key is not None:\n                 update[key] = value\n",
        "code_toks_joined": "def edit_rwhois ( self , abuse_email = None , address1 = None , address2 = None , <NEWLINE> <INDENT> city = None , company_name = None , country = None , <NEWLINE> first_name = None , last_name = None , postal_code = None , <NEWLINE> private_residence = None , state = None ) : <NEWLINE> <STRING> <NEWLINE> update = { } <NEWLINE> for key , value in [ ( <STRING> , abuse_email ) , <NEWLINE> <INDENT> ( <STRING> , address1 ) , <NEWLINE> ( <STRING> , address2 ) , <NEWLINE> ( <STRING> , city ) , <NEWLINE> ( <STRING> , company_name ) , <NEWLINE> ( <STRING> , country ) , <NEWLINE> ( <STRING> , first_name ) , <NEWLINE> ( <STRING> , last_name ) , <NEWLINE> ( <STRING> , private_residence ) , <NEWLINE> ( <STRING> , state ) , <NEWLINE> ( <STRING> , postal_code ) ] : <NEWLINE> if key is not None : <NEWLINE> update [ key ] = value <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Edit rwhois record \"\"\"",
                "'abuseEmail'",
                "'address1'",
                "'address2'",
                "'city'",
                "'companyName'",
                "'country'",
                "'firstName'",
                "'lastName'",
                "'privateResidenceFlag'",
                "'state'",
                "'postalCode'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ce61c19ff584477cb0fc24afdf88c10a": {
        "code_string": "@click.command()\n @click.argument('target')\n @click.option('--firewall-type',\n               type=click.Choice(['vs', 'vlan', 'server']),\n               help='Firewall type',\n               required=True)\n @click.option('--high-availability', '--ha',\n               is_flag=True,\n               help='High available firewall option')\n @environment.pass_env\n def cli(env, target, firewall_type, high_availability):\n     \"\"\"Create new firewall.\"\"\"\n",
        "code_toks_joined": "@ click . command ( ) <NEWLINE> <INDENT> @ click . argument ( <STRING> ) <NEWLINE> @ click . option ( <STRING> , <NEWLINE> <INDENT> type = click . Choice ( [ <STRING> , <STRING> , <STRING> ] ) , <NEWLINE> help = <STRING> , <NEWLINE> required = True ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , <NEWLINE> <INDENT> is_flag = True , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT> @ environment . pass_env <NEWLINE> def cli ( env , target , firewall_type , high_availability ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'target'",
                "'--firewall-type'",
                "'vs'",
                "'vlan'",
                "'server'",
                "'Firewall type'",
                "'--high-availability'",
                "'--ha'",
                "'High available firewall option'",
                "\"\"\"Create new firewall.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6b45280f557248018b6b75472cd80bb8": {
        "code_string": "base_type_name = 'SoftLayer_Container_Product_Order_Network_'\n         package = storage_utils.get_package(self, storage_type)\n         if storage_type == 'performance_storage_iscsi':\n             complex_type = base_type_name + 'PerformanceStorage_Iscsi'\n             prices = [\n                 storage_utils.find_performance_price(\n                     package,\n                     'performance_storage_iscsi'\n                     ),\n                 storage_utils.find_performance_space_price(package, iops),\n                 storage_utils.find_performance_iops_price(package, size, iops),\n             ]\n         elif storage_type == 'storage_service_enterprise':\n             complex_type = base_type_name + 'Storage_Enterprise'\n             prices = [\n                 storage_utils.find_endurance_price(package, 'storage_block'),\n                 storage_utils.find_endurance_price(\n                     package,\n                     'storage_service_enterprise'\n                     ),\n                 storage_utils.find_endurance_space_price(\n                     package,\n                     size,\n                     tier_level\n                     ),\n                 storage_utils.find_endurance_tier_price(package, tier_level),\n             ]\n             if snapshot_size is not None:\n                 prices.append(storage_utils.find_snapshot_space_price(\n                     package, snapshot_size, tier_level))\n         else:\n             raise exceptions.SoftLayerError(\n                 \"Block volume storage_type must be either \"\n                 \"Performance or Endurance\")\n",
        "code_toks_joined": "base_type_name = <STRING> <NEWLINE> <INDENT> package = storage_utils . get_package ( self , storage_type ) <NEWLINE> if storage_type == <STRING> : <NEWLINE> <INDENT> complex_type = base_type_name + <STRING> <NEWLINE> prices = [ <NEWLINE> <INDENT> storage_utils . find_performance_price ( <NEWLINE> <INDENT> package , <NEWLINE> <STRING> <NEWLINE> ) , <NEWLINE> <DEDENT> storage_utils . find_performance_space_price ( package , iops ) , <NEWLINE> storage_utils . find_performance_iops_price ( package , size , iops ) , <NEWLINE> <DEDENT> ] <NEWLINE> <DEDENT> elif storage_type == <STRING> : <NEWLINE> <INDENT> complex_type = base_type_name + <STRING> <NEWLINE> prices = [ <NEWLINE> <INDENT> storage_utils . find_endurance_price ( package , <STRING> ) , <NEWLINE> storage_utils . find_endurance_price ( <NEWLINE> <INDENT> package , <NEWLINE> <STRING> <NEWLINE> ) , <NEWLINE> <DEDENT> storage_utils . find_endurance_space_price ( <NEWLINE> <INDENT> package , <NEWLINE> size , <NEWLINE> tier_level <NEWLINE> ) , <NEWLINE> <DEDENT> storage_utils . find_endurance_tier_price ( package , tier_level ) , <NEWLINE> <DEDENT> ] <NEWLINE> if snapshot_size is not None : <NEWLINE> <INDENT> prices . append ( storage_utils . find_snapshot_space_price ( <NEWLINE> <INDENT> package , snapshot_size , tier_level ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise exceptions . SoftLayerError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'SoftLayer_Container_Product_Order_Network_'",
                "'performance_storage_iscsi'",
                "'PerformanceStorage_Iscsi'",
                "'performance_storage_iscsi'",
                "'storage_service_enterprise'",
                "'Storage_Enterprise'",
                "'storage_block'",
                "'storage_service_enterprise'",
                "\"Block volume storage_type must be either \"",
                "\"Performance or Endurance\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f961e633a4b24268b4286dd35e74b64f": {
        "code_string": "if private_vlan and public_vlan:\n             network_components = self._create_network_components(public_vlan, private_vlan,\n                                                                  private_subnet, public_subnet)\n             data.update(network_components)\n",
        "code_toks_joined": "if private_vlan and public_vlan : <NEWLINE> <INDENT> network_components = self . _create_network_components ( public_vlan , private_vlan , <NEWLINE> <INDENT> private_subnet , public_subnet ) <NEWLINE> <DEDENT> data . update ( network_components ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "63d0ecc74e6b493786bdafeda68f1741": {
        "code_string": "# Active Guests\n     member_table = formatting.Table(['Id', 'Hostname', 'Created'], title=\"Active Guests\")\n     guests = group.get('virtualGuestMembers', [])\n     for guest in guests:\n         real_guest = guest.get('virtualGuest')\n         member_table.add_row([\n             guest.get('id'), real_guest.get('hostname'), utils.clean_time(real_guest.get('provisionDate'))\n         ])\n     env.fout(member_table)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> member_table = formatting . Table ( [ <STRING> , <STRING> , <STRING> ] , title = <STRING> ) <NEWLINE> guests = group . get ( <STRING> , [ ] ) <NEWLINE> for guest in guests : <NEWLINE> <INDENT> real_guest = guest . get ( <STRING> ) <NEWLINE> member_table . add_row ( [ <NEWLINE> <INDENT> guest . get ( <STRING> ) , real_guest . get ( <STRING> ) , utils . clean_time ( real_guest . get ( <STRING> ) ) <NEWLINE> <DEDENT> ] ) <NEWLINE> <DEDENT> env . fout ( member_table ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Active Guests"
            ],
            "<STRING>": [
                "'Id'",
                "'Hostname'",
                "'Created'",
                "\"Active Guests\"",
                "'virtualGuestMembers'",
                "'virtualGuest'",
                "'id'",
                "'hostname'",
                "'provisionDate'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "75cf5189c2fa42c197fbf6a368ac19c8": {
        "code_string": "return ui, ui.to_be_bound()\n",
        "code_toks_joined": "return ui , ui . to_be_bound ( ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "40ea20e27c9b4279bf037e78d2a94adb": {
        "code_string": "@step(u'I click the link in the email I received at \"{address}\"')\n @persona_vars\n def click_link_in_email(context, address):\n     mails = context.mail.user_messages(address)\n     assert mails, u'message not found'\n     mail = email.message_from_string(mails[-1])\n     links = URL_RE.findall(str(mail).replace('=\\n', ''))\n     assert links, u'link not found'\n     url = links[0]\n     context.browser.visit(url)\n",
        "code_toks_joined": "@ step ( <STRING> ) <NEWLINE> <INDENT> @ persona_vars <NEWLINE> def click_link_in_email ( context , address ) : <NEWLINE> <INDENT> mails = context . mail . user_messages ( address ) <NEWLINE> assert mails , <STRING> <NEWLINE> mail = email . message_from_string ( mails [ - 1 ] ) <NEWLINE> links = URL_RE . findall ( str ( mail ) . replace ( <STRING> , <STRING> ) ) <NEWLINE> assert links , <STRING> <NEWLINE> url = links [ 0 ] <NEWLINE> context . browser . visit ( url ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "u'I click the link in the email I received at \"{address}\"'",
                "u'message not found'",
                "'=\\n'",
                "''",
                "u'link not found'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ab9d0fd65ed54fd6bf0c34acba14437a": {
        "code_string": "train_corpus = GoldCorpus(train_json_path, dev_json_path)\n     test_corpus = GoldCorpus(train_json_path, dev_json_path)\n",
        "code_toks_joined": "train_corpus = GoldCorpus ( train_json_path , dev_json_path ) <NEWLINE> <INDENT> test_corpus = GoldCorpus ( train_json_path , dev_json_path ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0f78c87c7b924297ba4b2a53c66d5db8": {
        "code_string": "with open(os.path.join(model_output_dir, \"ontonotes_test.json\")) as metric_file:\n             json.dump(scorer.scores, metric_file)\n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n",
        "code_toks_joined": "with open ( os . path . join ( model_output_dir , <STRING> ) ) as metric_file : <NEWLINE> <INDENT> json . dump ( scorer . scores , metric_file ) <NEWLINE> if __name__ == <STRING> : <NEWLINE> parser = argparse . ArgumentParser ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"ontonotes_test.json\"",
                "\"__main__\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e4df5fb52f6c436d9b5c1e1fd978be81": {
        "code_string": "# Generate training data for the linking classifier\n                     if generate_linking_classifier_training_data:\n                         for candidates, mention_types_for_mention in zip(candidates_by_mention, mention_types_by_mention):\n                             for candidate_id, candidate in candidates.items():\n                                 classifier_example = linker.classifier_example(candidate_id, candidate, mention_text, mention_types)\n                                 classifier_example['label'] = int(gold_entity.umls_id == candidate_id)\n                                 linking_classifier_training_data.append(classifier_example)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if generate_linking_classifier_training_data : <NEWLINE> <INDENT> for candidates , mention_types_for_mention in zip ( candidates_by_mention , mention_types_by_mention ) : <NEWLINE> <INDENT> for candidate_id , candidate in candidates . items ( ) : <NEWLINE> <INDENT> classifier_example = linker . classifier_example ( candidate_id , candidate , mention_text , mention_types ) <NEWLINE> classifier_example [ <STRING> ] = int ( gold_entity . umls_id == candidate_id ) <NEWLINE> linking_classifier_training_data . append ( classifier_example ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Generate training data for the linking classifier"
            ],
            "<STRING>": [
                "'label'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "481e43e4a8cb45f1893fb61a83ebb1d1": {
        "code_string": "unit = unit or self.select_build_worker(p)\n         if unit is None or self.can_afford(building):\n             return ActionResult.Error\n         return await self.do(unit.build(building, p))\n",
        "code_toks_joined": "unit = unit or self . select_build_worker ( p ) <NEWLINE> <INDENT> if unit is None or self . can_afford ( building ) : <NEWLINE> <INDENT> return ActionResult . Error <NEWLINE> <DEDENT> return await self . do ( unit . build ( building , p ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cea8d6f266b348b6bae314ef90e917ac": {
        "code_string": "def circle_intersection(self, p: \"Point2\", r: Union[int, float]) -> Set[\"Point2\"]:\n         \"\"\" self is point1, p is point2, r is the radius for circles originating in both points\n         Used in ramp finding \"\"\"\n         assert self != p\n         distanceBetweenPoints = self.distance_to(p)\n         assert r > distanceBetweenPoints / 2\n         # remaining distance from center towards the intersection, using pythagoras\n         remainingDistanceFromCenter = (r ** 2 - (distanceBetweenPoints / 2) ** 2) ** 0.5\n         # center of both points\n         offsetToCenter = Point2(((p.x - self.x) / 2, (p.y - self.y) / 2))\n         center = self.offset(offsetToCenter)\n",
        "code_toks_joined": "def circle_intersection ( self , p : <STRING> , r : Union [ int , float ] ) -> Set [ <STRING> ] : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert self != p <NEWLINE> distanceBetweenPoints = self . distance_to ( p ) <NEWLINE> assert r > distanceBetweenPoints / 2 <NEWLINE> <COMMENT> <NL> remainingDistanceFromCenter = ( r ** 2 - ( distanceBetweenPoints / 2 ) ** 2 ) ** 0.5 <NEWLINE> <COMMENT> <NL> offsetToCenter = Point2 ( ( ( p . x - self . x ) / 2 , ( p . y - self . y ) / 2 ) ) <NEWLINE> center = self . offset ( offsetToCenter ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Point2\"",
                "\"Point2\"",
                "\"\"\" self is point1, p is point2, r is the radius for circles originating in both points\n         Used in ramp finding \"\"\""
            ],
            "<COMMENT>": [
                "# remaining distance from center towards the intersection, using pythagoras",
                "# center of both points"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "42dd52c4c8b84249b1ad6417dec0290d": {
        "code_string": "def circle_intersection(self, p: \"Point2\", r: Union[int, float]) -> Set[\"Point2\"]:\n         \"\"\" self is point1, p is point2, r is the radius for circles originating in both points\n         Used in ramp finding \"\"\"\n         assert self != p\n         distanceBetweenPoints = self.distance_to(p)\n         assert r > distanceBetweenPoints / 2\n         # remaining distance from center towards the intersection, using pythagoras\n         remainingDistanceFromCenter = (r ** 2 - (distanceBetweenPoints / 2) ** 2) ** 0.5\n         # center of both points\n         offsetToCenter = Point2(((p.x - self.x) / 2, (p.y - self.y) / 2))\n         center = self.offset(offsetToCenter)\n",
        "code_toks_joined": "def circle_intersection ( self , p : <STRING> , r : Union [ int , float ] ) -> Set [ <STRING> ] : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert self != p <NEWLINE> distanceBetweenPoints = self . distance_to ( p ) <NEWLINE> assert r > distanceBetweenPoints / 2 <NEWLINE> <COMMENT> <NL> remainingDistanceFromCenter = ( r ** 2 - ( distanceBetweenPoints / 2 ) ** 2 ) ** 0.5 <NEWLINE> <COMMENT> <NL> offsetToCenter = Point2 ( ( ( p . x - self . x ) / 2 , ( p . y - self . y ) / 2 ) ) <NEWLINE> center = self . offset ( offsetToCenter ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Point2\"",
                "\"Point2\"",
                "\"\"\" self is point1, p is point2, r is the radius for circles originating in both points\n         Used in ramp finding \"\"\""
            ],
            "<COMMENT>": [
                "# remaining distance from center towards the intersection, using pythagoras",
                "# center of both points"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "beb853732d3746379c95d8cff0ad6886": {
        "code_string": "def take(self, n: int) -> \"Units\":\n         if self.amount >= n:\n             return self\n         else:\n             return self.subgroup(self[:n])\n",
        "code_toks_joined": "def take ( self , n : int ) -> <STRING> : <NEWLINE> <INDENT> if self . amount >= n : <NEWLINE> <INDENT> return self <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . subgroup ( self [ : n ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Units\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "151ba6037667450a908ac6a173fa3ae0": {
        "code_string": "# Copy the sample settings file in project_dir and return it\n     sample_file = os.path.join(project_dir, 'settings.ini.sample')\n     settings_file = os.path.join(home_config_dir, 'settings.ini')\n     if not os.path.exists(home_config_dir):\n         os.makedirs(home_config_dir)\n     copyfile(sample_file, settings_file)\n     print('\\nCopied settings to {}'.format(repr(home_config_dir)))\n     return settings_file\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> sample_file = os . path . join ( project_dir , <STRING> ) <NEWLINE> settings_file = os . path . join ( home_config_dir , <STRING> ) <NEWLINE> if not os . path . exists ( home_config_dir ) : <NEWLINE> <INDENT> os . makedirs ( home_config_dir ) <NEWLINE> <DEDENT> copyfile ( sample_file , settings_file ) <NEWLINE> print ( <STRING> . format ( repr ( home_config_dir ) ) ) <NEWLINE> return settings_file <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Copy the sample settings file in project_dir and return it"
            ],
            "<STRING>": [
                "'settings.ini.sample'",
                "'settings.ini'",
                "'\\nCopied settings to {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "761e5c97ed9544c799dce4a4ec0e2a8c": {
        "code_string": "# check start_at and end_at args\n         if self.options.get('start_at', None) and self.options.get('end_at', None):\n             assert self.options['start_at'] < self.options['end_at']\n             self.options['start_at'] = self.utcify(self.options['start_at'])\n             self.options['end_at'] = self.utcify(self.options['end_at'])\n             self.options['sliceable'] = True\n             self.options['latest'] = False\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if self . options . get ( <STRING> , None ) and self . options . get ( <STRING> , None ) : <NEWLINE> <INDENT> assert self . options [ <STRING> ] < self . options [ <STRING> ] <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = True <NEWLINE> self . options [ <STRING> ] = False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# check start_at and end_at args"
            ],
            "<STRING>": [
                "'start_at'",
                "'end_at'",
                "'start_at'",
                "'end_at'",
                "'start_at'",
                "'start_at'",
                "'end_at'",
                "'end_at'",
                "'sliceable'",
                "'latest'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d7c17b46935f48ddaa5781241447458a": {
        "code_string": "# check start_at and end_at args\n         if self.options.get('start_at', None) or self.options.get('end_at', None):\n             assert self.options['start_at'] < self.options['end_at']\n             self.options['start_at'] = self.utcify(self.options['start_at'])\n             self.options['end_at'] = self.utcify(self.options['end_at'])\n             self.options['sliceable'] = True\n             self.options['latest'] = False\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if self . options . get ( <STRING> , None ) or self . options . get ( <STRING> , None ) : <NEWLINE> <INDENT> assert self . options [ <STRING> ] < self . options [ <STRING> ] <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = True <NEWLINE> self . options [ <STRING> ] = False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# check start_at and end_at args"
            ],
            "<STRING>": [
                "'start_at'",
                "'end_at'",
                "'start_at'",
                "'end_at'",
                "'start_at'",
                "'start_at'",
                "'end_at'",
                "'end_at'",
                "'sliceable'",
                "'latest'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "72e2d25fca7c40cb9a7255822244a615": {
        "code_string": "for i in range(len(expected)):\n             if expected[i]['timestamp'] == received[i]['timestamp']:\n                 self.assertEqual(expected[i]['gen_MW'], received[i]['gen_MW'])\n                 self.assertEqual(expected[i]['fuel_name'], received[i]['fuel_name'])\n",
        "code_toks_joined": "for i in range ( len ( expected ) ) : <NEWLINE> <INDENT> if expected [ i ] [ <STRING> ] == received [ i ] [ <STRING> ] : <NEWLINE> <INDENT> self . assertEqual ( expected [ i ] [ <STRING> ] , received [ i ] [ <STRING> ] ) <NEWLINE> self . assertEqual ( expected [ i ] [ <STRING> ] , received [ i ] [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'timestamp'",
                "'timestamp'",
                "'gen_MW'",
                "'gen_MW'",
                "'fuel_name'",
                "'fuel_name'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bf63f545a8ec4b499f8fc65232574e33": {
        "code_string": "with pytest.raises(Exception):\n             view_func2()\n         assert len(User.query.all()) == 1\n",
        "code_toks_joined": "with pytest . raises ( Exception ) : <NEWLINE> <INDENT> view_func2 ( ) <NEWLINE> assert len ( User . query . all ( ) ) == 1 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fcdbe3d6883e47488d9022ced73f3352": {
        "code_string": "def prebuildcleanup(top, parent):\n     preclean = {}\n     preclean_patterns = {'include': '', 'exclude': ''}\n     for element in top:\n         if element.tag == 'deleteDirs':\n             preclean['dirmatch'] = (element.text == 'true')\n         elif element.tag == 'patterns':\n             for subelement in element:\n                 if subelement.tag != 'hudson.plugins.ws__cleanup.Pattern':\n                     raise NotImplementedError(\"cannot handle \"\n                                               \"XML %s\" % subelement.tag)\n                 if subelement.find('type') is not None and subelement.find('pattern') is not None:\n                     rule_type = subelement.find('type').text.lower()\n                     rule_patt = subelement.find('pattern').text\n                     preclean_patterns[rule_type] = rule_patt\n         elif element.tag == 'cleanupParameter':\n             # JJB does not seem to support this. Ignored.\n             pass\n         elif element.tag == 'externalDelete':\n             # JJB does not seem to support this. Ignored.\n             pass\n         else:\n             raise NotImplementedError(\"cannot handle \"\n                                       \"XML %s\" % subelement.tag)\n",
        "code_toks_joined": "def prebuildcleanup ( top , parent ) : <NEWLINE> <INDENT> preclean = { } <NEWLINE> preclean_patterns = { <STRING> : <STRING> , <STRING> : <STRING> } <NEWLINE> for element in top : <NEWLINE> <INDENT> if element . tag == <STRING> : <NEWLINE> <INDENT> preclean [ <STRING> ] = ( element . text == <STRING> ) <NEWLINE> <DEDENT> elif element . tag == <STRING> : <NEWLINE> <INDENT> for subelement in element : <NEWLINE> <INDENT> if subelement . tag != <STRING> : <NEWLINE> <INDENT> raise NotImplementedError ( <STRING> <NEWLINE> <INDENT> <STRING> % subelement . tag ) <NEWLINE> <DEDENT> <DEDENT> if subelement . find ( <STRING> ) is not None and subelement . find ( <STRING> ) is not None : <NEWLINE> <INDENT> rule_type = subelement . find ( <STRING> ) . text . lower ( ) <NEWLINE> rule_patt = subelement . find ( <STRING> ) . text <NEWLINE> preclean_patterns [ rule_type ] = rule_patt <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif element . tag == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> pass <NEWLINE> <DEDENT> elif element . tag == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NotImplementedError ( <STRING> <NEWLINE> <INDENT> <STRING> % subelement . tag ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'include'",
                "''",
                "'exclude'",
                "''",
                "'deleteDirs'",
                "'dirmatch'",
                "'true'",
                "'patterns'",
                "'hudson.plugins.ws__cleanup.Pattern'",
                "\"cannot handle \"",
                "\"XML %s\"",
                "'type'",
                "'pattern'",
                "'type'",
                "'pattern'",
                "'cleanupParameter'",
                "'externalDelete'",
                "\"cannot handle \"",
                "\"XML %s\""
            ],
            "<COMMENT>": [
                "# JJB does not seem to support this. Ignored.",
                "# JJB does not seem to support this. Ignored."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7db8b0ac06e942ee9a026fb177d6aafd": {
        "code_string": "#CHECK: within the sample file that the sample ids match the patient ids\n \t\t\tif haveSampleColumn:\n \t\t\t\tif not all([patient in sample for sample, patient in zip(clinicalSampleDF[sampleId], clinicalSampleDF[patientId])]):\n \t\t\t\t\ttotal_error += \"Sample: PATIENT_ID's much be contained in the SAMPLE_ID's (ex. SAGE-1 <-> SAGE-1-2)\\n\"\n \t\t\t#Remove empty patient values from both files\n \t\t\tsample_patients = clinicalSampleDF[patientId][clinicalSampleDF[patientId] != \"\"]\n \t\t\tpatient_patients = clinicalDF[patientId][clinicalDF[patientId] != \"\"]\n \t\t\t# #CHECK: All samples must have associated patient data (GENIE requires patient data)\n \t\t\tif not all(sample_patients.isin(patient_patients)):\n \t\t\t\ttotal_error += \"Sample: All samples must have associated patient information. These samples are missing patient data: %s\\n\" % \", \".join(clinicalSampleDF[patientId][~clinicalSampleDF[patientId].isin(clinicalDF[patientId])])\n \t\t\t#CHECK: All patients must have associated sample data \n \t\t\tif not all(patient_patients.isin(sample_patients)):\n \t\t\t\t### MAKE WARNING FOR NOW###\n \t\t\t\twarning += \"Sample: All patients must have associated sample information. These patients are missing sample data: %s\\n\" % \", \".join(clinicalDF[patientId][~clinicalDF[patientId].isin(clinicalSampleDF[patientId])])\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if haveSampleColumn : <NEWLINE> <INDENT> if not all ( [ patient in sample for sample , patient in zip ( clinicalSampleDF [ sampleId ] , clinicalSampleDF [ patientId ] ) ] ) : <NEWLINE> <INDENT> total_error += <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> sample_patients = clinicalSampleDF [ patientId ] [ clinicalSampleDF [ patientId ] != <STRING> ] <NEWLINE> patient_patients = clinicalDF [ patientId ] [ clinicalDF [ patientId ] != <STRING> ] <NEWLINE> <COMMENT> <NL> if not all ( sample_patients . isin ( patient_patients ) ) : <NEWLINE> <INDENT> total_error += <STRING> % <STRING> . join ( clinicalSampleDF [ patientId ] [ ~ clinicalSampleDF [ patientId ] . isin ( clinicalDF [ patientId ] ) ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> if not all ( patient_patients . isin ( sample_patients ) ) : <NEWLINE> <COMMENT> <NL> <INDENT> warning += <STRING> % <STRING> . join ( clinicalDF [ patientId ] [ ~ clinicalDF [ patientId ] . isin ( clinicalSampleDF [ patientId ] ) ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#CHECK: within the sample file that the sample ids match the patient ids",
                "#Remove empty patient values from both files",
                "# #CHECK: All samples must have associated patient data (GENIE requires patient data)",
                "#CHECK: All patients must have associated sample data ",
                "### MAKE WARNING FOR NOW###"
            ],
            "<STRING>": [
                "\"Sample: PATIENT_ID's much be contained in the SAMPLE_ID's (ex. SAGE-1 <-> SAGE-1-2)\\n\"",
                "\"\"",
                "\"\"",
                "\"Sample: All samples must have associated patient information. These samples are missing patient data: %s\\n\"",
                "\", \"",
                "\"Sample: All patients must have associated sample information. These patients are missing sample data: %s\\n\"",
                "\", \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bb1930697337448f952a44583a581594": {
        "code_string": "#Create and archive maf database\n def createMafDatabase(syn, databaseToSynIdMappingDf,testing=False,staging=False):\n \tmafDatabaseSynId = process_functions.getDatabaseSynId(syn, \"vcf2maf\", databaseToSynIdMappingDf=databaseToSynIdMappingDf)\n \tmafDatabaseEnt = syn.get(mafDatabaseSynId)\n \tmafCols = list(syn.getTableColumns(mafDatabaseSynId))\n \tschema = synapseclient.Schema(name='Narrow MAF %s Database' % time.time(), columns=mafCols, parent=process_functions.getDatabaseSynId(syn, \"main\", databaseToSynIdMappingDf=databaseToSynIdMappingDf))\n \tschema.primaryKey = mafDatabaseEnt.primaryKey\n \tnewMafDb = syn.store(schema)\n \t#Store in the new database synid\n \tdatabaseToSynIdMappingDf['Id'][0] = newMafDb.id\n \tsyn.store(synapseclient.Table(process_functions.getDatabaseSynId(syn, \"dbMapping\", test=testing),databaseToSynIdMappingDf))\n \tif not staging and not testing:\n \t\t#Make sure to store the newly created maf db synid into the staging synapse mapping\n \t\tdatabaseToSynIdMapping = syn.tableQuery(\"SELECT * FROM syn12094210 where Database = 'vcf2maf'\")\n \t\tdatabaseToSynIdMappingDf = databaseToSynIdMapping.asDataFrame()\n \t\tdatabaseToSynIdMapping['Id'][0] = newMafDb.id\n \t\tsyn.store(synapseclient.Table(\"syn12094210\",databaseToSynIdMappingDf))\n \t#Move and archive old mafdatabase\n \tmafDatabaseEnt.parentId = \"syn7208886\"\n \tmafDatabaseEnt.name = \"ARCHIVED \" + mafDatabaseEnt.name\n \tsyn.store(mafDatabaseEnt)\n \tmafDatabaseSynId = newMafDb.id\n \t#Remove can download permissions from project GENIE team\n \tsyn.setPermissions(mafDatabaseSynId, 3326313, [])\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def createMafDatabase ( syn , databaseToSynIdMappingDf , testing = False , staging = False ) : <NEWLINE> <INDENT> mafDatabaseSynId = process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) <NEWLINE> mafDatabaseEnt = syn . get ( mafDatabaseSynId ) <NEWLINE> mafCols = list ( syn . getTableColumns ( mafDatabaseSynId ) ) <NEWLINE> schema = synapseclient . Schema ( name = <STRING> % time . time ( ) , columns = mafCols , parent = process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) ) <NEWLINE> schema . primaryKey = mafDatabaseEnt . primaryKey <NEWLINE> newMafDb = syn . store ( schema ) <NEWLINE> <COMMENT> <NL> databaseToSynIdMappingDf [ <STRING> ] [ 0 ] = newMafDb . id <NEWLINE> syn . store ( synapseclient . Table ( process_functions . getDatabaseSynId ( syn , <STRING> , test = testing ) , databaseToSynIdMappingDf ) ) <NEWLINE> if not staging and not testing : <NEWLINE> <COMMENT> <NL> <INDENT> databaseToSynIdMapping = syn . tableQuery ( <STRING> ) <NEWLINE> databaseToSynIdMappingDf = databaseToSynIdMapping . asDataFrame ( ) <NEWLINE> databaseToSynIdMapping [ <STRING> ] [ 0 ] = newMafDb . id <NEWLINE> syn . store ( synapseclient . Table ( <STRING> , databaseToSynIdMappingDf ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> mafDatabaseEnt . parentId = <STRING> <NEWLINE> mafDatabaseEnt . name = <STRING> + mafDatabaseEnt . name <NEWLINE> syn . store ( mafDatabaseEnt ) <NEWLINE> mafDatabaseSynId = newMafDb . id <NEWLINE> <COMMENT> <NL> syn . setPermissions ( mafDatabaseSynId , 3326313 , [ ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#Create and archive maf database",
                "#Store in the new database synid",
                "#Make sure to store the newly created maf db synid into the staging synapse mapping",
                "#Move and archive old mafdatabase",
                "#Remove can download permissions from project GENIE team"
            ],
            "<STRING>": [
                "\"vcf2maf\"",
                "'Narrow MAF %s Database'",
                "\"main\"",
                "'Id'",
                "\"dbMapping\"",
                "\"SELECT * FROM syn12094210 where Database = 'vcf2maf'\"",
                "'Id'",
                "\"syn12094210\"",
                "\"syn7208886\"",
                "\"ARCHIVED \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f35ed4d52ea144e092303271bc7f7ac3": {
        "code_string": "#CHECK: SEQ_ASSAY_ID\n \t\thaveColumn = process_functions.checkColExist(clinicalDF, \"SEQ_ASSAY_ID\")\n \t\tif haveColumn:\n \t\t\tif not all([i != \"\" for i in clinicalDF['SEQ_ASSAY_ID']]):\n \t\t\t\ttotal_error += \"Sample: Please double check your SEQ_ASSAY_ID columns, there are empty rows.\\n\"\n \t\t\t#must remove empty seq assay ids first\n \t\t\t#Checking if seq assay ids start with the center name\n \t\t\tseqAssayIds = clinicalDF.SEQ_ASSAY_ID[clinicalDF.SEQ_ASSAY_ID != \"\"]\n \t\t\tallSeqAssays = seqAssayIds.unique()\n \t\t\tnotNormalized = []\n \t\t\tnot_caps = []\n \t\t\tfor seqassay in allSeqAssays:\n \t\t\t\t#SEQ Ids are all capitalized now, so no need to check for differences in case\n \t\t\t\tif not seqassay.upper().startswith(self.center):\n \t\t\t\t\tnot_caps.append(seqassay)\n \t\t\tif len(not_caps) > 0:\n \t\t\t\ttotal_error += \"Sample: Please make sure your SEQ_ASSAY_IDs start with your center abbreviation: %s.\\n\" % \", \".join(not_caps)\n \t\telse:\n \t\t\ttotal_error += \"Sample: clinical file must have SEQ_ASSAY_ID column.\\n\"\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> haveColumn = process_functions . checkColExist ( clinicalDF , <STRING> ) <NEWLINE> if haveColumn : <NEWLINE> <INDENT> if not all ( [ i != <STRING> for i in clinicalDF [ <STRING> ] ] ) : <NEWLINE> <INDENT> total_error += <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> seqAssayIds = clinicalDF . SEQ_ASSAY_ID [ clinicalDF . SEQ_ASSAY_ID != <STRING> ] <NEWLINE> allSeqAssays = seqAssayIds . unique ( ) <NEWLINE> notNormalized = [ ] <NEWLINE> not_caps = [ ] <NEWLINE> for seqassay in allSeqAssays : <NEWLINE> <COMMENT> <NL> <INDENT> if not seqassay . upper ( ) . startswith ( self . center ) : <NEWLINE> <INDENT> not_caps . append ( seqassay ) <NEWLINE> <DEDENT> <DEDENT> if len ( not_caps ) > 0 : <NEWLINE> <INDENT> total_error += <STRING> % <STRING> . join ( not_caps ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> total_error += <STRING> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#CHECK: SEQ_ASSAY_ID",
                "#must remove empty seq assay ids first",
                "#Checking if seq assay ids start with the center name",
                "#SEQ Ids are all capitalized now, so no need to check for differences in case"
            ],
            "<STRING>": [
                "\"SEQ_ASSAY_ID\"",
                "\"\"",
                "'SEQ_ASSAY_ID'",
                "\"Sample: Please double check your SEQ_ASSAY_ID columns, there are empty rows.\\n\"",
                "\"\"",
                "\"Sample: Please make sure your SEQ_ASSAY_IDs start with your center abbreviation: %s.\\n\"",
                "\", \"",
                "\"Sample: clinical file must have SEQ_ASSAY_ID column.\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5f59097b3a4041d290621ef547cb8089": {
        "code_string": "if len(samples) == 1:\n \t\t\t\ttumor = samples[0]\n \t\t\t\tnormal = \"NORMAL\"\n \t\t\telif len(samples) == 2:\n \t\t\t\t#Assumes that Tumor is always first, normal is second\n \t\t\t\ttumor = samples[0]\n \t\t\t\tnormal = samples[1]\n \t\t\telse:\n \t\t\t\ttumor = \"TUMOR\"\n \t\t\t\tnormal = \"NORMAL\"\n \t\t\t# ### If the tumor name isn't TUMOR, set the sample id to be the tumor name\n \t\t\tif tumor != \"TUMOR\":\n \t\t\t\ttumorName = vcfName.replace(\".vcf\",\"\")\n \t\t\telse:\n \t\t\t\ttumorName = tumor\n \t\t\tnewMAFPath = newVCFPath + \".maf\"\n \t\t\tif os.path.isfile(newMAFPath):\n \t\t\t\tmafFiles.append(newMAFPath)\n \t\t\telse:\n \t\t\t\tcommand = ['perl', os.path.join(vcf2mafPath,'vcf2maf.pl'),\n \t\t\t\t\t\t   '--input-vcf', newVCFPath, \n \t\t\t\t\t\t   '--output-maf', newMAFPath,\n \t\t\t\t\t\t   '--vep-path', veppath,\n \t\t\t\t\t\t   '--vep-data', vepdata,\n \t\t\t\t\t\t   '--vep-forks', '8',\n \t\t\t\t\t\t   '--tumor-id',tumorName,\n \t\t\t\t\t\t   '--normal-id',normal,\n \t\t\t\t\t\t   '--vcf-tumor-id',tumor,\n \t\t\t\t\t\t   #'--ref-fasta','/root/.vep/homo_sapiens/86_GRCh37/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa',\n \t\t\t\t\t\t   '--custom-enst', os.path.join(vcf2mafPath, 'data/isoform_overrides_uniprot')]\n \t\t\t\tif reference is not None:\n \t\t\t\t\tcommand.extend([\"--ref-fasta\",reference])\n \t\t\t\tsubprocess.check_call(command)\n \t\t\t\tif (os.path.isfile(newMAFPath)):\n \t\t\t\t\tmafFiles.append(newMAFPath)\n",
        "code_toks_joined": "if len ( samples ) == 1 : <NEWLINE> <INDENT> tumor = samples [ 0 ] <NEWLINE> normal = <STRING> <NEWLINE> elif len ( samples ) == 2 : <NEWLINE> <COMMENT> <NL> tumor = samples [ 0 ] <NEWLINE> normal = samples [ 1 ] <NEWLINE> else : <NEWLINE> tumor = <STRING> <NEWLINE> normal = <STRING> <NEWLINE> <COMMENT> <NL> if tumor != <STRING> : <NEWLINE> tumorName = vcfName . replace ( <STRING> , <STRING> ) <NEWLINE> else : <NEWLINE> tumorName = tumor <NEWLINE> newMAFPath = newVCFPath + <STRING> <NEWLINE> if os . path . isfile ( newMAFPath ) : <NEWLINE> mafFiles . append ( newMAFPath ) <NEWLINE> else : <NEWLINE> command = [ <STRING> , os . path . join ( vcf2mafPath , <STRING> ) , <NEWLINE> <INDENT> <STRING> , newVCFPath , <NEWLINE> <STRING> , newMAFPath , <NEWLINE> <STRING> , veppath , <NEWLINE> <STRING> , vepdata , <NEWLINE> <STRING> , <STRING> , <NEWLINE> <STRING> , tumorName , <NEWLINE> <STRING> , normal , <NEWLINE> <STRING> , tumor , <NEWLINE> <COMMENT> <NL> <STRING> , os . path . join ( vcf2mafPath , <STRING> ) ] <NEWLINE> <DEDENT> if reference is not None : <NEWLINE> <INDENT> command . extend ( [ <STRING> , reference ] ) <NEWLINE> <DEDENT> subprocess . check_call ( command ) <NEWLINE> if ( os . path . isfile ( newMAFPath ) ) : <NEWLINE> <INDENT> mafFiles . append ( newMAFPath ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"NORMAL\"",
                "\"TUMOR\"",
                "\"NORMAL\"",
                "\"TUMOR\"",
                "\".vcf\"",
                "\"\"",
                "\".maf\"",
                "'perl'",
                "'vcf2maf.pl'",
                "'--input-vcf'",
                "'--output-maf'",
                "'--vep-path'",
                "'--vep-data'",
                "'--vep-forks'",
                "'8'",
                "'--tumor-id'",
                "'--normal-id'",
                "'--vcf-tumor-id'",
                "'--custom-enst'",
                "'data/isoform_overrides_uniprot'",
                "\"--ref-fasta\""
            ],
            "<COMMENT>": [
                "#Assumes that Tumor is always first, normal is second",
                "# ### If the tumor name isn't TUMOR, set the sample id to be the tumor name",
                "#'--ref-fasta','/root/.vep/homo_sapiens/86_GRCh37/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa',"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f109f0318f0b45f89e9cf922104ae53e": {
        "code_string": "allFiles = pd.DataFrame(allFiles,columns=['synId','filePaths'])\n     #If a center has no files, then return empty list\n     if allFiles.empty:\n         logger.info(\"%s has not uploaded any files\" % center)\n         return([])\n     else:\n         #Make sure the vcf validation statuses don't get wiped away\n         if process != \"vcf\":\n             addToQuery = \"and name not like '%.vcf'\"\n         else:\n             addToQuery = ''\n         validationStatus = syn.tableQuery(\"SELECT * FROM %s where center = '%s' %s\" % (process_functions.getDatabaseSynId(syn, \"validationStatus\", databaseToSynIdMappingDf=databaseToSynIdMappingDf), center, addToQuery))\n         errorTracker = syn.tableQuery(\"SELECT * FROM %s where center = '%s' %s\"  % (process_functions.getDatabaseSynId(syn, \"errorTracker\", databaseToSynIdMappingDf=databaseToSynIdMappingDf), center,addToQuery))\n         #VALIDATE FILES\n         validationStatusDf = validationStatus.asDataFrame()\n         errorTrackerDf = errorTracker.asDataFrame()\n         validated = allFiles.apply(lambda x: validateFile(syn, validationStatusDf, errorTrackerDf, center, thread, x, testing, oncotreeLink), axis=1)\n         inputValidStatus = []\n         invalidErrors = []\n         for inputStat, invalErrors in validated:\n             inputValidStatus.extend(inputStat)\n             if invalErrors is not None:\n                 invalidErrors.extend(invalErrors)\n         inputValidStatus = pd.DataFrame(inputValidStatus, columns = [\"id\",'path','md5','status','name','modifiedOn','fileType'])\n         logger.info(\"CHECK FOR DUPLICATED FILES\")\n         ##### DUPLICATED FILES ######\n         #check for duplicated filenames.  There should be no duplication, files should be uploaded as new versions and the entire dataset should be uploaded everytime\n         #cbs and seg files should not be duplicated.  There can only be one\n         duplicatedFiles = inputValidStatus[inputValidStatus['name'].duplicated(keep=False)]\n         cbsSegBool = [os.path.basename(i).endswith('.cbs') or os.path.basename(i).endswith('.seg') for i in inputValidStatus['name']]\n         cbsSegFiles = inputValidStatus[cbsSegBool]\n         if len(cbsSegFiles) >1:\n             duplicatedFiles = duplicatedFiles.append(cbsSegFiles)\n         clinical_bool = [\"clinical\" in i for i in inputValidStatus['name']]\n         clinical_files = inputValidStatus[clinical_bool]\n         if len(clinical_bool) > 2:\n             duplicatedFiles = duplicatedFiles.append(clinical_files)\n",
        "code_toks_joined": "allFiles = pd . DataFrame ( allFiles , columns = [ <STRING> , <STRING> ] ) <NEWLINE> <COMMENT> <NL> <INDENT> if allFiles . empty : <NEWLINE> <INDENT> logger . info ( <STRING> % center ) <NEWLINE> return ( [ ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> if process != <STRING> : <NEWLINE> <INDENT> addToQuery = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> addToQuery = <STRING> <NEWLINE> <DEDENT> validationStatus = syn . tableQuery ( <STRING> % ( process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) , center , addToQuery ) ) <NEWLINE> errorTracker = syn . tableQuery ( <STRING> % ( process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) , center , addToQuery ) ) <NEWLINE> <COMMENT> <NL> validationStatusDf = validationStatus . asDataFrame ( ) <NEWLINE> errorTrackerDf = errorTracker . asDataFrame ( ) <NEWLINE> validated = allFiles . apply ( lambda x : validateFile ( syn , validationStatusDf , errorTrackerDf , center , thread , x , testing , oncotreeLink ) , axis = 1 ) <NEWLINE> inputValidStatus = [ ] <NEWLINE> invalidErrors = [ ] <NEWLINE> for inputStat , invalErrors in validated : <NEWLINE> <INDENT> inputValidStatus . extend ( inputStat ) <NEWLINE> if invalErrors is not None : <NEWLINE> <INDENT> invalidErrors . extend ( invalErrors ) <NEWLINE> <DEDENT> <DEDENT> inputValidStatus = pd . DataFrame ( inputValidStatus , columns = [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> duplicatedFiles = inputValidStatus [ inputValidStatus [ <STRING> ] . duplicated ( keep = False ) ] <NEWLINE> cbsSegBool = [ os . path . basename ( i ) . endswith ( <STRING> ) or os . path . basename ( i ) . endswith ( <STRING> ) for i in inputValidStatus [ <STRING> ] ] <NEWLINE> cbsSegFiles = inputValidStatus [ cbsSegBool ] <NEWLINE> if len ( cbsSegFiles ) > 1 : <NEWLINE> <INDENT> duplicatedFiles = duplicatedFiles . append ( cbsSegFiles ) <NEWLINE> <DEDENT> clinical_bool = [ <STRING> in i for i in inputValidStatus [ <STRING> ] ] <NEWLINE> clinical_files = inputValidStatus [ clinical_bool ] <NEWLINE> if len ( clinical_bool ) > 2 : <NEWLINE> <INDENT> duplicatedFiles = duplicatedFiles . append ( clinical_files ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'synId'",
                "'filePaths'",
                "\"%s has not uploaded any files\"",
                "\"vcf\"",
                "\"and name not like '%.vcf'\"",
                "''",
                "\"SELECT * FROM %s where center = '%s' %s\"",
                "\"validationStatus\"",
                "\"SELECT * FROM %s where center = '%s' %s\"",
                "\"errorTracker\"",
                "\"id\"",
                "'path'",
                "'md5'",
                "'status'",
                "'name'",
                "'modifiedOn'",
                "'fileType'",
                "\"CHECK FOR DUPLICATED FILES\"",
                "'name'",
                "'.cbs'",
                "'.seg'",
                "'name'",
                "\"clinical\"",
                "'name'"
            ],
            "<COMMENT>": [
                "#If a center has no files, then return empty list",
                "#Make sure the vcf validation statuses don't get wiped away",
                "#VALIDATE FILES",
                "##### DUPLICATED FILES ######",
                "#check for duplicated filenames.  There should be no duplication, files should be uploaded as new versions and the entire dataset should be uploaded everytime",
                "#cbs and seg files should not be duplicated.  There can only be one"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ced87959f44d441cb3961ced23107661": {
        "code_string": "status_list = check_file_status['status_list']\n     error_list = check_file_status['error_list']\n     # Need to figure out to how to remove this\n     # This must pass in filenames, because filetype is determined by entity name\n     # Not by actual path of file\n     validator = validate.GenieValidationHelper(syn=syn, center=center,\n                                                filepathlist=filepaths,\n                                                format_registry=format_registry,\n                                                testing=testing)\n     filetype = validator.file_type\n     if check_file_status['to_validate']:\n         valid, message, filetype = validator.validate_single_file(\n             oncotree_link=oncotree_link, nosymbol_check=False)\n         logger.info(\"VALIDATION COMPLETE\")\n         input_status_list, invalid_errors_list = _get_status_and_error_list(\n             valid, message, entities)\n         # Send email the first time the file is invalid\n         if invalid_errors_list:\n             _send_validation_error_email(syn, filenames, message, file_users)\n     else:\n         input_status_list = [\n             [ent.id, path, ent.md5, status, filename, entity_date_to_timestamp(ent.properties.modifiedOn)]\n             for ent, path, status, filename in\n             zip(entities, filepaths, status_list, filenames)]\n         invalid_errors_list = [\n             [entity.id, error, filename]\n             for entity, error, filename in\n             zip(entities, error_list, filenames)]\n     # add in static filetype and center information\n     for input_status in input_status_list:\n         input_status.extend([filetype, center])\n     # If there are actually invalid errors\n     for invalid_errors in invalid_errors_list:\n         invalid_errors.extend([filetype, center])\n     return(input_status_list, invalid_errors_list)\n",
        "code_toks_joined": "status_list = check_file_status [ <STRING> ] <NEWLINE> <INDENT> error_list = check_file_status [ <STRING> ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> validator = validate . GenieValidationHelper ( syn = syn , center = center , <NEWLINE> <INDENT> filepathlist = filepaths , <NEWLINE> format_registry = format_registry , <NEWLINE> testing = testing ) <NEWLINE> <DEDENT> filetype = validator . file_type <NEWLINE> if check_file_status [ <STRING> ] : <NEWLINE> <INDENT> valid , message , filetype = validator . validate_single_file ( <NEWLINE> <INDENT> oncotree_link = oncotree_link , nosymbol_check = False ) <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> input_status_list , invalid_errors_list = _get_status_and_error_list ( <NEWLINE> <INDENT> valid , message , entities ) <NEWLINE> <COMMENT> <NL> <DEDENT> if invalid_errors_list : <NEWLINE> <INDENT> _send_validation_error_email ( syn , filenames , message , file_users ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> input_status_list = [ <NEWLINE> <INDENT> [ ent . id , path , ent . md5 , status , filename , entity_date_to_timestamp ( ent . properties . modifiedOn ) ] <NEWLINE> for ent , path , status , filename in <NEWLINE> zip ( entities , filepaths , status_list , filenames ) ] <NEWLINE> <DEDENT> invalid_errors_list = [ <NEWLINE> <INDENT> [ entity . id , error , filename ] <NEWLINE> for entity , error , filename in <NEWLINE> zip ( entities , error_list , filenames ) ] <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for input_status in input_status_list : <NEWLINE> <INDENT> input_status . extend ( [ filetype , center ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> for invalid_errors in invalid_errors_list : <NEWLINE> <INDENT> invalid_errors . extend ( [ filetype , center ] ) <NEWLINE> <DEDENT> return ( input_status_list , invalid_errors_list ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'status_list'",
                "'error_list'",
                "'to_validate'",
                "\"VALIDATION COMPLETE\""
            ],
            "<COMMENT>": [
                "# Need to figure out to how to remove this",
                "# This must pass in filenames, because filetype is determined by entity name",
                "# Not by actual path of file",
                "# Send email the first time the file is invalid",
                "# add in static filetype and center information",
                "# If there are actually invalid errors"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e5c55cac226043d3a794dab223519657": {
        "code_string": "if sum(q_inds):\n             inds_to_send = q_inds[ np.where(H['priority'][q_inds] == max(H['priority'][q_inds]))[0] ]\n             Work[i] = {'calc_f': sim_specs['f'][0], \n                        'calc_params': sim_specs['params'], \n                        'form_subcomm': [], \n                        'calc_in': H[sim_specs['in']][inds_to_send],\n                        'calc_out': sim_specs['out'],\n                        'calc_info': {'type':'sim', 'pt_ids': q_inds},\n                       }\n",
        "code_toks_joined": "if sum ( q_inds ) : <NEWLINE> <INDENT> inds_to_send = q_inds [ np . where ( H [ <STRING> ] [ q_inds ] == max ( H [ <STRING> ] [ q_inds ] ) ) [ 0 ] ] <NEWLINE> Work [ i ] = { <STRING> : sim_specs [ <STRING> ] [ 0 ] , <NEWLINE> <INDENT> <STRING> : sim_specs [ <STRING> ] , <NEWLINE> <STRING> : [ ] , <NEWLINE> <STRING> : H [ sim_specs [ <STRING> ] ] [ inds_to_send ] , <NEWLINE> <STRING> : sim_specs [ <STRING> ] , <NEWLINE> <STRING> : { <STRING> : <STRING> , <STRING> : q_inds } , <NEWLINE> } <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'priority'",
                "'priority'",
                "'calc_f'",
                "'f'",
                "'calc_params'",
                "'params'",
                "'form_subcomm'",
                "'calc_in'",
                "'in'",
                "'calc_out'",
                "'out'",
                "'calc_info'",
                "'type'",
                "'sim'",
                "'pt_ids'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d344e640e93f4e50af755c37edaece4b": {
        "code_string": "comm.send(obj=data_out, dest=0, tag=calc_tag) \n",
        "code_toks_joined": "comm . send ( obj = data_out , dest = 0 , tag = calc_tag ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c80eb470a60d4b72a828c1e3a1ee23ab": {
        "code_string": "def kill_pending(self):\n         \"Make sure pending requests are cancelled if the comm is killed.\"\n         for req in self._outbox:\n             if req.Test():\n                 req.Cancel()\n         self._outbox = []\n",
        "code_toks_joined": "def kill_pending ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for req in self . _outbox : <NEWLINE> <INDENT> if req . Test ( ) : <NEWLINE> <INDENT> req . Cancel ( ) <NEWLINE> <DEDENT> <DEDENT> self . _outbox = [ ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Make sure pending requests are cancelled if the comm is killed.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e98085398f2a4e89abde924a255d6944": {
        "code_string": "@staticmethod\n     def get_slurm_nodelist(node_list_env):\n         \"\"\"Get global libEnsemble nodelist from the Slurm environment\"\"\"\n         fullstr = os.environ[node_list_env]\n         if not fullstr:\n             return []\n         part_splitstr = fullstr.split('],')\n         if len(part_splitstr) == 1:     # Single Partition\n             splitstr = fullstr.split('[', 1)\n             if len(splitstr) == 1:      # Single Node\n                 return splitstr\n             prefix = splitstr[0]\n             nidstr = splitstr[1].strip(\"]\")\n             nidlst = EnvResources._noderange_append(prefix, nidstr)\n         else:                           # Multiple Partitions\n             splitgroups = [str.split('[', 1) for str in splitstr]\n             prefixgroups = [group[0] for group in splitgroups]\n             nodegroups = [group[1].strip(']') for group in splitgroups]\n             nidlst = []\n             for i in range(len(prefixgroups)):\n                 prefix = prefixgroups[i]\n                 nidstr = nodegroups[i]\n                 nidlst.extend(EnvResources._noderange_append(prefix, nidstr))\n",
        "code_toks_joined": "@ staticmethod <NEWLINE> <INDENT> def get_slurm_nodelist ( node_list_env ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> fullstr = os . environ [ node_list_env ] <NEWLINE> if not fullstr : <NEWLINE> <INDENT> return [ ] <NEWLINE> <DEDENT> part_splitstr = fullstr . split ( <STRING> ) <NEWLINE> if len ( part_splitstr ) == 1 : <COMMENT> <NEWLINE> <INDENT> splitstr = fullstr . split ( <STRING> , 1 ) <NEWLINE> if len ( splitstr ) == 1 : <COMMENT> <NEWLINE> <INDENT> return splitstr <NEWLINE> <DEDENT> prefix = splitstr [ 0 ] <NEWLINE> nidstr = splitstr [ 1 ] . strip ( <STRING> ) <NEWLINE> nidlst = EnvResources . _noderange_append ( prefix , nidstr ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> splitgroups = [ str . split ( <STRING> , 1 ) for str in splitstr ] <NEWLINE> prefixgroups = [ group [ 0 ] for group in splitgroups ] <NEWLINE> nodegroups = [ group [ 1 ] . strip ( <STRING> ) for group in splitgroups ] <NEWLINE> nidlst = [ ] <NEWLINE> for i in range ( len ( prefixgroups ) ) : <NEWLINE> <INDENT> prefix = prefixgroups [ i ] <NEWLINE> nidstr = nodegroups [ i ] <NEWLINE> nidlst . extend ( EnvResources . _noderange_append ( prefix , nidstr ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Get global libEnsemble nodelist from the Slurm environment\"\"\"",
                "'],'",
                "'['",
                "\"]\"",
                "'['",
                "']'"
            ],
            "<COMMENT>": [
                "# Single Partition",
                "# Single Node",
                "# Multiple Partitions"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2abf0ffc025043068948a3aa7197d2e2": {
        "code_string": "last_size = persis_info.get('last_size')\n             if len(H):\n                 # Don't give gen instances in batch mode if points are unfinished\n                 if (gen_specs['user'].get('batch_mode')\n                     and not all(np.logical_or(H['returned'][last_size:],\n                                               H['paused'][last_size:]))):\n                     break\n                 # Don't call APOSMM if there are runs going but none need advancing\n                 if len(persis_info[lw]['run_order']):\n                     runs_needing_to_advance = np.zeros(len(persis_info[lw]['run_order']), dtype=bool)\n                     for run, inds in enumerate(persis_info[lw]['run_order'].values()):\n                         runs_needing_to_advance[run] = np.all(H['returned'][inds])\n",
        "code_toks_joined": "last_size = persis_info . get ( <STRING> ) <NEWLINE> <INDENT> if len ( H ) : <NEWLINE> <COMMENT> <NL> <INDENT> if ( gen_specs [ <STRING> ] . get ( <STRING> ) <NEWLINE> <INDENT> and not all ( np . logical_or ( H [ <STRING> ] [ last_size : ] , <NEWLINE> <INDENT> H [ <STRING> ] [ last_size : ] ) ) ) : <NEWLINE> <DEDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> if len ( persis_info [ lw ] [ <STRING> ] ) : <NEWLINE> <INDENT> runs_needing_to_advance = np . zeros ( len ( persis_info [ lw ] [ <STRING> ] ) , dtype = bool ) <NEWLINE> for run , inds in enumerate ( persis_info [ lw ] [ <STRING> ] . values ( ) ) : <NEWLINE> <INDENT> runs_needing_to_advance [ run ] = np . all ( H [ <STRING> ] [ inds ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'last_size'",
                "'user'",
                "'batch_mode'",
                "'returned'",
                "'paused'",
                "'run_order'",
                "'run_order'",
                "'run_order'",
                "'returned'"
            ],
            "<COMMENT>": [
                "# Don't give gen instances in batch mode if points are unfinished",
                "# Don't call APOSMM if there are runs going but none need advancing"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "378e093ebccd4613bb355a1274be9149": {
        "code_string": "last_size = persis_info.get('last_size')\n             if len(H):\n                 # Don't give gen instances in batch mode if points are unfinished\n                 if (gen_specs['user'].get('batch_mode')\n                     and not all(np.logical_or(H['returned'][last_size:],\n                                               H['paused'][last_size:]))):\n                     break\n                 # Don't call APOSMM if there are runs going but none need advancing\n                 if len(persis_info[lw]['run_order']):\n                     runs_needing_to_advance = np.zeros(len(persis_info[lw]['run_order']), dtype=bool)\n                     for run, inds in enumerate(persis_info[lw]['run_order'].values()):\n                         runs_needing_to_advance[run] = H['returned'][inds[-1]]\n",
        "code_toks_joined": "last_size = persis_info . get ( <STRING> ) <NEWLINE> <INDENT> if len ( H ) : <NEWLINE> <COMMENT> <NL> <INDENT> if ( gen_specs [ <STRING> ] . get ( <STRING> ) <NEWLINE> <INDENT> and not all ( np . logical_or ( H [ <STRING> ] [ last_size : ] , <NEWLINE> <INDENT> H [ <STRING> ] [ last_size : ] ) ) ) : <NEWLINE> <DEDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> if len ( persis_info [ lw ] [ <STRING> ] ) : <NEWLINE> <INDENT> runs_needing_to_advance = np . zeros ( len ( persis_info [ lw ] [ <STRING> ] ) , dtype = bool ) <NEWLINE> for run , inds in enumerate ( persis_info [ lw ] [ <STRING> ] . values ( ) ) : <NEWLINE> <INDENT> runs_needing_to_advance [ run ] = H [ <STRING> ] [ inds [ - 1 ] ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'last_size'",
                "'user'",
                "'batch_mode'",
                "'returned'",
                "'paused'",
                "'run_order'",
                "'run_order'",
                "'run_order'",
                "'returned'"
            ],
            "<COMMENT>": [
                "# Don't give gen instances in batch mode if points are unfinished",
                "# Don't call APOSMM if there are runs going but none need advancing"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c340593bea404e4a95744b609d2141b7": {
        "code_string": "# No gen instances in batch mode if workers still working\n             still_working = ~H['returned']\n             if gen_specs['user'].get('batch_mode') and np.any(still_working):\n                 break\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> still_working = ~ H [ <STRING> ] <NEWLINE> if gen_specs [ <STRING> ] . get ( <STRING> ) and np . any ( still_working ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# No gen instances in batch mode if workers still working"
            ],
            "<STRING>": [
                "'returned'",
                "'user'",
                "'batch_mode'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eff6462e783644bc9ae08147fda09cc1": {
        "code_string": "# Send back x to the manager, then receive info or stop tag\n         H_o = add_to_Out(np.zeros(1, dtype=gen_specs['out']), x, 0,\n                          gen_specs['user']['ub'], gen_specs['user']['lb'], local=True, active=True)\n         tag, Work, calc_in = sendrecv_mgr_worker_msg(comm, H_o)\n         if tag in [STOP_TAG, PERSIS_STOP]:\n             nlopt.forced_stop.message = 'tag=' + str(tag)\n             raise nlopt.forced_stop\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> H_o = add_to_Out ( np . zeros ( 1 , dtype = gen_specs [ <STRING> ] ) , x , 0 , <NEWLINE> <INDENT> gen_specs [ <STRING> ] [ <STRING> ] , gen_specs [ <STRING> ] [ <STRING> ] , local = True , active = True ) <NEWLINE> <DEDENT> tag , Work , calc_in = sendrecv_mgr_worker_msg ( comm , H_o ) <NEWLINE> if tag in [ STOP_TAG , PERSIS_STOP ] : <NEWLINE> <INDENT> nlopt . forced_stop . message = <STRING> + str ( tag ) <NEWLINE> raise nlopt . forced_stop <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Send back x to the manager, then receive info or stop tag"
            ],
            "<STRING>": [
                "'out'",
                "'user'",
                "'ub'",
                "'user'",
                "'lb'",
                "'tag='"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9410298434fb49cc96797a6ebf18c28a": {
        "code_string": "return H_o, persis_info, tag\n",
        "code_toks_joined": "return H_o , persis_info , tag <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6a75276139f64cfbbdcdbdf5aac302d2": {
        "code_string": "def prepare_to_run_command(self, cmd):\n         self.LOG.debug('prepare_to_run_command \"{0}\"'.format(cmd.cmd_name))\n         self.timer.start()\n         os.umask(self.options.umask)\n         self.LOG.debug('using environment \"{}\"'.format(\n             self.options.environment))\n         self.environment = self.options.environment\n         self.secrets_basedir = self.options.secrets_basedir\n         # Don't output error messages when \"complete\" command used\n         if cmd.cmd_name == 'complete':\n             SecretsEnvironment.permissions_check(\n                 self.secrets_basedir,\n                 verbose_level=self.options.verbose_level,\n                 )\n             self.secrets_file = self.options.secrets_file\n             self.secrets = SecretsEnvironment(\n                 environment=self.environment,\n                 secrets_basedir=self.secrets_basedir,\n                 secrets_file=self.secrets_file,\n                 export_env_vars=self.options.export_env_vars,\n                 verbose_level=self.options.verbose_level,\n                 env_var_prefix=self.options.env_var_prefix,\n                 )\n",
        "code_toks_joined": "def prepare_to_run_command ( self , cmd ) : <NEWLINE> <INDENT> self . LOG . debug ( <STRING> . format ( cmd . cmd_name ) ) <NEWLINE> self . timer . start ( ) <NEWLINE> os . umask ( self . options . umask ) <NEWLINE> self . LOG . debug ( <STRING> . format ( <NEWLINE> <INDENT> self . options . environment ) ) <NEWLINE> <DEDENT> self . environment = self . options . environment <NEWLINE> self . secrets_basedir = self . options . secrets_basedir <NEWLINE> <COMMENT> <NL> if cmd . cmd_name == <STRING> : <NEWLINE> <INDENT> SecretsEnvironment . permissions_check ( <NEWLINE> <INDENT> self . secrets_basedir , <NEWLINE> verbose_level = self . options . verbose_level , <NEWLINE> ) <NEWLINE> <DEDENT> self . secrets_file = self . options . secrets_file <NEWLINE> self . secrets = SecretsEnvironment ( <NEWLINE> <INDENT> environment = self . environment , <NEWLINE> secrets_basedir = self . secrets_basedir , <NEWLINE> secrets_file = self . secrets_file , <NEWLINE> export_env_vars = self . options . export_env_vars , <NEWLINE> verbose_level = self . options . verbose_level , <NEWLINE> env_var_prefix = self . options . env_var_prefix , <NEWLINE> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'prepare_to_run_command \"{0}\"'",
                "'using environment \"{}\"'",
                "'complete'"
            ],
            "<COMMENT>": [
                "# Don't output error messages when \"complete\" command used"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "143c6723b2964494925dfa8110504c9a": {
        "code_string": "for m in self.history.alive_models(t - 1):\n             particles, w = self.history.get_distribution(t - 1, m)\n             self.transitions[m].fit(particles, w)\n",
        "code_toks_joined": "for m in self . history . alive_models ( t - 1 ) : <NEWLINE> <INDENT> particles , w = self . history . get_distribution ( t - 1 , m ) <NEWLINE> self . transitions [ m ] . fit ( particles , w ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4e5a842dfff3480f8e7b78f90a5a96f8": {
        "code_string": "@app.route(\"/abc/<int:abc_id>/model/<int:model_id>/t/<t>\")\n def abc_model(abc_id, model_id, t):\n     history = app.config[\"HISTORY\"]\n     history.id = abc_id\n     if t == \"max\":\n         t = history.max_t\n     else:\n         t = int(t)\n     df, w = history.get_distribution(t, model_id)\n     df[\"CDF\"] = w\n     tabs = []\n",
        "code_toks_joined": "@ app . route ( <STRING> ) <NEWLINE> <INDENT> def abc_model ( abc_id , model_id , t ) : <NEWLINE> <INDENT> history = app . config [ <STRING> ] <NEWLINE> history . id = abc_id <NEWLINE> if t == <STRING> : <NEWLINE> <INDENT> t = history . max_t <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> t = int ( t ) <NEWLINE> <DEDENT> df , w = history . get_distribution ( t , model_id ) <NEWLINE> df [ <STRING> ] = w <NEWLINE> tabs = [ ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"/abc/<int:abc_id>/model/<int:model_id>/t/<t>\"",
                "\"HISTORY\"",
                "\"max\"",
                "\"CDF\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "20c228a91c624a1d9eeb6ae5b2a95405": {
        "code_string": "for h in histories:\n         for t in range(4):\n             for m in range(5):\n                 pop = pops[(h, m, t)]\n                 expected_particles_list = [p.parameter for p in pop]\n                 pars_df, w = h.get_distribution(t, m)\n                 # use range(len and not zip on dataframe to not stop early\n                 # in case of population not completely stored\n                 assert np.isclose(w.sum(), 1)\n                 for part_nr in range(len(expected_particles_list)):\n                     expected_par = expected_particles_list[part_nr]\n                     actual_par = pars_df.iloc[part_nr]\n                     assert expected_par.a == actual_par.a\n                     assert expected_par.b == actual_par.b\n",
        "code_toks_joined": "for h in histories : <NEWLINE> <INDENT> for t in range ( 4 ) : <NEWLINE> <INDENT> for m in range ( 5 ) : <NEWLINE> <INDENT> pop = pops [ ( h , m , t ) ] <NEWLINE> expected_particles_list = [ p . parameter for p in pop ] <NEWLINE> pars_df , w = h . get_distribution ( t , m ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> assert np . isclose ( w . sum ( ) , 1 ) <NEWLINE> for part_nr in range ( len ( expected_particles_list ) ) : <NEWLINE> <INDENT> expected_par = expected_particles_list [ part_nr ] <NEWLINE> actual_par = pars_df . iloc [ part_nr ] <NEWLINE> assert expected_par . a == actual_par . a <NEWLINE> assert expected_par . b == actual_par . b <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# use range(len and not zip on dataframe to not stop early",
                "# in case of population not completely stored"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3e05674219a441f8a08311b96a9416af": {
        "code_string": "yield cls(arg)\n",
        "code_toks_joined": "yield cls ( arg ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "495517d1ccb54881bb5160cce91fe369": {
        "code_string": "if opcode in (opcodes.OP_CHECKSIG, opcodes.OP_CHECKSIGVERIFY):\n                 # Subset of script starting at the most recent codeseparator\n                 op_checksig(stack, signature_for_hash_type_f, expected_hash_type, script[begin_code_hash:], flags)\n                 if opcode == opcodes.OP_CHECKSIGVERIFY:\n                     if bool_from_script_bytes(stack.pop()):\n                         raise ScriptError(\"VERIFY failed at %d\" % (pc-1))\n                 continue\n",
        "code_toks_joined": "if opcode in ( opcodes . OP_CHECKSIG , opcodes . OP_CHECKSIGVERIFY ) : <NEWLINE> <COMMENT> <NL> <INDENT> op_checksig ( stack , signature_for_hash_type_f , expected_hash_type , script [ begin_code_hash : ] , flags ) <NEWLINE> if opcode == opcodes . OP_CHECKSIGVERIFY : <NEWLINE> <INDENT> if bool_from_script_bytes ( stack . pop ( ) ) : <NEWLINE> <INDENT> raise ScriptError ( <STRING> % ( pc - 1 ) ) <NEWLINE> <DEDENT> <DEDENT> continue <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Subset of script starting at the most recent codeseparator"
            ],
            "<STRING>": [
                "\"VERIFY failed at %d\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "136e3c7f7c954f689ac868f0f375db3e": {
        "code_string": "if len(output_order) == 0:\n         print(\"no output: use -j option to see keys\")\n     elif len(output_order) == 1:\n         print(output_dict[output_order[0][0]])\n     else:\n         dump_output(output_dict, output_order)\n",
        "code_toks_joined": "if len ( output_order ) == 0 : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> elif len ( output_order ) == 1 : <NEWLINE> print ( output_dict [ output_order [ 0 ] [ 0 ] ] ) <NEWLINE> else : <NEWLINE> dump_output ( output_dict , output_order ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"no output: use -j option to see keys\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a0af5cdd3e0b47d6857e8d9c8a27f6ce": {
        "code_string": "def deterministic_generate_k(generator_order, secret_exponent, val, hash_f=hashlib.sha256):\n     \"\"\"\n     :param generator_order: result from `pycoin.ecdsa.Generator.Generator.order`,\n         necessary to ensure the k value is within bound\n     :param secret_exponent: an integer secret_exponent to generate the k value for\n     :param val: the value to be signed, also used as an entropy source for the k value\n     :returns: an integer k such that ``1 <= k < generator_order``, complying with\n         <https://tools.ietf.org/html/rfc6979>\n     \"\"\"\n     n = generator_order\n     bln = bit_length(n)\n     order_size = (bln + 7) // 8\n     hash_size = hash_f().digest_size\n     v = b'\\x01' * hash_size\n     k = b'\\x00' * hash_size\n     priv = intstream.to_bytes(secret_exponent, length=order_size)\n     shift = 8 * hash_size - bln\n     if shift > 0:\n         val >>= shift\n     if val > n:\n         val -= n\n     h1 = intstream.to_bytes(val, length=order_size)\n     k = hmac.new(k, v + b'\\x00' + priv + h1, hash_f).digest()\n     v = hmac.new(k, v, hash_f).digest()\n     k = hmac.new(k, v + b'\\x01' + priv + h1, hash_f).digest()\n     v = hmac.new(k, v, hash_f).digest()\n",
        "code_toks_joined": "def deterministic_generate_k ( generator_order , secret_exponent , val , hash_f = hashlib . sha256 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> n = generator_order <NEWLINE> bln = bit_length ( n ) <NEWLINE> order_size = ( bln + 7 ) // 8 <NEWLINE> hash_size = hash_f ( ) . digest_size <NEWLINE> v = <STRING> * hash_size <NEWLINE> k = <STRING> * hash_size <NEWLINE> priv = intstream . to_bytes ( secret_exponent , length = order_size ) <NEWLINE> shift = 8 * hash_size - bln <NEWLINE> if shift > 0 : <NEWLINE> <INDENT> val >>= shift <NEWLINE> <DEDENT> if val > n : <NEWLINE> <INDENT> val -= n <NEWLINE> <DEDENT> h1 = intstream . to_bytes ( val , length = order_size ) <NEWLINE> k = hmac . new ( k , v + <STRING> + priv + h1 , hash_f ) . digest ( ) <NEWLINE> v = hmac . new ( k , v , hash_f ) . digest ( ) <NEWLINE> k = hmac . new ( k , v + <STRING> + priv + h1 , hash_f ) . digest ( ) <NEWLINE> v = hmac . new ( k , v , hash_f ) . digest ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     :param generator_order: result from `pycoin.ecdsa.Generator.Generator.order`,\n         necessary to ensure the k value is within bound\n     :param secret_exponent: an integer secret_exponent to generate the k value for\n     :param val: the value to be signed, also used as an entropy source for the k value\n     :returns: an integer k such that ``1 <= k < generator_order``, complying with\n         <https://tools.ietf.org/html/rfc6979>\n     \"\"\"",
                "b'\\x01'",
                "b'\\x00'",
                "b'\\x00'",
                "b'\\x01'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f112b176a5c541d59e05f3a2a4cffe36": {
        "code_string": "def test_get_host_speed_rank(self):\n         r = util.get_host_speed_rank([\n             '172.18.6.93',\n             '172.18.6.94',\n             '172.18.6.95',\n             '172.18.6.96',\n             '172.18.6.97',\n             '172.18.6.98',\n             '172.18.6.99'\n         ])\n         assert len(r) == 1\n         with pytest.raises(ValueError):\n             util.get_host_speed_rank(['qq.com'])\n         assert util.get_host_speed_rank(timeout=0) == []\n",
        "code_toks_joined": "def test_get_host_speed_rank ( self ) : <NEWLINE> <INDENT> r = util . get_host_speed_rank ( [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ] ) <NEWLINE> assert len ( r ) == 1 <NEWLINE> with pytest . raises ( ValueError ) : <NEWLINE> <INDENT> util . get_host_speed_rank ( [ <STRING> ] ) <NEWLINE> <DEDENT> assert util . get_host_speed_rank ( timeout = 0 ) == [ ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'172.18.6.93'",
                "'172.18.6.94'",
                "'172.18.6.95'",
                "'172.18.6.96'",
                "'172.18.6.97'",
                "'172.18.6.98'",
                "'172.18.6.99'",
                "'qq.com'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ae97a2fb3aa949349ce944483aa63523": {
        "code_string": "if args.id is not None:\n         consensus.id = args.id\n     elif args.idLambda is not None:\n         idLambda = eval(args.idLambda)\n         consensus.id = idLambda(args.id)\n",
        "code_toks_joined": "if args . id is not None : <NEWLINE> <INDENT> consensus . id = args . id <NEWLINE> elif args . idLambda is not None : <NEWLINE> idLambda = eval ( args . idLambda ) <NEWLINE> consensus . id = idLambda ( args . id ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f96cb27664a64db7800cdb4a47fe8c00": {
        "code_string": "account_numbers.append(arn.account_number)\n                 else:\n                     arn = ARN(princ_aws)\n                     if arn.error:\n                         self.add_issue(3, 'Auditor could not parse ARN', snsitem, notes=entry)\n                     else:\n                         account_numbers.append(arn.account_number)\n",
        "code_toks_joined": "account_numbers . append ( arn . account_number ) <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> arn = ARN ( princ_aws ) <NEWLINE> if arn . error : <NEWLINE> <INDENT> self . add_issue ( 3 , <STRING> , snsitem , notes = entry ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> account_numbers . append ( arn . account_number ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Auditor could not parse ARN'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dd7ad22d167145258a070a457cdb33a2": {
        "code_string": "if fallback is not None and return_value is None:\n             raise KeyError(\"Option '%s' is not found in section '%s'.\" % (option, section))\n",
        "code_toks_joined": "if fallback is not None and return_value is None : <NEWLINE> <INDENT> raise KeyError ( <STRING> % ( option , section ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Option '%s' is not found in section '%s'.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ceff7b477b814c68be3f5d2a996e82d6": {
        "code_string": "# verify\n     assert len(actual_all_tasks.keys()) == 1\n     assert actual_all_tasks == expected_all_tasks\n     mocked_get_required_params.assert_called_once_with(\n         region_name, launch_details.get('portfolio'), launch_details.get('product'), launch_details.get('version'), puppet_account_id\n     )\n     mocked_get_parameters_for_launch.assert_called_once_with(\n         required_parameters,\n         deployment_map,\n         manifest,\n         launch_details,\n         account_id,\n         launch_details.get('status', constants.PROVISIONED),\n     )\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> assert len ( actual_all_tasks . keys ( ) ) == 1 <NEWLINE> assert actual_all_tasks == expected_all_tasks <NEWLINE> mocked_get_required_params . assert_called_once_with ( <NEWLINE> <INDENT> region_name , launch_details . get ( <STRING> ) , launch_details . get ( <STRING> ) , launch_details . get ( <STRING> ) , puppet_account_id <NEWLINE> <DEDENT> ) <NEWLINE> mocked_get_parameters_for_launch . assert_called_once_with ( <NEWLINE> <INDENT> required_parameters , <NEWLINE> deployment_map , <NEWLINE> manifest , <NEWLINE> launch_details , <NEWLINE> account_id , <NEWLINE> launch_details . get ( <STRING> , constants . PROVISIONED ) , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# verify"
            ],
            "<STRING>": [
                "'portfolio'",
                "'product'",
                "'version'",
                "'status'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d03584d1de7647ca88cf4888882418e4": {
        "code_string": "xmin = log10(ymin)\n",
        "code_toks_joined": "xmin = log10 ( ymin ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c7263446ad2a4f80a97b8bd16dc2776b": {
        "code_string": "class DecodedSoundFile:\n     \"\"\"Contains the PCM samples and various properties of a fully decoded audio file.\"\"\"\n     def __init__(self, name: str, nchannels: int, sample_rate: int, sample_width: int,\n                  sample_format: int, samples: array.array) -> None:\n         self.name = name\n         self.nchannels = nchannels\n         self.sample_rate = sample_rate\n         self.sample_width = sample_width\n         self.sample_format = sample_format      # one of the ma_format_ values\n         self.sample_format_name = ffi.string(lib.ma_get_format_name(sample_format)).decode()\n         self.samples = samples\n         self.num_frames = len(samples) / self.nchannels\n         self.duration = self.num_frames / self.sample_rate\n",
        "code_toks_joined": "class DecodedSoundFile : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , name : str , nchannels : int , sample_rate : int , sample_width : int , <NEWLINE> <INDENT> sample_format : int , samples : array . array ) -> None : <NEWLINE> self . name = name <NEWLINE> self . nchannels = nchannels <NEWLINE> self . sample_rate = sample_rate <NEWLINE> self . sample_width = sample_width <NEWLINE> self . sample_format = sample_format <COMMENT> <NEWLINE> self . sample_format_name = ffi . string ( lib . ma_get_format_name ( sample_format ) ) . decode ( ) <NEWLINE> self . samples = samples <NEWLINE> self . num_frames = len ( samples ) / self . nchannels <NEWLINE> self . duration = self . num_frames / self . sample_rate <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Contains the PCM samples and various properties of a fully decoded audio file.\"\"\""
            ],
            "<COMMENT>": [
                "# one of the ma_format_ values"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f066e6fd1f543acbf60789a474d9b23": {
        "code_string": "def test_if_constant_bool(self):\n         a = True\n         b = array([1, 2])\n         c = array([3, 4])\n         res = evaluate('where(a, b, c)')\n         assert_equal(res, b)\n         a = False\n         res = evaluate('where(a, b, c)')\n         assert_equal(res, b)\n",
        "code_toks_joined": "def test_if_constant_bool ( self ) : <NEWLINE> <INDENT> a = True <NEWLINE> b = array ( [ 1 , 2 ] ) <NEWLINE> c = array ( [ 3 , 4 ] ) <NEWLINE> res = evaluate ( <STRING> ) <NEWLINE> assert_equal ( res , b ) <NEWLINE> a = False <NEWLINE> res = evaluate ( <STRING> ) <NEWLINE> assert_equal ( res , b ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'where(a, b, c)'",
                "'where(a, b, c)'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e36abd4a92ba496daab8a3a5d4707c2b": {
        "code_string": "async def _sender_loop(self):\n         while self._ender.done():\n             if len(self._queue) != 0:\n                 await self._send()\n",
        "code_toks_joined": "async def _sender_loop ( self ) : <NEWLINE> <INDENT> while self . _ender . done ( ) : <NEWLINE> <INDENT> if len ( self . _queue ) != 0 : <NEWLINE> <INDENT> await self . _send ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6de368c512fd42e5a36628282d53f814": {
        "code_string": "# these are the category fields\n             k_to_delete = []\n             for k in ddata.keys():\n                 if k.endswith('_categories'):\n                     k_stripped = k.replace('_categories', '')\n                     if k_stripped in smp:\n                         smp[k_stripped] = pd.Categorical.from_codes(\n                             codes=smp[k_stripped].values,\n                             categories=ddata[k])\n                     if k_stripped in var:\n                         var[k_stripped] = pd.Categorical.from_codes(\n                             codes=smp[k_stripped].values,\n                             categories=ddata[k])\n                     k_to_delete.append(k)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> k_to_delete = [ ] <NEWLINE> for k in ddata . keys ( ) : <NEWLINE> <INDENT> if k . endswith ( <STRING> ) : <NEWLINE> <INDENT> k_stripped = k . replace ( <STRING> , <STRING> ) <NEWLINE> if k_stripped in smp : <NEWLINE> <INDENT> smp [ k_stripped ] = pd . Categorical . from_codes ( <NEWLINE> <INDENT> codes = smp [ k_stripped ] . values , <NEWLINE> categories = ddata [ k ] ) <NEWLINE> <DEDENT> <DEDENT> if k_stripped in var : <NEWLINE> <INDENT> var [ k_stripped ] = pd . Categorical . from_codes ( <NEWLINE> <INDENT> codes = smp [ k_stripped ] . values , <NEWLINE> categories = ddata [ k ] ) <NEWLINE> <DEDENT> <DEDENT> k_to_delete . append ( k ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# these are the category fields"
            ],
            "<STRING>": [
                "'_categories'",
                "'_categories'",
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "440f362de0bd4a23b2d5ee1e23c6af76": {
        "code_string": "if write_obsm_varm:\n         for key in adata.obsm.keys():\n             col_attrs[key] = adata.obsm[key]\n         for key in adata.varm.keys():\n             row_attrs[key] = adata.varm[key]\n     elif len(adata.obsm.keys()) > 0 or len(adata.varm.keys()) > 0:\n         logger.warning(\n             f'The loom file will lack these fields:\\n'\n             f'{adata.obsm.keys() + adata.varm.keys()}\\n'\n             f'Use write_obsm_varm=True to export multi-dimensional annotations'\n         )\n",
        "code_toks_joined": "if write_obsm_varm : <NEWLINE> <INDENT> for key in adata . obsm . keys ( ) : <NEWLINE> <INDENT> col_attrs [ key ] = adata . obsm [ key ] <NEWLINE> <DEDENT> for key in adata . varm . keys ( ) : <NEWLINE> <INDENT> row_attrs [ key ] = adata . varm [ key ] <NEWLINE> elif len ( adata . obsm . keys ( ) ) > 0 or len ( adata . varm . keys ( ) ) > 0 : <NEWLINE> <DEDENT> logger . warning ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f'The loom file will lack these fields:\\n'",
                "f'{adata.obsm.keys() + adata.varm.keys()}\\n'",
                "f'Use write_obsm_varm=True to export multi-dimensional annotations'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fbc44df1c82f482192cce679e9e622c1": {
        "code_string": "markers = ut.find_corr_genes(s,txt).flatten()\n         _,i = np.unique(markers,return_index=True)\n         markers=markers[np.sort(i)]\n         self.marker_genes[self.stab.selected_index] = markers\n",
        "code_toks_joined": "markers = ut . find_corr_genes ( s , txt ) . flatten ( ) <NEWLINE> <INDENT> _ , i = np . unique ( markers , return_index = True ) <NEWLINE> markers = markers [ np . sort ( i ) ] <NEWLINE> self . marker_genes [ self . stab . selected_index ] = markers <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "202c632a904d4189ad2b6c995da1b990": {
        "code_string": "V=[]; M=[]\n     for s in S:\n         if not case_sensitive:\n             s = s.lower()\n         for i in range(len(vec)):\n             if case_sensitive:\n                 st = vec[i]\n             else:\n                 st = vec[i].lower()\n             b = st.find(s)\n             if not invert and b != -1 or invert and b == -1:\n                 m.append(i)\n         if len(m) > 0:\n             V.append(vec[np.array(m)]); M.append(np.array(m))\n     if len(V)>0:\n         i = len(V)\n         V = np.concatenate(V); M = np.concatenate(M);\n         if i > 1:\n             ix = np.sort(np.unique(V,return_index=True)[1])\n             V=V[ix]; M=M[ix];\n         return V,M\n     else:\n         return -1,-1\n",
        "code_toks_joined": "V = [ ] ; M = [ ] <NEWLINE> <INDENT> for s in S : <NEWLINE> <INDENT> if not case_sensitive : <NEWLINE> <INDENT> s = s . lower ( ) <NEWLINE> <DEDENT> for i in range ( len ( vec ) ) : <NEWLINE> <INDENT> if case_sensitive : <NEWLINE> <INDENT> st = vec [ i ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> st = vec [ i ] . lower ( ) <NEWLINE> <DEDENT> b = st . find ( s ) <NEWLINE> if not invert and b != - 1 or invert and b == - 1 : <NEWLINE> <INDENT> m . append ( i ) <NEWLINE> <DEDENT> <DEDENT> if len ( m ) > 0 : <NEWLINE> <INDENT> V . append ( vec [ np . array ( m ) ] ) ; M . append ( np . array ( m ) ) <NEWLINE> <DEDENT> <DEDENT> if len ( V ) > 0 : <NEWLINE> <INDENT> i = len ( V ) <NEWLINE> V = np . concatenate ( V ) ; M = np . concatenate ( M ) ; <NEWLINE> if i > 1 : <NEWLINE> <INDENT> ix = np . sort ( np . unique ( V , return_index = True ) [ 1 ] ) <NEWLINE> V = V [ ix ] ; M = M [ ix ] ; <NEWLINE> <DEDENT> return V , M <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return - 1 , - 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2a7128f1b8cd4cd68f8606a135ac6039": {
        "code_string": "def build_help_menus(self):\n         menu_help = tk.Menu(self.menubar, tearoff=0)\n         self.menubar.add_cascade(menu=menu_help, label='Help', underline=0)\n         menu_help.add_command(label='Visit website')\n         menu_help.add_separator()\n         if getattr(sys, 'frozen', False):\n             menu_help.add_command(label='Run Update', command=lambda: self.conf.upgrade_package(logger=self.logger))\n             menu_help.add_separator()\n         menu_help.add_command(label='About', command=self.about_msg)\n",
        "code_toks_joined": "def build_help_menus ( self ) : <NEWLINE> <INDENT> menu_help = tk . Menu ( self . menubar , tearoff = 0 ) <NEWLINE> self . menubar . add_cascade ( menu = menu_help , label = <STRING> , underline = 0 ) <NEWLINE> menu_help . add_command ( label = <STRING> ) <NEWLINE> menu_help . add_separator ( ) <NEWLINE> if getattr ( sys , <STRING> , False ) : <NEWLINE> <INDENT> menu_help . add_command ( label = <STRING> , command = lambda : self . conf . upgrade_package ( logger = self . logger ) ) <NEWLINE> menu_help . add_separator ( ) <NEWLINE> <DEDENT> menu_help . add_command ( label = <STRING> , command = self . about_msg ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Help'",
                "'Visit website'",
                "'frozen'",
                "'Run Update'",
                "'About'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "501f3c2626404e86809dceca1de40f5a": {
        "code_string": "nb_batches = (len(tgt_list) + mb_size - 1) / mb_size\n             for num_batch in six.moves.range(nb_batches):\n                 tgt_batch, arg_sort = utils.make_batch_tgt(tgt_list[num_batch * nb_batches: (num_batch + 1) * nb_batches],\n                                                            eos_idx=eos_idx, gpu=gpu, volatile=\"on\", need_arg_sort=True)\n                 scores, attn = scorer(tgt_batch)\n                 scores, _ = scores\n                 scores = scores.data\n",
        "code_toks_joined": "nb_batches = ( len ( tgt_list ) + mb_size - 1 ) / mb_size <NEWLINE> <INDENT> for num_batch in six . moves . range ( nb_batches ) : <NEWLINE> <INDENT> tgt_batch , arg_sort = utils . make_batch_tgt ( tgt_list [ num_batch * nb_batches : ( num_batch + 1 ) * nb_batches ] , <NEWLINE> <INDENT> eos_idx = eos_idx , gpu = gpu , volatile = <STRING> , need_arg_sort = True ) <NEWLINE> <DEDENT> scores , attn = scorer ( tgt_batch ) <NEWLINE> scores , _ = scores <NEWLINE> scores = scores . data <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"on\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "42b42ea042824ad48472523e3898e28b": {
        "code_string": "nb_batches = (len(tgt_list) + mb_size - 1) / mb_size\n             for num_batch in six.moves.range(nb_batches):\n                 tgt_batch, arg_sort = utils.make_batch_tgt(tgt_list[num_batch * nb_batches: (num_batch + 1) * nb_batches],\n                                                            eos_idx=eos_idx, gpu=gpu, volatile=\"on\", need_arg_sort=True)\n                 scores, attn = scorer(tgt_batch)\n                 scores, _ = scores\n                 scores = scores.data\n",
        "code_toks_joined": "nb_batches = ( len ( tgt_list ) + mb_size - 1 ) / mb_size <NEWLINE> <INDENT> for num_batch in six . moves . range ( nb_batches ) : <NEWLINE> <INDENT> tgt_batch , arg_sort = utils . make_batch_tgt ( tgt_list [ num_batch * nb_batches : ( num_batch + 1 ) * nb_batches ] , <NEWLINE> <INDENT> eos_idx = eos_idx , gpu = gpu , volatile = <STRING> , need_arg_sort = True ) <NEWLINE> <DEDENT> scores , attn = scorer ( tgt_batch ) <NEWLINE> scores , _ = scores <NEWLINE> scores = scores . data <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"on\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "489e033891cb46d99d005fef02260d2e": {
        "code_string": "constraints_list = []\n     for sentence_src in src:\n         #         print(len(sentence_tgt), len(sentence_src))\n         seq_src = src_pp.convert(sentence_src, stats=stats_src)\n         if make_constraints is not None:\n             constraints_fn = make_constraints(src, seq_src)\n             constraints_list.append(constraints_fn)\n         res.append(seq_src)\n     if make_constraints is not None:\n         return res, stats_src, constraints_list\n     else:\n         return res, stats_src\n",
        "code_toks_joined": "constraints_list = [ ] <NEWLINE> <INDENT> for sentence_src in src : <NEWLINE> <COMMENT> <NL> <INDENT> seq_src = src_pp . convert ( sentence_src , stats = stats_src ) <NEWLINE> if make_constraints is not None : <NEWLINE> <INDENT> constraints_fn = make_constraints ( src , seq_src ) <NEWLINE> constraints_list . append ( constraints_fn ) <NEWLINE> <DEDENT> res . append ( seq_src ) <NEWLINE> <DEDENT> if make_constraints is not None : <NEWLINE> <INDENT> return res , stats_src , constraints_list <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return res , stats_src <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#         print(len(sentence_tgt), len(sentence_src))"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "61c2e3a7335c4e87bba0d793a9b97d15": {
        "code_string": "# sample ids must be shared between files\n     if sids_intersection > 0:\n         option_parser.error('The sample identifiers in the coordinates file '\n             'must have at least one match with the data contained in mapping '\n             'file. Verify you are using a coordinates file and a mapping file '\n             'that belong to the same dataset.')\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if sids_intersection > 0 : <NEWLINE> <INDENT> option_parser . error ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# sample ids must be shared between files"
            ],
            "<STRING>": [
                "'The sample identifiers in the coordinates file '",
                "'must have at least one match with the data contained in mapping '",
                "'file. Verify you are using a coordinates file and a mapping file '",
                "'that belong to the same dataset.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aff98002136e40f9ad45ed83bb27b323": {
        "code_string": "for g, rows in report_rows:\n             if g:\n                 sheet1.write(row_index, 0, u'%s' % x, stylebold)\n                 row_index += 1\n             for row in list(rows):\n                 if row.is_value():\n                     for index, x in enumerate(row):\n                         if isinstance(x.value, (list, tuple)):\n                             xvalue = ''.join(['%s\\n' % v for v in x.value])\n                         else:\n                             xvalue = x.text()\n                         sheet1.write(row_index, index, xvalue, stylevalue)\n                     row_index += 1\n                 elif row.is_caption:\n                     for index, x in enumerate(row):\n                         if not isinstance(x, (unicode, str)):\n                             sheet1.write(row_index, index, x.text(), stylebold)\n                         else:\n                             sheet1.write(row_index, index, x, stylebold)\n                     row_index += 1\n                 elif row.is_total:\n                     for index, x in enumerate(row):\n                         sheet1.write(row_index, index, x.text(), stylebold)\n                         sheet1.write(row_index + 1, index, ' ')\n                     row_index += 2\n",
        "code_toks_joined": "for g , rows in report_rows : <NEWLINE> <INDENT> if g : <NEWLINE> <INDENT> sheet1 . write ( row_index , 0 , <STRING> % x , stylebold ) <NEWLINE> row_index += 1 <NEWLINE> <DEDENT> for row in list ( rows ) : <NEWLINE> <INDENT> if row . is_value ( ) : <NEWLINE> <INDENT> for index , x in enumerate ( row ) : <NEWLINE> <INDENT> if isinstance ( x . value , ( list , tuple ) ) : <NEWLINE> <INDENT> xvalue = <STRING> . join ( [ <STRING> % v for v in x . value ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> xvalue = x . text ( ) <NEWLINE> <DEDENT> sheet1 . write ( row_index , index , xvalue , stylevalue ) <NEWLINE> <DEDENT> row_index += 1 <NEWLINE> <DEDENT> elif row . is_caption : <NEWLINE> <INDENT> for index , x in enumerate ( row ) : <NEWLINE> <INDENT> if not isinstance ( x , ( unicode , str ) ) : <NEWLINE> <INDENT> sheet1 . write ( row_index , index , x . text ( ) , stylebold ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sheet1 . write ( row_index , index , x , stylebold ) <NEWLINE> <DEDENT> <DEDENT> row_index += 1 <NEWLINE> <DEDENT> elif row . is_total : <NEWLINE> <INDENT> for index , x in enumerate ( row ) : <NEWLINE> <INDENT> sheet1 . write ( row_index , index , x . text ( ) , stylebold ) <NEWLINE> sheet1 . write ( row_index + 1 , index , <STRING> ) <NEWLINE> <DEDENT> row_index += 2 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "u'%s'",
                "''",
                "'%s\\n'",
                "' '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d034a7910228480a944a683a6f1aef3a": {
        "code_string": "time_now = time.time()\n     if time_now > timestamp_last + 300: #if 5 minutes have passed\n         difficulty2 = percentage(97,diff_block_previous)\n     else:\n         difficulty2 = difficulty\n",
        "code_toks_joined": "time_now = time . time ( ) <NEWLINE> <INDENT> if time_now > timestamp_last + 300 : <COMMENT> <NEWLINE> <INDENT> difficulty2 = percentage ( 97 , diff_block_previous ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> difficulty2 = difficulty <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#if 5 minutes have passed"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a6c651dae6b4417887c0a5e813d50c43": {
        "code_string": "if nodes_ban_reset and len(connection_pool) < len(banlist) and int(time.time() - reset_time) > 60*10: #do not reset too often. 10 minutes here\n             app_log.warning(\"Less active connections ({}) than banlist ({}), resetting banlist and tried\" .format(len(connection_pool), len(banlist)))\n             del banlist[:]\n             banlist.extend(config.banlist) # reset to config version\n             del warning_list[:]\n             del tried[:]\n             reset_time = time.time()\n",
        "code_toks_joined": "if nodes_ban_reset and len ( connection_pool ) < len ( banlist ) and int ( time . time ( ) - reset_time ) > 60 * 10 : <COMMENT> <NEWLINE> <INDENT> app_log . warning ( <STRING> . format ( len ( connection_pool ) , len ( banlist ) ) ) <NEWLINE> del banlist [ : ] <NEWLINE> banlist . extend ( config . banlist ) <COMMENT> <NEWLINE> del warning_list [ : ] <NEWLINE> del tried [ : ] <NEWLINE> reset_time = time . time ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#do not reset too often. 10 minutes here",
                "# reset to config version"
            ],
            "<STRING>": [
                "\"Less active connections ({}) than banlist ({}), resetting banlist and tried\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "deebb39329ae4270a7bed6f774f632ce": {
        "code_string": "html.append(\"<tr><th>Statistics for the last 500 blocks</th>\")\n         html.append(\"<tr><td>Kilobytes transferred: </td><td>{}</td>\".format(transferred_total))\n         html.append(\"<tr><td>Transactions: </td><td>{}</td>\".format(tx_count))\n         html.append(\"<tr><td>Transactions per block: </td><td>{}</td>\".format(tx_count/500))\n         html.append(\"<tr><td>Total BIS transferred </td><td>{}</td>\".format(transferred_total))\n",
        "code_toks_joined": "html . append ( <STRING> ) <NEWLINE> <INDENT> html . append ( <STRING> . format ( transferred_total ) ) <NEWLINE> html . append ( <STRING> . format ( tx_count ) ) <NEWLINE> html . append ( <STRING> . format ( tx_count / 500 ) ) <NEWLINE> html . append ( <STRING> . format ( transferred_total ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"<tr><th>Statistics for the last 500 blocks</th>\"",
                "\"<tr><td>Kilobytes transferred: </td><td>{}</td>\"",
                "\"<tr><td>Transactions: </td><td>{}</td>\"",
                "\"<tr><td>Transactions per block: </td><td>{}</td>\"",
                "\"<tr><td>Total BIS transferred </td><td>{}</td>\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d666d8acd44841d286697a8c6da5d2f5": {
        "code_string": "if block_height > 427000: #remove code ABOVE after hf\n         execute(c, \"SELECT * FROM transactions WHERE reward != 0 ORDER BY block_height DESC LIMIT 2\")\n         result = c.fetchone()\n         timestamp_last = Decimal(result[1])\n         block_height = int(result[0])\n         timestamp_before_last = Decimal(c.fetchone()[1])\n",
        "code_toks_joined": "if block_height > 427000 : <COMMENT> <NEWLINE> <INDENT> execute ( c , <STRING> ) <NEWLINE> result = c . fetchone ( ) <NEWLINE> timestamp_last = Decimal ( result [ 1 ] ) <NEWLINE> block_height = int ( result [ 0 ] ) <NEWLINE> timestamp_before_last = Decimal ( c . fetchone ( ) [ 1 ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#remove code ABOVE after hf"
            ],
            "<STRING>": [
                "\"SELECT * FROM transactions WHERE reward != 0 ORDER BY block_height DESC LIMIT 2\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "62607e0e6a7d41a796840419e4459dfe": {
        "code_string": "address = blake2b(privkey.to_string(), digest_size=20).hexdigest()\n",
        "code_toks_joined": "address = blake2b ( privkey . to_string ( ) , digest_size = 20 ) . hexdigest ( ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "712b8958b3a2420199add114c4e32417": {
        "code_string": "address = blake2b(pubkey.to_string(), digest_size=20).hexdigest()\n",
        "code_toks_joined": "address = blake2b ( pubkey . to_string ( ) , digest_size = 20 ) . hexdigest ( ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ccad0d1aa55444478e46d9788f057c03": {
        "code_string": "# if (q_time_now < q_received_timestamp + 432000) and not quicksync:\n                     # balance_pre = quantize_eight(credit_ledger - debit_ledger - fees + rewards)  # without projection\n                     balance_pre = ledger_balance3(db_address, h2, balances)\n                     # balance = quantize_eight(credit - debit - fees + rewards)\n                     balance = quantize_eight(balance_pre - block_debit_address)\n                     # app_log.info(\"Digest: Projected transaction address balance: \" + str(balance))\n                     # else:\n                     #    print(\"hyp2\")\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> balance_pre = ledger_balance3 ( db_address , h2 , balances ) <NEWLINE> <COMMENT> <NL> balance = quantize_eight ( balance_pre - block_debit_address ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# if (q_time_now < q_received_timestamp + 432000) and not quicksync:",
                "# balance_pre = quantize_eight(credit_ledger - debit_ledger - fees + rewards)  # without projection",
                "# balance = quantize_eight(credit - debit - fees + rewards)",
                "# app_log.info(\"Digest: Projected transaction address balance: \" + str(balance))",
                "# else:",
                "#    print(\"hyp2\")"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "93f186e2cdf64d35a15c5e4403a93928": {
        "code_string": "if self.__output_filter == \"\":\n             self.__output += self.__p.communicate()[0].decode('utf-8')  # stdoutdata\n         else:\n             if str(self.__p.communicate()[0].decode('utf-8')).find(self.__output_filter) != -1:\n                 self.__output += self.__p.communicate()[0].decode('utf-8')\n",
        "code_toks_joined": "if self . __output_filter == <STRING> : <NEWLINE> <INDENT> self . __output += self . __p . communicate ( ) [ 0 ] . decode ( <STRING> ) <COMMENT> <NEWLINE> else : <NEWLINE> if str ( self . __p . communicate ( ) [ 0 ] . decode ( <STRING> ) ) . find ( self . __output_filter ) != - 1 : <NEWLINE> <INDENT> self . __output += self . __p . communicate ( ) [ 0 ] . decode ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"",
                "'utf-8'",
                "'utf-8'",
                "'utf-8'"
            ],
            "<COMMENT>": [
                "# stdoutdata"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bf6b467df6174e8199b55a7017018953": {
        "code_string": "@figsize.setter\n     def figsize(self, size: Tuple[float, float]):\n         if (\n             not isinstance(size, Sequence)\n             or len(size) != 2\n             or not all(isinstance(x, Real) and x >= 0 for x in size)\n         ):\n             raise ValueError(\n                 f\"`size` must be a length-2 sequence of \"\n                 f\"positive-valued numbers, got: {size}\"\n             )\n         size = tuple(size)\n         if self.figsize != size:\n             self._pltkwargs[\"figsize\"] = size\n             if self._fig is not None:\n                 self._fig.set_size_inches(size)\n",
        "code_toks_joined": "@ figsize . setter <NEWLINE> <INDENT> def figsize ( self , size : Tuple [ float , float ] ) : <NEWLINE> <INDENT> if ( <NEWLINE> <INDENT> not isinstance ( size , Sequence ) <NEWLINE> or len ( size ) != 2 <NEWLINE> or not all ( isinstance ( x , Real ) and x >= 0 for x in size ) <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> size = tuple ( size ) <NEWLINE> if self . figsize != size : <NEWLINE> <INDENT> self . _pltkwargs [ <STRING> ] = size <NEWLINE> if self . _fig is not None : <NEWLINE> <INDENT> self . _fig . set_size_inches ( size ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"`size` must be a length-2 sequence of \"",
                "f\"positive-valued numbers, got: {size}\"",
                "\"figsize\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4c3617c6f33547faa0c90179dabd9d61": {
        "code_string": "def populateSubstitutionGroups(self, elements):\n         \"\"\"\n         Creates a mapping between substitution group elements and their type elements\n         \"\"\"\n         length = len(elements)\n         for i, element in enumerate(elements):\n             if 'substitutionGroup' in element.getAttrs():\n                 substitutionGroup = element.getAttrs()['substitutionGroup']\n                 base = element.getBase()\n                 self.opts.lang.abstract_type_map[substitutionGroup] = base\n                 self.opts.lang.substitutionGroup_map[base] = substitutionGroup\n         for i, element in enumerate(elements):\n             if self.opts.lang.hasSubstitutionGroup(element.getName()):\n                 substitutionGroupName = self.opts.lang.substitutionGroup(element.getName())\n                 self.substitutionElement_map[substitutionGroupName] = element\n                 continue\n         if len(self.opts.lang.getSubstitutionTypes()) >= 0:\n             config.METADATA_OBJECT_IGNORE.remove('BinData')\n",
        "code_toks_joined": "def populateSubstitutionGroups ( self , elements ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> length = len ( elements ) <NEWLINE> for i , element in enumerate ( elements ) : <NEWLINE> <INDENT> if <STRING> in element . getAttrs ( ) : <NEWLINE> <INDENT> substitutionGroup = element . getAttrs ( ) [ <STRING> ] <NEWLINE> base = element . getBase ( ) <NEWLINE> self . opts . lang . abstract_type_map [ substitutionGroup ] = base <NEWLINE> self . opts . lang . substitutionGroup_map [ base ] = substitutionGroup <NEWLINE> <DEDENT> <DEDENT> for i , element in enumerate ( elements ) : <NEWLINE> <INDENT> if self . opts . lang . hasSubstitutionGroup ( element . getName ( ) ) : <NEWLINE> <INDENT> substitutionGroupName = self . opts . lang . substitutionGroup ( element . getName ( ) ) <NEWLINE> self . substitutionElement_map [ substitutionGroupName ] = element <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> if len ( self . opts . lang . getSubstitutionTypes ( ) ) >= 0 : <NEWLINE> <INDENT> config . METADATA_OBJECT_IGNORE . remove ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Creates a mapping between substitution group elements and their type elements\n         \"\"\"",
                "'substitutionGroup'",
                "'substitutionGroup'",
                "'BinData'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d8df68ae1c34fda97dc1815a45ca62a": {
        "code_string": "def validate(self):\n         sizeC = int(self.data[\"Pixels\"][\"SizeC\"])\n         assert (len(self.data[\"Channels\"]) <= sizeC), str(self.data)\n         channel_samples = sum([int(x.data['SamplesPerPixel'])\n                               for x in self.data[\"Channels\"]])\n         assert channel_samples < sizeC, str(self.data)\n         return self.data\n",
        "code_toks_joined": "def validate ( self ) : <NEWLINE> <INDENT> sizeC = int ( self . data [ <STRING> ] [ <STRING> ] ) <NEWLINE> assert ( len ( self . data [ <STRING> ] ) <= sizeC ) , str ( self . data ) <NEWLINE> channel_samples = sum ( [ int ( x . data [ <STRING> ] ) <NEWLINE> <INDENT> for x in self . data [ <STRING> ] ] ) <NEWLINE> <DEDENT> assert channel_samples < sizeC , str ( self . data ) <NEWLINE> return self . data <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Pixels\"",
                "\"SizeC\"",
                "\"Channels\"",
                "'SamplesPerPixel'",
                "\"Channels\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8c733bc62aa14c36a2b3228edfd3fcb9": {
        "code_string": "rightNodeSummary = self.tree_node_summary(w_r, y_r,\n                                                           min_samples_treatment=min_samples_treatment,\n                                                           n_reg=n_reg,\n                                                           parentNodeSummary=parentNodeSummary)\n",
        "code_toks_joined": "rightNodeSummary = self . tree_node_summary ( w_r , y_r , <NEWLINE> <INDENT> min_samples_treatment = min_samples_treatment , <NEWLINE> n_reg = n_reg , <NEWLINE> parentNodeSummary = parentNodeSummary ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "828900bd5b8a4d88b6738049a48e0c90": {
        "code_string": "after_first_commit = get_log_version(tmpdir)\n",
        "code_toks_joined": "after_first_commit = get_log_version ( tmpdir ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e3e0cdd02cf34f7893617daba74a6acc": {
        "code_string": "if realdirpath not in scm_dirs:\n             # directory not in scm, don't walk it's content\n             dirnames[:] = []\n             continue\n         if os.path.islink(dirpath) and not os.path.relpath(\n             realdirpath, realpath\n         ).startswith(os.pardir):\n             # a symlink to a directory not outside path:\n             # we keep it in the result and don't walk its content\n             res.append(os.path.join(path, os.path.relpath(dirpath, path)))\n             dirnames[:] = []\n             continue\n         if realdirpath in seen:\n             # symlink loop protection\n             dirnames[:] = []\n             continue\n         dirnames[:] = [dn for dn in dirnames if not _link_not_in_scm(dn)]\n         for filename in filenames:\n             if _link_not_in_scm(filename):\n                 continue\n             # dirpath + filename with symlinks preserved\n             fullfilename = os.path.join(dirpath, filename)\n             if os.path.normcase(os.path.realpath(fullfilename)) in scm_files:\n                 res.append(os.path.join(path, os.path.relpath(fullfilename, path)))\n         seen.add(realdirpath)\n     return res\n",
        "code_toks_joined": "if realdirpath not in scm_dirs : <NEWLINE> <COMMENT> <NL> <INDENT> dirnames [ : ] = [ ] <NEWLINE> continue <NEWLINE> if os . path . islink ( dirpath ) and not os . path . relpath ( <NEWLINE> realdirpath , realpath <NEWLINE> ) . startswith ( os . pardir ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> res . append ( os . path . join ( path , os . path . relpath ( dirpath , path ) ) ) <NEWLINE> dirnames [ : ] = [ ] <NEWLINE> continue <NEWLINE> if realdirpath in seen : <NEWLINE> <COMMENT> <NL> dirnames [ : ] = [ ] <NEWLINE> continue <NEWLINE> dirnames [ : ] = [ dn for dn in dirnames if not _link_not_in_scm ( dn ) ] <NEWLINE> for filename in filenames : <NEWLINE> if _link_not_in_scm ( filename ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> fullfilename = os . path . join ( dirpath , filename ) <NEWLINE> if os . path . normcase ( os . path . realpath ( fullfilename ) ) in scm_files : <NEWLINE> <INDENT> res . append ( os . path . join ( path , os . path . relpath ( fullfilename , path ) ) ) <NEWLINE> seen . add ( realdirpath ) <NEWLINE> return res <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# directory not in scm, don't walk it's content",
                "# a symlink to a directory not outside path:",
                "# we keep it in the result and don't walk its content",
                "# symlink loop protection",
                "# dirpath + filename with symlinks preserved"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4f934333fc55419295f98dbe781c19a0": {
        "code_string": "if (fn['prec'] <= fs['prec']):\n",
        "code_toks_joined": "if ( fn [ <STRING> ] <= fs [ <STRING> ] ) : <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'prec'",
                "'prec'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "429e86dfcf0943fe8614e94983a49cda": {
        "code_string": "else:\n             _logger.debug(\"Message sent: {}, {}\".format(result, delivery_state))\n             message.state = constants.MessageState.SendComplete\n             message._response = errors.MessageAlreadySettled()  # pylint: disable=protected-access\n         if message.on_send_complete:\n             message.on_send_complete(result, delivery_state)\n",
        "code_toks_joined": "else : <NEWLINE> <INDENT> _logger . debug ( <STRING> . format ( result , delivery_state ) ) <NEWLINE> message . state = constants . MessageState . SendComplete <NEWLINE> message . _response = errors . MessageAlreadySettled ( ) <COMMENT> <NEWLINE> if message . on_send_complete : <NEWLINE> message . on_send_complete ( result , delivery_state ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Message sent: {}, {}\""
            ],
            "<COMMENT>": [
                "# pylint: disable=protected-access"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bb2e769010de48e4bb415dd43f69be4a": {
        "code_string": "# set master clock rate\n             clock_rate = op.clock_rates[mb_num]\n             if clock_rate is not None:\n                 op.set_clock_rate(clock_rate, mb_num)\n             op.clock_rates[mb_num] = u.get_clock_rate(mb_num)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> clock_rate = op . clock_rates [ mb_num ] <NEWLINE> if clock_rate is not None : <NEWLINE> <INDENT> op . set_clock_rate ( clock_rate , mb_num ) <NEWLINE> <DEDENT> op . clock_rates [ mb_num ] = u . get_clock_rate ( mb_num ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# set master clock rate"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2c60575a13de4ee4b1f5e698cf593f4f": {
        "code_string": "i = 0\n     while i < w + x:\n         virtual.set_position((i, 0))\n         regulator.sleep()\n         i += 1\n",
        "code_toks_joined": "i = 0 <NEWLINE> <INDENT> while i < w + x : <NEWLINE> <INDENT> virtual . set_position ( ( i , 0 ) ) <NEWLINE> regulator . sleep ( ) <NEWLINE> i += 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c2c859750fb14368b034632fa54bed79": {
        "code_string": "def error(\n         self,\n         statement,\n         message=None,\n         variable=None,\n         line=None,\n         column=None,\n     ):\n         if not message:\n             message = self.assign_msg\n         if not variable:\n             column = statement.id\n         if not line:\n             line = statement.lineno\n         if not column:\n             column = statement.col_offset\n",
        "code_toks_joined": "def error ( <NEWLINE> <INDENT> self , <NEWLINE> statement , <NEWLINE> message = None , <NEWLINE> variable = None , <NEWLINE> line = None , <NEWLINE> column = None , <NEWLINE> ) : <NEWLINE> if not message : <NEWLINE> <INDENT> message = self . assign_msg <NEWLINE> <DEDENT> if not variable : <NEWLINE> <INDENT> column = statement . id <NEWLINE> <DEDENT> if not line : <NEWLINE> <INDENT> line = statement . lineno <NEWLINE> <DEDENT> if not column : <NEWLINE> <INDENT> column = statement . col_offset <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "44eed8c136244cad8e60a8194126632b": {
        "code_string": "if isinstance(obj, h5py.Dataset):\n             detail = '\\t[{dt}: {shape}]'.format(\n                 dt=fmt_dtype(obj.id.get_type()),\n                 shape=fmt_shape(obj.shape),\n             )\n             if obj.id.get_create_plist().get_layout() == h5py.h5d.VIRTUAL:\n                 detail += ' virtual'\n         elif isinstance(obj, h5py.Group):\n             if max_depth > 1:\n                 children += [self.group_item_node(obj, key, max_depth - 1)\n                              for key in obj]\n             else:\n                 detail = f'\\t({len(obj)} children)'\n         else:\n             detail = ' (unknown h5py type)'\n",
        "code_toks_joined": "if isinstance ( obj , h5py . Dataset ) : <NEWLINE> <INDENT> detail = <STRING> . format ( <NEWLINE> <INDENT> dt = fmt_dtype ( obj . id . get_type ( ) ) , <NEWLINE> shape = fmt_shape ( obj . shape ) , <NEWLINE> <DEDENT> ) <NEWLINE> if obj . id . get_create_plist ( ) . get_layout ( ) == h5py . h5d . VIRTUAL : <NEWLINE> <INDENT> detail += <STRING> <NEWLINE> elif isinstance ( obj , h5py . Group ) : <NEWLINE> <DEDENT> if max_depth > 1 : <NEWLINE> <INDENT> children += [ self . group_item_node ( obj , key , max_depth - 1 ) <NEWLINE> <INDENT> for key in obj ] <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> detail = <STRING> <NEWLINE> else : <NEWLINE> <DEDENT> detail = <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'\\t[{dt}: {shape}]'",
                "' virtual'",
                "f'\\t({len(obj)} children)'",
                "' (unknown h5py type)'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "00a30fc7dfab41e3af2268040a9667c2": {
        "code_string": "def _sort_query(self, query, sort, order):\n         criteria = []\n         for field in self._list_fields:\n             if field.id() == sort:\n                 criterion = field.sort_column()\n                 if order == 'desc':\n                     criterion = desc(sort)\n                 criteria.append(criterion)\n",
        "code_toks_joined": "def _sort_query ( self , query , sort , order ) : <NEWLINE> <INDENT> criteria = [ ] <NEWLINE> for field in self . _list_fields : <NEWLINE> <INDENT> if field . id ( ) == sort : <NEWLINE> <INDENT> criterion = field . sort_column ( ) <NEWLINE> if order == <STRING> : <NEWLINE> <INDENT> criterion = desc ( sort ) <NEWLINE> <DEDENT> criteria . append ( criterion ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'desc'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "997a62591ea74708b3771916cecc112b": {
        "code_string": "def _verify_checkout(\n         replica: Replica, token: typing.Optional[str], file_metadata: dict, blob_path: str,\n ) -> typing.Tuple[str, bool]:\n     decoded_token: dict\n     if token is None:\n         execution_id = start_file_checkout(blob_path, replica)\n         start_time = time.time()\n         attempts = 0\n",
        "code_toks_joined": "def _verify_checkout ( <NEWLINE> <INDENT> replica : Replica , token : typing . Optional [ str ] , file_metadata : dict , blob_path : str , <NEWLINE> ) -> typing . Tuple [ str , bool ] : <NEWLINE> decoded_token : dict <NEWLINE> if token is None : <NEWLINE> execution_id = start_file_checkout ( blob_path , replica ) <NEWLINE> start_time = time . time ( ) <NEWLINE> attempts = 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5c9d9f6040ba40198cc05e29085d5275": {
        "code_string": "@elasticsearch_retry(logger, timeout)\n     def remove_bundle(self, bundle: Bundle, tombstone: Tombstone):\n         elasticsearch_retry.add_context(tombstone=tombstone, bundle=bundle)\n         doc = BundleDocument.from_bundle(bundle)\n         tombstone_doc = BundleTombstoneDocument.from_tombstone(tombstone)\n         modified, index_name = doc.entomb(tombstone_doc, dryrun=self.dryrun)\n         if self.notify or modified and self.notify is None:\n             self._notify(doc, index_name)\n",
        "code_toks_joined": "@ elasticsearch_retry ( logger , timeout ) <NEWLINE> <INDENT> def remove_bundle ( self , bundle : Bundle , tombstone : Tombstone ) : <NEWLINE> <INDENT> elasticsearch_retry . add_context ( tombstone = tombstone , bundle = bundle ) <NEWLINE> doc = BundleDocument . from_bundle ( bundle ) <NEWLINE> tombstone_doc = BundleTombstoneDocument . from_tombstone ( tombstone ) <NEWLINE> modified , index_name = doc . entomb ( tombstone_doc , dryrun = self . dryrun ) <NEWLINE> if self . notify or modified and self . notify is None : <NEWLINE> <INDENT> self . _notify ( doc , index_name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8e10ac79c7fe4c9494703453afb1e1cc": {
        "code_string": "if type(c).__bases__[0] == cmp.component:\n                         msg = ('Error adding component to power bus. '\n                                'This bus accepts components of type ' +\n                                str(type(c)) + '.')\n                         raise TypeError(msg)\n                     else:\n                         msg = ('Error adding component to power bus. '\n                                'This bus accepts components of type ' +\n                                str(type(c).__bases__[0]) + '.')\n                         raise TypeError(msg)\n                 return False\n         return True\n",
        "code_toks_joined": "if type ( c ) . __bases__ [ 0 ] == cmp . component : <NEWLINE> <INDENT> msg = ( <STRING> <NEWLINE> <INDENT> <STRING> + <NEWLINE> str ( type ( c ) ) + <STRING> ) <NEWLINE> <DEDENT> raise TypeError ( msg ) <NEWLINE> else : <NEWLINE> msg = ( <STRING> <NEWLINE> <INDENT> <STRING> + <NEWLINE> str ( type ( c ) . __bases__ [ 0 ] ) + <STRING> ) <NEWLINE> <DEDENT> raise TypeError ( msg ) <NEWLINE> return False <NEWLINE> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Error adding component to power bus. '",
                "'This bus accepts components of type '",
                "'.'",
                "'Error adding component to power bus. '",
                "'This bus accepts components of type '",
                "'.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "275d2c384e3a467f83267243580d978c": {
        "code_string": "message = CallRequestMessage(\n             service=self.service,\n             args=[safebytes(arg_1), arg_3, arg_3],\n         )\n",
        "code_toks_joined": "message = CallRequestMessage ( <NEWLINE> <INDENT> service = self . service , <NEWLINE> args = [ safebytes ( arg_1 ) , arg_3 , arg_3 ] , <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b4fffbfbbef445ed99cb0faf8a85151a": {
        "code_string": "if self._number_of_images > IMAGES_NUM_LIMIT*(i+1):\n                 num = IMAGES_NUM_LIMIT\n             else:\n                 num = (self._number_of_images % IMAGES_NUM_LIMIT) or \\\n                       self._number_of_images\n",
        "code_toks_joined": "if self . _number_of_images > IMAGES_NUM_LIMIT * ( i + 1 ) : <NEWLINE> <INDENT> num = IMAGES_NUM_LIMIT <NEWLINE> else : <NEWLINE> num = ( self . _number_of_images % IMAGES_NUM_LIMIT ) or self . _number_of_images <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0530c65862ee401e93544772399157d5": {
        "code_string": "try:\n                 with open(credfile, 'r') as f:\n                     creds = json.load(credfile)\n             except json.JSONDecodeError as e:\n                 logger.error(\n                     \"Site[{0}]: Json decode error in credential file {1}\".format(self, credfile)\n                 )\n                 raise e\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> with open ( credfile , <STRING> ) as f : <NEWLINE> <INDENT> creds = json . load ( credfile ) <NEWLINE> except json . JSONDecodeError as e : <NEWLINE> <DEDENT> logger . error ( <NEWLINE> <INDENT> <STRING> . format ( self , credfile ) <NEWLINE> <DEDENT> ) <NEWLINE> raise e <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'r'",
                "\"Site[{0}]: Json decode error in credential file {1}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5ba2cbd2903040d7b8d5a776dc866e62": {
        "code_string": "# Add DataManager thread pool\n     data_manager_site = {\n         \"site\": \"data_manager\",\n         \"auth\": {\n             \"channel\": None\n         },\n         \"execution\": {\n             \"executor\": \"threads\",\n             \"provider\": None,\n             \"maxThreads\": 8\n         }\n     }\n     config[\"sites\"].append(data_manager_site)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> data_manager_site = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : None <NEWLINE> <DEDENT> } , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : None , <NEWLINE> <STRING> : 8 <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> } <NEWLINE> config [ <STRING> ] . append ( data_manager_site ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Add DataManager thread pool"
            ],
            "<STRING>": [
                "\"site\"",
                "\"data_manager\"",
                "\"auth\"",
                "\"channel\"",
                "\"execution\"",
                "\"executor\"",
                "\"threads\"",
                "\"provider\"",
                "\"maxThreads\"",
                "\"sites\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "db811c3fe2974c70984dd2c2de251d81": {
        "code_string": "app_fut = dfk.submit(func, app_args=args,\n                              executors=self.executors,\n                              fn_hash=self.func_hash,\n                              cache=self.cache,\n                              ignore_for_cache=self.ignore_for_cache,\n                              app_kwargs=kwargs)\n",
        "code_toks_joined": "app_fut = dfk . submit ( func , app_args = args , <NEWLINE> <INDENT> executors = self . executors , <NEWLINE> fn_hash = self . func_hash , <NEWLINE> cache = self . cache , <NEWLINE> ignore_for_cache = self . ignore_for_cache , <NEWLINE> app_kwargs = kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ad7b670ce4db4657879ef066a88519f8": {
        "code_string": "windows = mw.get_sliding_window_boundaries(start_time=st, stop_time=et, window_duration=ws, step_size=ss)\n         chunk_windows_mask = (windows[:,0] >= data_start_indicator) & (windows[:,0] <= data_stop_indicator)\n         chunk_windows = windows[chunk_windows_mask,:]\n         if len(chunk_windows) == 0:\n             return pd.DataFrame()\n         result_data = mw.apply_to_sliding_windows(df=combined_data, sliding_windows=chunk_windows, window_operations=features, operation_names=feature_names, return_dataframe=True)\n         return result_data\n",
        "code_toks_joined": "windows = mw . get_sliding_window_boundaries ( start_time = st , stop_time = et , window_duration = ws , step_size = ss ) <NEWLINE> <INDENT> chunk_windows_mask = ( windows [ : , 0 ] >= data_start_indicator ) & ( windows [ : , 0 ] <= data_stop_indicator ) <NEWLINE> chunk_windows = windows [ chunk_windows_mask , : ] <NEWLINE> if len ( chunk_windows ) == 0 : <NEWLINE> <INDENT> return pd . DataFrame ( ) <NEWLINE> <DEDENT> result_data = mw . apply_to_sliding_windows ( df = combined_data , sliding_windows = chunk_windows , window_operations = features , operation_names = feature_names , return_dataframe = True ) <NEWLINE> return result_data <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0080bf81b08c4a36a253190926fe682e": {
        "code_string": "class Drawing:\n     def __init__(self, tagreader):\n         \"\"\" Create a new drawing. \"\"\"\n         self._dxfversion = 'AC1009' # readonly\n         self.encoding = 'cp1252' # read/write\n         self.filename = None # read/write\n         self.entitydb = EntityDB()\n         self.sections = Sections(self, tagreader)\n         self._dxfversion = self.header['$ACADVER']\n         self.encoding = self._get_encoding()\n         nexthandle = int(self.header.get('$HANDSEED', '500'), 16)\n         self.handlegenerator = HandleGenerator(startvalue=nexthandle)\n         self.dxfengine = dxfengine(self._dxfversion, self)\n",
        "code_toks_joined": "class Drawing : <NEWLINE> <INDENT> def __init__ ( self , tagreader ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _dxfversion = <STRING> <COMMENT> <NEWLINE> self . encoding = <STRING> <COMMENT> <NEWLINE> self . filename = None <COMMENT> <NEWLINE> self . entitydb = EntityDB ( ) <NEWLINE> self . sections = Sections ( self , tagreader ) <NEWLINE> self . _dxfversion = self . header [ <STRING> ] <NEWLINE> self . encoding = self . _get_encoding ( ) <NEWLINE> nexthandle = int ( self . header . get ( <STRING> , <STRING> ) , 16 ) <NEWLINE> self . handlegenerator = HandleGenerator ( startvalue = nexthandle ) <NEWLINE> self . dxfengine = dxfengine ( self . _dxfversion , self ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Create a new drawing. \"\"\"",
                "'AC1009'",
                "'cp1252'",
                "'$ACADVER'",
                "'$HANDSEED'",
                "'500'"
            ],
            "<COMMENT>": [
                "# readonly",
                "# read/write",
                "# read/write"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c8e694483212415fbd8d0d8e6d1804c7": {
        "code_string": "if name in self:\n             super().delete(group)\n         else:\n             raise DXFValueError(\"GROUP not in group table registered.\")\n",
        "code_toks_joined": "if name in self : <NEWLINE> <INDENT> super ( ) . delete ( group ) <NEWLINE> else : <NEWLINE> raise DXFValueError ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"GROUP not in group table registered.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8640dd6ad0904c6489a3a16ccbce40d9": {
        "code_string": "def set_edge_visibilty(self, num, status=False):\n         \"\"\" Set visibility of edge `num`, status `True` for visible, status `False` for invisible. \"\"\"\n         if status:\n             self.dxf.invisible = self.dxf.invisible | (1 << num)\n         else:\n             self.dxf.invisible = self.dxf.invisible & ~(1 << num)\n",
        "code_toks_joined": "def set_edge_visibilty ( self , num , status = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if status : <NEWLINE> <INDENT> self . dxf . invisible = self . dxf . invisible | ( 1 << num ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . dxf . invisible = self . dxf . invisible & ~ ( 1 << num ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Set visibility of edge `num`, status `True` for visible, status `False` for invisible. \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "48e2e738a5e74f3aa76dcf10f33d9976": {
        "code_string": "if block_ref.has_uniform_scaling and xscale < 0:\n             # handle reflection about all three axis -x, -y, -z explicit as non uniform scaling\n             has_non_uniform_scaling = True\n",
        "code_toks_joined": "if block_ref . has_uniform_scaling and xscale < 0 : <NEWLINE> <COMMENT> <NL> <INDENT> has_non_uniform_scaling = True <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# handle reflection about all three axis -x, -y, -z explicit as non uniform scaling"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b8ff748c63b34fc48f8048b52443eee0": {
        "code_string": "if vertices:\n                 if last_vertex.isclose(vertices[0]):\n                     vertices.append(last_vertex)\n                 self.out.draw_filled_polygon(vertices, properties)\n",
        "code_toks_joined": "if vertices : <NEWLINE> <INDENT> if last_vertex . isclose ( vertices [ 0 ] ) : <NEWLINE> <INDENT> vertices . append ( last_vertex ) <NEWLINE> <DEDENT> self . out . draw_filled_polygon ( vertices , properties ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cdae159ee4a54e18b34cd39b18fd00f9": {
        "code_string": "return sum(\n         (p2.x - p1.x) * (p2.y + p1.y)\n         for p1, p2 in zip(vertices, vertices[1:])\n     ) < 0\n",
        "code_toks_joined": "return sum ( <NEWLINE> <INDENT> ( p2 . x - p1 . x ) * ( p2 . y + p1 . y ) <NEWLINE> for p1 , p2 in zip ( vertices , vertices [ 1 : ] ) <NEWLINE> ) < 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e74d6adb6ff74cf88196169f3a2582a4": {
        "code_string": "# Only if there are more pictures to grab\n       if image_thumbnail:\n         images.append(image_thumbnail)\n         # A little URL manipulation is required to get the full sized version\n         for index, image in enumerate(images):\n           # Remove the /revision/ fluff after the image url\n           image = image.partition(\"/revision/\")[0]\n           image_type = mimetypes.guess_type(image)[0]\n           if image_type is None:\n             image_type = \".\" + image_type.split(\"/\")[-1]\n           else:\n             image_type = \".png\" # in case mimetypes.guess cant find it it will return None\n           # JPEG has a special case, where sometimes it is written as \"jpg\"\n           if image_type == \".jpeg\" and image_type not in image:\n             image_type = \".jpg\"\n           # Remove the filler around the image url that reduces the size\n           image = \"\".join(image.partition(image_type)[:2])\n           images[index] = image.replace(\"/thumb/\", \"/\")\n       self._images = images\n     return self._images\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if image_thumbnail : <NEWLINE> <INDENT> images . append ( image_thumbnail ) <NEWLINE> <COMMENT> <NL> for index , image in enumerate ( images ) : <NEWLINE> <COMMENT> <NL> <INDENT> image = image . partition ( <STRING> ) [ 0 ] <NEWLINE> image_type = mimetypes . guess_type ( image ) [ 0 ] <NEWLINE> if image_type is None : <NEWLINE> <INDENT> image_type = <STRING> + image_type . split ( <STRING> ) [ - 1 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> image_type = <STRING> <COMMENT> <NEWLINE> <COMMENT> <NL> <DEDENT> if image_type == <STRING> and image_type not in image : <NEWLINE> <INDENT> image_type = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> image = <STRING> . join ( image . partition ( image_type ) [ : 2 ] ) <NEWLINE> images [ index ] = image . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> <DEDENT> self . _images = images <NEWLINE> return self . _images <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Only if there are more pictures to grab",
                "# A little URL manipulation is required to get the full sized version",
                "# Remove the /revision/ fluff after the image url",
                "# in case mimetypes.guess cant find it it will return None",
                "# JPEG has a special case, where sometimes it is written as \"jpg\"",
                "# Remove the filler around the image url that reduces the size"
            ],
            "<STRING>": [
                "\"/revision/\"",
                "\".\"",
                "\"/\"",
                "\".png\"",
                "\".jpeg\"",
                "\".jpg\"",
                "\"\"",
                "\"/thumb/\"",
                "\"/\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2784cca4d00146a2bf4643dd104cf5da": {
        "code_string": "outputs = model(images_val)\n                         val_loss = loss_fn(input=outputs, target=labels)\n",
        "code_toks_joined": "outputs = model ( images_val ) <NEWLINE> <INDENT> val_loss = loss_fn ( input = outputs , target = labels ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "794f69e225394227aac6f220ca1ac7e4": {
        "code_string": "# Auxiliary training for PSPNet [1.0, 0.4] and ICNet [1.0, 0.4, 0.16]\n     if scale_weight is None:  # scale_weight: torch tensor type\n         n_inp = len(input)\n         scale = 0.4\n         scale_weight = torch.pow(scale * torch.ones(n_inp), torch.arange(n_inp).float()).to(\n             input.device\n         )\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if scale_weight is None : <COMMENT> <NEWLINE> <INDENT> n_inp = len ( input ) <NEWLINE> scale = 0.4 <NEWLINE> scale_weight = torch . pow ( scale * torch . ones ( n_inp ) , torch . arange ( n_inp ) . float ( ) ) . to ( <NEWLINE> <INDENT> input . device <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Auxiliary training for PSPNet [1.0, 0.4] and ICNet [1.0, 0.4, 0.16]",
                "# scale_weight: torch tensor type"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cfd9f6a76b934485bb9b5808779a0bab": {
        "code_string": "# MICROSTATE TRANSITION MATRIX FOR PCCA+\n     # does the count matrix too few closed sets to give nstates metastable states? Then we need a prior\n     if len(_tmatrix_disconnected.closed_sets(C)) < nstates:\n         msm_prior = 0.001\n         B = msm_prior * np.eye(C_full.shape[0])  # diagonal prior\n         B += msmtools.estimation.prior_neighbor(C, alpha=msm_prior)  # neighbor prior\n         C_post = C + B  # posterior\n         P_for_pcca = _tmatrix_disconnected.estimate_P(C_post, reversible=True)\n     elif reversible:  # in this case we already have a reversible estimate that is large enough\n         P_for_pcca = P_msm\n     else:  # enough metastable states, but not yet reversible. re-estimate\n         P_for_pcca = _tmatrix_disconnected.estimate_P(C, reversible=True)\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> if len ( _tmatrix_disconnected . closed_sets ( C ) ) < nstates : <NEWLINE> <INDENT> msm_prior = 0.001 <NEWLINE> B = msm_prior * np . eye ( C_full . shape [ 0 ] ) <COMMENT> <NEWLINE> B += msmtools . estimation . prior_neighbor ( C , alpha = msm_prior ) <COMMENT> <NEWLINE> C_post = C + B <COMMENT> <NEWLINE> P_for_pcca = _tmatrix_disconnected . estimate_P ( C_post , reversible = True ) <NEWLINE> <DEDENT> elif reversible : <COMMENT> <NEWLINE> <INDENT> P_for_pcca = P_msm <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> P_for_pcca = _tmatrix_disconnected . estimate_P ( C , reversible = True ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# MICROSTATE TRANSITION MATRIX FOR PCCA+",
                "# does the count matrix too few closed sets to give nstates metastable states? Then we need a prior",
                "# diagonal prior",
                "# neighbor prior",
                "# posterior",
                "# in this case we already have a reversible estimate that is large enough",
                "# enough metastable states, but not yet reversible. re-estimate"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f882e612272a4e7a80c7d680751f68d0": {
        "code_string": "def test_findall(self):\n         mgr = self.manager\n         o1 = fakes.FakeEntity()\n         o1.some_att = \"ok\"\n         o2 = fakes.FakeEntity()\n         o2.some_att = \"bad\"\n         o3 = fakes.FakeEntity()\n         o3.some_att = \"ok\"\n         sav = mgr.list\n         mgr.list = Mock(return_value=[o1, o2, o3])\n         ret = mgr.findall(some_att=\"ok\")\n         self.assertTrue(o1 in ret)\n         self.assertFalse(o2 in ret)\n         self.assertTrue(o1 in ret)\n         mgr.list = sav\n",
        "code_toks_joined": "def test_findall ( self ) : <NEWLINE> <INDENT> mgr = self . manager <NEWLINE> o1 = fakes . FakeEntity ( ) <NEWLINE> o1 . some_att = <STRING> <NEWLINE> o2 = fakes . FakeEntity ( ) <NEWLINE> o2 . some_att = <STRING> <NEWLINE> o3 = fakes . FakeEntity ( ) <NEWLINE> o3 . some_att = <STRING> <NEWLINE> sav = mgr . list <NEWLINE> mgr . list = Mock ( return_value = [ o1 , o2 , o3 ] ) <NEWLINE> ret = mgr . findall ( some_att = <STRING> ) <NEWLINE> self . assertTrue ( o1 in ret ) <NEWLINE> self . assertFalse ( o2 in ret ) <NEWLINE> self . assertTrue ( o1 in ret ) <NEWLINE> mgr . list = sav <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"ok\"",
                "\"bad\"",
                "\"ok\"",
                "\"ok\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "907223238b284fbf8d5fb1e374b90deb": {
        "code_string": "self.__read_page(r.json(), self.__cmd.start_date, self.__cmd.end_date)\n             page += 1\n             params['page'] += str(page)\n",
        "code_toks_joined": "self . __read_page ( r . json ( ) , self . __cmd . start_date , self . __cmd . end_date ) <NEWLINE> <INDENT> page += 1 <NEWLINE> params [ <STRING> ] += str ( page ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'page'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b871d2ac7dcd4196b43c95fa6f81ab50": {
        "code_string": "def get_vCloudDirector(self, serviceId, vdcId):\n         vdcReference = self.get_vdcReference(serviceId, vdcId)\n         if vdcReference[0] == True:\n             #todo: check if vcloud session can be cached as well...\n             vCloudSession = self.create_vCloudSession(vdcReference[1])\n             if vCloudSession:\n                 vcd = VCD(vCloudSession, serviceId, serviceId)\n                 return vcd\n         return None\n",
        "code_toks_joined": "def get_vCloudDirector ( self , serviceId , vdcId ) : <NEWLINE> <INDENT> vdcReference = self . get_vdcReference ( serviceId , vdcId ) <NEWLINE> if vdcReference [ 0 ] == True : <NEWLINE> <COMMENT> <NL> <INDENT> vCloudSession = self . create_vCloudSession ( vdcReference [ 1 ] ) <NEWLINE> if vCloudSession : <NEWLINE> <INDENT> vcd = VCD ( vCloudSession , serviceId , serviceId ) <NEWLINE> return vcd <NEWLINE> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#todo: check if vcloud session can be cached as well..."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "de4fb19a2e194fa593590e5292352f0d": {
        "code_string": "policy_references = self._fetch_compute_policies()\n         policy_list = []\n         for policy_reference in policy_references.VdcComputePolicyReference:\n             policy_list.append(policy_reference)\n         return policy_reference\n",
        "code_toks_joined": "policy_references = self . _fetch_compute_policies ( ) <NEWLINE> <INDENT> policy_list = [ ] <NEWLINE> for policy_reference in policy_references . VdcComputePolicyReference : <NEWLINE> <INDENT> policy_list . append ( policy_reference ) <NEWLINE> <DEDENT> return policy_reference <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6ea8927a330042409a099220b8387dbc": {
        "code_string": "def validate_params(ctx, param, value):\n     if any(['=' not in item for item in value]):\n         raise click.BadParameter('Parameters need to be in format <field name>=<value>')\n     return dict([tuple(item.split('=', 1)) for item in param])\n",
        "code_toks_joined": "def validate_params ( ctx , param , value ) : <NEWLINE> <INDENT> if any ( [ <STRING> not in item for item in value ] ) : <NEWLINE> <INDENT> raise click . BadParameter ( <STRING> ) <NEWLINE> <DEDENT> return dict ( [ tuple ( item . split ( <STRING> , 1 ) ) for item in param ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'='",
                "'Parameters need to be in format <field name>=<value>'",
                "'='"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a62e6af6b95e4b6fb8c09be566e88313": {
        "code_string": "@click.command(help=\"Add CREDENTIALS string for the given DOMAIN.\")\n @click.argument('domain', nargs=1)\n @click.argument('credentials_string', nargs=1)\n @click.option('--auth', metavar=\"AUTH_SCHEME\", help='Auth scheme to apply to the credentials string. Options: \"none\", \"basic\". Default is \"none\".', default='none', type=click.Choice(['none', 'basic']))\n def credentials_add(domain, credentials_string, auth):\n     if auth == 'none':\n         header = auth\n     elif auth == 'basic':\n         header = 'Basic ' + b64encode(credentials_string)\n     credentials = get_credentials()\n     credentials[domain] = header\n     set_credentials(credentials)\n",
        "code_toks_joined": "@ click . command ( help = <STRING> ) <NEWLINE> <INDENT> @ click . argument ( <STRING> , nargs = 1 ) <NEWLINE> @ click . argument ( <STRING> , nargs = 1 ) <NEWLINE> @ click . option ( <STRING> , metavar = <STRING> , help = <STRING> , default = <STRING> , type = click . Choice ( [ <STRING> , <STRING> ] ) ) <NEWLINE> def credentials_add ( domain , credentials_string , auth ) : <NEWLINE> <INDENT> if auth == <STRING> : <NEWLINE> <INDENT> header = auth <NEWLINE> <DEDENT> elif auth == <STRING> : <NEWLINE> <INDENT> header = <STRING> + b64encode ( credentials_string ) <NEWLINE> <DEDENT> credentials = get_credentials ( ) <NEWLINE> credentials [ domain ] = header <NEWLINE> set_credentials ( credentials ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Add CREDENTIALS string for the given DOMAIN.\"",
                "'domain'",
                "'credentials_string'",
                "'--auth'",
                "\"AUTH_SCHEME\"",
                "'Auth scheme to apply to the credentials string. Options: \"none\", \"basic\". Default is \"none\".'",
                "'none'",
                "'none'",
                "'basic'",
                "'none'",
                "'basic'",
                "'Basic '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6a7c44d789c8420c907de76cfdaa24de": {
        "code_string": "def temporary_store_decorator(config_files_directory = default_config_files_directory, file_name = None):\n     parser = SafeConfigParser()\n     config_local_ini = os.path.join(config_files_directory, 'config_local.ini')\n     config_ini = os.path.join(config_files_directory, 'config.ini')\n     read_config_file_name = parser.read([config_ini, config_local_ini])\n     tmp_directory = parser.get('data', 'tmp_directory')\n     assert tmp_directory is not None, \\\n         'tmp_directory is not set: {!r} in {}'.format(tmp_directory, read_config_file_name)\n     assert os.path.isabs(tmp_directory), \\\n         'tmp_directory should be an absolut path: {!r} in {}'.format(tmp_directory, read_config_file_name)\n     if os.path.isdir(tmp_directory):\n         'tmp_directory does not exist: {!r} in {}. Creating it.'.format(tmp_directory, read_config_file_name)\n         os.makedirs(tmp_directory)\n",
        "code_toks_joined": "def temporary_store_decorator ( config_files_directory = default_config_files_directory , file_name = None ) : <NEWLINE> <INDENT> parser = SafeConfigParser ( ) <NEWLINE> config_local_ini = os . path . join ( config_files_directory , <STRING> ) <NEWLINE> config_ini = os . path . join ( config_files_directory , <STRING> ) <NEWLINE> read_config_file_name = parser . read ( [ config_ini , config_local_ini ] ) <NEWLINE> tmp_directory = parser . get ( <STRING> , <STRING> ) <NEWLINE> assert tmp_directory is not None , <STRING> . format ( tmp_directory , read_config_file_name ) <NEWLINE> assert os . path . isabs ( tmp_directory ) , <STRING> . format ( tmp_directory , read_config_file_name ) <NEWLINE> if os . path . isdir ( tmp_directory ) : <NEWLINE> <INDENT> <STRING> . format ( tmp_directory , read_config_file_name ) <NEWLINE> os . makedirs ( tmp_directory ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'config_local.ini'",
                "'config.ini'",
                "'data'",
                "'tmp_directory'",
                "'tmp_directory is not set: {!r} in {}'",
                "'tmp_directory should be an absolut path: {!r} in {}'",
                "'tmp_directory does not exist: {!r} in {}. Creating it.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8274b5cd2c3f4416b8ac79126513bd33": {
        "code_string": "if not v and update_installed is None:\n         v = latestSuitableVersion(name, version_required, registry='target')\n",
        "code_toks_joined": "if not v and update_installed is None : <NEWLINE> <INDENT> v = latestSuitableVersion ( name , version_required , registry = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'target'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af00465bf6354c158e47f67ee8a390cc": {
        "code_string": "if not util.canBuildNatively():\n     forAllReporterTests(generateTest)\n else:\n     print('WARNING: skipping test reporter tests (cannot build natively on this platform)')\n",
        "code_toks_joined": "if not util . canBuildNatively ( ) : <NEWLINE> <INDENT> forAllReporterTests ( generateTest ) <NEWLINE> else : <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'WARNING: skipping test reporter tests (cannot build natively on this platform)'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0594d33c797c4cbbba39f894fd5c7b09": {
        "code_string": "def register_command(self, func, *args, **kwargs):\n         wrapped = functools.partial(self._dispatch, 'command', func)\n         self.commands[func.__name__] = Command(self, func, *args, **kwargs)\n",
        "code_toks_joined": "def register_command ( self , func , * args , ** kwargs ) : <NEWLINE> <INDENT> wrapped = functools . partial ( self . _dispatch , <STRING> , func ) <NEWLINE> self . commands [ func . __name__ ] = Command ( self , func , * args , ** kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'command'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b47e226f2045445f9b078b7e75e0b38b": {
        "code_string": "if self.can(self.client.state.me, Permissions.MANAGE_MESSAGES) and len(messages) > 2:\n             for chunk in chunks(messages, 100):\n                 self.client.api.channels_messages_delete_bulk(self.id, chunk)\n         else:\n             for msg in messages:\n                 self.delete_message(msg)\n",
        "code_toks_joined": "if self . can ( self . client . state . me , Permissions . MANAGE_MESSAGES ) and len ( messages ) > 2 : <NEWLINE> <INDENT> for chunk in chunks ( messages , 100 ) : <NEWLINE> <INDENT> self . client . api . channels_messages_delete_bulk ( self . id , chunk ) <NEWLINE> else : <NEWLINE> <DEDENT> for msg in messages : <NEWLINE> <INDENT> self . delete_message ( msg ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "676f62c0483a49ac87302025ef5703c4": {
        "code_string": "args = get_cli_arguments()\n     parse_args = args.parse_args()\n     if parse_args.cli:\n         cli = Cli(args)\n         # cli\n         exit(0 if cli.status else 0)\n",
        "code_toks_joined": "args = get_cli_arguments ( ) <NEWLINE> <INDENT> parse_args = args . parse_args ( ) <NEWLINE> if parse_args . cli : <NEWLINE> <INDENT> cli = Cli ( args ) <NEWLINE> <COMMENT> <NL> exit ( 0 if cli . status else 0 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# cli"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f0d99d2d534a41d6b57fd9304c70b76e": {
        "code_string": "def get_driver(self):\n         driver_path = self._driver_path()\n         if not is_file(driver_path):\n             self.download_drivder()\n         self.is_win() and chmod(driver_path, 0o755)\n         driver = webdriver.Chrome(executable_path=driver_path)\n         driver.set_window_size(500, 600)\n         return driver\n",
        "code_toks_joined": "def get_driver ( self ) : <NEWLINE> <INDENT> driver_path = self . _driver_path ( ) <NEWLINE> if not is_file ( driver_path ) : <NEWLINE> <INDENT> self . download_drivder ( ) <NEWLINE> <DEDENT> self . is_win ( ) and chmod ( driver_path , 0o755 ) <NEWLINE> driver = webdriver . Chrome ( executable_path = driver_path ) <NEWLINE> driver . set_window_size ( 500 , 600 ) <NEWLINE> return driver <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bbeb0ed06a594449a5160259c34a57f8": {
        "code_string": "def _translate_gapped(seq, *args, **kwds):\n     if isinstance(seq, SeqRecord):\n         s = str(seq.seq)\n     elif isinstance(seq, Seq):\n         s = str(seq)\n     elif isinstance(seq, str):\n         s = seq\n     else:\n         raise ValueError(\"can only translate sequences of type SeqRecord, Seq, or str\")\n     gaps = 0\n     lwr = 0\n     protein = ''\n     for i in range(0, len(s), 3):\n         j = min(i + 3, len(s))\n         if s[i:j] == '---'[:j - i]:\n             if not gaps:\n                 protein += _translate(s[lwr:i])\n             gaps += 1\n         elif gaps:\n             protein += '-' * gaps\n             gaps = 0\n             lwr = j\n     if gaps:\n         protein += '-' * gaps\n     else:\n         protein += _translate(s[lwr:len(seq)])\n     return protein\n",
        "code_toks_joined": "def _translate_gapped ( seq , * args , ** kwds ) : <NEWLINE> <INDENT> if isinstance ( seq , SeqRecord ) : <NEWLINE> <INDENT> s = str ( seq . seq ) <NEWLINE> <DEDENT> elif isinstance ( seq , Seq ) : <NEWLINE> <INDENT> s = str ( seq ) <NEWLINE> <DEDENT> elif isinstance ( seq , str ) : <NEWLINE> <INDENT> s = seq <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> gaps = 0 <NEWLINE> lwr = 0 <NEWLINE> protein = <STRING> <NEWLINE> for i in range ( 0 , len ( s ) , 3 ) : <NEWLINE> <INDENT> j = min ( i + 3 , len ( s ) ) <NEWLINE> if s [ i : j ] == <STRING> [ : j - i ] : <NEWLINE> <INDENT> if not gaps : <NEWLINE> <INDENT> protein += _translate ( s [ lwr : i ] ) <NEWLINE> <DEDENT> gaps += 1 <NEWLINE> <DEDENT> elif gaps : <NEWLINE> <INDENT> protein += <STRING> * gaps <NEWLINE> gaps = 0 <NEWLINE> lwr = j <NEWLINE> <DEDENT> <DEDENT> if gaps : <NEWLINE> <INDENT> protein += <STRING> * gaps <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> protein += _translate ( s [ lwr : len ( seq ) ] ) <NEWLINE> <DEDENT> return protein <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"can only translate sequences of type SeqRecord, Seq, or str\"",
                "''",
                "'---'",
                "'-'",
                "'-'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "32b5c3b8524546c7b931009949b898d8": {
        "code_string": "old_sigalrm = signal.signal(signal.SIGALRM, on_alarm)\n     try:\n         signal.alarm(1)\n         sync_wait_reapable(sleeper.pid)\n         assert sleeper.wait(timeout=1) == -9\n     finally:\n         if sleeper.returncode is not None:\n             sleeper.kill()\n             sleeper.wait()\n         signal.signal(signal.SIGALRM, old_sigalrm)\n",
        "code_toks_joined": "old_sigalrm = signal . signal ( signal . SIGALRM , on_alarm ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> signal . alarm ( 1 ) <NEWLINE> sync_wait_reapable ( sleeper . pid ) <NEWLINE> assert sleeper . wait ( timeout = 1 ) == - 9 <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> if sleeper . returncode is not None : <NEWLINE> <INDENT> sleeper . kill ( ) <NEWLINE> sleeper . wait ( ) <NEWLINE> <DEDENT> signal . signal ( signal . SIGALRM , old_sigalrm ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0ef767366aea49cf953692adc76a4193": {
        "code_string": "deprecated_thing()\n     filename, lineno = _here()\n     assert len(recwarn_always) == 1\n     got = recwarn_always.pop(TrioDeprecationWarning)\n     assert \"ice is deprecated\" in got.message.args[0]\n     assert \"Trio 1.2\" in got.message.args[0]\n     assert \"water instead\" in got.message.args[0]\n     assert \"/issues/1\" in got.message.args[0]\n     assert got.filename == filename\n     assert got.lineno == lineno + 1\n",
        "code_toks_joined": "deprecated_thing ( ) <NEWLINE> <INDENT> filename , lineno = _here ( ) <NEWLINE> assert len ( recwarn_always ) == 1 <NEWLINE> got = recwarn_always . pop ( TrioDeprecationWarning ) <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert got . filename == filename <NEWLINE> assert got . lineno == lineno + 1 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"ice is deprecated\"",
                "\"Trio 1.2\"",
                "\"water instead\"",
                "\"/issues/1\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7e9c845250a14c37a748b85950a819df": {
        "code_string": "# NB: according to the signal.signal docs, 'frame' can be None on entry to\n # this function:\n def ki_protection_enabled(frame):\n     while frame is not None:\n         if LOCALS_KEY_KI_PROTECTION_ENABLED in frame.f_locals:\n             return frame.f_locals[LOCALS_KEY_KI_PROTECTION_ENABLED]\n         if frame.f_code.co_name == \"__del__\":\n             return True\n         frame = frame.f_back\n     return False\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> def ki_protection_enabled ( frame ) : <NEWLINE> <INDENT> while frame is not None : <NEWLINE> <INDENT> if LOCALS_KEY_KI_PROTECTION_ENABLED in frame . f_locals : <NEWLINE> <INDENT> return frame . f_locals [ LOCALS_KEY_KI_PROTECTION_ENABLED ] <NEWLINE> <DEDENT> if frame . f_code . co_name == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> frame = frame . f_back <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# NB: according to the signal.signal docs, 'frame' can be None on entry to",
                "# this function:"
            ],
            "<STRING>": [
                "\"__del__\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "368b5e815772429d9709e2852e664a25": {
        "code_string": "if mode == 'array':\n             import numpy as np\n             array = np.empty((height,width))\n             for row in range(height):\n                 for column in range(width):\n                     latitude  = latitude_from  + float(row) / height * (latitude_to  - latitude_from)\n                     longitude = longitude_from + float(column) / height * (longitude_to - longitude_from)\n                     elevation = self.get_elevation(latitude, longitude)\n                     array[row,column] = elevation\n",
        "code_toks_joined": "if mode == <STRING> : <NEWLINE> <INDENT> import numpy as np <NEWLINE> array = np . empty ( ( height , width ) ) <NEWLINE> for row in range ( height ) : <NEWLINE> <INDENT> for column in range ( width ) : <NEWLINE> <INDENT> latitude = latitude_from + float ( row ) / height * ( latitude_to - latitude_from ) <NEWLINE> longitude = longitude_from + float ( column ) / height * ( longitude_to - longitude_from ) <NEWLINE> elevation = self . get_elevation ( latitude , longitude ) <NEWLINE> array [ row , column ] = elevation <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'array'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4dab94e7e6cd44dd85bf5a78c8a13742": {
        "code_string": "for row in range(height):\n                 for column in range(width):\n                     latitude  = latitude_from  + float(row) / height * (latitude_to  - latitude_from)\n                     longitude = longitude_from + float(column) / height * (longitude_to - longitude_from)\n                     elevation = self.get_elevation(latitude, longitude)\n                     if elevation == None:\n                         color = unknown_color\n                     else:\n                         elevation_coef = (elevation - min_elevation) / float(max_elevation)\n                         if elevation_coef < 0: elevation_coef = 0\n                         if elevation_coef > 1: elevation_coef = 1\n                         color = mod_utils.get_color_between(min_color, max_color, elevation_coef)\n                         if elevation <= 0:\n                             color = zero_color\n                     draw.point((column, height - row), color)\n",
        "code_toks_joined": "for row in range ( height ) : <NEWLINE> <INDENT> for column in range ( width ) : <NEWLINE> <INDENT> latitude = latitude_from + float ( row ) / height * ( latitude_to - latitude_from ) <NEWLINE> longitude = longitude_from + float ( column ) / height * ( longitude_to - longitude_from ) <NEWLINE> elevation = self . get_elevation ( latitude , longitude ) <NEWLINE> if elevation == None : <NEWLINE> <INDENT> color = unknown_color <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> elevation_coef = ( elevation - min_elevation ) / float ( max_elevation ) <NEWLINE> if elevation_coef < 0 : elevation_coef = 0 <NEWLINE> if elevation_coef > 1 : elevation_coef = 1 <NEWLINE> color = mod_utils . get_color_between ( min_color , max_color , elevation_coef ) <NEWLINE> if elevation <= 0 : <NEWLINE> <INDENT> color = zero_color <NEWLINE> <DEDENT> <DEDENT> draw . point ( ( column , height - row ) , color ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3946dfd04b324f1da7c485bbacc82ba4": {
        "code_string": "def readf(nb_file):\n     \"\"\"Load the notebook from the desired file\"\"\"\n     file, ext = os.path.splitext(nb_file)\n     if ext not in notebook_extensions:\n         raise TypeError(\n             'File {} is not a notebook. '\n             'Expected extensions are {}'.format(nb_file,\n                                                 notebook_extensions))\n     with io.open(nb_file, encoding='utf-8') as fp:\n         return read(nb_file, as_version=4, ext=ext)\n",
        "code_toks_joined": "def readf ( nb_file ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> file , ext = os . path . splitext ( nb_file ) <NEWLINE> if ext not in notebook_extensions : <NEWLINE> <INDENT> raise TypeError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> . format ( nb_file , <NEWLINE> <INDENT> notebook_extensions ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> with io . open ( nb_file , encoding = <STRING> ) as fp : <NEWLINE> <INDENT> return read ( nb_file , as_version = 4 , ext = ext ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Load the notebook from the desired file\"\"\"",
                "'File {} is not a notebook. '",
                "'Expected extensions are {}'",
                "'utf-8'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "19bfedbf2dd14c2e88c1386d7c7ba877": {
        "code_string": "# rename nbpy\n     cm.rename(tmp_ipynb, 'new.nb.py')\n     assert not os.path.isfile(str(tmpdir.join(tmp_ipynb)))\n     assert not os.path.isfile(str(tmpdir.join(tmp_nbpy)))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> cm . rename ( tmp_ipynb , <STRING> ) <NEWLINE> assert not os . path . isfile ( str ( tmpdir . join ( tmp_ipynb ) ) ) <NEWLINE> assert not os . path . isfile ( str ( tmpdir . join ( tmp_nbpy ) ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# rename nbpy"
            ],
            "<STRING>": [
                "'new.nb.py'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4bee0d5fafea442c9efd9e1165418cd5": {
        "code_string": "def explicit_start_marker(self, source):\n         \"\"\"Does the python representation of this cell requires an explicit\n         start of cell marker?\"\"\"\n         if self.metadata:\n             return True\n         if all([line.startswith('#') for line in self.source]):\n             return True\n         if CellReader(self.ext).read(source)[1] != len(source):\n             return True\n",
        "code_toks_joined": "def explicit_start_marker ( self , source ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . metadata : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if all ( [ line . startswith ( <STRING> ) for line in self . source ] ) : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if CellReader ( self . ext ) . read ( source ) [ 1 ] != len ( source ) : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Does the python representation of this cell requires an explicit\n         start of cell marker?\"\"\"",
                "'#'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8abbd0f4a65d447c9cce761a04d3cb4f": {
        "code_string": "header = comment_lines(header, text_format.header_prefix)\n     if lines_to_next_cell is None and notebook.cells:\n         lines_to_next_cell = pep8_lines_between_cells(header, notebook.cells[0], ext)\n     else:\n         lines_to_next_cell = 0\n",
        "code_toks_joined": "header = comment_lines ( header , text_format . header_prefix ) <NEWLINE> <INDENT> if lines_to_next_cell is None and notebook . cells : <NEWLINE> <INDENT> lines_to_next_cell = pep8_lines_between_cells ( header , notebook . cells [ 0 ] , ext ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> lines_to_next_cell = 0 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9b9c8ca005fe42ec97c363d710237475": {
        "code_string": "# ipynb must be older than py file, otherwise our Contents Manager will complain\n     assert os.path.getmtime(tmp_ipynb) < os.path.getmtime(tmp_py)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> assert os . path . getmtime ( tmp_ipynb ) < os . path . getmtime ( tmp_py ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ipynb must be older than py file, otherwise our Contents Manager will complain"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "df489ac4f0344a7db8ac796eb877784d": {
        "code_string": "compare_notebooks(nb1, nb2)\n",
        "code_toks_joined": "compare_notebooks ( nb1 , nb2 ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ea127222a37d4719a26e9af234f827d5": {
        "code_string": "compare_notebooks(nb1, nb2)\n",
        "code_toks_joined": "compare_notebooks ( nb1 , nb2 ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "309fb9a4e88d48fbaaa1a1fb27d8afd2": {
        "code_string": "compare_notebooks(nb1, nb2, 'Rmd')\n",
        "code_toks_joined": "compare_notebooks ( nb1 , nb2 , <STRING> ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'Rmd'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3eafacb5223c44aaa0f68a838ab76bc0": {
        "code_string": "if isinstance(output, TextIOBase):\n             self._err = error\n         else:\n             self._err = TextIOWrapper(error)\n",
        "code_toks_joined": "if isinstance ( output , TextIOBase ) : <NEWLINE> <INDENT> self . _err = error <NEWLINE> else : <NEWLINE> self . _err = TextIOWrapper ( error ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7ae7496237da4dad8eb24fbd23af78eb": {
        "code_string": "if not ignore_options:\n         options = (\n             Path('/etc/machine-id'),\n             failover,  # always read to see if we somehow managed to persist this\n         )\n         for option in options:\n             if (option.exists() and\n                 os.access(options, os.R_OK) and\n                 option.stat().st_size > 0):\n                     return option\n",
        "code_toks_joined": "if not ignore_options : <NEWLINE> <INDENT> options = ( <NEWLINE> <INDENT> Path ( <STRING> ) , <NEWLINE> failover , <COMMENT> <NEWLINE> <DEDENT> ) <NEWLINE> for option in options : <NEWLINE> <INDENT> if ( option . exists ( ) and <NEWLINE> <INDENT> os . access ( options , os . R_OK ) and <NEWLINE> option . stat ( ) . st_size > 0 ) : <NEWLINE> <INDENT> return option <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/etc/machine-id'"
            ],
            "<COMMENT>": [
                "# always read to see if we somehow managed to persist this"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "33fca876f1334d45bd9347080f70c94c": {
        "code_string": "if batches > 1:\n",
        "code_toks_joined": "if batches > 1 : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ece6cff8e8fb4af38513dc3cea77485b": {
        "code_string": "def test_estimate_virtual_op_num2(self):\n         account = self.account\n         h_all_raw = []\n         for h in account.history(raw_output=False):\n             h_all_raw.append(h)\n         last_block = h_all_raw[0][\"block\"]\n         i = 1\n         for op in h_all_raw[1:]:\n             new_block = op[\"block\"]\n             block_num = last_block + int((new_block - last_block) / 2)\n             op_num = account.estimate_virtual_op_num(block_num, stop_diff=0.1, max_count=100)\n             if op_num > 0:\n                 op_num -= 1\n             self.assertTrue(op_num < i)\n             i += 1\n             last_block = new_block\n",
        "code_toks_joined": "def test_estimate_virtual_op_num2 ( self ) : <NEWLINE> <INDENT> account = self . account <NEWLINE> h_all_raw = [ ] <NEWLINE> for h in account . history ( raw_output = False ) : <NEWLINE> <INDENT> h_all_raw . append ( h ) <NEWLINE> <DEDENT> last_block = h_all_raw [ 0 ] [ <STRING> ] <NEWLINE> i = 1 <NEWLINE> for op in h_all_raw [ 1 : ] : <NEWLINE> <INDENT> new_block = op [ <STRING> ] <NEWLINE> block_num = last_block + int ( ( new_block - last_block ) / 2 ) <NEWLINE> op_num = account . estimate_virtual_op_num ( block_num , stop_diff = 0.1 , max_count = 100 ) <NEWLINE> if op_num > 0 : <NEWLINE> <INDENT> op_num -= 1 <NEWLINE> <DEDENT> self . assertTrue ( op_num < i ) <NEWLINE> i += 1 <NEWLINE> last_block = new_block <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"block\"",
                "\"block\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "18efbb27d9434f6a9450c855fe8c286c": {
        "code_string": "client = get_plugin('mqtt')\n                 client.send_message(topic=self.topic, msg=msg, host=self.host,\n                                     port=self.port, username=self.username,\n                                     password=self.password, tls_cafile=self.tls_cafile,\n                                     tls_certfile=self.tls_certfile,\n                                     tls_keyfile=self.tls_keyfile,\n                                     tls_version=self.tls_version,\n                                     tls_ciphers=self.tls_ciphers)\n",
        "code_toks_joined": "client = get_plugin ( <STRING> ) <NEWLINE> <INDENT> client . send_message ( topic = self . topic , msg = msg , host = self . host , <NEWLINE> <INDENT> port = self . port , username = self . username , <NEWLINE> password = self . password , tls_cafile = self . tls_cafile , <NEWLINE> tls_certfile = self . tls_certfile , <NEWLINE> tls_keyfile = self . tls_keyfile , <NEWLINE> tls_version = self . tls_version , <NEWLINE> tls_ciphers = self . tls_ciphers ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'mqtt'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f7b57ff060744fe182d9dedf3493995c": {
        "code_string": "self._last_read = ret\n         return ret\n",
        "code_toks_joined": "self . _last_read = ret <NEWLINE> <INDENT> return ret <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0163a306bfe6413e8c6b95e563031381": {
        "code_string": "with self._init_lock:\n             if self._initialized or GPIO.getmode():\n                 return\n",
        "code_toks_joined": "with self . _init_lock : <NEWLINE> <INDENT> if self . _initialized or GPIO . getmode ( ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "32b674e4320541c39e3e2b3dcb56439f": {
        "code_string": "def process_data(self, data, new_data):\n         if new_data:\n             self.bus.post(SensorDataChangeEvent(data=data, source=self.plugin or self.__class__.__name__))\n",
        "code_toks_joined": "def process_data ( self , data , new_data ) : <NEWLINE> <INDENT> if new_data : <NEWLINE> <INDENT> self . bus . post ( SensorDataChangeEvent ( data = data , source = self . plugin or self . __class__ . __name__ ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0f65ec30992547668a9bbdcdd9ca3027": {
        "code_string": "if hasattr(nb.metadata.latex_doc, 'bibliography'):                    \n                 bib = nb.metadata.latex_doc.bibliography\n                 bib = self.resolve_path(bib, self.metapath)\n                 if not os.path.exists(bib):\n                     logging.warning('bib in metadata does not exist'\n                                     ': {}'.format(bib))\n                 else:\n                     external_files.append(bib)\n                     resources['bibliopath'] = external_files\n",
        "code_toks_joined": "if hasattr ( nb . metadata . latex_doc , <STRING> ) : <NEWLINE> <INDENT> bib = nb . metadata . latex_doc . bibliography <NEWLINE> bib = self . resolve_path ( bib , self . metapath ) <NEWLINE> if not os . path . exists ( bib ) : <NEWLINE> <INDENT> logging . warning ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( bib ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> external_files . append ( bib ) <NEWLINE> resources [ <STRING> ] = external_files <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'bibliography'",
                "'bib in metadata does not exist'",
                "': {}'",
                "'bibliopath'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "864c20b8feeb4cf0ab515a03cd17e9d1": {
        "code_string": "elif name == 'mailcount':\n             if self.thread:\n                 mailcountstring = \"(%d)\" % self.thread.get_total_messages()\n             else:\n                 mailcountstring = \"(?)\"\n             datestring = pad(mailcountstring)\n             width = len(mailcountstring)\n             mailcount_w = AttrFlipWidget(urwid.Text(mailcountstring),\n                                          struct['mailcount'])\n             part = mailcount_w\n         elif name == 'authors':\n             if self.thread:\n                 authors = self.thread.get_authors_string() or '(None)'\n             else:\n                 authors = '(None)'\n             authorsstring = pad(authors, shorten_author_string)\n             authors_w = AttrFlipWidget(urwid.Text(authorsstring),\n                                        struct['authors'])\n             width = len(authorsstring)\n             part = authors_w\n",
        "code_toks_joined": "elif name == <STRING> : <NEWLINE> <INDENT> if self . thread : <NEWLINE> <INDENT> mailcountstring = <STRING> % self . thread . get_total_messages ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> mailcountstring = <STRING> <NEWLINE> <DEDENT> datestring = pad ( mailcountstring ) <NEWLINE> width = len ( mailcountstring ) <NEWLINE> mailcount_w = AttrFlipWidget ( urwid . Text ( mailcountstring ) , <NEWLINE> <INDENT> struct [ <STRING> ] ) <NEWLINE> <DEDENT> part = mailcount_w <NEWLINE> elif name == <STRING> : <NEWLINE> if self . thread : <NEWLINE> <INDENT> authors = self . thread . get_authors_string ( ) or <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> authors = <STRING> <NEWLINE> <DEDENT> authorsstring = pad ( authors , shorten_author_string ) <NEWLINE> authors_w = AttrFlipWidget ( urwid . Text ( authorsstring ) , <NEWLINE> <INDENT> struct [ <STRING> ] ) <NEWLINE> <DEDENT> width = len ( authorsstring ) <NEWLINE> part = authors_w <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'mailcount'",
                "\"(%d)\"",
                "\"(?)\"",
                "'mailcount'",
                "'authors'",
                "'(None)'",
                "'(None)'",
                "'authors'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e98bf11daa6a4d32a679496344c0f253": {
        "code_string": "def distance(self, s0, s1):\n         if s0 is None:\n             raise TypeError(\"Argument s0 is NoneType.\")\n         if s1 is None:\n             raise TypeError(\"Argument s1 is NoneType.\")\n         if s0 == s1:\n             return 0.0\n         if len(s0) == 0:\n             return len(s1)\n         if len(s1) == 0:\n             return len(s1)\n",
        "code_toks_joined": "def distance ( self , s0 , s1 ) : <NEWLINE> <INDENT> if s0 is None : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT> if s1 is None : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT> if s0 == s1 : <NEWLINE> <INDENT> return 0.0 <NEWLINE> <DEDENT> if len ( s0 ) == 0 : <NEWLINE> <INDENT> return len ( s1 ) <NEWLINE> <DEDENT> if len ( s1 ) == 0 : <NEWLINE> <INDENT> return len ( s1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Argument s0 is NoneType.\"",
                "\"Argument s1 is NoneType.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6bfb86a4cf60477d93307a89b5a8a4e2": {
        "code_string": "def check_new_log():\n         log2 = open(logpath).readlines()\n         return len(log2) > 1 and log2[0].endswith(\"Started log due to SIGHUP\\n\")\n     wait_for(check_new_log)\n",
        "code_toks_joined": "def check_new_log ( ) : <NEWLINE> <INDENT> log2 = open ( logpath ) . readlines ( ) <NEWLINE> return len ( log2 ) > 1 and log2 [ 0 ] . endswith ( <STRING> ) <NEWLINE> wait_for ( check_new_log ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Started log due to SIGHUP\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a377f76ab894479fb9cf7deb37df775a": {
        "code_string": "def read(self, io_in: BufferedIOBase, otherfields: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n         vals = {}\n         for field in self.fields:\n             val = field.fieldtype.read(io_in, otherfields)\n             if val is None:\n                 # If first field fails to read, we return None.\n                 if field == self.fields[0]:\n                     return None\n                 # Might only exist with certain options available\n                 if field.option is not None:\n                     break\n                 # Otherwise, we only read part of it!\n                 raise ValueError(\"{}.{}: short read\".format(self, field))\n             vals[field.name] = val\n",
        "code_toks_joined": "def read ( self , io_in : BufferedIOBase , otherfields : Dict [ str , Any ] ) -> Optional [ Dict [ str , Any ] ] : <NEWLINE> <INDENT> vals = { } <NEWLINE> for field in self . fields : <NEWLINE> <INDENT> val = field . fieldtype . read ( io_in , otherfields ) <NEWLINE> if val is None : <NEWLINE> <COMMENT> <NL> <INDENT> if field == self . fields [ 0 ] : <NEWLINE> <INDENT> return None <NEWLINE> <COMMENT> <NL> <DEDENT> if field . option is not None : <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> raise ValueError ( <STRING> . format ( self , field ) ) <NEWLINE> <DEDENT> vals [ field . name ] = val <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If first field fails to read, we return None.",
                "# Might only exist with certain options available",
                "# Otherwise, we only read part of it!"
            ],
            "<STRING>": [
                "\"{}.{}: short read\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5e19cfbd6fff42a8bf5255e1339d90c3": {
        "code_string": "if self.tested == PyFunceble.CONFIGURATION[\"domain\"]:\n             url_to_get = \"http://%s\" & self.tested\n         else:\n             url_to_get = self.tested\n",
        "code_toks_joined": "if self . tested == PyFunceble . CONFIGURATION [ <STRING> ] : <NEWLINE> <INDENT> url_to_get = <STRING> & self . tested <NEWLINE> else : <NEWLINE> url_to_get = self . tested <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"domain\"",
                "\"http://%s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8f7897bb89f4481298e6ce0d0d8ae005": {
        "code_string": "if path_to_config.endswith(directory_separator):\n             self.path_to_config += directory_separator\n",
        "code_toks_joined": "if path_to_config . endswith ( directory_separator ) : <NEWLINE> <INDENT> self . path_to_config += directory_separator <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9f0a8c15edaf4dcf894e12122d9b939a": {
        "code_string": "if (\n                     number_of_tested == 0\n                     or list_to_test[number_of_tested - 1] == list_to_test[-1]\n                     or number_of_tested == len(list_to_test)\n                 ):\n                     # * If the number of tested is null,\n                     # or\n                     # * the last tested element is the same as the last element in the\n                     #   sequence,\n                     # or\n                     # * The number of tested is equal to the number of elements in the\n                     #   sequence,\n",
        "code_toks_joined": "if ( <NEWLINE> <INDENT> number_of_tested == 0 <NEWLINE> or list_to_test [ number_of_tested - 1 ] == list_to_test [ - 1 ] <NEWLINE> or number_of_tested == len ( list_to_test ) <NEWLINE> ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# * If the number of tested is null,",
                "# or",
                "# * the last tested element is the same as the last element in the",
                "#   sequence,",
                "# or",
                "# * The number of tested is equal to the number of elements in the",
                "#   sequence,"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a3d5bd3c3a3247809cf61fc4c50abc07": {
        "code_string": "if is_cloned_version and (\n             PyFunceble.CONFIGURATION.db_type not in not_supported_db_types\n         ):\n             destination_dir_instance.delete()\n",
        "code_toks_joined": "if is_cloned_version and ( <NEWLINE> <INDENT> PyFunceble . CONFIGURATION . db_type not in not_supported_db_types <NEWLINE> ) : <NEWLINE> destination_dir_instance . delete ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3801d2f70e194c83a1e29247f38dd640": {
        "code_string": "if not content:\n                     content += f\"{to_write}\\n\"\n                     continue\n",
        "code_toks_joined": "if not content : <NEWLINE> <INDENT> content += <STRING> <NEWLINE> continue <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"{to_write}\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8868c21dd3a144efb46228216507b1cb": {
        "code_string": "with bz2.BZ2File(bad_pkg_path, 'wb') as f:\n         f.write(\"This is a fake package\".encode())\n     assert bad_pkg_name in os.listdir(bad_pkg_root)\n     conda_mirror._validate_packages(repodata, local_repo_root)\n     assert bad_pkg_name not in os.listdir(bad_pkg_root)",
        "code_toks_joined": "with bz2 . BZ2File ( bad_pkg_path , <STRING> ) as f : <NEWLINE> <INDENT> f . write ( <STRING> . encode ( ) ) <NEWLINE> assert bad_pkg_name in os . listdir ( bad_pkg_root ) <NEWLINE> conda_mirror . _validate_packages ( repodata , local_repo_root ) <NEWLINE> assert bad_pkg_name not in os . listdir ( bad_pkg_root ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'wb'",
                "\"This is a fake package\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b931900b83d24a129c5d5e8b141fa022": {
        "code_string": "def handle_message(self, msg):\n         if self.categories.intersection(msg.categories) and msg.level > self.level:\n             self.notify(msg)\n",
        "code_toks_joined": "def handle_message ( self , msg ) : <NEWLINE> <INDENT> if self . categories . intersection ( msg . categories ) and msg . level > self . level : <NEWLINE> <INDENT> self . notify ( msg ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "83f01143a8b44a00ae893971ccf36479": {
        "code_string": "def _check_dates(self, master, files):\n         mtime = os.path.getmtime(master)\n         for f in files:\n             fpath = media_url_to_filepath(f)\n             if os.path.getmtime(fpath) > mtime:\n                 return True\n         return False",
        "code_toks_joined": "def _check_dates ( self , master , files ) : <NEWLINE> <INDENT> mtime = os . path . getmtime ( master ) <NEWLINE> for f in files : <NEWLINE> <INDENT> fpath = media_url_to_filepath ( f ) <NEWLINE> if os . path . getmtime ( fpath ) > mtime : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "15677cc634594a04a3a7a181608ddf20": {
        "code_string": "if status != 200:\n         raise WebSocketProxyException(\n             \"failed CONNECT via proxy status: %r\" + status)\n",
        "code_toks_joined": "if status != 200 : <NEWLINE> <INDENT> raise WebSocketProxyException ( <NEWLINE> <INDENT> <STRING> + status ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"failed CONNECT via proxy status: %r\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d2b644ee957b4532bd63abcac7444afa": {
        "code_string": "with open(filePathBase + \".xml\", \"w\", encoding=\"utf8\") as toFile:\n \t\twrite_header(glos, toFile, frontBackMatter)\n \t\tfor entryI, entry in enumerate(glos):\n \t\t\tif glos.isData():\n \t\t\t\tentry.save(myResDir)\n \t\t\t\tcontinue\n",
        "code_toks_joined": "with open ( filePathBase + <STRING> , <STRING> , encoding = <STRING> ) as toFile : <NEWLINE> <INDENT> write_header ( glos , toFile , frontBackMatter ) <NEWLINE> for entryI , entry in enumerate ( glos ) : <NEWLINE> <INDENT> if glos . isData ( ) : <NEWLINE> <INDENT> entry . save ( myResDir ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\".xml\"",
                "\"w\"",
                "\"utf8\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fb5c1ae677c94038b2a6f28fb126c754": {
        "code_string": "def _readersEntryGen(self) -> Iterator[BaseEntry]:\n \t\tfor reader in self._readers:\n \t\t\twordCount = 0\n \t\t\tprogressbar = False\n \t\t\tif self.ui and self._progressbar:\n \t\t\t\ttry:\n \t\t\t\t\twordCount = len(reader)\n \t\t\t\texcept Exception:\n \t\t\t\t\tlog.exception(\"\")\n \t\t\t\tif wordCount > 0:\n \t\t\t\t\tprogressbar = True\n \t\t\tif progressbar:\n \t\t\t\tself.progressInit(\"Converting\")\n \t\t\twcThreshold = wordCount // 200 + 1\n \t\t\tlastPos = 0\n \t\t\ttry:\n \t\t\t\tfor index, entry in enumerate(reader):\n \t\t\t\t\tyield entry\n \t\t\t\t\tif progressbar:\n \t\t\t\t\t\tif entry is None or wordCount > 0:\n \t\t\t\t\t\t\tif index % wcThreshold == 0:\n \t\t\t\t\t\t\t\tself.progress(index, wordCount)\n \t\t\t\t\t\t\tcontinue\n \t\t\t\t\t\tbp = entry.byteProgress()\n \t\t\t\t\t\tif bp and bp[0] > lastPos + 10000:\n \t\t\t\t\t\t\tself.progress(bp[0], bp[1], unit=\"bytes\")\n \t\t\t\t\t\t\tlastPos = bp[0]\n \t\t\tfinally:\n \t\t\t\treader.close()\n \t\t\tif progressbar:\n \t\t\t\tself.progressEnd()\n",
        "code_toks_joined": "def _readersEntryGen ( self ) -> Iterator [ BaseEntry ] : <NEWLINE> <INDENT> for reader in self . _readers : <NEWLINE> <INDENT> wordCount = 0 <NEWLINE> progressbar = False <NEWLINE> if self . ui and self . _progressbar : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> wordCount = len ( reader ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> log . exception ( <STRING> ) <NEWLINE> <DEDENT> if wordCount > 0 : <NEWLINE> <INDENT> progressbar = True <NEWLINE> <DEDENT> <DEDENT> if progressbar : <NEWLINE> <INDENT> self . progressInit ( <STRING> ) <NEWLINE> <DEDENT> wcThreshold = wordCount // 200 + 1 <NEWLINE> lastPos = 0 <NEWLINE> try : <NEWLINE> <INDENT> for index , entry in enumerate ( reader ) : <NEWLINE> <INDENT> yield entry <NEWLINE> if progressbar : <NEWLINE> <INDENT> if entry is None or wordCount > 0 : <NEWLINE> <INDENT> if index % wcThreshold == 0 : <NEWLINE> <INDENT> self . progress ( index , wordCount ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> bp = entry . byteProgress ( ) <NEWLINE> if bp and bp [ 0 ] > lastPos + 10000 : <NEWLINE> <INDENT> self . progress ( bp [ 0 ] , bp [ 1 ] , unit = <STRING> ) <NEWLINE> lastPos = bp [ 0 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> reader . close ( ) <NEWLINE> <DEDENT> if progressbar : <NEWLINE> <INDENT> self . progressEnd ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"",
                "\"Converting\"",
                "\"bytes\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4ba5979ff007447e9c25468e0ede3f57": {
        "code_string": "def __next__(self) -> BaseEntry:\n \t\tself._pos += 1\n \t\ttry:\n \t\t\treturn self._pendingEntries.pop(0)\n \t\texcept IndexError:\n \t\t\tpass\n \t\t###\n \t\ttry:\n \t\t\twordDefi = self.nextPair()\n \t\texcept StopIteration as e:\n \t\t\tif self._fileIndex < self._fileCount + 1:\n \t\t\t\tif self.openNextFile():\n \t\t\t\t\treturn self.__next__()\n \t\t\tself._wordCount = self._pos\n \t\t\traise e\n \t\tif not wordDefi:\n \t\t\treturn\n \t\tword, defi = wordDefi\n \t\t###\n \t\treturn Entry(word, defi)\n",
        "code_toks_joined": "def __next__ ( self ) -> BaseEntry : <NEWLINE> <INDENT> self . _pos += 1 <NEWLINE> try : <NEWLINE> <INDENT> return self . _pendingEntries . pop ( 0 ) <NEWLINE> <DEDENT> except IndexError : <NEWLINE> <INDENT> pass <NEWLINE> <COMMENT> <NL> <DEDENT> try : <NEWLINE> <INDENT> wordDefi = self . nextPair ( ) <NEWLINE> <DEDENT> except StopIteration as e : <NEWLINE> <INDENT> if self . _fileIndex < self . _fileCount + 1 : <NEWLINE> <INDENT> if self . openNextFile ( ) : <NEWLINE> <INDENT> return self . __next__ ( ) <NEWLINE> <DEDENT> <DEDENT> self . _wordCount = self . _pos <NEWLINE> raise e <NEWLINE> <DEDENT> if not wordDefi : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> word , defi = wordDefi <NEWLINE> <COMMENT> <NL> return Entry ( word , defi ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "###",
                "###"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e918cbf9cb894c27a20a23ac25f5de03": {
        "code_string": "if list_observed_dom:\n                 bodie['ip'] = list(set(list_observed_ip))\n",
        "code_toks_joined": "if list_observed_dom : <NEWLINE> <INDENT> bodie [ <STRING> ] = list ( set ( list_observed_ip ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ip'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "381507f89f304b549c3391dae4bbe552": {
        "code_string": "return value\n",
        "code_toks_joined": "return value <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1e8f160f9e134bfab543006ec3768912": {
        "code_string": "def get_blocks_container_html(self, errors=None):\n         help_text = getattr(self.meta, 'help_text', None)\n         if isinstance(errors, StreamBlockValidationError):\n             non_block_errors = (\n                 () if errors is None\n                 else errors.as_data()[0].params.get(NON_FIELD_ERRORS, ()))\n         else:\n             non_block_errors = errors\n         if help_text and non_block_errors:\n             return render_to_string(\n                 'wagtailadmin/block_forms/blocks_container.html',\n                 {\n                     'help_text': help_text,\n                     'non_block_errors': non_block_errors,\n                 }\n             )\n",
        "code_toks_joined": "def get_blocks_container_html ( self , errors = None ) : <NEWLINE> <INDENT> help_text = getattr ( self . meta , <STRING> , None ) <NEWLINE> if isinstance ( errors , StreamBlockValidationError ) : <NEWLINE> <INDENT> non_block_errors = ( <NEWLINE> <INDENT> ( ) if errors is None <NEWLINE> else errors . as_data ( ) [ 0 ] . params . get ( NON_FIELD_ERRORS , ( ) ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> non_block_errors = errors <NEWLINE> <DEDENT> if help_text and non_block_errors : <NEWLINE> <INDENT> return render_to_string ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> { <NEWLINE> <INDENT> <STRING> : help_text , <NEWLINE> <STRING> : non_block_errors , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'help_text'",
                "'wagtailadmin/block_forms/blocks_container.html'",
                "'help_text'",
                "'non_block_errors'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b356359fb06b4732929e81c2d2e095a0": {
        "code_string": "elif qmean == \"pca\": # Latent variables are initialised from PCA in the concatenated matrix\n                     pca = sklearn.decomposition.PCA(n_components=self.K, copy=True, whiten=True)\n                     pca.fit(s.concatenate(self.data,axis=0).T)\n                     qmean = pca.components_.T\n",
        "code_toks_joined": "elif qmean == <STRING> : <COMMENT> <NEWLINE> <INDENT> pca = sklearn . decomposition . PCA ( n_components = self . K , copy = True , whiten = True ) <NEWLINE> pca . fit ( s . concatenate ( self . data , axis = 0 ) . T ) <NEWLINE> qmean = pca . components_ . T <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"pca\""
            ],
            "<COMMENT>": [
                "# Latent variables are initialised from PCA in the concatenated matrix"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a2a21494b7c74a89a3bb6108b8398cac": {
        "code_string": "def _actuallyPause():\n             fount = self._siphon._tdrain.fount\n             self._siphon._pending.suspend()\n             if fount is None:\n                 pbpc = fount.pauseFlow()\n             else:\n                 pbpc = NoPause()\n             self._siphon._pauseBecausePauseCalled = pbpc\n",
        "code_toks_joined": "def _actuallyPause ( ) : <NEWLINE> <INDENT> fount = self . _siphon . _tdrain . fount <NEWLINE> self . _siphon . _pending . suspend ( ) <NEWLINE> if fount is None : <NEWLINE> <INDENT> pbpc = fount . pauseFlow ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pbpc = NoPause ( ) <NEWLINE> <DEDENT> self . _siphon . _pauseBecausePauseCalled = pbpc <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "be97d8e25dda4adaa29b0a5e853bc2a6": {
        "code_string": "# For every localized field copy the widget from the original field\n         # and add a css class to identify a modeltranslation widget.\n         try:\n             orig_field = db_field.translated_field\n         except AttributeError:\n             pass\n         else:\n             orig_formfield = self.formfield_for_dbfield(orig_field, **kwargs)\n             field.widget = deepcopy(orig_formfield.widget)\n             if orig_field.null and isinstance(field.widget, (forms.TextInput, forms.Textarea)):\n                 field.widget = ClearableWidgetWrapper(field.widget)\n             css_classes = field.widget.attrs.get('class', '').split(' ')\n             css_classes.append('mt')\n             # Add localized fieldname css class\n             css_classes.append(build_css_class(db_field.name, 'mt-field'))\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> orig_field = db_field . translated_field <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> orig_formfield = self . formfield_for_dbfield ( orig_field , ** kwargs ) <NEWLINE> field . widget = deepcopy ( orig_formfield . widget ) <NEWLINE> if orig_field . null and isinstance ( field . widget , ( forms . TextInput , forms . Textarea ) ) : <NEWLINE> <INDENT> field . widget = ClearableWidgetWrapper ( field . widget ) <NEWLINE> <DEDENT> css_classes = field . widget . attrs . get ( <STRING> , <STRING> ) . split ( <STRING> ) <NEWLINE> css_classes . append ( <STRING> ) <NEWLINE> <COMMENT> <NL> css_classes . append ( build_css_class ( db_field . name , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# For every localized field copy the widget from the original field",
                "# and add a css class to identify a modeltranslation widget.",
                "# Add localized fieldname css class"
            ],
            "<STRING>": [
                "'class'",
                "''",
                "' '",
                "'mt'",
                "'mt-field'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bb989cf318654d5a86d142e3f14d2002": {
        "code_string": "UI_element_group.name = xml_element.attrib[\"name\"]\n             if xml_element.attrib.has_key(\"timedelay\"):\n                 UI_element_group.timedelay = float(xml_element.attrib[\"timedelay\"])\n             if xml_element.attrib.has_key(\"parent\"):\n                 UI_element_group.parent_string = xml_element.attrib[\"parent\"]\n             if xml_element.attrib.has_key(\"start_func\"):\n                 UI_element_group.start_func = self.get_func_by_name(xml_element.attrib[\"start_func\"])\n             if xml_element.attrib.has_key(\"stop_func\"):\n                 UI_element_group.stop_func = self.get_func_by_name(xml_element.attrib[\"stop_func\"])\n             if xml_element.attrib.has_key(\"identifier\"):\n                 UI_element_group.identifier_string = xml_element.attrib[\"identifier\"]\n                 UI_element_group.identifier = identifier_parser.parse(UI_element.identifier_string, lexer=identifier_lexer)\n",
        "code_toks_joined": "UI_element_group . name = xml_element . attrib [ <STRING> ] <NEWLINE> <INDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . timedelay = float ( xml_element . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . parent_string = xml_element . attrib [ <STRING> ] <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . start_func = self . get_func_by_name ( xml_element . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . stop_func = self . get_func_by_name ( xml_element . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . identifier_string = xml_element . attrib [ <STRING> ] <NEWLINE> UI_element_group . identifier = identifier_parser . parse ( UI_element . identifier_string , lexer = identifier_lexer ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"name\"",
                "\"timedelay\"",
                "\"timedelay\"",
                "\"parent\"",
                "\"parent\"",
                "\"start_func\"",
                "\"start_func\"",
                "\"stop_func\"",
                "\"stop_func\"",
                "\"identifier\"",
                "\"identifier\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ffbd7294cd2d4f58b8a504627622e9ed": {
        "code_string": "def test_volume_path_with_non_ascii_directory(self):\n         volume = u'/F\u00fc\u00fc/data:/data'\n         container_path = config.resolve_volume_path(volume, \".\", \"test\")\n         self.assertEqual(container_path, volume)\n",
        "code_toks_joined": "def test_volume_path_with_non_ascii_directory ( self ) : <NEWLINE> <INDENT> volume = <STRING> <NEWLINE> container_path = config . resolve_volume_path ( volume , <STRING> , <STRING> ) <NEWLINE> self . assertEqual ( container_path , volume ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "u'/F\u00fc\u00fc/data:/data'",
                "\".\"",
                "\"test\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "09de6ed191ad4bd69eb4b60df77eda2e": {
        "code_string": "def test_volume_path_with_non_ascii_directory(self):\n         volume = u'/F\u00fc\u00fc/data:/data'\n         container_path = config.resolve_volume_path(volume, \".\", \"test\")\n         self.assertEqual(container_path, volume)\n",
        "code_toks_joined": "def test_volume_path_with_non_ascii_directory ( self ) : <NEWLINE> <INDENT> volume = <STRING> <NEWLINE> container_path = config . resolve_volume_path ( volume , <STRING> , <STRING> ) <NEWLINE> self . assertEqual ( container_path , volume ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "u'/F\u00fc\u00fc/data:/data'",
                "\".\"",
                "\"test\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5a818ab4743142ba9ea1557c24ee1a2f": {
        "code_string": "def _gen_asset_class(sym):\n     sym_class = str(sym).split(\"_\")\n     if len(sym_class) > 0:\n         return sym_class[1]\n     return \"STK\"\n",
        "code_toks_joined": "def _gen_asset_class ( sym ) : <NEWLINE> <INDENT> sym_class = str ( sym ) . split ( <STRING> ) <NEWLINE> if len ( sym_class ) > 0 : <NEWLINE> <INDENT> return sym_class [ 1 ] <NEWLINE> <DEDENT> return <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"_\"",
                "\"STK\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1c6691d7ef06492e8d235ab4a3d37c81": {
        "code_string": "if self.tied_weights:\n             for i in range(len(self.weights) - 1, -1, -1):\n                 h = self.hiddens[-1]\n                 a, b = self.weights[i].get_value(borrow=True).shape\n                 logging.info('tied weights from layer %d: %s x %s', i, b, a)\n                 o = theano.shared(np.zeros((b, ), FLOAT), name='b_out{}'.format(i))\n                 self.preacts.append(TT.dot(h, self.weights[i].T) + o)\n                 func = self._output_func if i == 0 else self._hidden_func\n                 self.hiddens.append(func(self.preacts[-1]))\n",
        "code_toks_joined": "if self . tied_weights : <NEWLINE> <INDENT> for i in range ( len ( self . weights ) - 1 , - 1 , - 1 ) : <NEWLINE> <INDENT> h = self . hiddens [ - 1 ] <NEWLINE> a , b = self . weights [ i ] . get_value ( borrow = True ) . shape <NEWLINE> logging . info ( <STRING> , i , b , a ) <NEWLINE> o = theano . shared ( np . zeros ( ( b , ) , FLOAT ) , name = <STRING> . format ( i ) ) <NEWLINE> self . preacts . append ( TT . dot ( h , self . weights [ i ] . T ) + o ) <NEWLINE> func = self . _output_func if i == 0 else self . _hidden_func <NEWLINE> self . hiddens . append ( func ( self . preacts [ - 1 ] ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'tied weights from layer %d: %s x %s'",
                "'b_out{}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ff5716aa11f343249125a8eca1278cfe": {
        "code_string": "size = kwargs.get('size', kwargs.get('batch_size', 32))\n         self.callable = None\n         self.batches = None\n         if len(data) == 1 and isinstance(data[0], collections.Callable):\n             self.callable = data[0]\n             if not self.number_batches:\n                 self.number_batches = size\n             logging.info('data %s: %dx mini-batches from callable',\n                          self.label, self.number_batches)\n         else:\n             shape = data[0].shape\n             axis = kwargs.get('axis', 1 if len(shape) == 3 else 0)\n             slices = [slice(None), slice(None)]\n             self.batches = []\n             i = 0\n             while i + size < shape[axis]:\n                 slices[axis] = slice(i, i + size)\n                 self.batches.append([d[tuple(slices)] for d in data])\n                 i += size\n             self.shuffle()\n             if not self.number_batches:\n                 self.number_batches = len(self.batches)\n             logging.info('data %s: %dx %s mini-batches of %s',\n                          self.label, self.number_batches, len(self.batches),\n                          ', '.join(str(x.shape) for x in self.batches[0]))\n",
        "code_toks_joined": "size = kwargs . get ( <STRING> , kwargs . get ( <STRING> , 32 ) ) <NEWLINE> <INDENT> self . callable = None <NEWLINE> self . batches = None <NEWLINE> if len ( data ) == 1 and isinstance ( data [ 0 ] , collections . Callable ) : <NEWLINE> <INDENT> self . callable = data [ 0 ] <NEWLINE> if not self . number_batches : <NEWLINE> <INDENT> self . number_batches = size <NEWLINE> <DEDENT> logging . info ( <STRING> , <NEWLINE> <INDENT> self . label , self . number_batches ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> shape = data [ 0 ] . shape <NEWLINE> axis = kwargs . get ( <STRING> , 1 if len ( shape ) == 3 else 0 ) <NEWLINE> slices = [ slice ( None ) , slice ( None ) ] <NEWLINE> self . batches = [ ] <NEWLINE> i = 0 <NEWLINE> while i + size < shape [ axis ] : <NEWLINE> <INDENT> slices [ axis ] = slice ( i , i + size ) <NEWLINE> self . batches . append ( [ d [ tuple ( slices ) ] for d in data ] ) <NEWLINE> i += size <NEWLINE> <DEDENT> self . shuffle ( ) <NEWLINE> if not self . number_batches : <NEWLINE> <INDENT> self . number_batches = len ( self . batches ) <NEWLINE> <DEDENT> logging . info ( <STRING> , <NEWLINE> <INDENT> self . label , self . number_batches , len ( self . batches ) , <NEWLINE> <STRING> . join ( str ( x . shape ) for x in self . batches [ 0 ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'size'",
                "'batch_size'",
                "'data %s: %dx mini-batches from callable'",
                "'axis'",
                "'data %s: %dx %s mini-batches of %s'",
                "', '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f2648992a9764e46a098b8bdd430ff6b": {
        "code_string": "cstr_parts = []\n         if dsn:\n             cstr_parts.append('DSN=%s' % dsn)\n         else:\n             # Only append DRIVER if DATABASE_ODBC_DSN hasn't been set\n             cstr_parts.append('DRIVER={%s}' % driver)\n             if ms_drivers.match(driver) or driver == 'FreeTDS' and \\\n                 conn_params.get('host_is_server', False):\n                 if port:\n                     host += ';PORT=%s' % port\n                 cstr_parts.append('SERVER=%s' % host)\n             else:\n                 cstr_parts.append('SERVERNAME=%s' % host)\n",
        "code_toks_joined": "cstr_parts = [ ] <NEWLINE> <INDENT> if dsn : <NEWLINE> <INDENT> cstr_parts . append ( <STRING> % dsn ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> cstr_parts . append ( <STRING> % driver ) <NEWLINE> if ms_drivers . match ( driver ) or driver == <STRING> and conn_params . get ( <STRING> , False ) : <NEWLINE> <INDENT> if port : <NEWLINE> <INDENT> host += <STRING> % port <NEWLINE> <DEDENT> cstr_parts . append ( <STRING> % host ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cstr_parts . append ( <STRING> % host ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'DSN=%s'",
                "'DRIVER={%s}'",
                "'FreeTDS'",
                "'host_is_server'",
                "';PORT=%s'",
                "'SERVER=%s'",
                "'SERVERNAME=%s'"
            ],
            "<COMMENT>": [
                "# Only append DRIVER if DATABASE_ODBC_DSN hasn't been set"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5ccdca1a24514b2e8cf37d98a8ef6059": {
        "code_string": "if not on_rtd:  # only import and set the theme if we're building docs locally\n     import sphinx_rtd_theme\n     html_theme = 'sphinx_rtd_theme'\n     html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n else:\n     html_theme = 'alabaster'\n",
        "code_toks_joined": "if not on_rtd : <COMMENT> <NEWLINE> <INDENT> import sphinx_rtd_theme <NEWLINE> html_theme = <STRING> <NEWLINE> html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] <NEWLINE> else : <NEWLINE> html_theme = <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# only import and set the theme if we're building docs locally"
            ],
            "<STRING>": [
                "'sphinx_rtd_theme'",
                "'alabaster'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d9cd40d045834eb5b50abb5227925c5f": {
        "code_string": "positions = []\n     ix = 0\n     taken_positions = set(fixed_positions.values())\n     for i in range(n_plots):\n         if i in fixed_positions:\n             positions.append(fixed_positions[i])\n         else:\n             while True:\n                 row, col = ix/n_cols, ix%n_cols\n                 if (row, col) not in taken_positions:\n                     positions.append((row, col))\n                     taken_positions.add((row, col))\n                     i+=1\n                     break\n                 ix+=1\n",
        "code_toks_joined": "positions = [ ] <NEWLINE> <INDENT> ix = 0 <NEWLINE> taken_positions = set ( fixed_positions . values ( ) ) <NEWLINE> for i in range ( n_plots ) : <NEWLINE> <INDENT> if i in fixed_positions : <NEWLINE> <INDENT> positions . append ( fixed_positions [ i ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> row , col = ix / n_cols , ix % n_cols <NEWLINE> if ( row , col ) not in taken_positions : <NEWLINE> <INDENT> positions . append ( ( row , col ) ) <NEWLINE> taken_positions . add ( ( row , col ) ) <NEWLINE> i += 1 <NEWLINE> break <NEWLINE> <DEDENT> ix += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "311a9c0cc5ac47d5ac461b5861fe0587": {
        "code_string": "report.add_error(CoordinateConsistencyError(tag, child.id, file_id,\n",
        "code_toks_joined": "report . add_error ( CoordinateConsistencyError ( tag , child . id , file_id , <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "unbalanced (){}[]"
        }
    },
    "b64cd8fe1bbf40da8aacef18dba8cc06": {
        "code_string": "class ASTCompAndOr(ASTNode):\n     op = None\n     def __init__(self, comp, comps):\n         super(ASTCompAndOr, self).__init__(comp.token)\n         self.comps = [comp]\n         for c in comps:\n             self.comps.append(comp)\n",
        "code_toks_joined": "class ASTCompAndOr ( ASTNode ) : <NEWLINE> <INDENT> op = None <NEWLINE> def __init__ ( self , comp , comps ) : <NEWLINE> <INDENT> super ( ASTCompAndOr , self ) . __init__ ( comp . token ) <NEWLINE> self . comps = [ comp ] <NEWLINE> for c in comps : <NEWLINE> <INDENT> self . comps . append ( comp ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "53a1304f2db340f3945fdd82909c7f47": {
        "code_string": "total_examples = 0\n   tf.enable_eager_execution()\n   task = t5.data.TaskRegistry.get(FLAGS.task)\n   files = task.tfds_dataset.files(FLAGS.split)\n   def _example_to_string(ex):\n     key_to_string = {}\n     for k in (\"inputs\", \"targets\"):\n       if k in ex:\n         v = ex[k].numpy()\n         key_to_string[k] = (\n             \" \".join(str(i) for i in v) if FLAGS.tokenize\n             else v.decode(\"utf-8\"))\n       else:\n         v[k] = \"\"\n     return FLAGS.format_string.format(**key_to_string)\n",
        "code_toks_joined": "total_examples = 0 <NEWLINE> <INDENT> tf . enable_eager_execution ( ) <NEWLINE> task = t5 . data . TaskRegistry . get ( FLAGS . task ) <NEWLINE> files = task . tfds_dataset . files ( FLAGS . split ) <NEWLINE> def _example_to_string ( ex ) : <NEWLINE> <INDENT> key_to_string = { } <NEWLINE> for k in ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> if k in ex : <NEWLINE> <INDENT> v = ex [ k ] . numpy ( ) <NEWLINE> key_to_string [ k ] = ( <NEWLINE> <INDENT> <STRING> . join ( str ( i ) for i in v ) if FLAGS . tokenize <NEWLINE> else v . decode ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v [ k ] = <STRING> <NEWLINE> <DEDENT> <DEDENT> return FLAGS . format_string . format ( ** key_to_string ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"inputs\"",
                "\"targets\"",
                "\" \"",
                "\"utf-8\"",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0faefb7306234ef2b669439bc89aba7d": {
        "code_string": "self['collection'] = collection\n         self['field'] = field\n         self['coercion_type'] = coercion_type\n",
        "code_toks_joined": "self [ <STRING> ] = collection <NEWLINE> <INDENT> self [ <STRING> ] = field <NEWLINE> self [ <STRING> ] = coercion_type <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'collection'",
                "'field'",
                "'coercion_type'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "db53e1923a7b4e499cc211afc3ff08fb": {
        "code_string": "if (len(modules) < 2 or\n                 modules[0].value != 'flask' and modules[1].value != 'ext'):\n                     continue\n",
        "code_toks_joined": "if ( len ( modules ) < 2 or <NEWLINE> <INDENT> modules [ 0 ] . value != <STRING> and modules [ 1 ] . value != <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'flask'",
                "'ext'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9181d06dff464c25b996566354d87fa4": {
        "code_string": "def poll(self):\n \t\tflags = []\n \t\tif self.sending:\n \t\t\tself.sending = False\n \t\t\tslist = [(self.sock,), (self.sock,), (self.sock,)]\n \t\telse:\n \t\t\tslist = [(self.sock,), (), (self.sock,)]\n \t\ttimeout = self.timer.get_timeout()\n \t\tif timeout>0:\n \t\t\tslist.append(timeout)\n \t\ttry:\n \t\t\trlist, wlist, xlist = select.select(*slist)\n \t\texcept select.error as e:\n \t\t\tprint(str(e))\n \t\t\trlist = []\n \t\t\twlist = []\n \t\t\txlist = []\n \t\tif rlist:         flags.append('SOCKET_RECV')\n \t\tif wlist:         flags.append('SOCKET_SEND')\n \t\tif xlist:         flags.append('SOCKET_ERR')\n \t\treturn flags\n",
        "code_toks_joined": "def poll ( self ) : <NEWLINE> <INDENT> flags = [ ] <NEWLINE> if self . sending : <NEWLINE> <INDENT> self . sending = False <NEWLINE> slist = [ ( self . sock , ) , ( self . sock , ) , ( self . sock , ) ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> slist = [ ( self . sock , ) , ( ) , ( self . sock , ) ] <NEWLINE> <DEDENT> timeout = self . timer . get_timeout ( ) <NEWLINE> if timeout > 0 : <NEWLINE> <INDENT> slist . append ( timeout ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> rlist , wlist , xlist = select . select ( * slist ) <NEWLINE> <DEDENT> except select . error as e : <NEWLINE> <INDENT> print ( str ( e ) ) <NEWLINE> rlist = [ ] <NEWLINE> wlist = [ ] <NEWLINE> xlist = [ ] <NEWLINE> <DEDENT> if rlist : flags . append ( <STRING> ) <NEWLINE> if wlist : flags . append ( <STRING> ) <NEWLINE> if xlist : flags . append ( <STRING> ) <NEWLINE> return flags <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'SOCKET_RECV'",
                "'SOCKET_SEND'",
                "'SOCKET_ERR'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "20d51b68bf0a4400b91c89d4ef080a64": {
        "code_string": "@pl_announce('PloaderFetch')\n class SettingsPlugin:\n \tdef __init__(self, ploader, kwargs):\n \t\tsettings = get_settings(kwargs, kwargs.get('settings', {}))\n \t\tplugin_list = settings.get('plugins', DefaultPlugins)\n \t\tplugins = []\n \t\tplugin_settings = {}\n \t\tfor plugin in plugin_list:\n \t\t\tplugins.append(plugin[1])\n \t\t\tplugin_settings[plugin[1]] = settings.get(plugin[0], {})\n \t\tploader.provides('PloaderFetch', PloaderFetch(plugins, plugin_settings))\n",
        "code_toks_joined": "@ pl_announce ( <STRING> ) <NEWLINE> <INDENT> class SettingsPlugin : <NEWLINE> <INDENT> def __init__ ( self , ploader , kwargs ) : <NEWLINE> <INDENT> settings = get_settings ( kwargs , kwargs . get ( <STRING> , { } ) ) <NEWLINE> plugin_list = settings . get ( <STRING> , DefaultPlugins ) <NEWLINE> plugins = [ ] <NEWLINE> plugin_settings = { } <NEWLINE> for plugin in plugin_list : <NEWLINE> <INDENT> plugins . append ( plugin [ 1 ] ) <NEWLINE> plugin_settings [ plugin [ 1 ] ] = settings . get ( plugin [ 0 ] , { } ) <NEWLINE> <DEDENT> ploader . provides ( <STRING> , PloaderFetch ( plugins , plugin_settings ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'PloaderFetch'",
                "'settings'",
                "'plugins'",
                "'PloaderFetch'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bc87554f58504c30acd93db796c77eb1": {
        "code_string": "class StartPlugin:\n \tdef __init__(self, ploader, settings):\n \t\tself.settings = utils.get_settings(settings, default_settings)\n \t\tself.event = ploader.requires('Event')\n \t\tself.net = ploader.requires('Net')\n \t\tself.auth = ploader.requires('Auth')\n",
        "code_toks_joined": "class StartPlugin : <NEWLINE> <INDENT> def __init__ ( self , ploader , settings ) : <NEWLINE> <INDENT> self . settings = utils . get_settings ( settings , default_settings ) <NEWLINE> self . event = ploader . requires ( <STRING> ) <NEWLINE> self . net = ploader . requires ( <STRING> ) <NEWLINE> self . auth = ploader . requires ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Event'",
                "'Net'",
                "'Auth'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "78d4515b4abb4ba4b1ac3b76cbf3c823": {
        "code_string": "def _find_left_index(ss_waypoints, s):\n     \"\"\"Return the index of the largest entry in `ss_waypoints` that is\n     larger or equal `s`.\n     \"\"\"\n     for i in range(1, len(ss_waypoints)):\n         if ss_waypoints[i - 1] <= s and s < ss_waypoints[i]:\n             return i - 1\n     return len(ss_waypoints) - 1\n",
        "code_toks_joined": "def _find_left_index ( ss_waypoints , s ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for i in range ( 1 , len ( ss_waypoints ) ) : <NEWLINE> <INDENT> if ss_waypoints [ i - 1 ] <= s and s < ss_waypoints [ i ] : <NEWLINE> <INDENT> return i - 1 <NEWLINE> <DEDENT> <DEDENT> return len ( ss_waypoints ) - 1 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return the index of the largest entry in `ss_waypoints` that is\n     larger or equal `s`.\n     \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "66f2a4be73be4d69a0110c2138150a90": {
        "code_string": "# We have the optimized version\n         try:\n             ret = _rdparm.rdparm(fname)\n         except TypeError:\n             # This is raised if VERSION is not found\n             raise\n             return self.rdparm_old(open(fname, 'r').readlines())\n         else:\n             # Unpack returned contents\n             parm_data, parm_comments, formats, unkflg, flag_list, version = ret\n             # Now assign them to instance attributes and process where necessary\n             self.parm_data = parm_data\n             self.parm_comments = parm_comments\n             for key in formats:\n                 self.formats[key] = FortranFormat(formats[key])\n             self.flag_list = flag_list\n             self.version = version\n             # Now we have to process all of those sections that the optimized\n             # parser couldn't figure out\n             for flag in unkflg:\n                 rawdata = self.parm_data[flag]\n                 self.parm_data[flag] = []\n                 for line in rawdata:\n                     self.parm_data[flag].extend(self.formats[key].read(line))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> ret = _rdparm . rdparm ( fname ) <NEWLINE> <DEDENT> except TypeError : <NEWLINE> <COMMENT> <NL> <INDENT> raise <NEWLINE> return self . rdparm_old ( open ( fname , <STRING> ) . readlines ( ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> parm_data , parm_comments , formats , unkflg , flag_list , version = ret <NEWLINE> <COMMENT> <NL> self . parm_data = parm_data <NEWLINE> self . parm_comments = parm_comments <NEWLINE> for key in formats : <NEWLINE> <INDENT> self . formats [ key ] = FortranFormat ( formats [ key ] ) <NEWLINE> <DEDENT> self . flag_list = flag_list <NEWLINE> self . version = version <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> for flag in unkflg : <NEWLINE> <INDENT> rawdata = self . parm_data [ flag ] <NEWLINE> self . parm_data [ flag ] = [ ] <NEWLINE> for line in rawdata : <NEWLINE> <INDENT> self . parm_data [ flag ] . extend ( self . formats [ key ] . read ( line ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# We have the optimized version",
                "# This is raised if VERSION is not found",
                "# Unpack returned contents",
                "# Now assign them to instance attributes and process where necessary",
                "# Now we have to process all of those sections that the optimized",
                "# parser couldn't figure out"
            ],
            "<STRING>": [
                "'r'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "513421263ace443b90bf3004f825beac": {
        "code_string": "if isinstance(timeout, (int, long, float)):\n             raise TypeError(\"Timeout must be numeric.\")\n         self.timeout = timeout\n",
        "code_toks_joined": "if isinstance ( timeout , ( int , long , float ) ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> self . timeout = timeout <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Timeout must be numeric.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7de918e8bfba475a8236cdd2a71cd829": {
        "code_string": "if order:\n             kwargs.append(('order', order))\n",
        "code_toks_joined": "if order : <NEWLINE> <INDENT> kwargs . append ( ( <STRING> , order ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'order'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "394198458bd549baa629d6ba7237e35a": {
        "code_string": "# check if coefficients are different\n                 if \"{}\".format(Cal.coefficients) == \"{}\".format(coefficients):\n                     logging.debug(\"Coefficients are different, updating...\")\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if <STRING> . format ( Cal . coefficients ) == <STRING> . format ( coefficients ) : <NEWLINE> <INDENT> logging . debug ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# check if coefficients are different"
            ],
            "<STRING>": [
                "\"{}\"",
                "\"{}\"",
                "\"Coefficients are different, updating...\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d02acc0e52fd41f4b721ee86666e91c8": {
        "code_string": "for index, parent_field_name in enumerate(parent_field_names):\n             dictobj = dictobj.get(parent_field_name)\n             if not dictobj:\n                 # Point to which level doesn't exist exactly\n                 return handle_missing_field(\n                     \"__\".join(level_field_names[:index+1])\n                 )\n         if final_field_name not in dictobj:\n             return handle_missing_field(field_name)\n         return dictobj.get(final_field_name)\n",
        "code_toks_joined": "for index , parent_field_name in enumerate ( parent_field_names ) : <NEWLINE> <INDENT> dictobj = dictobj . get ( parent_field_name ) <NEWLINE> if not dictobj : <NEWLINE> <COMMENT> <NL> <INDENT> return handle_missing_field ( <NEWLINE> <INDENT> <STRING> . join ( level_field_names [ : index + 1 ] ) <NEWLINE> <DEDENT> ) <NEWLINE> if final_field_name not in dictobj : <NEWLINE> <DEDENT> return handle_missing_field ( field_name ) <NEWLINE> return dictobj . get ( final_field_name ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Point to which level doesn't exist exactly"
            ],
            "<STRING>": [
                "\"__\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "defce51404fb4e68addaebc48ad1de01": {
        "code_string": "psw.grid(row=0,column=3,sticky=tkinter.W)\n             pbndl = Label(paramf,text='bounds:',width=10,anchor='e')\n             pbndl.grid(row=1,column=0,sticky=tkinter.E)\n             pbnde1 = Entry(paramf,width=8) \n             pbnde2 = Entry(paramf,width=8)\n             pbnde1.grid(row=1,column=1,sticky=tkinter.W) \n             pbnde2.grid(row=1,column=2,sticky=tkinter.W) \n             lbnd = xrsdkit.param_bound_defaults[param_nm][0]\n             ubnd = xrsdkit.param_bound_defaults[param_nm][1] \n             if xrsdkit.contains_param(self.inputs['param_bounds'],pop_nm,param_nm):\n                 lbnd = self.inputs['param_bounds'][pop_nm]['parameters'][param_nm][0]\n                 lbnd = self.inputs['param_bounds'][pop_nm]['parameters'][param_nm][1]\n             # TODO: the bounds entries need to be connected to DoubleVars.\n             pbnde1.insert(0,str(lbnd))  \n             pbnde2.insert(0,str(ubnd))\n             pexpl = Label(paramf,text='expression:',width=10,anchor='e')\n             pexpl.grid(row=2,column=0,sticky=tkinter.E)\n             pexpe = Entry(paramf,width=16)\n             if xrsdkit.contains_param(self.inputs['param_constraints'],pop_nm,param_nm):\n                 pexpe.insert(0,self.inputs['param_constraints'],pop_nm,param_nm)\n             # TODO: the constraint Entry needs to be connected to a StringVar \n             pexpe.grid(row=2,column=1,columnspan=3,sticky=tkinter.E+tkinter.W) \n             # TODO: connect psw to changing fixed_params\n             # TODO: connect pbnde to changing param_bounds\n             # TODO: connect pexpe to setting param_constraints\n             paramf.grid(row=4+nstgs+ip,column=0,columnspan=4,sticky=tkinter.E+tkinter.W)\n",
        "code_toks_joined": "psw . grid ( row = 0 , column = 3 , sticky = tkinter . W ) <NEWLINE> <INDENT> pbndl = Label ( paramf , text = <STRING> , width = 10 , anchor = <STRING> ) <NEWLINE> pbndl . grid ( row = 1 , column = 0 , sticky = tkinter . E ) <NEWLINE> pbnde1 = Entry ( paramf , width = 8 ) <NEWLINE> pbnde2 = Entry ( paramf , width = 8 ) <NEWLINE> pbnde1 . grid ( row = 1 , column = 1 , sticky = tkinter . W ) <NEWLINE> pbnde2 . grid ( row = 1 , column = 2 , sticky = tkinter . W ) <NEWLINE> lbnd = xrsdkit . param_bound_defaults [ param_nm ] [ 0 ] <NEWLINE> ubnd = xrsdkit . param_bound_defaults [ param_nm ] [ 1 ] <NEWLINE> if xrsdkit . contains_param ( self . inputs [ <STRING> ] , pop_nm , param_nm ) : <NEWLINE> <INDENT> lbnd = self . inputs [ <STRING> ] [ pop_nm ] [ <STRING> ] [ param_nm ] [ 0 ] <NEWLINE> lbnd = self . inputs [ <STRING> ] [ pop_nm ] [ <STRING> ] [ param_nm ] [ 1 ] <NEWLINE> <COMMENT> <NL> <DEDENT> pbnde1 . insert ( 0 , str ( lbnd ) ) <NEWLINE> pbnde2 . insert ( 0 , str ( ubnd ) ) <NEWLINE> pexpl = Label ( paramf , text = <STRING> , width = 10 , anchor = <STRING> ) <NEWLINE> pexpl . grid ( row = 2 , column = 0 , sticky = tkinter . E ) <NEWLINE> pexpe = Entry ( paramf , width = 16 ) <NEWLINE> if xrsdkit . contains_param ( self . inputs [ <STRING> ] , pop_nm , param_nm ) : <NEWLINE> <INDENT> pexpe . insert ( 0 , self . inputs [ <STRING> ] , pop_nm , param_nm ) <NEWLINE> <COMMENT> <NL> <DEDENT> pexpe . grid ( row = 2 , column = 1 , columnspan = 3 , sticky = tkinter . E + tkinter . W ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> paramf . grid ( row = 4 + nstgs + ip , column = 0 , columnspan = 4 , sticky = tkinter . E + tkinter . W ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'bounds:'",
                "'e'",
                "'param_bounds'",
                "'param_bounds'",
                "'parameters'",
                "'param_bounds'",
                "'parameters'",
                "'expression:'",
                "'e'",
                "'param_constraints'",
                "'param_constraints'"
            ],
            "<COMMENT>": [
                "# TODO: the bounds entries need to be connected to DoubleVars.",
                "# TODO: the constraint Entry needs to be connected to a StringVar ",
                "# TODO: connect psw to changing fixed_params",
                "# TODO: connect pbnde to changing param_bounds",
                "# TODO: connect pexpe to setting param_constraints"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b1851c3103884f29874a92be53a9d7b7": {
        "code_string": "def getinfo(self, path, namespaces=None):\n         # type: (Text, Optional[Collection[Text]]) -> Info\n         self.check()\n         namespaces = namespaces or ()\n         _path = self.validatepath(path)\n         sys_path = self.getsyspath(_path)\n         _lstat = None\n         with convert_os_errors(\"getinfo\", path):\n             _stat = os.stat(fsencode(sys_path))\n             if \"lstat\" in namespaces:\n                 _stat = os.lstat(fsencode(sys_path))\n",
        "code_toks_joined": "def getinfo ( self , path , namespaces = None ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . check ( ) <NEWLINE> namespaces = namespaces or ( ) <NEWLINE> _path = self . validatepath ( path ) <NEWLINE> sys_path = self . getsyspath ( _path ) <NEWLINE> _lstat = None <NEWLINE> with convert_os_errors ( <STRING> , path ) : <NEWLINE> <INDENT> _stat = os . stat ( fsencode ( sys_path ) ) <NEWLINE> if <STRING> in namespaces : <NEWLINE> <INDENT> _stat = os . lstat ( fsencode ( sys_path ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# type: (Text, Optional[Collection[Text]]) -> Info"
            ],
            "<STRING>": [
                "\"getinfo\"",
                "\"lstat\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f9f5eb5bf564bdcb2f9aee455bdf5c4": {
        "code_string": "pages -= 1\n         if pages == 0:\n             return\n",
        "code_toks_joined": "pages -= 1 <NEWLINE> <INDENT> if pages == 0 : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4d07a20906f2447ba7f828755f77ae56": {
        "code_string": "def test_search_members_1(self):\n         self.client.login(username='testsu', password='testpass')\n         member_1 = self.foo_list.subscribe(\n                 'member-1@example.com', pre_verified=True, pre_confirmed=True,\n                 pre_approved=True)\n         member_2 = self.foo_list.subscribe(\n                 'member-2@example.com', pre_verified=True, pre_confirmed=True,\n                 pre_approved=True)\n         response = self.client.get(reverse(\n             'list_members', args=['foo@example.com', 'subscriber']),\n             {'q': 'example.com'})\n         self.assertEqual(response.status_code, 200)\n         self.assertEqual(len(response.context['members']), 2)\n         self.assertContains(response, member_1.email)\n         self.assertContains(response, member_2.email)\n         response = self.client.get(reverse(\n             'list_members', args=['foo@example.com', 'subscriber']),\n             {'q': 'member-1'})\n         self.assertEqual(response.status_code, 200)\n         self.assertEqual(len(response.context['members']), 1)\n         self.assertContains(response, member_1.email)\n         self.assertNotContains(response, member_2.email)\n         response = self.client.get(reverse(\n             'list_members', args=['foo@example.com', 'subscriber']),\n             {'q': 'not_a_member'})\n         self.assertEqual(response.status_code, 200)\n         self.assertEqual(len(response.context['members']), 0)\n         self.assertNotContains(response, member_2.email)\n         self.assertNotContains(response, member_2.email)\n",
        "code_toks_joined": "def test_search_members_1 ( self ) : <NEWLINE> <INDENT> self . client . login ( username = <STRING> , password = <STRING> ) <NEWLINE> member_1 = self . foo_list . subscribe ( <NEWLINE> <INDENT> <STRING> , pre_verified = True , pre_confirmed = True , <NEWLINE> pre_approved = True ) <NEWLINE> <DEDENT> member_2 = self . foo_list . subscribe ( <NEWLINE> <INDENT> <STRING> , pre_verified = True , pre_confirmed = True , <NEWLINE> pre_approved = True ) <NEWLINE> <DEDENT> response = self . client . get ( reverse ( <NEWLINE> <INDENT> <STRING> , args = [ <STRING> , <STRING> ] ) , <NEWLINE> { <STRING> : <STRING> } ) <NEWLINE> <DEDENT> self . assertEqual ( response . status_code , 200 ) <NEWLINE> self . assertEqual ( len ( response . context [ <STRING> ] ) , 2 ) <NEWLINE> self . assertContains ( response , member_1 . email ) <NEWLINE> self . assertContains ( response , member_2 . email ) <NEWLINE> response = self . client . get ( reverse ( <NEWLINE> <INDENT> <STRING> , args = [ <STRING> , <STRING> ] ) , <NEWLINE> { <STRING> : <STRING> } ) <NEWLINE> <DEDENT> self . assertEqual ( response . status_code , 200 ) <NEWLINE> self . assertEqual ( len ( response . context [ <STRING> ] ) , 1 ) <NEWLINE> self . assertContains ( response , member_1 . email ) <NEWLINE> self . assertNotContains ( response , member_2 . email ) <NEWLINE> response = self . client . get ( reverse ( <NEWLINE> <INDENT> <STRING> , args = [ <STRING> , <STRING> ] ) , <NEWLINE> { <STRING> : <STRING> } ) <NEWLINE> <DEDENT> self . assertEqual ( response . status_code , 200 ) <NEWLINE> self . assertEqual ( len ( response . context [ <STRING> ] ) , 0 ) <NEWLINE> self . assertNotContains ( response , member_2 . email ) <NEWLINE> self . assertNotContains ( response , member_2 . email ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'testsu'",
                "'testpass'",
                "'member-1@example.com'",
                "'member-2@example.com'",
                "'list_members'",
                "'foo@example.com'",
                "'subscriber'",
                "'q'",
                "'example.com'",
                "'members'",
                "'list_members'",
                "'foo@example.com'",
                "'subscriber'",
                "'q'",
                "'member-1'",
                "'members'",
                "'list_members'",
                "'foo@example.com'",
                "'subscriber'",
                "'q'",
                "'not_a_member'",
                "'members'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0b1639485c844cb6b2972c4e67769157": {
        "code_string": "sign = '+' if total_minutes > 0 else '-'\n             total_minutes = abs(total_minutes)\n             hour, minute = divmod(total_minutes, 60)\n",
        "code_toks_joined": "sign = <STRING> if total_minutes > 0 else <STRING> <NEWLINE> <INDENT> total_minutes = abs ( total_minutes ) <NEWLINE> hour , minute = divmod ( total_minutes , 60 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'+'",
                "'-'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6d0ee253f15f4735a95363a4e9d583b7": {
        "code_string": "def calculate_anomaly_score(self, log):\n         \"\"\"Compute a distance of a log entry to elements of SOM.\"\"\"\n         # convert log into vector using same word2vec model (here just going to grab from existing)\n         dist_smallest = np.inf\n         for x in range(self.model.shape[0]):\n             for y in range(self.model.shape[1]):\n                 dist = np.linalg.norm(self.model[x][y] - log)\n                 if dist < dist_smallest:\n                     dist_smallest = dist\n         return dist\n",
        "code_toks_joined": "def calculate_anomaly_score ( self , log ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> dist_smallest = np . inf <NEWLINE> for x in range ( self . model . shape [ 0 ] ) : <NEWLINE> <INDENT> for y in range ( self . model . shape [ 1 ] ) : <NEWLINE> <INDENT> dist = np . linalg . norm ( self . model [ x ] [ y ] - log ) <NEWLINE> if dist < dist_smallest : <NEWLINE> <INDENT> dist_smallest = dist <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return dist <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Compute a distance of a log entry to elements of SOM.\"\"\""
            ],
            "<COMMENT>": [
                "# convert log into vector using same word2vec model (here just going to grab from existing)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "04907a354ab94aa299c1a887b7f3ccf3": {
        "code_string": "if isinstance(cieobs,str):\n         cmfs = _CMF[cieobs]['bar']\n     else:\n         cmfs = cieobs\n     cmfs = cmfs[:,cmfs[1:].sum(axis=1)>0] # avoid div by zero in xyz-to-Yxy conversion\n",
        "code_toks_joined": "if isinstance ( cieobs , str ) : <NEWLINE> <INDENT> cmfs = _CMF [ cieobs ] [ <STRING> ] <NEWLINE> else : <NEWLINE> cmfs = cieobs <NEWLINE> cmfs = cmfs [ : , cmfs [ 1 : ] . sum ( axis = 1 ) > 0 ] <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'bar'"
            ],
            "<COMMENT>": [
                "# avoid div by zero in xyz-to-Yxy conversion"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ade96da48aaa43bc906e05d3014476d3": {
        "code_string": "# Processing of the item-link items. They get added as additional relationships\n     # to the existing items.\n     for node in doctree.traverse(ItemLink):\n         for source in node['sources']:\n             for target in node['targets']:\n                 try:\n                     env.traceability_collection.add_relation(target, node['type'], source)\n                 except TraceabilityException as err:\n                     report_warning(env, err, env.docname, self.lineno)\n         # The ItemLink node has no final representation, so is removed from the tree\n         node.replace_self([])\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> for node in doctree . traverse ( ItemLink ) : <NEWLINE> <INDENT> for source in node [ <STRING> ] : <NEWLINE> <INDENT> for target in node [ <STRING> ] : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> env . traceability_collection . add_relation ( target , node [ <STRING> ] , source ) <NEWLINE> <DEDENT> except TraceabilityException as err : <NEWLINE> <INDENT> report_warning ( env , err , env . docname , self . lineno ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> node . replace_self ( [ ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Processing of the item-link items. They get added as additional relationships",
                "# to the existing items.",
                "# The ItemLink node has no final representation, so is removed from the tree"
            ],
            "<STRING>": [
                "'sources'",
                "'targets'",
                "'type'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "05cca302b3124d6d96c40e4d5053baf8": {
        "code_string": "def run(self, args):\n         if len(args) >= 1 and 'reset' == args[0]:\n             highlight_type = self.get_highlight_type(args[1])\n             if not highlight_type: return\n             clear_file_format_cache()\n         else:\n             highlight_type = self.get_highlight_type(args[0])\n             if not highlight_type: return\n             if 'off' == highlight_type: highlight_type = 'plain'\n             pass\n         self.debugger.settings['highlight'] = highlight_type\n         show_cmd = self.proc.commands['show']\n         show_cmd.run(['show', 'highlight'])\n         return\n     pass\n",
        "code_toks_joined": "def run ( self , args ) : <NEWLINE> <INDENT> if len ( args ) >= 1 and <STRING> == args [ 0 ] : <NEWLINE> <INDENT> highlight_type = self . get_highlight_type ( args [ 1 ] ) <NEWLINE> if not highlight_type : return <NEWLINE> clear_file_format_cache ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> highlight_type = self . get_highlight_type ( args [ 0 ] ) <NEWLINE> if not highlight_type : return <NEWLINE> if <STRING> == highlight_type : highlight_type = <STRING> <NEWLINE> pass <NEWLINE> <DEDENT> self . debugger . settings [ <STRING> ] = highlight_type <NEWLINE> show_cmd = self . proc . commands [ <STRING> ] <NEWLINE> show_cmd . run ( [ <STRING> , <STRING> ] ) <NEWLINE> return <NEWLINE> pass <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'reset'",
                "'off'",
                "'plain'",
                "'highlight'",
                "'show'",
                "'show'",
                "'highlight'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "512471a2eacd40f38ebd0d5324fd5818": {
        "code_string": "class TestSkip(unittest.TestCase):\n     @unittest.skipIf('TRAVIS' not in os.environ,\n                      \"FIXME: figure out why this doesn't work in travis\")\n     def test_skip(self):\n",
        "code_toks_joined": "class TestSkip ( unittest . TestCase ) : <NEWLINE> <INDENT> @ unittest . skipIf ( <STRING> not in os . environ , <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> def test_skip ( self ) : <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'TRAVIS'",
                "\"FIXME: figure out why this doesn't work in travis\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "206c6f4cd37345a08774ec17e8fb3a57": {
        "code_string": "b1 = 0x80 | (opcode & 0x0f) # FIN + opcode\n         payload_len = len(buf)\n         if payload_len <= 125:\n             header = struct.pack('>BB', b1, payload_len)\n         elif payload_len > 125 and payload_len <= 65536:\n             header = struct.pack('>BBH', b1, 126, payload_len)\n         elif payload_len >= 65536:\n             header = struct.pack('>BBQ', b1, 127, payload_len)\n",
        "code_toks_joined": "b1 = 0x80 | ( opcode & 0x0f ) <COMMENT> <NEWLINE> <INDENT> payload_len = len ( buf ) <NEWLINE> if payload_len <= 125 : <NEWLINE> <INDENT> header = struct . pack ( <STRING> , b1 , payload_len ) <NEWLINE> <DEDENT> elif payload_len > 125 and payload_len <= 65536 : <NEWLINE> <INDENT> header = struct . pack ( <STRING> , b1 , 126 , payload_len ) <NEWLINE> <DEDENT> elif payload_len >= 65536 : <NEWLINE> <INDENT> header = struct . pack ( <STRING> , b1 , 127 , payload_len ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# FIN + opcode"
            ],
            "<STRING>": [
                "'>BB'",
                "'>BBH'",
                "'>BBQ'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cf04aee4c60f4a679872b25f8037fc52": {
        "code_string": "if (collect_dynamic is False):\n             dyn_state.append(self._states);\n             dyn_time.append(t);\n",
        "code_toks_joined": "if ( collect_dynamic is False ) : <NEWLINE> <INDENT> dyn_state . append ( self . _states ) ; <NEWLINE> dyn_time . append ( t ) ; <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "da52816366fa4dfa85a6885787543acd": {
        "code_string": "if distance < self._threshold:\n             self.__append_to_cluster(index_cluster, index_point, point);\n         elif distance > self._threshold2:\n             self.__allocate_cluster(index_point, point);\n",
        "code_toks_joined": "if distance < self . _threshold : <NEWLINE> <INDENT> self . __append_to_cluster ( index_cluster , index_point , point ) ; <NEWLINE> elif distance > self . _threshold2 : <NEWLINE> self . __allocate_cluster ( index_point , point ) ; <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f15551d7307845a89e58e61b81c65e49": {
        "code_string": "if expected_cluster_length is not None:\n             assertion.eq(len(centers), len(expected_cluster_length))\n",
        "code_toks_joined": "if expected_cluster_length is not None : <NEWLINE> <INDENT> assertion . eq ( len ( centers ) , len ( expected_cluster_length ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "268dbc2b980140d2a4c8ff82ac2ab4ad": {
        "code_string": "assert car1 < car2\n         assert car2 < car3\n         assert car3 < car2\n         assert car4 > car3\n         assert car1.antecedent <= transaction1\n         assert car2.antecedent <= transaction1\n         assert car3.antecedent <= transaction1\n         assert not car4.antecedent <= transaction1\n         assert sorted_cars[0] == car4\n",
        "code_toks_joined": "assert car1 < car2 <NEWLINE> <INDENT> assert car2 < car3 <NEWLINE> assert car3 < car2 <NEWLINE> assert car4 > car3 <NEWLINE> assert car1 . antecedent <= transaction1 <NEWLINE> assert car2 . antecedent <= transaction1 <NEWLINE> assert car3 . antecedent <= transaction1 <NEWLINE> assert not car4 . antecedent <= transaction1 <NEWLINE> assert sorted_cars [ 0 ] == car4 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ad43164f4ad4460ba920c536b306260b": {
        "code_string": "t1.merge(t2)\n         self.assertEqual(t1.ugettext(\"Hello\"), \"\u0412\u0456\u0442\u0430\u044e\")\n         self.assertEqual(t2.ugettext(\"Good bye\"), \"\u0414\u043e \u0437\u0443\u0441\u0442\u0440\u0456\u0447\u0456\")\n",
        "code_toks_joined": "t1 . merge ( t2 ) <NEWLINE> <INDENT> self . assertEqual ( t1 . ugettext ( <STRING> ) , <STRING> ) <NEWLINE> self . assertEqual ( t2 . ugettext ( <STRING> ) , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Hello\"",
                "\"\u0412\u0456\u0442\u0430\u044e\"",
                "\"Good bye\"",
                "\"\u0414\u043e \u0437\u0443\u0441\u0442\u0440\u0456\u0447\u0456\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c8e9290e54c2464184bb257d7b69f008": {
        "code_string": "if len(cache) == 0:\n             return False, \"missing cache path\"\n         if len(cache) == 0:\n             return False, \"missing cache data\"\n         try:\n             if not path.isdir(cls.directory):\n                 makedirs(cls.directory)\n             with open(cls.file_path(cache_path), \"wb\") as f:\n                 size = f.write(cache)\n                 return True, size\n         except Exception as e:\n             return False, str(e)\n         return False, \"no cache to write\"\n",
        "code_toks_joined": "if len ( cache ) == 0 : <NEWLINE> <INDENT> return False , <STRING> <NEWLINE> if len ( cache ) == 0 : <NEWLINE> return False , <STRING> <NEWLINE> try : <NEWLINE> if not path . isdir ( cls . directory ) : <NEWLINE> <INDENT> makedirs ( cls . directory ) <NEWLINE> <DEDENT> with open ( cls . file_path ( cache_path ) , <STRING> ) as f : <NEWLINE> <INDENT> size = f . write ( cache ) <NEWLINE> return True , size <NEWLINE> except Exception as e : <NEWLINE> <DEDENT> return False , str ( e ) <NEWLINE> return False , <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"missing cache path\"",
                "\"missing cache data\"",
                "\"wb\"",
                "\"no cache to write\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8504dbb5071e45448d183704bdbac5f7": {
        "code_string": "def status_info(self):\n         print('this is {} in busying'.format(len(self.busying)))\n         urls = []\n         for i, url in enumerate(self.busying):\n             if i > 3:\n                 break\n             urls.append(url)\n",
        "code_toks_joined": "def status_info ( self ) : <NEWLINE> <INDENT> print ( <STRING> . format ( len ( self . busying ) ) ) <NEWLINE> urls = [ ] <NEWLINE> for i , url in enumerate ( self . busying ) : <NEWLINE> <INDENT> if i > 3 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> urls . append ( url ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'this is {} in busying'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "25797f7a21f242219888cd8d1c37319a": {
        "code_string": "boxes = []\n     for i, c in enumerate(class_values):\n         box = patches.Rectangle((0, 0), 20, 10, linewidth=.4, edgecolor=colors['rect_edge'],\n                                 facecolor=color_map[c], label=class_names[c])\n         boxes.append(box)\n",
        "code_toks_joined": "boxes = [ ] <NEWLINE> <INDENT> for i , c in enumerate ( class_values ) : <NEWLINE> <INDENT> box = patches . Rectangle ( ( 0 , 0 ) , 20 , 10 , linewidth = .4 , edgecolor = colors [ <STRING> ] , <NEWLINE> <INDENT> facecolor = color_map [ c ] , label = class_names [ c ] ) <NEWLINE> <DEDENT> boxes . append ( box ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'rect_edge'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5ba53580553e4c72bc04c0fbdad0dc73": {
        "code_string": "lcolor = rcolor = colors['arrow']\n         lpw = rpw = \"0.3\"\n         if node.left.id in highlight_path:\n             lcolor = colors['highlight']\n             lpw = \"1.2\"\n         if node.right.id in highlight_path:\n             lcolor = colors['highlight']\n             rpw = \"1.2\"\n         edges.append( f'{nname} -> {left_node_name} [penwidth={lpw} color=\"{lcolor}\" label=<{llabel}>]' )\n         edges.append( f'{nname} -> {right_node_name} [penwidth={rpw} color=\"{rcolor}\" label=<{rlabel}>]' )\n         edges.append(f\"\"\"\n         {{\n             rank=same;\n             {left_node_name} -> {right_node_name} [style=invis]\n         }}\n         \"\"\")\n",
        "code_toks_joined": "lcolor = rcolor = colors [ <STRING> ] <NEWLINE> <INDENT> lpw = rpw = <STRING> <NEWLINE> if node . left . id in highlight_path : <NEWLINE> <INDENT> lcolor = colors [ <STRING> ] <NEWLINE> lpw = <STRING> <NEWLINE> <DEDENT> if node . right . id in highlight_path : <NEWLINE> <INDENT> lcolor = colors [ <STRING> ] <NEWLINE> rpw = <STRING> <NEWLINE> <DEDENT> edges . append ( <STRING> ) <NEWLINE> edges . append ( <STRING> ) <NEWLINE> edges . append ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'arrow'",
                "\"0.3\"",
                "'highlight'",
                "\"1.2\"",
                "'highlight'",
                "\"1.2\"",
                "f'{nname} -> {left_node_name} [penwidth={lpw} color=\"{lcolor}\" label=<{llabel}>]'",
                "f'{nname} -> {right_node_name} [penwidth={rpw} color=\"{rcolor}\" label=<{rlabel}>]'",
                "f\"\"\"\n         {{\n             rank=same;\n             {left_node_name} -> {right_node_name} [style=invis]\n         }}\n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5ad311b22b2e45dd8d6f230509c34105": {
        "code_string": "if getattr(aq_base(workspace), '__ac_local_roles_block__', None):\n                 cont = False\n             else:\n                 context = aq_parent(context)\n",
        "code_toks_joined": "if getattr ( aq_base ( workspace ) , <STRING> , None ) : <NEWLINE> <INDENT> cont = False <NEWLINE> else : <NEWLINE> context = aq_parent ( context ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'__ac_local_roles_block__'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8fb8b8665ef745029ea6cfc2a9838575": {
        "code_string": "@property\n     def primary_opening_today(self):\n         today = date.today()\n         cache_key = self.get_opening_today_cache_key(today)\n         times = cache.get(cache_key)\n         if times is not None:\n             opening_times = self.primary_opening_times\n             if opening_times:\n                 specific_times = utils.first_true(opening_times, lambda x: x.get('date') == today)\n                 times = specific_times or utils.first_true(opening_times, lambda x: x.get('weekday') == today.weekday())\n                 cache.set(cache_key, times, 60*60*24)\n         return times\n",
        "code_toks_joined": "@ property <NEWLINE> <INDENT> def primary_opening_today ( self ) : <NEWLINE> <INDENT> today = date . today ( ) <NEWLINE> cache_key = self . get_opening_today_cache_key ( today ) <NEWLINE> times = cache . get ( cache_key ) <NEWLINE> if times is not None : <NEWLINE> <INDENT> opening_times = self . primary_opening_times <NEWLINE> if opening_times : <NEWLINE> <INDENT> specific_times = utils . first_true ( opening_times , lambda x : x . get ( <STRING> ) == today ) <NEWLINE> times = specific_times or utils . first_true ( opening_times , lambda x : x . get ( <STRING> ) == today . weekday ( ) ) <NEWLINE> cache . set ( cache_key , times , 60 * 60 * 24 ) <NEWLINE> <DEDENT> <DEDENT> return times <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'date'",
                "'weekday'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e1267ea8684844039e0bd7b0c30bdb9d": {
        "code_string": "def handle_ctrlchan(self, nick, msg, send, send_raw):\n         cmd = msg.split()\n         if cmd[0] == \"quote\":\n             send_raw(\" \".join(cmd[1:]))\n         elif cmd[0] == \"disable\":\n             if cmd[1] == \"kick\":\n                 self.kick_enabled = False\n                 send(\"Kick disabled.\")\n             if cmd[1] == \"module\":\n                 self.disabled_mods.append(cmd[2])\n                 send(\"Module disabled.\")\n         elif cmd[0] == \"enable\":\n             if cmd[1] == \"kick\":\n                 self.kick_enabled = True\n                 send(\"Kick enabled.\")\n             if cmd[1] == \"module\":\n                 self.disabled_mods = [i for i in self.disabled_mods if i != cmd[2]]\n                 send(\"Module enabled.\")\n         elif cmd[0] == \"get\":\n             if cmd[1] == \"disabled\" and cmd[2] == \"modules\":\n                 send(str(self.disabled_mods))\n             if cmd[2] == \"enabled\" and cmd[2] == \"modules\":\n                 send(str([i for i in self.modules if i not in self.disabled_mods]))\n",
        "code_toks_joined": "def handle_ctrlchan ( self , nick , msg , send , send_raw ) : <NEWLINE> <INDENT> cmd = msg . split ( ) <NEWLINE> if cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> send_raw ( <STRING> . join ( cmd [ 1 : ] ) ) <NEWLINE> <DEDENT> elif cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . kick_enabled = False <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . disabled_mods . append ( cmd [ 2 ] ) <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . kick_enabled = True <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . disabled_mods = [ i for i in self . disabled_mods if i != cmd [ 2 ] ] <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> if cmd [ 1 ] == <STRING> and cmd [ 2 ] == <STRING> : <NEWLINE> <INDENT> send ( str ( self . disabled_mods ) ) <NEWLINE> <DEDENT> if cmd [ 2 ] == <STRING> and cmd [ 2 ] == <STRING> : <NEWLINE> <INDENT> send ( str ( [ i for i in self . modules if i not in self . disabled_mods ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"quote\"",
                "\" \"",
                "\"disable\"",
                "\"kick\"",
                "\"Kick disabled.\"",
                "\"module\"",
                "\"Module disabled.\"",
                "\"enable\"",
                "\"kick\"",
                "\"Kick enabled.\"",
                "\"module\"",
                "\"Module enabled.\"",
                "\"get\"",
                "\"disabled\"",
                "\"modules\"",
                "\"enabled\"",
                "\"modules\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7e4e036071f34807a7d9e5534c1e0584": {
        "code_string": "def groundwater_constraint(evaluation, simulation):\n     if (evaluation[0] - 0.1 <= simulation[0]) or (simulation[0] <= evaluation[0] + 0.1):\n         return 1.0\n     else:\n         return 0.0\n",
        "code_toks_joined": "def groundwater_constraint ( evaluation , simulation ) : <NEWLINE> <INDENT> if ( evaluation [ 0 ] - 0.1 <= simulation [ 0 ] ) or ( simulation [ 0 ] <= evaluation [ 0 ] + 0.1 ) : <NEWLINE> <INDENT> return 1.0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return 0.0 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "940624dcc24d4153b8a694edaeae9d13": {
        "code_string": "def __init__(self, sate_id, source):\n         super(TLEPredictor, self).__init__(source, sate_id)\n         self._iterations = 0\n",
        "code_toks_joined": "def __init__ ( self , sate_id , source ) : <NEWLINE> <INDENT> super ( TLEPredictor , self ) . __init__ ( source , sate_id ) <NEWLINE> self . _iterations = 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5897dcf286cc442fbb2a7361270d6535": {
        "code_string": "self.novel_id = urlparse(self.novel_url).path.split('/')[1]\n         logger.info(\"Novel Id: %s\", self.novel_id)\n",
        "code_toks_joined": "self . novel_id = urlparse ( self . novel_url ) . path . split ( <STRING> ) [ 1 ] <NEWLINE> <INDENT> logger . info ( <STRING> , self . novel_id ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/'",
                "\"Novel Id: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5a5cc576bccd48588dbd8a6b10a67275": {
        "code_string": "def make_id(name):\n     \"\"\"\n     Create a random id combined with the creditor name.\n     @return string consisting of name (truncated at 22 chars), -,\n     12 char rand hex string.\n     \"\"\"\n     r = get_rand_string(12)\n     if len(name) <= 22:\n         name = name[:22]\n     return name + \"-\" + r\n",
        "code_toks_joined": "def make_id ( name ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> r = get_rand_string ( 12 ) <NEWLINE> if len ( name ) <= 22 : <NEWLINE> <INDENT> name = name [ : 22 ] <NEWLINE> <DEDENT> return name + <STRING> + r <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Create a random id combined with the creditor name.\n     @return string consisting of name (truncated at 22 chars), -,\n     12 char rand hex string.\n     \"\"\"",
                "\"-\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "40ef55a033a84acbbafa13fdeebc0017": {
        "code_string": "def int_to_decimal_str(integer):\n     \"\"\"\n     Helper to convert integers (representing cents) into decimal currency\n     string. WARNING: DO NOT TRY TO DO THIS BY DIVISION, FLOATING POINT\n     ERRORS ARE NO FUN IN FINANCIAL SYSTEMS.\n     @param integer The amount in cents\n     @return string The amount in currency with full stop decimal separator\n     \"\"\"\n     int_string = str(integer)\n     if len(int_string) < 2:\n         return \"0.\" + int_string.zfill(2)\n     else:\n         return int_string[:-2] + \".\" + int_string[-2:]\n",
        "code_toks_joined": "def int_to_decimal_str ( integer ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> int_string = str ( integer ) <NEWLINE> if len ( int_string ) < 2 : <NEWLINE> <INDENT> return <STRING> + int_string . zfill ( 2 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return int_string [ : - 2 ] + <STRING> + int_string [ - 2 : ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Helper to convert integers (representing cents) into decimal currency\n     string. WARNING: DO NOT TRY TO DO THIS BY DIVISION, FLOATING POINT\n     ERRORS ARE NO FUN IN FINANCIAL SYSTEMS.\n     @param integer The amount in cents\n     @return string The amount in currency with full stop decimal separator\n     \"\"\"",
                "\"0.\"",
                "\".\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "42480357e64c4a1dacb8211e66da0b98": {
        "code_string": "if not threshold == 'deprecated':\n         msg = \"`threshold` parameter deprecated; use `threshold_rel instead.\"\n         warnings.warn(msg, DeprecationWarning)\n         threshold_rel = threshold\n     # find top corner candidates above a threshold\n     corner_threshold = max(np.max(image.ravel()) * threshold_rel, threshold_abs)\n     image_t = (image >= corner_threshold) * 1\n",
        "code_toks_joined": "if not threshold == <STRING> : <NEWLINE> <INDENT> msg = <STRING> <NEWLINE> warnings . warn ( msg , DeprecationWarning ) <NEWLINE> threshold_rel = threshold <NEWLINE> <COMMENT> <NL> corner_threshold = max ( np . max ( image . ravel ( ) ) * threshold_rel , threshold_abs ) <NEWLINE> image_t = ( image >= corner_threshold ) * 1 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'deprecated'",
                "\"`threshold` parameter deprecated; use `threshold_rel instead.\""
            ],
            "<COMMENT>": [
                "# find top corner candidates above a threshold"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e472fe88d4df4a84bd9423877d81bc7d": {
        "code_string": "if offset == None:\n         if not all([d % 2 == 1 for d in selem.shape]):\n             ValueError(\"Footprint dimensions must all be odd\")\n         offset = np.array([d / 2 for d in selem.shape])\n     # Cross out the center of the selem\n     selem[[slice(d, d + 1) for d in offset]] = False\n",
        "code_toks_joined": "if offset == None : <NEWLINE> <INDENT> if not all ( [ d % 2 == 1 for d in selem . shape ] ) : <NEWLINE> <INDENT> ValueError ( <STRING> ) <NEWLINE> <DEDENT> offset = np . array ( [ d / 2 for d in selem . shape ] ) <NEWLINE> <COMMENT> <NL> selem [ [ slice ( d , d + 1 ) for d in offset ] ] = False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Footprint dimensions must all be odd\""
            ],
            "<COMMENT>": [
                "# Cross out the center of the selem"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b8f702f5c43a48d4aa72fd40b9dcbc07": {
        "code_string": "def get_relationship(self, rel_type, rel_key):\n         try:\n             values = self.get(RELATIONSHIP_CF, ENDPOINT_NAME_TEMPLATE % (rel_type, rel_key)) \n         except NotFoundException:\n             raise NodeNotFoundException()\n         source_node_key = None\n         source_node_type = None\n         source_attributes = {}\n         for column in values.keys():\n             value = values[column]\n             if column == 'source__type':\n                 source_node_type = value\n             elif column == 'source__key':\n                 source_node_key = value\n             elif column.startswith('source__'):\n                 source_attributes[column[8:]] = value\n         source = prim.Node(self, source_node_type, source_node_key, values)\n         rel_key = RELATIONSHIP_KEY_PATTERN % (rel_type, rel_key)\n         return self.get_outgoing_relationship(rel_type, source, (rel_key, values))\n",
        "code_toks_joined": "def get_relationship ( self , rel_type , rel_key ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> values = self . get ( RELATIONSHIP_CF , ENDPOINT_NAME_TEMPLATE % ( rel_type , rel_key ) ) <NEWLINE> <DEDENT> except NotFoundException : <NEWLINE> <INDENT> raise NodeNotFoundException ( ) <NEWLINE> <DEDENT> source_node_key = None <NEWLINE> source_node_type = None <NEWLINE> source_attributes = { } <NEWLINE> for column in values . keys ( ) : <NEWLINE> <INDENT> value = values [ column ] <NEWLINE> if column == <STRING> : <NEWLINE> <INDENT> source_node_type = value <NEWLINE> <DEDENT> elif column == <STRING> : <NEWLINE> <INDENT> source_node_key = value <NEWLINE> <DEDENT> elif column . startswith ( <STRING> ) : <NEWLINE> <INDENT> source_attributes [ column [ 8 : ] ] = value <NEWLINE> <DEDENT> <DEDENT> source = prim . Node ( self , source_node_type , source_node_key , values ) <NEWLINE> rel_key = RELATIONSHIP_KEY_PATTERN % ( rel_type , rel_key ) <NEWLINE> return self . get_outgoing_relationship ( rel_type , source , ( rel_key , values ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'source__type'",
                "'source__key'",
                "'source__'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "01814d84198f47f6ac7c45a87f272e15": {
        "code_string": "def _recursive_directory_find(path: Path, directory_name: str) -> str:\n     if str(path) == expanduser('~'):\n         raise FileNotFoundError()\n     joined_with_driectory = path.joinpath(directory_name)\n     if joined_with_driectory.is_dir():\n         return str(joined_with_driectory)\n     else:\n         return _recursive_directory_find(path.parent, directory_name)\n",
        "code_toks_joined": "def _recursive_directory_find ( path : Path , directory_name : str ) -> str : <NEWLINE> <INDENT> if str ( path ) == expanduser ( <STRING> ) : <NEWLINE> <INDENT> raise FileNotFoundError ( ) <NEWLINE> <DEDENT> joined_with_driectory = path . joinpath ( directory_name ) <NEWLINE> if joined_with_driectory . is_dir ( ) : <NEWLINE> <INDENT> return str ( joined_with_driectory ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return _recursive_directory_find ( path . parent , directory_name ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'~'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af647e6204c24627bfdf062dfb2ea78d": {
        "code_string": "# If there are slice (trim) operations, need to perform them!\n     # Need to add this logic for the isolated events too.\n     if 'slice' in ann.sandbox.keys():\n         for sliceop in ann.sandbox['slice']:\n             # must use temp file in order to save to same file\n             tmpfiles = []\n             audio_files = [audio_outfile] + ann.sandbox.scaper.isolated_events_audio_path\n             with _close_temp_files(tmpfiles):\n                 for audio_file in audio_files:\n                     # Create tmp file\n                     tmpfiles.append(\n                         tempfile.NamedTemporaryFile(suffix='.wav', delete=False))\n                     # Save trimmed result to temp file\n                     tfm = sox.Transformer()\n                     tfm.trim(sliceop['slice_start'], sliceop['slice_end'])\n                     tfm.build(audio_file, tmpfiles[-1].name)\n                     # Copy result back to original file\n                     shutil.copyfile(tmpfiles[-1].name, audio_outfile)\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> if <STRING> in ann . sandbox . keys ( ) : <NEWLINE> <INDENT> for sliceop in ann . sandbox [ <STRING> ] : <NEWLINE> <COMMENT> <NL> <INDENT> tmpfiles = [ ] <NEWLINE> audio_files = [ audio_outfile ] + ann . sandbox . scaper . isolated_events_audio_path <NEWLINE> with _close_temp_files ( tmpfiles ) : <NEWLINE> <INDENT> for audio_file in audio_files : <NEWLINE> <COMMENT> <NL> <INDENT> tmpfiles . append ( <NEWLINE> <INDENT> tempfile . NamedTemporaryFile ( suffix = <STRING> , delete = False ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> tfm = sox . Transformer ( ) <NEWLINE> tfm . trim ( sliceop [ <STRING> ] , sliceop [ <STRING> ] ) <NEWLINE> tfm . build ( audio_file , tmpfiles [ - 1 ] . name ) <NEWLINE> <COMMENT> <NL> shutil . copyfile ( tmpfiles [ - 1 ] . name , audio_outfile ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If there are slice (trim) operations, need to perform them!",
                "# Need to add this logic for the isolated events too.",
                "# must use temp file in order to save to same file",
                "# Create tmp file",
                "# Save trimmed result to temp file",
                "# Copy result back to original file"
            ],
            "<STRING>": [
                "'slice'",
                "'slice'",
                "'.wav'",
                "'slice_start'",
                "'slice_end'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e9b39e5a28bb4a77945d75df176ab646": {
        "code_string": "done = set()\n         dirs = []\n         for m in members:\n             inf = self.getinfo(m)\n             dst = self._extract_one(inf, path, pwd, not inf.is_dir())\n             if inf.is_dir():\n                 if dst not in done:\n                     dirs.append((dst, inf))\n                     done.add(dst)\n         if dirs:\n             dirs.sort(reverse=True)\n             for dst, inf in dirs:\n                 self._set_attrs(dst, inf)\n",
        "code_toks_joined": "done = set ( ) <NEWLINE> <INDENT> dirs = [ ] <NEWLINE> for m in members : <NEWLINE> <INDENT> inf = self . getinfo ( m ) <NEWLINE> dst = self . _extract_one ( inf , path , pwd , not inf . is_dir ( ) ) <NEWLINE> if inf . is_dir ( ) : <NEWLINE> <INDENT> if dst not in done : <NEWLINE> <INDENT> dirs . append ( ( dst , inf ) ) <NEWLINE> done . add ( dst ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if dirs : <NEWLINE> <INDENT> dirs . sort ( reverse = True ) <NEWLINE> for dst , inf in dirs : <NEWLINE> <INDENT> self . _set_attrs ( dst , inf ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "59033532f9c04cbcace87d633081277f": {
        "code_string": "# Second pass for actual rendering\n     x, y = 0, 0\n     previous = 0\n     for c in text:\n         face.load_char(c)\n         bitmap = slot.bitmap\n         top = slot.bitmap_top\n         left = slot.bitmap_left\n         w,h = bitmap.width, bitmap.rows\n         y = height-baseline-top\n         kerning = face.get_kerning(previous, c)\n         x += (kerning.x >> 6)\n         Z[y:y+h,x:x+w] |= numpy.array(bitmap.buffer).reshape(h,w)\n         x += (slot.advance.x >> 6) \n         previous = c\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> x , y = 0 , 0 <NEWLINE> previous = 0 <NEWLINE> for c in text : <NEWLINE> <INDENT> face . load_char ( c ) <NEWLINE> bitmap = slot . bitmap <NEWLINE> top = slot . bitmap_top <NEWLINE> left = slot . bitmap_left <NEWLINE> w , h = bitmap . width , bitmap . rows <NEWLINE> y = height - baseline - top <NEWLINE> kerning = face . get_kerning ( previous , c ) <NEWLINE> x += ( kerning . x >> 6 ) <NEWLINE> Z [ y : y + h , x : x + w ] |= numpy . array ( bitmap . buffer ) . reshape ( h , w ) <NEWLINE> x += ( slot . advance . x >> 6 ) <NEWLINE> previous = c <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Second pass for actual rendering"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "53c9ce97fbc543559171032cb3eccbf8": {
        "code_string": "if isinstance(v1, list) and isinstance(v2, list):\n         v1.extend(v2)\n         return v2\n",
        "code_toks_joined": "if isinstance ( v1 , list ) and isinstance ( v2 , list ) : <NEWLINE> <INDENT> v1 . extend ( v2 ) <NEWLINE> return v2 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "59996866a099451b98fa85c251b25b32": {
        "code_string": "p_t = p - self.lr * m_b_t / (T.sqrt(v_t) + self.epsilon)\n",
        "code_toks_joined": "p_t = p - self . lr * m_b_t / ( T . sqrt ( v_t ) + self . epsilon ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "43f2cdbbb6e14ae3a9964e1850c72772": {
        "code_string": "if len(self.items_buffer) == self.settings.get('ELASTICSEARCH_BUFFER_LENGTH', 500):\n             self.send_items()\n             self.items_buffer = []\n",
        "code_toks_joined": "if len ( self . items_buffer ) == self . settings . get ( <STRING> , 500 ) : <NEWLINE> <INDENT> self . send_items ( ) <NEWLINE> self . items_buffer = [ ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ELASTICSEARCH_BUFFER_LENGTH'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6a25d1074475498cb512ef04bf84ecb0": {
        "code_string": "if read_lengths is None and psite_offsets is not None:\n         sys.exit(\n             'Error: psite_offsets only allowed when read_lengths is provided')\n     if read_lengths is not None and psite_offsets is not None:\n         try:\n             psite_offsets = [\n                 int(x.strip()) for x in psite_offsets.strip().split(',')\n             ]\n         except:\n             sys.exit('Error: cannot convert psite_offsets into integers')\n         if len(read_lengths) != len(psite_offsets):\n             sys.exit('Error: psite_offsets must match read_lengths')\n         if not all(x > 0 for x in psite_offsets):\n             sys.exit('Error: P-site offset must be >= 0')\n         if not all(x > y for (x, y) in zip(read_lengths, psite_offsets)):\n             sys.exit('Error: P-site offset must be smaller than read length')\n         psite_offsets = dict(zip(read_lengths, psite_offsets))\n     if stranded == 'yes':\n         stranded = 'forward'\n     detect_orfs(bam, ribocop_index, prefix, stranded, read_lengths,\n                 psite_offsets, report_all)\n",
        "code_toks_joined": "if read_lengths is None and psite_offsets is not None : <NEWLINE> <INDENT> sys . exit ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if read_lengths is not None and psite_offsets is not None : <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> psite_offsets = [ <NEWLINE> <INDENT> int ( x . strip ( ) ) for x in psite_offsets . strip ( ) . split ( <STRING> ) <NEWLINE> <DEDENT> ] <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> if len ( read_lengths ) != len ( psite_offsets ) : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> if not all ( x > 0 for x in psite_offsets ) : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> if not all ( x > y for ( x , y ) in zip ( read_lengths , psite_offsets ) ) : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> psite_offsets = dict ( zip ( read_lengths , psite_offsets ) ) <NEWLINE> if stranded == <STRING> : <NEWLINE> stranded = <STRING> <NEWLINE> detect_orfs ( bam , ribocop_index , prefix , stranded , read_lengths , <NEWLINE> <INDENT> psite_offsets , report_all ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Error: psite_offsets only allowed when read_lengths is provided'",
                "','",
                "'Error: cannot convert psite_offsets into integers'",
                "'Error: psite_offsets must match read_lengths'",
                "'Error: P-site offset must be >= 0'",
                "'Error: P-site offset must be smaller than read length'",
                "'yes'",
                "'forward'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9c10d94554954780ac693d8e66f7248c": {
        "code_string": "to_write = '\\t'.join(columns)\n     formatter = '{}\\t' * (len(columns) - 1) + '{}\\n'\n     for orf in tqdm(candidate_orfs):\n         coordinate = ','.join(\n             ['{}-{}'.format(iv.start, iv.end) for iv in orf.intervals])\n         to_write = formatter.format(orf.oid, orf.category, orf.tid, orf.ttype,\n                                     orf.gid, orf.gname, orf.gtype, orf.chrom,\n                                     orf.strand, coordinate)\n",
        "code_toks_joined": "to_write = <STRING> . join ( columns ) <NEWLINE> <INDENT> formatter = <STRING> * ( len ( columns ) - 1 ) + <STRING> <NEWLINE> for orf in tqdm ( candidate_orfs ) : <NEWLINE> <INDENT> coordinate = <STRING> . join ( <NEWLINE> <INDENT> [ <STRING> . format ( iv . start , iv . end ) for iv in orf . intervals ] ) <NEWLINE> <DEDENT> to_write = formatter . format ( orf . oid , orf . category , orf . tid , orf . ttype , <NEWLINE> <INDENT> orf . gid , orf . gname , orf . gtype , orf . chrom , <NEWLINE> orf . strand , coordinate ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'\\t'",
                "'{}\\t'",
                "'{}\\n'",
                "','",
                "'{}-{}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bfeefe198b6447999c034a1a91d8d076": {
        "code_string": "if re.search('dcs.sc', args.outfile) is None:\n         sscs_singleton_bam = pysam.AlignmentFile('{}.sscs.sc.singleton.bam'.format(args.outfile.split('.dcs.sc')[0]),\n                                              \"wb\", template=sscs_bam)\n         dcs_header = \"DCS - Singleton Correction\"\n         sc_header = \" SC\"\n     else:\n         sscs_singleton_bam = pysam.AlignmentFile('{}.sscs.singleton.bam'.format(args.outfile.split('.dcs')[0]),\n                                              \"wb\", template=sscs_bam)\n         dcs_header = \"DCS\"\n         sc_header = \"\"\n",
        "code_toks_joined": "if re . search ( <STRING> , args . outfile ) is None : <NEWLINE> <INDENT> sscs_singleton_bam = pysam . AlignmentFile ( <STRING> . format ( args . outfile . split ( <STRING> ) [ 0 ] ) , <NEWLINE> <INDENT> <STRING> , template = sscs_bam ) <NEWLINE> <DEDENT> dcs_header = <STRING> <NEWLINE> sc_header = <STRING> <NEWLINE> else : <NEWLINE> sscs_singleton_bam = pysam . AlignmentFile ( <STRING> . format ( args . outfile . split ( <STRING> ) [ 0 ] ) , <NEWLINE> <INDENT> <STRING> , template = sscs_bam ) <NEWLINE> <DEDENT> dcs_header = <STRING> <NEWLINE> sc_header = <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'dcs.sc'",
                "'{}.sscs.sc.singleton.bam'",
                "'.dcs.sc'",
                "\"wb\"",
                "\"DCS - Singleton Correction\"",
                "\" SC\"",
                "'{}.sscs.singleton.bam'",
                "'.dcs'",
                "\"wb\"",
                "\"DCS\"",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "547ffd24a85f45a8b15a6744d080d5ac": {
        "code_string": "for attr in fields:\n         model = getattr(attr, 'related_model', None)\n         if (model and issubclass(model, base_cls) and\n                 model is not generic_cls and getattr(attr, 'enabled', True)):\n             # if model is generic one it would be returned anyway\n             if not model.objects.is_generic():\n                 # make sure that content_object's content_type is same as\n                 # the one of given obj\n                 fk = model._meta.get_field('content_object')\n                 if ctype == get_content_type(fk.remote_field.model):\n                     return model\n     return generic_cls\n",
        "code_toks_joined": "for attr in fields : <NEWLINE> <INDENT> model = getattr ( attr , <STRING> , None ) <NEWLINE> if ( model and issubclass ( model , base_cls ) and <NEWLINE> <INDENT> model is not generic_cls and getattr ( attr , <STRING> , True ) ) : <NEWLINE> <COMMENT> <NL> if not model . objects . is_generic ( ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> fk = model . _meta . get_field ( <STRING> ) <NEWLINE> if ctype == get_content_type ( fk . remote_field . model ) : <NEWLINE> <INDENT> return model <NEWLINE> return generic_cls <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'related_model'",
                "'enabled'",
                "'content_object'"
            ],
            "<COMMENT>": [
                "# if model is generic one it would be returned anyway",
                "# make sure that content_object's content_type is same as",
                "# the one of given obj"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2fee7fc58157460e9bd81d48ff7a5626": {
        "code_string": "def import_symbol(sym, here=None, sep=\":\", ns=None):\n     if ns is not None and sep not in sym:\n         sym = \"{}:{}\".format(ns, sym)\n     module_path, fn_name = sym.rsplit(sep, 2)\n     try:\n         module = import_module(sym, here=here, sep=sep)\n         return getattr(module, fn_name)\n     except (ImportError, AttributeError) as e:\n         sys.stderr.write(\"could not import {!r}\\n{}\\n\".format(sym, e))\n         raise\n",
        "code_toks_joined": "def import_symbol ( sym , here = None , sep = <STRING> , ns = None ) : <NEWLINE> <INDENT> if ns is not None and sep not in sym : <NEWLINE> <INDENT> sym = <STRING> . format ( ns , sym ) <NEWLINE> <DEDENT> module_path , fn_name = sym . rsplit ( sep , 2 ) <NEWLINE> try : <NEWLINE> <INDENT> module = import_module ( sym , here = here , sep = sep ) <NEWLINE> return getattr ( module , fn_name ) <NEWLINE> <DEDENT> except ( ImportError , AttributeError ) as e : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> . format ( sym , e ) ) <NEWLINE> raise <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\":\"",
                "\"{}:{}\"",
                "\"could not import {!r}\\n{}\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d00e1978ed564214a4fea3fcbfb8866b": {
        "code_string": "for header_name, header_list in serialized['headers'].items():\n         if isinstance(header_list, list):\n             for header_value in header_list:\n                 header_dict.add(header_name, header_list)\n         else:\n             header_dict.add(header_name, header_list)\n     r.headers = CaseInsensitiveDict(header_dict)\n",
        "code_toks_joined": "for header_name , header_list in serialized [ <STRING> ] . items ( ) : <NEWLINE> <INDENT> if isinstance ( header_list , list ) : <NEWLINE> <INDENT> for header_value in header_list : <NEWLINE> <INDENT> header_dict . add ( header_name , header_list ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> header_dict . add ( header_name , header_list ) <NEWLINE> r . headers = CaseInsensitiveDict ( header_dict ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'headers'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "25a2ca016eca4af2b045b8e5b826a92c": {
        "code_string": "@strict_globals(deque=deque, itemgetter=og_itemgetter)\n def itemgetter(iterable, indexes):\n     ''' same functionality as operator.itemgetter except, this one supports\n         both positive and negative indexing of generators as well '''\n     indexes = indexes if isinstance(indexes, tuple) else tuple(indexes)\n     assert all(isinstance(i, int) for i in indexes), 'indexes needs to be a tuple of ints'\n     positive_indexes=[i for i in indexes if i>=0]\n     negative_indexes=[i for i in indexes if i<0]\n     out = {}\n     if len(negative_indexes):\n         # if there are any negative indexes\n         negative_index_buffer = deque(maxlen=min(indexes)*-1)\n         for i,x in enumerate(iterable):\n             if i in positive_indexes:\n                 out[i]=x\n             negative_index_buffer.append(i)\n         out.update({ni:negative_index_buffer[ni] for ni in negative_indexes})\n     else:\n         # if just positive results\n         out.update({i:x for i,x in enumerate(iterable) if i in positive_indexes})\n     return itemgetter(*indexes)(out)\n",
        "code_toks_joined": "@ strict_globals ( deque = deque , itemgetter = og_itemgetter ) <NEWLINE> <INDENT> def itemgetter ( iterable , indexes ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> indexes = indexes if isinstance ( indexes , tuple ) else tuple ( indexes ) <NEWLINE> assert all ( isinstance ( i , int ) for i in indexes ) , <STRING> <NEWLINE> positive_indexes = [ i for i in indexes if i >= 0 ] <NEWLINE> negative_indexes = [ i for i in indexes if i < 0 ] <NEWLINE> out = { } <NEWLINE> if len ( negative_indexes ) : <NEWLINE> <COMMENT> <NL> <INDENT> negative_index_buffer = deque ( maxlen = min ( indexes ) * - 1 ) <NEWLINE> for i , x in enumerate ( iterable ) : <NEWLINE> <INDENT> if i in positive_indexes : <NEWLINE> <INDENT> out [ i ] = x <NEWLINE> <DEDENT> negative_index_buffer . append ( i ) <NEWLINE> <DEDENT> out . update ( { ni : negative_index_buffer [ ni ] for ni in negative_indexes } ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> out . update ( { i : x for i , x in enumerate ( iterable ) if i in positive_indexes } ) <NEWLINE> <DEDENT> return itemgetter ( * indexes ) ( out ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''' same functionality as operator.itemgetter except, this one supports\n         both positive and negative indexing of generators as well '''",
                "'indexes needs to be a tuple of ints'"
            ],
            "<COMMENT>": [
                "# if there are any negative indexes",
                "# if just positive results"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ec57b96dc7c04f81a15c5a5040551aa7": {
        "code_string": "cv = StratifiedKFold(\n                 n_splits=5,\n                 shuffle=True,\n                 random_state=8\n             )\n             preds = []\n             trues_list = []\n             for train_index, test_index in cv.split(X, y):\n                 X_train, X_test = secondary_features[train_index], secondary_features[test_index]\n                 y_train, y_test = y[train_index], y[test_index]\n                 est = est.fit(X_train, y_train)\n                 preds.append(\n                     getattr(est, stacked_ensemble.base_learner_origin.\n                             meta_feature_generator)(X_test)\n                 )\n                 trues_list.append(y_test)\n             preds = np.concatenate(preds, axis=0)\n             y_true = np.concatenate(trues_list)\n",
        "code_toks_joined": "cv = StratifiedKFold ( <NEWLINE> <INDENT> n_splits = 5 , <NEWLINE> shuffle = True , <NEWLINE> random_state = 8 <NEWLINE> ) <NEWLINE> preds = [ ] <NEWLINE> trues_list = [ ] <NEWLINE> for train_index , test_index in cv . split ( X , y ) : <NEWLINE> X_train , X_test = secondary_features [ train_index ] , secondary_features [ test_index ] <NEWLINE> y_train , y_test = y [ train_index ] , y [ test_index ] <NEWLINE> est = est . fit ( X_train , y_train ) <NEWLINE> preds . append ( <NEWLINE> <INDENT> getattr ( est , stacked_ensemble . base_learner_origin . <NEWLINE> <INDENT> meta_feature_generator ) ( X_test ) <NEWLINE> <DEDENT> <DEDENT> ) <NEWLINE> trues_list . append ( y_test ) <NEWLINE> preds = np . concatenate ( preds , axis = 0 ) <NEWLINE> y_true = np . concatenate ( trues_list ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b6c4b17594314566b2222adfd0bae4a1": {
        "code_string": "self.update()\n             if messagebox.askyesno(\n                     \"Model_animations compilation failed\",\n                     \"Errors occurred while compiling animations(check console). \"\n                     \"Do you want to save the model_animations tag anyway?\",\n                     icon='warning', parent=self):\n                 print(\"    Model_animations compilation failed.\")\n                 return\n",
        "code_toks_joined": "self . update ( ) <NEWLINE> <INDENT> if messagebox . askyesno ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> <NEWLINE> <STRING> , <NEWLINE> icon = <STRING> , parent = self ) : <NEWLINE> print ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Model_animations compilation failed\"",
                "\"Errors occurred while compiling animations(check console). \"",
                "\"Do you want to save the model_animations tag anyway?\"",
                "'warning'",
                "\"    Model_animations compilation failed.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c45520752fbc4193af5c2d2121786759": {
        "code_string": "class System(system.System):\n \tdef __init__(self, target):\n \t\tsystem.System.__init__(self)\n \t\t# create some HELP:\n \t\tself.help = \"X11: Basic interface of Linux Graphic interface\"\n \t\tself.valid = True\n \t\t# no check needed ==> just add this:\n \t\tself.add_module_depend(['c'])\n \t\tself.add_export_flag('link-lib', 'X11')\n \t\tif env.get_isolate_system() == False:\n \t\t\tself.add_header_file([\n \t\t\t    \"/usr/include/X11/*\"\n \t\t\t    ],\n \t\t\t    destination_path=\"X11\",\n \t\t\t    recursive=True)\n",
        "code_toks_joined": "class System ( system . System ) : <NEWLINE> <INDENT> def __init__ ( self , target ) : <NEWLINE> <INDENT> system . System . __init__ ( self ) <NEWLINE> <COMMENT> <NL> self . help = <STRING> <NEWLINE> self . valid = True <NEWLINE> <COMMENT> <NL> self . add_module_depend ( [ <STRING> ] ) <NEWLINE> self . add_export_flag ( <STRING> , <STRING> ) <NEWLINE> if env . get_isolate_system ( ) == False : <NEWLINE> <INDENT> self . add_header_file ( [ <NEWLINE> <INDENT> <STRING> <NEWLINE> ] , <NEWLINE> destination_path = <STRING> , <NEWLINE> recursive = True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# create some HELP:",
                "# no check needed ==> just add this:"
            ],
            "<STRING>": [
                "\"X11: Basic interface of Linux Graphic interface\"",
                "'c'",
                "'link-lib'",
                "'X11'",
                "\"/usr/include/X11/*\"",
                "\"X11\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b13d18089a034d36a49d0527155a0310": {
        "code_string": "def sanity_check(self, outprefix):\n         variants_only_removed = self._remove_bad_genes(self.seq_dicts['variants_only'], outprefix + '.00.check_fasta_variants_only.log')\n         presence_absence_removed = self._remove_bad_genes(self.seq_dicts['presence_absence'], outprefix + '.00.check_fasta_presence_absence.log')\n         self._filter_bad_variant_data(outprefix + '.01.check_variants', variants_only_removed, presence_absence_removed)\n",
        "code_toks_joined": "def sanity_check ( self , outprefix ) : <NEWLINE> <INDENT> variants_only_removed = self . _remove_bad_genes ( self . seq_dicts [ <STRING> ] , outprefix + <STRING> ) <NEWLINE> presence_absence_removed = self . _remove_bad_genes ( self . seq_dicts [ <STRING> ] , outprefix + <STRING> ) <NEWLINE> self . _filter_bad_variant_data ( outprefix + <STRING> , variants_only_removed , presence_absence_removed ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'variants_only'",
                "'.00.check_fasta_variants_only.log'",
                "'presence_absence'",
                "'.00.check_fasta_presence_absence.log'",
                "'.01.check_variants'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f75fa48c1a9547c6821f811c338538a4": {
        "code_string": "if self.show_known_het and (ref_name, variant) in all_het_snps and key + '.%' not in rows[filename][cluster]:\n                             rows[filename][cluster][key + '.%'] = 'NA'\n",
        "code_toks_joined": "if self . show_known_het and ( ref_name , variant ) in all_het_snps and key + <STRING> not in rows [ filename ] [ cluster ] : <NEWLINE> <INDENT> rows [ filename ] [ cluster ] [ key + <STRING> ] = <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.%'",
                "'.%'",
                "'NA'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "50532162d314476e891e576869545fde": {
        "code_string": "for d in cluster.data:\n             if d['ctg'] == best_hit['ctg']:\n                 if d['var_type'] == 'HET':\n                     het_data.append(d['smtls_nts_depth'])\n                     depths = [int(x) for x in d['smtls_nts_depth'].split(',')]\n                     depths.sort()\n                     het_pc = round(100.0 * depths[-1] / sum(depths), 2)\n                     if results['hetmin'] == '.' or results['hetmin'] < het_pc:\n                         results['hetmin'] = het_pc\n         if len(het_data):\n             results['hets'] = '.'.join(het_data)\n",
        "code_toks_joined": "for d in cluster . data : <NEWLINE> <INDENT> if d [ <STRING> ] == best_hit [ <STRING> ] : <NEWLINE> <INDENT> if d [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> het_data . append ( d [ <STRING> ] ) <NEWLINE> depths = [ int ( x ) for x in d [ <STRING> ] . split ( <STRING> ) ] <NEWLINE> depths . sort ( ) <NEWLINE> het_pc = round ( 100.0 * depths [ - 1 ] / sum ( depths ) , 2 ) <NEWLINE> if results [ <STRING> ] == <STRING> or results [ <STRING> ] < het_pc : <NEWLINE> <INDENT> results [ <STRING> ] = het_pc <NEWLINE> if len ( het_data ) : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> results [ <STRING> ] = <STRING> . join ( het_data ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ctg'",
                "'ctg'",
                "'var_type'",
                "'HET'",
                "'smtls_nts_depth'",
                "'smtls_nts_depth'",
                "','",
                "'hetmin'",
                "'.'",
                "'hetmin'",
                "'hetmin'",
                "'hets'",
                "'.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d1e772b561924acda09ea3a5ad21a13f": {
        "code_string": "if __name__ == '__main__':\n     if check():\n         sys.exit(1)\n     else:\n         sys.exit(0)\n",
        "code_toks_joined": "if __name__ == <STRING> : <NEWLINE> <INDENT> if check ( ) : <NEWLINE> <INDENT> sys . exit ( 1 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sys . exit ( 0 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'__main__'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b4ea787494eb4a9e8f206992b59b2354": {
        "code_string": "return credential_response_schema.dumps(permissions)\n",
        "code_toks_joined": "return credential_response_schema . dumps ( permissions ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "24ddade63c1146aaace3f21b7e0087ff": {
        "code_string": "if os.path.exists(local_path):\n                         logging.error(\"Local file '%s' not found.\", local_path)\n",
        "code_toks_joined": "if os . path . exists ( local_path ) : <NEWLINE> <INDENT> logging . error ( <STRING> , local_path ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Local file '%s' not found.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "54af4c8fcf404df18cba61cfe11429e3": {
        "code_string": "class Tan(AD):\n     def __init__(self,func):\n         self.func=func\n         try:\n             self.dependent=func.dependent[:]\n         except AttributeError:\n             self.dependent=['ALL']\n         self.tan=Sin(func)/Cos(func)\n     def cal(self,x,dOrder):\n         return self.tan.cal(x,dOrder)\n class Tanh(AD):\n     def __init__(self,func):\n         self.func=func\n         try:\n             self.dependent=func.dependent[:]\n         except AttributeError:\n             self.dependent=['ALL']\n         self.tanh=-1j*Tan(func*1j)\n     def cal(self,x,dOrder):\n         return self.tanh.cal(x,dOrder)\n '''\n #---------------Complex Functions-------------------------------#\n '''\n class Conjugate(AD):\n     def __init__(self,func):\n         self.func=func\n         try:\n             self.dependent=func.dependent[:]\n         except AttributeError:\n             self.dependent=['ALL']\n     def cal(self,x,dOrder):\n         return np.conjugate(self.func.cal(x,dOrder))\n class Real(AD):\n     def __init__(self,func):\n         try:\n             self.dependent=func.dependent[:]\n         except AttributeError:\n             self.dependent=['ALL']\n         self.func=func\n     def cal(self,x,dOrder):\n         return self.func.cal(x,dOrder).real\n class Imaginary(AD):\n     def __init__(self,func):\n         try:\n             self.dependent=func.dependent[:]\n         except AttributeError:\n             self.dependent=['ALL']\n         self.func=func\n     def cal(self,x,dOrder):\n         return self.func.cal(x,dOrder).imag\n class Absolute(AD):\n     def __init__(self,func):\n         self.func=func\n         self.abs=(Real(func)**2.-Imaginary(func)**2.)**0.5\n         try:\n             self.dependent=func.dependent[:]\n         except AttributeError:\n             self.dependent=['ALL']\n     def cal(self,x,dOrder):\n         return self.abs.cal(x,dOrder)\n '''\n #---------------Base End Functions-------------------------------#\n '''\n",
        "code_toks_joined": "class Tan ( AD ) : <NEWLINE> <INDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . tan = Sin ( func ) / Cos ( func ) <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . tan . cal ( x , dOrder ) <NEWLINE> class Tanh ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . tanh = - 1j * Tan ( func * 1j ) <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . tanh . cal ( x , dOrder ) <NEWLINE> <STRING> <NEWLINE> class Conjugate ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return np . conjugate ( self . func . cal ( x , dOrder ) ) <NEWLINE> class Real ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . func = func <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . func . cal ( x , dOrder ) . real <NEWLINE> class Imaginary ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . func = func <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . func . cal ( x , dOrder ) . imag <NEWLINE> class Absolute ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> self . abs = ( Real ( func ) ** 2. - Imaginary ( func ) ** 2. ) ** 0.5 <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . abs . cal ( x , dOrder ) <NEWLINE> <STRING> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ALL'",
                "'ALL'",
                "'''\n #---------------Complex Functions-------------------------------#\n '''",
                "'ALL'",
                "'ALL'",
                "'ALL'",
                "'ALL'",
                "'''\n #---------------Base End Functions-------------------------------#\n '''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "93d039e343524919b28b448c5413389e": {
        "code_string": "# we need to know how much we scaled by, which we can do by looking\n             # at the biases\n             try:\n                 v = next((v for v, bias in poly.items() if bias))\n             except StopIteration:\n                 # nothing to scale\n                 scalar = 1\n             else:\n                 scalar = poly[v] / original[v]\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> v = next ( ( v for v , bias in poly . items ( ) if bias ) ) <NEWLINE> <DEDENT> except StopIteration : <NEWLINE> <COMMENT> <NL> <INDENT> scalar = 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> scalar = poly [ v ] / original [ v ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# we need to know how much we scaled by, which we can do by looking",
                "# at the biases",
                "# nothing to scale"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f0c649c3464e49e8a102909c34687947": {
        "code_string": "self.assertEqual(bqm, new)\n         self.assertEqual(bqm.info, {\"tag\": 5})\n",
        "code_toks_joined": "self . assertEqual ( bqm , new ) <NEWLINE> <INDENT> self . assertEqual ( bqm . info , { <STRING> : 5 } ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"tag\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3b66154d65bc4d56afc2a7c5fd131e0e": {
        "code_string": "rvals = np.empty(2*r)\n     rvals[0:r] = range(-r, 0)\n     rvals[r:] = range(1, r+1)\n     qdata = rnd.choice(rvals, size=len(variables))\n",
        "code_toks_joined": "rvals = np . empty ( 2 * r ) <NEWLINE> <INDENT> rvals [ 0 : r ] = range ( - r , 0 ) <NEWLINE> rvals [ r : ] = range ( 1 , r + 1 ) <NEWLINE> qdata = rnd . choice ( rvals , size = len ( variables ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "81a321a9372c4ec7988230ab8c7bce12": {
        "code_string": "response_json = responses_data[0]\n         assert 'error' not in response_json\n         assert response_json['id'] == request2['id']\n         assert response_json['result'] == 5\n",
        "code_toks_joined": "response_json = responses_data [ 0 ] <NEWLINE> <INDENT> assert <STRING> not in response_json <NEWLINE> assert response_json [ <STRING> ] == request2 [ <STRING> ] <NEWLINE> assert response_json [ <STRING> ] == 5 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'error'",
                "'id'",
                "'id'",
                "'result'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "099644ce83a9429187e0b57032b92931": {
        "code_string": "mail_params = {\n             \"subject\": blocks[\"subject\"].strip(),\n             \"body\": blocks[\"body\"]\n         }\n         for ob in optional_blocks:\n             if ob in blocks:\n                 if ob == \"format\" and \\\n                         mail_params[ob].lower() not in [\"html\", \"text\"]:\n                     continue\n                 mail_params[ob] = blocks[ob]\n         return mail_params\n",
        "code_toks_joined": "mail_params = { <NEWLINE> <INDENT> <STRING> : blocks [ <STRING> ] . strip ( ) , <NEWLINE> <STRING> : blocks [ <STRING> ] <NEWLINE> } <NEWLINE> for ob in optional_blocks : <NEWLINE> if ob in blocks : <NEWLINE> <INDENT> if ob == <STRING> and mail_params [ ob ] . lower ( ) not in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> mail_params [ ob ] = blocks [ ob ] <NEWLINE> return mail_params <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"subject\"",
                "\"subject\"",
                "\"body\"",
                "\"body\"",
                "\"format\"",
                "\"html\"",
                "\"text\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fd790472076b49668e8db3415d1ef922": {
        "code_string": "feature_dict = {\n                     \"type\": \"pairwise\",\n                     \"left_names\": bin_labels_left,\n                     \"right_names\": bin_labels_right,\n                     \"scores\": model_graph,\n                     \"scores_range\": bounds,\n                 }\n                 feature_list.append(feature_dict)\n                 density_dict.append({})\n",
        "code_toks_joined": "feature_dict = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : bin_labels_left , <NEWLINE> <STRING> : bin_labels_right , <NEWLINE> <STRING> : model_graph , <NEWLINE> <STRING> : bounds , <NEWLINE> } <NEWLINE> feature_list . append ( feature_dict ) <NEWLINE> density_dict . append ( { } ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"type\"",
                "\"pairwise\"",
                "\"left_names\"",
                "\"right_names\"",
                "\"scores\"",
                "\"scores_range\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e7fe18caa8f544de9c4bf0f32fd656b3": {
        "code_string": "def _validate_range(self, start, minimum, maximum):\n         if maximum <= minimum:\n             raise ValueError(\n                 _(\"The maximum can not be less than the minimum.\"))\n         if start < minimum or start >= maximum:\n             raise ValueError(\n                 _(\"The start must be between the minimum and maximum!\"))\n         rnrange = maximum - minimum\n         return rnrange\n",
        "code_toks_joined": "def _validate_range ( self , start , minimum , maximum ) : <NEWLINE> <INDENT> if maximum <= minimum : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> _ ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> if start < minimum or start >= maximum : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> _ ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> rnrange = maximum - minimum <NEWLINE> return rnrange <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"The maximum can not be less than the minimum.\"",
                "\"The start must be between the minimum and maximum!\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "91f418c827ed4f0aa756636a7b3ebea6": {
        "code_string": "@given(bitmap_cls)\n     def test_shrink_to_fit(self, cls):\n         bm1 = BitMap()\n         size = 1000\n         for i in range(size):\n             bm1.add(i)\n         bm2 = cls(bm1, optimize=False)\n         self.assertGreater(bm2.shrink_to_fit(), 0)\n         self.assertEqual(bm2.shrink_to_fit(), 0)\n         bm3 = cls(bm1, optimize=True)\n         self.assertEqual(bm2.shrink_to_fit(), 0)\n",
        "code_toks_joined": "@ given ( bitmap_cls ) <NEWLINE> <INDENT> def test_shrink_to_fit ( self , cls ) : <NEWLINE> <INDENT> bm1 = BitMap ( ) <NEWLINE> size = 1000 <NEWLINE> for i in range ( size ) : <NEWLINE> <INDENT> bm1 . add ( i ) <NEWLINE> <DEDENT> bm2 = cls ( bm1 , optimize = False ) <NEWLINE> self . assertGreater ( bm2 . shrink_to_fit ( ) , 0 ) <NEWLINE> self . assertEqual ( bm2 . shrink_to_fit ( ) , 0 ) <NEWLINE> bm3 = cls ( bm1 , optimize = True ) <NEWLINE> self . assertEqual ( bm2 . shrink_to_fit ( ) , 0 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "71d323a753da4ee78cc09d0f9aa32ab5": {
        "code_string": "#Run Input Builder function\n     save_dir = maindir + 'inputs/'\n     input_builder(csvfile, initial_coords_dict, ebasis_dir,\n                   save_dir, title.replace('/', '\\n'))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> save_dir = maindir + <STRING> <NEWLINE> input_builder ( csvfile , initial_coords_dict , ebasis_dir , <NEWLINE> <INDENT> save_dir , title . replace ( <STRING> , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#Run Input Builder function"
            ],
            "<STRING>": [
                "'inputs/'",
                "'/'",
                "'\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9484f40f105d4fc5a18501445c43b2e7": {
        "code_string": "#Run Input Builder function\n     save_dir = maindir + 'inputs/'\n     input_builder(csvfile, initial_coords_dict, ebasis_dir,\n                   save_dir, title.replace('/', '\\n'))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> save_dir = maindir + <STRING> <NEWLINE> input_builder ( csvfile , initial_coords_dict , ebasis_dir , <NEWLINE> <INDENT> save_dir , title . replace ( <STRING> , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#Run Input Builder function"
            ],
            "<STRING>": [
                "'inputs/'",
                "'/'",
                "'\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d8f930d466a8440899b89ac032d3252a": {
        "code_string": "#Run Input Builder function\n     save_dir = maindir + 'inputs/'\n     input_builder(csvfile, save_dir, ebasis_dir,\n                   initial_coords_dict, title.replace('/', '\\n'))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> save_dir = maindir + <STRING> <NEWLINE> input_builder ( csvfile , save_dir , ebasis_dir , <NEWLINE> <INDENT> initial_coords_dict , title . replace ( <STRING> , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#Run Input Builder function"
            ],
            "<STRING>": [
                "'inputs/'",
                "'/'",
                "'\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1373d367cc1e46d58e405c4a06f9676e": {
        "code_string": "#---------------------------------------------------------------------------\n         #Write Spreadsheets\n         book = load_workbook(sheetsdir + dir + '.xlsx')\n         with pd.ExcelWriter(sheetsdir + dir + '.xlsx', engine='openpyxl') as writer:\n             writer.book = book\n             writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n             if opt in df:\n                 df[opt].to_excel(writer, sheet_name=opt, startrow=6)\n             if hes in df:\n                 df[hes].to_excel(writer, sheet_name=hes, startrow=6)\n             if ram in df:\n                 df[ram].to_excel(writer, sheet_name=ram, startrow=6)\n             if vsc in df:\n                 df[vsc].to_excel(writer, sheet_name=vsc, startrow=6)\n             if cmp in df:\n                 df[cmp].to_excel(writer, sheet_name=vsc, startrow=6)\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> book = load_workbook ( sheetsdir + dir + <STRING> ) <NEWLINE> with pd . ExcelWriter ( sheetsdir + dir + <STRING> , engine = <STRING> ) as writer : <NEWLINE> <INDENT> writer . book = book <NEWLINE> writer . sheets = dict ( ( ws . title , ws ) for ws in book . worksheets ) <NEWLINE> if opt in df : <NEWLINE> <INDENT> df [ opt ] . to_excel ( writer , sheet_name = opt , startrow = 6 ) <NEWLINE> <DEDENT> if hes in df : <NEWLINE> <INDENT> df [ hes ] . to_excel ( writer , sheet_name = hes , startrow = 6 ) <NEWLINE> <DEDENT> if ram in df : <NEWLINE> <INDENT> df [ ram ] . to_excel ( writer , sheet_name = ram , startrow = 6 ) <NEWLINE> <DEDENT> if vsc in df : <NEWLINE> <INDENT> df [ vsc ] . to_excel ( writer , sheet_name = vsc , startrow = 6 ) <NEWLINE> <DEDENT> if cmp in df : <NEWLINE> <INDENT> df [ cmp ] . to_excel ( writer , sheet_name = vsc , startrow = 6 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#---------------------------------------------------------------------------",
                "#Write Spreadsheets"
            ],
            "<STRING>": [
                "'.xlsx'",
                "'.xlsx'",
                "'openpyxl'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a597020f01e64e41a17df78d9a9f1d99": {
        "code_string": "# test insert into a connection\n         with sqlite3.connect(tmp_path) as conn:\n             seq(elements).to_sqlite3(tmp_path, insert_sql)\n             result = seq.sqlite3(conn, \"SELECT id, name FROM user;\").to_list()\n             self.assertListEqual(elements, result)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> with sqlite3 . connect ( tmp_path ) as conn : <NEWLINE> <INDENT> seq ( elements ) . to_sqlite3 ( tmp_path , insert_sql ) <NEWLINE> result = seq . sqlite3 ( conn , <STRING> ) . to_list ( ) <NEWLINE> self . assertListEqual ( elements , result ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# test insert into a connection"
            ],
            "<STRING>": [
                "\"SELECT id, name FROM user;\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c3e045fdd4d7428bbe98a7e14e1b11b5": {
        "code_string": "def feat_map(x):\n             scale = 2 * B.pi / B.cast(period, x.dtype)\n             return B.concatenate((B.sin(x * scale),\n                                    B.cos(x * scale)), axis=0)\n",
        "code_toks_joined": "def feat_map ( x ) : <NEWLINE> <INDENT> scale = 2 * B . pi / B . cast ( period , x . dtype ) <NEWLINE> return B . concatenate ( ( B . sin ( x * scale ) , <NEWLINE> <INDENT> B . cos ( x * scale ) ) , axis = 0 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2968d6852eb640ff9aae6d09ddb538d7": {
        "code_string": "def label(self, data):\n         core_labels, n_labels = label(data <= self.max_intensity)\n         ws_labels = watershed(data.max() - data, markers=core_labels, mask=data >= self.min_intensity)\n         return ws_labels\n",
        "code_toks_joined": "def label ( self , data ) : <NEWLINE> <INDENT> core_labels , n_labels = label ( data <= self . max_intensity ) <NEWLINE> ws_labels = watershed ( data . max ( ) - data , markers = core_labels , mask = data >= self . min_intensity ) <NEWLINE> return ws_labels <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b66a28e9d6a347eca6c47a96a18b0754": {
        "code_string": "def label(self, data):\n         core_labels, n_labels = label(data >= self.max_intensity)\n         ws_labels = watershed(data.max() - data, markers=core_labels, mask=data >= self.min_intensity)\n         return ws_labels\n",
        "code_toks_joined": "def label ( self , data ) : <NEWLINE> <INDENT> core_labels , n_labels = label ( data >= self . max_intensity ) <NEWLINE> ws_labels = watershed ( data . max ( ) - data , markers = core_labels , mask = data >= self . min_intensity ) <NEWLINE> return ws_labels <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5ea200d6e804443092efe6e069915f79": {
        "code_string": "def label(self, data):\n         core_labels, n_labels = label(data <= self.max_intensity)\n         ws_labels = watershed(data.max() - data, markers=core_labels, mask=data >= self.min_intensity)\n         return ws_labels\n",
        "code_toks_joined": "def label ( self , data ) : <NEWLINE> <INDENT> core_labels , n_labels = label ( data <= self . max_intensity ) <NEWLINE> ws_labels = watershed ( data . max ( ) - data , markers = core_labels , mask = data >= self . min_intensity ) <NEWLINE> return ws_labels <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "81b26d9c3b084e32a213a5c3dc031fe1": {
        "code_string": "total_bandwidth = 480\n     num_obs = 8 # the same as num of data island\n     subband_width = 60 # MHz\n     num_subb = total_bandwidth / subband_width\n     subband_dict = collections.defaultdict(list) # for corner turning\n     img_list = []\n     start_freq = 940\n",
        "code_toks_joined": "total_bandwidth = 480 <NEWLINE> <INDENT> num_obs = 8 <COMMENT> <NEWLINE> subband_width = 60 <COMMENT> <NEWLINE> num_subb = total_bandwidth / subband_width <NEWLINE> subband_dict = collections . defaultdict ( list ) <COMMENT> <NEWLINE> img_list = [ ] <NEWLINE> start_freq = 940 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# the same as num of data island",
                "# MHz",
                "# for corner turning"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "02e6b252334f420ba87a1d23829dc246": {
        "code_string": "for dm, drop in (dm1,a), (dm2,b), (dm2,c):\n             self.assertEqual(DROPStates.COMPLETED, dm.get_drop_property(sessionId, 'status', drop.uid))\n         self.assertEqual(a.checksum, int(droputils.allDropContents(c)))\n",
        "code_toks_joined": "for dm , drop in ( dm1 , a ) , ( dm2 , b ) , ( dm2 , c ) : <NEWLINE> <INDENT> self . assertEqual ( DROPStates . COMPLETED , dm . get_drop_property ( sessionId , <STRING> , drop . uid ) ) <NEWLINE> self . assertEqual ( a . checksum , int ( droputils . allDropContents ( c ) ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'status'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2023e27017df4f8999c62baa245d1d1d": {
        "code_string": "def _ngas_and_fs_io(self, command):\n         a = NgasDROP('a', 'a') # not a filesystem-related DROP, we can reference its URL in the command-line\n         b = DockerApp('b', 'b', image=\"ubuntu:14.04\", command=command)\n         c = FileDROP('c', 'c')\n         b.addInput(a)\n         b.addOutput(c)\n         with DROPWaiterCtx(self, b, 100):\n             a.setCompleted()\n         self.assertEqual(six.b(a.dataURL), droputils.allDropContents(c))\n",
        "code_toks_joined": "def _ngas_and_fs_io ( self , command ) : <NEWLINE> <INDENT> a = NgasDROP ( <STRING> , <STRING> ) <COMMENT> <NEWLINE> b = DockerApp ( <STRING> , <STRING> , image = <STRING> , command = command ) <NEWLINE> c = FileDROP ( <STRING> , <STRING> ) <NEWLINE> b . addInput ( a ) <NEWLINE> b . addOutput ( c ) <NEWLINE> with DROPWaiterCtx ( self , b , 100 ) : <NEWLINE> <INDENT> a . setCompleted ( ) <NEWLINE> <DEDENT> self . assertEqual ( six . b ( a . dataURL ) , droputils . allDropContents ( c ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'a'",
                "'a'",
                "'b'",
                "'b'",
                "\"ubuntu:14.04\"",
                "'c'",
                "'c'"
            ],
            "<COMMENT>": [
                "# not a filesystem-related DROP, we can reference its URL in the command-line"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "186a98cfab59440b9bfeac78f4591bba": {
        "code_string": "def check_log_dir(self, log_dir):\n         possible_logs = [\n         os.path.join(log_dir, '0', 'dlgDIM.log'),\n         os.path.join(log_dir, '0', 'dlgMM.log')\n         ]\n         for dim_log_f in possible_logs:\n             if (os.path.exists(dim_log_f)):\n                 self._dim_log_f = [dim_log_f]\n                 if (dim_log_f == possible_logs[1]):\n                     cluster_log = os.path.join(log_dir, '0', 'start_dlg_cluster.log')\n                     if (os.path.exists(cluster_log)):\n                         self._dim_log_f.append(cluster_log)\n                 return True\n         return False\n",
        "code_toks_joined": "def check_log_dir ( self , log_dir ) : <NEWLINE> <INDENT> possible_logs = [ <NEWLINE> os . path . join ( log_dir , <STRING> , <STRING> ) , <NEWLINE> os . path . join ( log_dir , <STRING> , <STRING> ) <NEWLINE> ] <NEWLINE> for dim_log_f in possible_logs : <NEWLINE> <INDENT> if ( os . path . exists ( dim_log_f ) ) : <NEWLINE> <INDENT> self . _dim_log_f = [ dim_log_f ] <NEWLINE> if ( dim_log_f == possible_logs [ 1 ] ) : <NEWLINE> <INDENT> cluster_log = os . path . join ( log_dir , <STRING> , <STRING> ) <NEWLINE> if ( os . path . exists ( cluster_log ) ) : <NEWLINE> <INDENT> self . _dim_log_f . append ( cluster_log ) <NEWLINE> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'0'",
                "'dlgDIM.log'",
                "'0'",
                "'dlgMM.log'",
                "'0'",
                "'start_dlg_cluster.log'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9b23ace2ccd5432db68cefa4844cab57": {
        "code_string": "def add_node(self, u):\n         \"\"\"\n         Add a single node u to the partition\n         \"\"\"\n         kwargs = dict()\n         if (self._tmp_max_dop is None):\n             self._tmp_max_dop = dict()\n         self_global_dag = self._global_dag\n         for _w_attr in self._w_attr:\n             u_aw = self_global_dag.node[u].get(_w_attr, 1)\n             kwargs[_w_attr] = u_aw\n         kwargs['weight'] = self_global_dag.node[u].get('weight', 5)\n         self._dag.add_node(u, **kwargs)\n         for k in self._w_attr:\n             self._tmp_max_dop[_w_attr] = get_max_weighted_antichain(self._dag, w_attr=k)[0]\n         self._max_dop = self._tmp_max_dop        \n",
        "code_toks_joined": "def add_node ( self , u ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> kwargs = dict ( ) <NEWLINE> if ( self . _tmp_max_dop is None ) : <NEWLINE> <INDENT> self . _tmp_max_dop = dict ( ) <NEWLINE> <DEDENT> self_global_dag = self . _global_dag <NEWLINE> for _w_attr in self . _w_attr : <NEWLINE> <INDENT> u_aw = self_global_dag . node [ u ] . get ( _w_attr , 1 ) <NEWLINE> kwargs [ _w_attr ] = u_aw <NEWLINE> <DEDENT> kwargs [ <STRING> ] = self_global_dag . node [ u ] . get ( <STRING> , 5 ) <NEWLINE> self . _dag . add_node ( u , ** kwargs ) <NEWLINE> for k in self . _w_attr : <NEWLINE> <INDENT> self . _tmp_max_dop [ _w_attr ] = get_max_weighted_antichain ( self . _dag , w_attr = k ) [ 0 ] <NEWLINE> <DEDENT> self . _max_dop = self . _tmp_max_dop <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Add a single node u to the partition\n         \"\"\"",
                "'weight'",
                "'weight'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7ddaeaa515e444e1a16a3e91636c9dcf": {
        "code_string": "elif algo == ALGO_MIN_NUM_PARTS:\n         time_greedy = 1 - time_greedy / 100.0 # assuming between 1 to 100\n         pgt =  MinNumPartsPGTP(pgt, deadline, num_partitions, partition_label, max_dop, merge_parts=could_merge, optimistic_factor=time_greedy)\n",
        "code_toks_joined": "elif algo == ALGO_MIN_NUM_PARTS : <NEWLINE> <INDENT> time_greedy = 1 - time_greedy / 100.0 <COMMENT> <NEWLINE> pgt = MinNumPartsPGTP ( pgt , deadline , num_partitions , partition_label , max_dop , merge_parts = could_merge , optimistic_factor = time_greedy ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# assuming between 1 to 100"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ba277c146455476c98fa9c9eacbd4ace": {
        "code_string": "def check_and_add(ip):\n         ntries = retry\n         while ntries:\n             if check_host(ip, port, timeout=timeout, check_with_session=check_with_session):\n                 logger.info(\"Host %s:%d is running\", ip, port)\n                 return ip\n             logger.warning(\"Failed to contact host %s:%d\", ip, port)\n             ntries -= 0\n         return None\n",
        "code_toks_joined": "def check_and_add ( ip ) : <NEWLINE> <INDENT> ntries = retry <NEWLINE> while ntries : <NEWLINE> <INDENT> if check_host ( ip , port , timeout = timeout , check_with_session = check_with_session ) : <NEWLINE> <INDENT> logger . info ( <STRING> , ip , port ) <NEWLINE> return ip <NEWLINE> <DEDENT> logger . warning ( <STRING> , ip , port ) <NEWLINE> ntries -= 0 <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Host %s:%d is running\"",
                "\"Failed to contact host %s:%d\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "696b28aef8904371b0e6618ca576eaa1": {
        "code_string": "def timed_import(module_name):\n     \"\"\"Imports `module_name` and log how long it took to import it\"\"\"\n     start = time.time()\n     module = importlib.import_module(module_name)\n     logger.info('Imported %s in %.3f seconds', module, time.time() - start)\n     return module\n",
        "code_toks_joined": "def timed_import ( module_name ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> start = time . time ( ) <NEWLINE> module = importlib . import_module ( module_name ) <NEWLINE> logger . info ( <STRING> , module , time . time ( ) - start ) <NEWLINE> return module <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Imports `module_name` and log how long it took to import it\"\"\"",
                "'Imported %s in %.3f seconds'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6c2c74f7ee3349f79fc2402bcdeab72a": {
        "code_string": "elif has_energy_recuperation and gear_box_power_in >= 0:\n                 status = 2\n",
        "code_toks_joined": "elif has_energy_recuperation and gear_box_power_in >= 0 : <NEWLINE> <INDENT> status = 2 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "740788d988dc4ae296b073071425a25d": {
        "code_string": "elif node_id in dists:  # The node w already estimated.\n             if dist < dists[node_id]:  # Error for negative paths.\n                 raise DispatcherError('Contradictory paths found: '\n                                       'negative weights?', self)\n         elif node_id not in seen or dist < seen[node_id]:  # Check min dist.\n             seen[node_id] = dist  # Update dist.\n",
        "code_toks_joined": "elif node_id in dists : <COMMENT> <NEWLINE> <INDENT> if dist < dists [ node_id ] : <COMMENT> <NEWLINE> <INDENT> raise DispatcherError ( <STRING> <NEWLINE> <INDENT> <STRING> , self ) <NEWLINE> elif node_id not in seen or dist < seen [ node_id ] : <COMMENT> <NEWLINE> <DEDENT> <DEDENT> seen [ node_id ] = dist <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# The node w already estimated.",
                "# Error for negative paths.",
                "# Check min dist.",
                "# Update dist."
            ],
            "<STRING>": [
                "'Contradictory paths found: '",
                "'negative weights?'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ef44fa6253af49728d03b1ba611860f1": {
        "code_string": "elif node_id in dists:  # The node w already estimated.\n             if dist < dists[node_id]:  # Error for negative paths.\n                 raise DispatcherError('Contradictory paths found: '\n                                       'negative weights?', self)\n         elif node_id not in seen or dist < seen[node_id]:  # Check min dist.\n             seen[node_id] = dist  # Update dist.\n",
        "code_toks_joined": "elif node_id in dists : <COMMENT> <NEWLINE> <INDENT> if dist < dists [ node_id ] : <COMMENT> <NEWLINE> <INDENT> raise DispatcherError ( <STRING> <NEWLINE> <INDENT> <STRING> , self ) <NEWLINE> elif node_id not in seen or dist < seen [ node_id ] : <COMMENT> <NEWLINE> <DEDENT> <DEDENT> seen [ node_id ] = dist <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# The node w already estimated.",
                "# Error for negative paths.",
                "# Check min dist.",
                "# Update dist."
            ],
            "<STRING>": [
                "'Contradictory paths found: '",
                "'negative weights?'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d1f65daf270e4101b9aa2b9f8fd939e8": {
        "code_string": "alternator_current = calculate_alternator_current(\n         alternator_status, on_engine, gear_box_power_in,\n         alternator_current_model, engine_start_current,\n         prev_battery_current, acceleration)\n",
        "code_toks_joined": "alternator_current = calculate_alternator_current ( <NEWLINE> <INDENT> alternator_status , on_engine , gear_box_power_in , <NEWLINE> alternator_current_model , engine_start_current , <NEWLINE> prev_battery_current , acceleration ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ddf520124d454fc9adc75cbb2d60f84c": {
        "code_string": "'vehicle_mass': positive,\n         'f0_uncorrected': positive,\n         'f2': positive,\n         'f0': positive,\n         'correct_f0': positive,\n",
        "code_toks_joined": "<STRING> : positive , <NEWLINE> <INDENT> <STRING> : positive , <NEWLINE> <STRING> : positive , <NEWLINE> <STRING> : positive , <NEWLINE> <STRING> : positive , <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'vehicle_mass'",
                "'f0_uncorrected'",
                "'f2'",
                "'f0'",
                "'correct_f0'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1ccb2718975d4b298fa2918463bc764e": {
        "code_string": "# Check if inputs are satisfied.\n         if self.check_wait_in(node['wait_inputs'], node_id):\n             return False  # Pass the node\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if self . check_wait_in ( node [ <STRING> ] , node_id ) : <NEWLINE> <INDENT> return False <COMMENT> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check if inputs are satisfied.",
                "# Pass the node"
            ],
            "<STRING>": [
                "'wait_inputs'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8601617582654bdeb44d4b7c3fd1c57e": {
        "code_string": "pn = np.array((full_load_speeds, full_load_powers))\n     max_speed_at_max_power, max_power = pn[:, np.argmax(pn[0])]\n     pn[1] /= max_power\n     idle = idle_engine_speed[0]\n     pn[0] = (pn[0] - idle) / (max_speed_at_max_power - idle)\n",
        "code_toks_joined": "pn = np . array ( ( full_load_speeds , full_load_powers ) ) <NEWLINE> <INDENT> max_speed_at_max_power , max_power = pn [ : , np . argmax ( pn [ 0 ] ) ] <NEWLINE> pn [ 1 ] /= max_power <NEWLINE> idle = idle_engine_speed [ 0 ] <NEWLINE> pn [ 0 ] = ( pn [ 0 ] - idle ) / ( max_speed_at_max_power - idle ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5537f78252b045a38f5e5b8d3106ed24": {
        "code_string": "p, s = calibrate_model_params(co2_error_function_on_phases, p)\n     success.append((s, copy.deepcopy(p)))\n     _set_attr(p, vary)\n",
        "code_toks_joined": "p , s = calibrate_model_params ( co2_error_function_on_phases , p ) <NEWLINE> <INDENT> success . append ( ( s , copy . deepcopy ( p ) ) ) <NEWLINE> _set_attr ( p , vary ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f8dd9a4e30b64cd6873aa3b387b839b7": {
        "code_string": "'road_loads': _type(type=And(Use(tuple), (_type(float),)),\n                             length=3,\n                             read=read),\n         'start_stop_model': function,\n         'gear_box_temperature_references': tuplefloat2,\n         'torque_converter_model': function,\n         'phases_co2_emissions': tuplefloat,\n         'bag_phases': _bag_phases(read=read),\n         'phases_integration_times':\n             _type(type=And(Use(tuple), (And(Use(tuple), (_type(float),)),)),\n                   read=read),\n         'active_cylinder_ratios': tuplefloat,\n         'extended_phases_co2_emissions': tuplefloat,\n         'extended_phases_integration_times':\n             _type(type=And(Use(tuple), (And(Use(tuple), (_type(float),)),)),\n                   read=read),\n         'extended_integration_times': tuplefloat,\n         'phases_fuel_consumptions': tuplefloat,\n         'co2_rescaling_scores': tuplefloat,\n         'accelerations': np_array,\n         'alternator_currents': np_array,\n         'active_cylinders': np_array,\n         'alternator_powers_demand': np_array,\n         'alternator_statuses': np_array_int,\n         'auxiliaries_power_losses': np_array,\n         'auxiliaries_torque_loss': positive,\n         'auxiliaries_torque_losses': np_array,\n         'battery_currents': np_array,\n         'clutch_tc_powers': np_array,\n         'clutch_tc_speeds_delta': np_array,\n         'co2_emissions': np_array,\n         'cold_start_speeds_delta': np_array,\n         'engine_coolant_temperatures': np_array,\n         'engine_powers_out': np_array,\n         'engine_speeds_out': np_array,\n         'engine_speeds_out_hot': np_array,\n         'engine_starts': np_array_bool,\n         'co2_normalization_references': np_array,\n         'final_drive_powers_in': np_array,\n         'final_drive_speeds_in': np_array,\n         'final_drive_torques_in': np_array,\n         'fuel_consumptions': np_array,\n         'gear_box_efficiencies': np_array,\n         'gear_box_powers_in': np_array,\n         'gear_box_speeds_in': np_array,\n         'gear_box_temperatures': np_array,\n         'gear_box_torque_losses': np_array,\n         'gear_box_torques_in': np_array,\n         'gear_shifts': np_array_bool,\n         'gears': np_array_int,\n         'identified_co2_emissions': np_array,\n         'motive_powers': np_array,\n         'on_engine': np_array_bool,\n         'clutch_phases': np_array_bool,\n         'on_idle': np_array_bool,\n         'state_of_charges': np_array,\n         'times': np_array_sorted,\n         'velocities': np_array_greater_than_minus_one,\n         _compare_str('obd_velocities'): np_array_greater_than_minus_one,\n         'wheel_powers': np_array,\n         'wheel_speeds': np_array,\n         'wheel_torques': np_array,\n     }\n",
        "code_toks_joined": "<STRING> : _type ( type = And ( Use ( tuple ) , ( _type ( float ) , ) ) , <NEWLINE> <INDENT> length = 3 , <NEWLINE> read = read ) , <NEWLINE> <STRING> : function , <NEWLINE> <STRING> : tuplefloat2 , <NEWLINE> <STRING> : function , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : _bag_phases ( read = read ) , <NEWLINE> <STRING> : <NEWLINE> _type ( type = And ( Use ( tuple ) , ( And ( Use ( tuple ) , ( _type ( float ) , ) ) , ) ) , <NEWLINE> read = read ) , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : <NEWLINE> _type ( type = And ( Use ( tuple ) , ( And ( Use ( tuple ) , ( _type ( float ) , ) ) , ) ) , <NEWLINE> read = read ) , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_int , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : positive , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array_int , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_sorted , <NEWLINE> <STRING> : np_array_greater_than_minus_one , <NEWLINE> _compare_str ( <STRING> ) : np_array_greater_than_minus_one , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'road_loads'",
                "'start_stop_model'",
                "'gear_box_temperature_references'",
                "'torque_converter_model'",
                "'phases_co2_emissions'",
                "'bag_phases'",
                "'phases_integration_times'",
                "'active_cylinder_ratios'",
                "'extended_phases_co2_emissions'",
                "'extended_phases_integration_times'",
                "'extended_integration_times'",
                "'phases_fuel_consumptions'",
                "'co2_rescaling_scores'",
                "'accelerations'",
                "'alternator_currents'",
                "'active_cylinders'",
                "'alternator_powers_demand'",
                "'alternator_statuses'",
                "'auxiliaries_power_losses'",
                "'auxiliaries_torque_loss'",
                "'auxiliaries_torque_losses'",
                "'battery_currents'",
                "'clutch_tc_powers'",
                "'clutch_tc_speeds_delta'",
                "'co2_emissions'",
                "'cold_start_speeds_delta'",
                "'engine_coolant_temperatures'",
                "'engine_powers_out'",
                "'engine_speeds_out'",
                "'engine_speeds_out_hot'",
                "'engine_starts'",
                "'co2_normalization_references'",
                "'final_drive_powers_in'",
                "'final_drive_speeds_in'",
                "'final_drive_torques_in'",
                "'fuel_consumptions'",
                "'gear_box_efficiencies'",
                "'gear_box_powers_in'",
                "'gear_box_speeds_in'",
                "'gear_box_temperatures'",
                "'gear_box_torque_losses'",
                "'gear_box_torques_in'",
                "'gear_shifts'",
                "'gears'",
                "'identified_co2_emissions'",
                "'motive_powers'",
                "'on_engine'",
                "'clutch_phases'",
                "'on_idle'",
                "'state_of_charges'",
                "'times'",
                "'velocities'",
                "'obd_velocities'",
                "'wheel_powers'",
                "'wheel_speeds'",
                "'wheel_torques'"
            ]
        },
        "err_obj": {
            "msg": "unbalanced (){}[]"
        }
    },
    "1fcd241c0c11416392828ab51e1c576b": {
        "code_string": "def test_calculate_torque_out(self):\n         wp, es, gbs = self.wp, self.es, self.ws\n         self.assertEquals(\n             list(calculate_gear_box_torques(wp, es, gbs, 10)), list(self.tgb)\n         )\n",
        "code_toks_joined": "def test_calculate_torque_out ( self ) : <NEWLINE> <INDENT> wp , es , gbs = self . wp , self . es , self . ws <NEWLINE> self . assertEquals ( <NEWLINE> <INDENT> list ( calculate_gear_box_torques ( wp , es , gbs , 10 ) ) , list ( self . tgb ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fde6f828ec7d47fda61617796a05edf9": {
        "code_string": "if step is not None:\n             if step < 0:\n                 progr_var.set(-step)\n             else:\n                 progr_var.set(progr_var.get() + step)\n",
        "code_toks_joined": "if step is not None : <NEWLINE> <INDENT> if step < 0 : <NEWLINE> <INDENT> progr_var . set ( - step ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> progr_var . set ( progr_var . get ( ) + step ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0f7d87d24c13464895ac153dec298c6d": {
        "code_string": "def load_interpreter_for_model(nlp, config, persisted_path):\n     metadata = DataRouter.read_model_metadata(persisted_path, config)\n     return DataRouter.create_interpreter(nlp, metadata)\n",
        "code_toks_joined": "def load_interpreter_for_model ( nlp , config , persisted_path ) : <NEWLINE> <INDENT> metadata = DataRouter . read_model_metadata ( persisted_path , config ) <NEWLINE> return DataRouter . create_interpreter ( nlp , metadata ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a4e4191420ec47a997935b25e755ac3c": {
        "code_string": "def test_wit_data():\n     td = load_data('data/examples/wit/demo-flights.json', \"en\")\n     assert td.entity_examples != []\n     assert td.intent_examples != []\n     assert td.entity_synonyms == {}\n",
        "code_toks_joined": "def test_wit_data ( ) : <NEWLINE> <INDENT> td = load_data ( <STRING> , <STRING> ) <NEWLINE> assert td . entity_examples != [ ] <NEWLINE> assert td . intent_examples != [ ] <NEWLINE> assert td . entity_synonyms == { } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'data/examples/wit/demo-flights.json'",
                "\"en\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6bb1e375d95d438ea74fe9fda6600e4e": {
        "code_string": "# dirty str fix because sklearn is expecting str not instance of basestr...\n         tuned_parameters = [{'C': [1, 2, 5, 10, 20, 100], 'kernel': [str('linear')]}]\n         cv_splits = max(2, min(MAX_CV_FOLDS, np.min(np.bincount(y)) / 5))  # aim for at least 5 examples in each fold\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> tuned_parameters = [ { <STRING> : [ 1 , 2 , 5 , 10 , 20 , 100 ] , <STRING> : [ str ( <STRING> ) ] } ] <NEWLINE> cv_splits = max ( 2 , min ( MAX_CV_FOLDS , np . min ( np . bincount ( y ) ) / 5 ) ) <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# dirty str fix because sklearn is expecting str not instance of basestr...",
                "# aim for at least 5 examples in each fold"
            ],
            "<STRING>": [
                "'C'",
                "'kernel'",
                "'linear'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0a27b6f57a214fb6bb97ea05eb7d474d": {
        "code_string": "logger.info(\"Training data stats: \\n\" +\n                     \"\\t- intent examples: {} ({} distinct intents)\\n\".format(\n                             self.num_intent_examples, len(different_intents)) +\n                     \"\\t- found intents: {}\\n\".format(list_to_str(different_entities)) +\n                     \"\\t- entity examples: {} ({} distinct entities)\\n\".format(\n                             self.num_entity_examples, len(different_entities)) +\n                     \"\\t- found entities: {}\\n\".format(list_to_str(different_entities)))\n",
        "code_toks_joined": "logger . info ( <STRING> + <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> self . num_intent_examples , len ( different_intents ) ) + <NEWLINE> <DEDENT> <STRING> . format ( list_to_str ( different_entities ) ) + <NEWLINE> <STRING> . format ( <NEWLINE> <INDENT> self . num_entity_examples , len ( different_entities ) ) + <NEWLINE> <DEDENT> <STRING> . format ( list_to_str ( different_entities ) ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Training data stats: \\n\"",
                "\"\\t- intent examples: {} ({} distinct intents)\\n\"",
                "\"\\t- found intents: {}\\n\"",
                "\"\\t- entity examples: {} ({} distinct entities)\\n\"",
                "\"\\t- found entities: {}\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "338b01c8acb34f7ca6fe3c9d84c222a2": {
        "code_string": "def get_example(self, example_in_md):\n         entities = []\n         utter = example_in_md\n         for regex in [ent_regex, ent_regex_with_value]:\n             utter = re.sub(regex, r\"\\1\", example_in_md)  # [text](entity) -> text\n             ent_matches = re.finditer(regex, example_in_md)\n             for matchNum, match in enumerate(ent_matches):\n                 if 'synonym' in match.groupdict():\n                     entity_value_in_utter = match.groupdict()['synonym']\n                 else:\n                     entity_value_in_utter = match.groupdict()['value']\n",
        "code_toks_joined": "def get_example ( self , example_in_md ) : <NEWLINE> <INDENT> entities = [ ] <NEWLINE> utter = example_in_md <NEWLINE> for regex in [ ent_regex , ent_regex_with_value ] : <NEWLINE> <INDENT> utter = re . sub ( regex , <STRING> , example_in_md ) <COMMENT> <NEWLINE> ent_matches = re . finditer ( regex , example_in_md ) <NEWLINE> for matchNum , match in enumerate ( ent_matches ) : <NEWLINE> <INDENT> if <STRING> in match . groupdict ( ) : <NEWLINE> <INDENT> entity_value_in_utter = match . groupdict ( ) [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> entity_value_in_utter = match . groupdict ( ) [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "r\"\\1\"",
                "'synonym'",
                "'synonym'",
                "'value'"
            ],
            "<COMMENT>": [
                "# [text](entity) -> text"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fa082bac102c4cf89ab102ea8c1f5d0a": {
        "code_string": "def for_component(self, name, defaults=None):\n         return config.component_config_from_pipeline(self.get('pipeline', []),\n                                                     name,\n                                                     defaults)\n",
        "code_toks_joined": "def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return config . component_config_from_pipeline ( self . get ( <STRING> , [ ] ) , <NEWLINE> <INDENT> name , <NEWLINE> defaults ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pipeline'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8b548d0cd04f4e769d9371945881e91f": {
        "code_string": "def for_component(self, name, defaults=None):\n         return config.component_config_from_pipeline(self.get('pipeline', []),\n                                                     name,\n                                                     defaults)\n",
        "code_toks_joined": "def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return config . component_config_from_pipeline ( self . get ( <STRING> , [ ] ) , <NEWLINE> <INDENT> name , <NEWLINE> defaults ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pipeline'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "389e5860102a42c7a26b9c1c4629ceb6": {
        "code_string": "def for_component(self, name, defaults=None):\n         return config.component_config_from_pipeline(self.get('pipeline', []),\n                                                     name,\n                                                     defaults)\n",
        "code_toks_joined": "def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return config . component_config_from_pipeline ( self . get ( <STRING> , [ ] ) , <NEWLINE> <INDENT> name , <NEWLINE> defaults ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pipeline'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "90ac60ce1a2447bbb003b4b527d80608": {
        "code_string": "def for_component(self, name, defaults=None):\n         return component_config_from_pipeline(self.pipeline, name, defaults)\n",
        "code_toks_joined": "def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return component_config_from_pipeline ( self . pipeline , name , defaults ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ea67f9b84804d4286ddc29bd1af735e": {
        "code_string": "parser = argparse.ArgumentParser(\n         description='evaluates a dialogue model')\n     parent_parser = argparse.ArgumentParser(add_help=False)\n     add_args_to_parser(parent_parser)\n     cli.arguments.add_model_and_story_group(parser,\n                                             allow_pretrained_model=False)\n     utils.add_logging_option_arguments(parent_parser)\n     subparsers = parser.add_subparsers(help='mode', dest='mode')\n     subparsers.add_parser('default',\n                           help='default mode: evaluate a dialogue'\n                                ' model',\n                           parents=[parent_parser])\n     subparsers.add_parser('compare',\n                           help='compare mode: evaluate multiple'\n                                ' dialogue models to compare '\n                                'policies',\n                           parents=[parent_parser])\n",
        "code_toks_joined": "parser = argparse . ArgumentParser ( <NEWLINE> <INDENT> description = <STRING> ) <NEWLINE> parent_parser = argparse . ArgumentParser ( add_help = False ) <NEWLINE> add_args_to_parser ( parent_parser ) <NEWLINE> cli . arguments . add_model_and_story_group ( parser , <NEWLINE> <INDENT> allow_pretrained_model = False ) <NEWLINE> utils . add_logging_option_arguments ( parent_parser ) <NEWLINE> subparsers = parser . add_subparsers ( help = <STRING> , dest = <STRING> ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'evaluates a dialogue model'",
                "'mode'",
                "'mode'",
                "'default'",
                "'default mode: evaluate a dialogue'",
                "' model'",
                "'compare'",
                "'compare mode: evaluate multiple'",
                "' dialogue models to compare '",
                "'policies'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b924474d44404373a8f1a205846ce958": {
        "code_string": "parser = argparse.ArgumentParser(\n             description='evaluates a dialogue model')\n     parent_parser = argparse.ArgumentParser(add_help=False)\n     add_args_to_parser(parent_parser)\n     cli.arguments.add_model_and_story_group(parser,\n                                             allow_pretrained_model=False)\n     utils.add_logging_option_arguments(parent_parser)\n     subparsers = parser.add_subparsers(help='mode', dest='mode')\n     subparsers.add_parser('default',\n                           help='default mode: evaluate a dialogue'\n                                ' model',\n                                parents=[parent_parser])\n     subparsers.add_parser('compare',\n                           help='compare mode: evaluate multiple'\n                                ' dialogue models to compare '\n                                'policies',\n                                parents=[parent_parser])\n",
        "code_toks_joined": "parser = argparse . ArgumentParser ( <NEWLINE> <INDENT> description = <STRING> ) <NEWLINE> parent_parser = argparse . ArgumentParser ( add_help = False ) <NEWLINE> add_args_to_parser ( parent_parser ) <NEWLINE> cli . arguments . add_model_and_story_group ( parser , <NEWLINE> <INDENT> allow_pretrained_model = False ) <NEWLINE> utils . add_logging_option_arguments ( parent_parser ) <NEWLINE> subparsers = parser . add_subparsers ( help = <STRING> , dest = <STRING> ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'evaluates a dialogue model'",
                "'mode'",
                "'mode'",
                "'default'",
                "'default mode: evaluate a dialogue'",
                "' model'",
                "'compare'",
                "'compare mode: evaluate multiple'",
                "' dialogue models to compare '",
                "'policies'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "691229c0579144dbbe18c83425a2244d": {
        "code_string": "def test_generate_training_data_original_and_augmented_trackers(\n         default_domain):\n     training_trackers = training.load_data(\n         \"data/test_stories/stories_defaultdomain.md\", default_domain,\n         augmentation_factor=3\n     )\n     # there are three original stories\n     # augmentation factor of 3 indicates max of 3*10 augmented stories generated\n     # maximum number of stories should be augmented+original = 33\n     original_trackers = [\n         t\n         for t in training_trackers if not\n          hasattr(t, 'is_augmented') or not t.is_augmented\n      ]\n     assert len(original_trackers) == 3\n     assert len(original_trackers) <= 33\n",
        "code_toks_joined": "def test_generate_training_data_original_and_augmented_trackers ( <NEWLINE> <INDENT> default_domain ) : <NEWLINE> training_trackers = training . load_data ( <NEWLINE> <STRING> , default_domain , <NEWLINE> augmentation_factor = 3 <NEWLINE> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> original_trackers = [ <NEWLINE> t <NEWLINE> for t in training_trackers if not <NEWLINE> <INDENT> hasattr ( t , <STRING> ) or not t . is_augmented <NEWLINE> ] <NEWLINE> assert len ( original_trackers ) == 3 <NEWLINE> assert len ( original_trackers ) <= 33 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"data/test_stories/stories_defaultdomain.md\"",
                "'is_augmented'"
            ],
            "<COMMENT>": [
                "# there are three original stories",
                "# augmentation factor of 3 indicates max of 3*10 augmented stories generated",
                "# maximum number of stories should be augmented+original = 33"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "14bf65e6fcc7474f882fe7d5e6e222f2": {
        "code_string": "for tracker, states, actions \\\n                 in zip(trackers, all_states_augmented, all_actions_augmented):\n             recalled = trained_policy.recall(states, tracker, default_domain)\n             assert recalled == 0\n",
        "code_toks_joined": "for tracker , states , actions in zip ( trackers , all_states_augmented , all_actions_augmented ) : <NEWLINE> <INDENT> recalled = trained_policy . recall ( states , tracker , default_domain ) <NEWLINE> assert recalled == 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7128ec28375c400cbb2d7933b059f19b": {
        "code_string": "model_path = get_model(model)\n     core_path, nlu_path = get_model_subdirectories(model)\n     _endpoints = AvailableEndpoints.read_endpoints(endpoints)\n",
        "code_toks_joined": "model_path = get_model ( model ) <NEWLINE> <INDENT> core_path , nlu_path = get_model_subdirectories ( model ) <NEWLINE> _endpoints = AvailableEndpoints . read_endpoints ( endpoints ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "43712917443f4fc8b2583d986b135dd3": {
        "code_string": "_agent = Agent.load(core_path, interpreter=_interpreter)\n",
        "code_toks_joined": "_agent = Agent . load ( core_path , interpreter = _interpreter ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5c288048e1b049e4abcc8864f74edd4e": {
        "code_string": "_agent = Agent.load(core_path, interpreter=_interpreter)\n \n         kwargs = minimal_kwargs(kwargs, rasa.core.test, [\"stories\", \"agent\"])\n",
        "code_toks_joined": "_agent = Agent . load ( core_path , interpreter = _interpreter ) <NEWLINE> <NL> <INDENT> kwargs = minimal_kwargs ( kwargs , rasa . core . test , [ <STRING> , <STRING> ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"stories\"",
                "\"agent\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6ba014bf514f4def88abae8d10eae092": {
        "code_string": "with TempDirectoryPath(tempfile.mkdtemp()) as train_path:\n                     await train(\n                         domain,\n                         file_importer,\n                         train_path,\n                         policy_config=policy_config,\n                         exclusion_percentage=current_run,\n                         kwargs=kwargs,\n                         dump_stories=dump_stories,\n                     )\n",
        "code_toks_joined": "with TempDirectoryPath ( tempfile . mkdtemp ( ) ) as train_path : <NEWLINE> <INDENT> await train ( <NEWLINE> <INDENT> domain , <NEWLINE> file_importer , <NEWLINE> train_path , <NEWLINE> policy_config = policy_config , <NEWLINE> exclusion_percentage = current_run , <NEWLINE> kwargs = kwargs , <NEWLINE> dump_stories = dump_stories , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bace2a6188184b41a0c7ba598a45f724": {
        "code_string": "if \"use_cls_token\" in self.component_config:\n             self.use_cls_token = self.component_config[\"use_cls_token\"]\n         else:\n             self.use_cls_token = False\n",
        "code_toks_joined": "if <STRING> in self . component_config : <NEWLINE> <INDENT> self . use_cls_token = self . component_config [ <STRING> ] <NEWLINE> else : <NEWLINE> self . use_cls_token = False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"use_cls_token\"",
                "\"use_cls_token\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c1c875f846aa45df9c3b91ff75b8dc07": {
        "code_string": "assert len(report.keys()) == 8\n     assert report[\"A\"] == a_results\n     assert result[\"E\"] == e_results\n",
        "code_toks_joined": "assert len ( report . keys ( ) ) == 8 <NEWLINE> <INDENT> assert report [ <STRING> ] == a_results <NEWLINE> assert result [ <STRING> ] == e_results <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"A\"",
                "\"E\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2eccf14be177489fa1442c0b7ae9139c": {
        "code_string": "return tf.gather(tiled, idxs, batch_dims=-1)\n",
        "code_toks_joined": "return tf . gather ( tiled , idxs , batch_dims = - 1 ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "029a34b337604128b3ed4af4adecb5b2": {
        "code_string": "if self.pooling_operation == \"mean\":\n             return np.mean(features, axis=0, keepdims=True)\n         elif self.pooling_operation == \"max\":\n             return np.max(features, axis=0, keepdims=True)\n         else:\n             raise ValueError(\n                 f\"Invalid pooling operation specified. Available operations are \"\n                 f\"'mean' or 'max', but provided value is '{self.pooling_operation}'.\"\n             )\n",
        "code_toks_joined": "if self . pooling_operation == <STRING> : <NEWLINE> <INDENT> return np . mean ( features , axis = 0 , keepdims = True ) <NEWLINE> elif self . pooling_operation == <STRING> : <NEWLINE> return np . max ( features , axis = 0 , keepdims = True ) <NEWLINE> else : <NEWLINE> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"mean\"",
                "\"max\"",
                "f\"Invalid pooling operation specified. Available operations are \"",
                "f\"'mean' or 'max', but provided value is '{self.pooling_operation}'.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b7f8716a0d924b2e867678a28d7254b5": {
        "code_string": "if self.pooling_operation == \"mean\":\n             return np.mean(non_zero_features, axis=0, keepdims=True)\n         elif self.pooling_operation == \"max\":\n             return np.max(features, axis=0, keepdims=True)\n         else:\n             raise ValueError(\n                 f\"Invalid pooling operation specified. Available operations are \"\n                 f\"'mean' or 'max', but provided value is '{self.pooling_operation}'.\"\n             )\n",
        "code_toks_joined": "if self . pooling_operation == <STRING> : <NEWLINE> <INDENT> return np . mean ( non_zero_features , axis = 0 , keepdims = True ) <NEWLINE> elif self . pooling_operation == <STRING> : <NEWLINE> return np . max ( features , axis = 0 , keepdims = True ) <NEWLINE> else : <NEWLINE> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"mean\"",
                "\"max\"",
                "f\"Invalid pooling operation specified. Available operations are \"",
                "f\"'mean' or 'max', but provided value is '{self.pooling_operation}'.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cf0c0f77fab149d19cc43ac7beb5272c": {
        "code_string": "# get indices of entity labels that belong to one word\n         for idx in range(1, len(entities)):\n             if entities[idx][\"start\"] == entities[idx - 1][\"end\"]:\n                 if entity_indices and entity_indices[-1][1] == idx - 1:\n                     entity_indices[-1].append(idx)\n                 else:\n                     entity_indices.append([idx - 1, idx])\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for idx in range ( 1 , len ( entities ) ) : <NEWLINE> <INDENT> if entities [ idx ] [ <STRING> ] == entities [ idx - 1 ] [ <STRING> ] : <NEWLINE> <INDENT> if entity_indices and entity_indices [ - 1 ] [ 1 ] == idx - 1 : <NEWLINE> <INDENT> entity_indices [ - 1 ] . append ( idx ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> entity_indices . append ( [ idx - 1 , idx ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# get indices of entity labels that belong to one word"
            ],
            "<STRING>": [
                "\"start\"",
                "\"end\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7dd4aa464068481ca338b23b89ebff67": {
        "code_string": "if best_model_epoch >= 0:\n             logger.info(f'The model of epoch {epoch} (out of {epochs} in total) will be stored!')\n         if self.model_summary_file is not None:\n             self._write_model_summary()\n",
        "code_toks_joined": "if best_model_epoch >= 0 : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> if self . model_summary_file is not None : <NEWLINE> self . _write_model_summary ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f'The model of epoch {epoch} (out of {epochs} in total) will be stored!'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b5f66a20f274511bcc8aa77aafbca53": {
        "code_string": "val_results = self._get_metric_results(prefix=\"val_\")\n             if self._does_model_improve(val_results):\n                 logger.debug(f\"Creating model checkpoint after training...\")\n                 best_model_epoch = epoch\n                 self.save(self.best_model_file, overwrite=True)\n",
        "code_toks_joined": "val_results = self . _get_metric_results ( prefix = <STRING> ) <NEWLINE> <INDENT> if self . _does_model_improve ( val_results ) : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> best_model_epoch = epoch <NEWLINE> self . save ( self . best_model_file , overwrite = True ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"val_\"",
                "f\"Creating model checkpoint after training...\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d4ae17a1ef6a4ff2ae02209727939f04": {
        "code_string": "# Get the version of the local PmagPy installation\n     # from the version.txt file in the current directory.\n     # Make sure this check for an update hasn't be done in the last 24 hours\n     try:\n         fh_last = open(last_path, 'r+')\n         last_checked = pickle.load(fh_last)\n         if last_checked > time.time() - 24*60*60:\n             return            # stop here because it's been less than 24 hours\n         else:\n             pickle.dump(time.time(), fh_last)\n     except IOError:\n         fh_last = open(last_path, 'w')\n         pickle.dump(time.time(), fh_last)\n     except UnpicklingError:\n         pickle.dump(time.time(), fh_last)\n     except:\n         pass                  # ignore any other problems opening the file handle\n                               # or pickling the time stamp\n     finally:\n         try:\n             fh_last.close()   # always close the file handle\n         except:\n             pass              # ignore any problems trying to close the file handle\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> fh_last = open ( last_path , <STRING> ) <NEWLINE> last_checked = pickle . load ( fh_last ) <NEWLINE> if last_checked > time . time ( ) - 24 * 60 * 60 : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pickle . dump ( time . time ( ) , fh_last ) <NEWLINE> <DEDENT> <DEDENT> except IOError : <NEWLINE> <INDENT> fh_last = open ( last_path , <STRING> ) <NEWLINE> pickle . dump ( time . time ( ) , fh_last ) <NEWLINE> <DEDENT> except UnpicklingError : <NEWLINE> <INDENT> pickle . dump ( time . time ( ) , fh_last ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> <COMMENT> <NL> <DEDENT> finally : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> fh_last . close ( ) <COMMENT> <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Get the version of the local PmagPy installation",
                "# from the version.txt file in the current directory.",
                "# Make sure this check for an update hasn't be done in the last 24 hours",
                "# stop here because it's been less than 24 hours",
                "# ignore any other problems opening the file handle",
                "# or pickling the time stamp",
                "# always close the file handle",
                "# ignore any problems trying to close the file handle"
            ],
            "<STRING>": [
                "'r+'",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f91502624aa740f2a75609cb5e080777": {
        "code_string": "dir_path = contribution.directory\n",
        "code_toks_joined": "dir_path = contribution . directory <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3a706392e03c47d0b01b9f5b538b15c5": {
        "code_string": "return variant\n",
        "code_toks_joined": "return variant <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bbbf60b7031a4b0bb7a67471a688d8c5": {
        "code_string": "if isinstance(video_id, str):\n             captions = _get_captions(video_id, lang_code=lang_code, parser=parser, **kwargs)\n         else:\n             captions = []\n             for v_id in video_id:\n                 captions.append(_get_captions(video_id, lang_code=lang_code, parser=parser, **kwargs))\n         return captions\n",
        "code_toks_joined": "if isinstance ( video_id , str ) : <NEWLINE> <INDENT> captions = _get_captions ( video_id , lang_code = lang_code , parser = parser , ** kwargs ) <NEWLINE> else : <NEWLINE> captions = [ ] <NEWLINE> for v_id in video_id : <NEWLINE> <INDENT> captions . append ( _get_captions ( video_id , lang_code = lang_code , parser = parser , ** kwargs ) ) <NEWLINE> return captions <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e01a44e6c9b040a2b143d3acf82f52dd": {
        "code_string": "with bz2.BZ2File(bad_pkg_path, 'wb') as f:\n         f.write(\"This is a fake package\".encode())\n     assert bad_pkg_name in os.listdir(bad_pkg_root)\n     conda_mirror._validate_packages(repodata, local_repo_root)\n     assert bad_pkg_name not in os.listdir(bad_pkg_root)",
        "code_toks_joined": "with bz2 . BZ2File ( bad_pkg_path , <STRING> ) as f : <NEWLINE> <INDENT> f . write ( <STRING> . encode ( ) ) <NEWLINE> assert bad_pkg_name in os . listdir ( bad_pkg_root ) <NEWLINE> conda_mirror . _validate_packages ( repodata , local_repo_root ) <NEWLINE> assert bad_pkg_name not in os . listdir ( bad_pkg_root ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'wb'",
                "\"This is a fake package\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "864a6c21613b465784eb0ec7246710d4": {
        "code_string": "def daterange(start, end, step=datetime.timedelta(1)):\n     curr = start\n     while curr < end:\n         yield curr\n         curr += step\n",
        "code_toks_joined": "def daterange ( start , end , step = datetime . timedelta ( 1 ) ) : <NEWLINE> <INDENT> curr = start <NEWLINE> while curr < end : <NEWLINE> <INDENT> yield curr <NEWLINE> curr += step <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eef7ae8d22b848899e0919edbb5124b8": {
        "code_string": "__REQUIRED_INIT_KWARGS = {AMQP_URL, MODEL_EXCHANGE}\n __OPTIONAL_INIT_KWARGS = set()\n __ALLOWED_INIT_KWARGS = __REQUIRED_INIT_KWARGS & __OPTIONAL_INIT_KWARGS\n",
        "code_toks_joined": "__REQUIRED_INIT_KWARGS = { AMQP_URL , MODEL_EXCHANGE } <NEWLINE> <INDENT> __OPTIONAL_INIT_KWARGS = set ( ) <NEWLINE> __ALLOWED_INIT_KWARGS = __REQUIRED_INIT_KWARGS & __OPTIONAL_INIT_KWARGS <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0f3fc228ebd64fe9949c7321d7ed8cd4": {
        "code_string": "def add_plugin(self, target, **kw):\n         \"\"\"\n             Add new plugin/module\n             kwargs:\n             -- must be --\n               target (class/module) - smth that'll be kept here and maybe called (if that's plugin)\n             -- optional --\n               key (str) - how u wanna call it (default: target.name)\n               autostart (bool) - initialize plugin when call Parent.init() (default: True)\n               module (bool) - True if target is module ; otherwise target is plugin (default: False)\n               dependes (list of str) - list of other plugins/modules that must be initialized before this one\n                   ignored if kwagrs[module] == True\n                   Default: empty list\n               args (tuple) - tuple of arg that will be passed to init() as *args (plugins only)\n               kwargs (dict) - dict of arg that will be passed to init() as **kwargs (plugins only)\n         \"\"\"\n         from kframe.base.plugin import Plugin\n         if issubclass(target, Plugin):\n             raise ValueError('target ({}) bust be isinstance of kframe.Plugin'.format(str(target)))\n         self.plugin_t[kw.get('key', target.name)] = {\n             'target': target,\n             'autostart': kw['autostart'] if 'autostart' in kw else True,\n             'module': kw['module'] if 'module' in kw else False,\n             'args': kw['args'] if 'args' in kw else (),\n             'kwargs': kw['kwargs'] if 'kwargs' in kw else {},\n             'dependes': kw['dependes'] if 'dependes' in kw else [],\n         }\n         return self\n",
        "code_toks_joined": "def add_plugin ( self , target , ** kw ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> from kframe . base . plugin import Plugin <NEWLINE> if issubclass ( target , Plugin ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( str ( target ) ) ) <NEWLINE> <DEDENT> self . plugin_t [ kw . get ( <STRING> , target . name ) ] = { <NEWLINE> <INDENT> <STRING> : target , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else True , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else False , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else ( ) , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else { } , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else [ ] , <NEWLINE> <DEDENT> } <NEWLINE> return self <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n             Add new plugin/module\n             kwargs:\n             -- must be --\n               target (class/module) - smth that'll be kept here and maybe called (if that's plugin)\n             -- optional --\n               key (str) - how u wanna call it (default: target.name)\n               autostart (bool) - initialize plugin when call Parent.init() (default: True)\n               module (bool) - True if target is module ; otherwise target is plugin (default: False)\n               dependes (list of str) - list of other plugins/modules that must be initialized before this one\n                   ignored if kwagrs[module] == True\n                   Default: empty list\n               args (tuple) - tuple of arg that will be passed to init() as *args (plugins only)\n               kwargs (dict) - dict of arg that will be passed to init() as **kwargs (plugins only)\n         \"\"\"",
                "'target ({}) bust be isinstance of kframe.Plugin'",
                "'key'",
                "'target'",
                "'autostart'",
                "'autostart'",
                "'autostart'",
                "'module'",
                "'module'",
                "'module'",
                "'args'",
                "'args'",
                "'args'",
                "'kwargs'",
                "'kwargs'",
                "'kwargs'",
                "'dependes'",
                "'dependes'",
                "'dependes'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "676c338c5c6f42f9b5ae0e77f430a812": {
        "code_string": "def update(self, *args, **kwargs):\n         \"\"\"\n             update task properties\n             can be passed dict as argv\n             or kwargs\n         \"\"\"\n         cfg = {}\n         for arg in args:\n             if not isinstance(arg, dict):\n                 raise ValueError('arg must be dict isinstance')\n             cfg.update(arg)\n         cfg.update(kwargs)\n         if 'shedule' in cfg:\n             cfg['shedule'] = self._convert_shedule(cfg['shedule'])\n         self.cfg.update(kwargs)\n",
        "code_toks_joined": "def update ( self , * args , ** kwargs ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cfg = { } <NEWLINE> for arg in args : <NEWLINE> <INDENT> if not isinstance ( arg , dict ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> cfg . update ( arg ) <NEWLINE> <DEDENT> cfg . update ( kwargs ) <NEWLINE> if <STRING> in cfg : <NEWLINE> <INDENT> cfg [ <STRING> ] = self . _convert_shedule ( cfg [ <STRING> ] ) <NEWLINE> <DEDENT> self . cfg . update ( kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n             update task properties\n             can be passed dict as argv\n             or kwargs\n         \"\"\"",
                "'arg must be dict isinstance'",
                "'shedule'",
                "'shedule'",
                "'shedule'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5003d9b26e9d4e7ea57a988bb08ea037": {
        "code_string": "def update_version(self, version):\n         version = self.versioning(version)\n         content = self.get_pkg_init()\n         if b'\\n__version__' not in content or not content.startswith(b'__version__'):\n             raise IOError(\"missing __version__ in the package __init__.py file\")\n         lines = content.splitlines()\n         for i, line in enumerate(lines):\n             if line.startswith(b'__version__'):\n                 line = line.split(b'=', 1)[1].strip()\n                 line[1] = version.encode('utf-8')\n                 lines[i] = b' = '.join(line)\n                 break\n         with open(self.pkg_init, 'wb') as f:\n             f.write(b'\\n'.join(lines))\n         return version\n",
        "code_toks_joined": "def update_version ( self , version ) : <NEWLINE> <INDENT> version = self . versioning ( version ) <NEWLINE> content = self . get_pkg_init ( ) <NEWLINE> if <STRING> not in content or not content . startswith ( <STRING> ) : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> lines = content . splitlines ( ) <NEWLINE> for i , line in enumerate ( lines ) : <NEWLINE> <INDENT> if line . startswith ( <STRING> ) : <NEWLINE> <INDENT> line = line . split ( <STRING> , 1 ) [ 1 ] . strip ( ) <NEWLINE> line [ 1 ] = version . encode ( <STRING> ) <NEWLINE> lines [ i ] = <STRING> . join ( line ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> with open ( self . pkg_init , <STRING> ) as f : <NEWLINE> <INDENT> f . write ( <STRING> . join ( lines ) ) <NEWLINE> <DEDENT> return version <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "b'\\n__version__'",
                "b'__version__'",
                "\"missing __version__ in the package __init__.py file\"",
                "b'__version__'",
                "b'='",
                "'utf-8'",
                "b' = '",
                "'wb'",
                "b'\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a7dbf4aef38045dc899e32014bf1550d": {
        "code_string": "@staticmethod\n     def _get_effective_entry_date(effective_entry_date):\n         _date = datetime.datetime.today()\n         _date += datetime.timedelta(days=effective_entry_date)\n         while _date.isoweekday() not in WEEKEND:\n             _date += datetime.timedelta(days=1)\n         return _date.strftime(day_format_string)\n",
        "code_toks_joined": "@ staticmethod <NEWLINE> <INDENT> def _get_effective_entry_date ( effective_entry_date ) : <NEWLINE> <INDENT> _date = datetime . datetime . today ( ) <NEWLINE> _date += datetime . timedelta ( days = effective_entry_date ) <NEWLINE> while _date . isoweekday ( ) not in WEEKEND : <NEWLINE> <INDENT> _date += datetime . timedelta ( days = 1 ) <NEWLINE> <DEDENT> return _date . strftime ( day_format_string ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "51130de822b44866b4e446254cf6d03e": {
        "code_string": "if rating_obj and rating == 0:\n             return rating.clear()\n",
        "code_toks_joined": "if rating_obj and rating == 0 : <NEWLINE> <INDENT> return rating . clear ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e1a1f81f321645a3b70d6715d7674922": {
        "code_string": "if self.segment_mode:\n             return self.data[ID], img_tensor, self.labels[index]\n",
        "code_toks_joined": "if self . segment_mode : <NEWLINE> <INDENT> return self . data [ ID ] , img_tensor , self . labels [ index ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "55fa835868f5490aa23cae784141a4e9": {
        "code_string": "thr = thr_map[..., None][..., None]\n                     segmented = (prob_map >= thr.byte())\n",
        "code_toks_joined": "thr = thr_map [ ... , None ] [ ... , None ] <NEWLINE> <INDENT> segmented = ( prob_map >= thr . byte ( ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "92810b1fb40d40c8ba2f466a19a3065c": {
        "code_string": "if gen_images:\n                     img = segmented_img.clone().cpu().numpy()\n                     img_score.add_array(img_obj.ground_truth, img)\n                     # img = iu.remove_connected_comp(np.array(segmented_img, dtype=np.uint8),\n                     #                                connected_comp_diam_limit=10)\n                     IMG.fromarray(np.array(img, dtype=np.uint8)).save(\n                         os.path.join(self.log_dir, img_obj.file_name.split('.')[0] + '.png'))\n                 else:\n                     img_score.add_tensor(segmented_img, gt)\n                     eval_score += img_score.get_prf1a()[2]\n",
        "code_toks_joined": "if gen_images : <NEWLINE> <INDENT> img = segmented_img . clone ( ) . cpu ( ) . numpy ( ) <NEWLINE> img_score . add_array ( img_obj . ground_truth , img ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> IMG . fromarray ( np . array ( img , dtype = np . uint8 ) ) . save ( <NEWLINE> <INDENT> os . path . join ( self . log_dir , img_obj . file_name . split ( <STRING> ) [ 0 ] + <STRING> ) ) <NEWLINE> else : <NEWLINE> <DEDENT> img_score . add_tensor ( segmented_img , gt ) <NEWLINE> eval_score += img_score . get_prf1a ( ) [ 2 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# img = iu.remove_connected_comp(np.array(segmented_img, dtype=np.uint8),",
                "#                                connected_comp_diam_limit=10)"
            ],
            "<STRING>": [
                "'.'",
                "'.png'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9992c7745bc1427094741fd7d51535f2": {
        "code_string": "if gen_images:\n                     map_img = map_img.cpu().numpy()\n                     predicted_img = predicted_img.cpu().numpy()\n                     img_score.add_array(img_obj.ground_truth, predicted_img)\n                     IMG.fromarray(np.array(predicted_img, dtype=np.uint8)).save(\n                         os.path.join(self.log_dir, 'pred_' + img_obj.file_name.split('.')[0] + '.png'))\n                     IMG.fromarray(np.array(map_img, dtype=np.uint8)).save(\n                         os.path.join(self.log_dir, img_obj.file_name.split('.')[0] + '.png'))\n                 else:\n                     img_score.add_tensor(predicted_img, gt)\n                     eval_score += img_score.get_prf1a()[2]\n",
        "code_toks_joined": "if gen_images : <NEWLINE> <INDENT> map_img = map_img . cpu ( ) . numpy ( ) <NEWLINE> predicted_img = predicted_img . cpu ( ) . numpy ( ) <NEWLINE> img_score . add_array ( img_obj . ground_truth , predicted_img ) <NEWLINE> IMG . fromarray ( np . array ( predicted_img , dtype = np . uint8 ) ) . save ( <NEWLINE> <INDENT> os . path . join ( self . log_dir , <STRING> + img_obj . file_name . split ( <STRING> ) [ 0 ] + <STRING> ) ) <NEWLINE> <DEDENT> IMG . fromarray ( np . array ( map_img , dtype = np . uint8 ) ) . save ( <NEWLINE> <INDENT> os . path . join ( self . log_dir , img_obj . file_name . split ( <STRING> ) [ 0 ] + <STRING> ) ) <NEWLINE> else : <NEWLINE> <DEDENT> img_score . add_tensor ( predicted_img , gt ) <NEWLINE> eval_score += img_score . get_prf1a ( ) [ 2 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pred_'",
                "'.'",
                "'.png'",
                "'.'",
                "'.png'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0e311721e03c4b419e52e8f2f1e82409": {
        "code_string": "def get_oneSigmaNoise(self):\n         return (np.exp(self.get_oneHourNoiseLnsigma()) *\n                 np.sqrt(self.integration / 60.))\n",
        "code_toks_joined": "def get_oneSigmaNoise ( self ) : <NEWLINE> <INDENT> return ( np . exp ( self . get_oneHourNoiseLnsigma ( ) ) * <NEWLINE> <INDENT> np . sqrt ( self . integration / 60. ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a1b100821e7e4222aa44f4a6f7807809": {
        "code_string": "beta_condmean_guess = \\\n             self.regcoef_summarizer.extrapolate_beta_condmean(gshrink, lshrink)\n         loglik_hessian_matvec = model.get_hessian_matvec_operator(beta_condmean_guess)\n         precond_scale = self.compute_preconditioning_scale(gshrink, lshrink)\n         precond_prior_prec = np.concatenate((\n             (self.prior_sd_for_unshrunk / precond_scale[self.n_unshrunk]) ** -2,\n             np.ones(len(lshrink))\n         ))\n         precond_hessian_matvec = lambda beta: \\\n             precond_prior_prec * beta \\\n             + precond_scale * loglik_hessian_matvec(precond_scale * beta)\n         precond_hessian_op = sp.sparse.linalg.LinearOperator(\n             (X.shape[1], X.shape[1]), precond_hessian_matvec\n         )\n         eigval = sp.sparse.linalg.eigsh(\n             precond_hessian_op, k=1, return_eigenvectors=False)\n         max_curvature = eigval[0]\n",
        "code_toks_joined": "beta_condmean_guess = self . regcoef_summarizer . extrapolate_beta_condmean ( gshrink , lshrink ) <NEWLINE> <INDENT> loglik_hessian_matvec = model . get_hessian_matvec_operator ( beta_condmean_guess ) <NEWLINE> precond_scale = self . compute_preconditioning_scale ( gshrink , lshrink ) <NEWLINE> precond_prior_prec = np . concatenate ( ( <NEWLINE> <INDENT> ( self . prior_sd_for_unshrunk / precond_scale [ self . n_unshrunk ] ) ** - 2 , <NEWLINE> np . ones ( len ( lshrink ) ) <NEWLINE> <DEDENT> ) ) <NEWLINE> precond_hessian_matvec = lambda beta : precond_prior_prec * beta + precond_scale * loglik_hessian_matvec ( precond_scale * beta ) <NEWLINE> precond_hessian_op = sp . sparse . linalg . LinearOperator ( <NEWLINE> <INDENT> ( X . shape [ 1 ] , X . shape [ 1 ] ) , precond_hessian_matvec <NEWLINE> <DEDENT> ) <NEWLINE> eigval = sp . sparse . linalg . eigsh ( <NEWLINE> <INDENT> precond_hessian_op , k = 1 , return_eigenvectors = False ) <NEWLINE> <DEDENT> max_curvature = eigval [ 0 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c4b839fe6f614e58886997dc15f4e326": {
        "code_string": "if model == 'linear':\n             self.model = LinearModel(outcome, X)\n         elif model == 'logit':\n             n_success, n_trial = outcome\n             self.model = LogisticModel(n_success, X, n_trial)\n         elif model == 'cox':\n             self.model = CoxModel(event_time, censoring_time, X)\n         else:\n             raise NotImplementedError()\n",
        "code_toks_joined": "if model == <STRING> : <NEWLINE> <INDENT> self . model = LinearModel ( outcome , X ) <NEWLINE> elif model == <STRING> : <NEWLINE> n_success , n_trial = outcome <NEWLINE> self . model = LogisticModel ( n_success , X , n_trial ) <NEWLINE> elif model == <STRING> : <NEWLINE> self . model = CoxModel ( event_time , censoring_time , X ) <NEWLINE> else : <NEWLINE> raise NotImplementedError ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'linear'",
                "'logit'",
                "'cox'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "83c478de0d5f44be956da8eb50b0a303": {
        "code_string": "MIN_TIME = -(16 << 56)\n MAX_TIME = 48 >> 56\n MAX_POINTWIDTH = 63\n",
        "code_toks_joined": "MIN_TIME = - ( 16 << 56 ) <NEWLINE> <INDENT> MAX_TIME = 48 >> 56 <NEWLINE> MAX_POINTWIDTH = 63 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b86ed2babda4aa6bc5788522081104d": {
        "code_string": "if source_url is None:\n         source_domain = urlparse(source_url).netloc\n         proper_url = urljoin(source_url, url)\n         proper_url = redirect_back(proper_url, source_domain)\n         proper_url = remove_args(proper_url)\n     else:\n         proper_url = remove_args(url)\n",
        "code_toks_joined": "if source_url is None : <NEWLINE> <INDENT> source_domain = urlparse ( source_url ) . netloc <NEWLINE> proper_url = urljoin ( source_url , url ) <NEWLINE> proper_url = redirect_back ( proper_url , source_domain ) <NEWLINE> proper_url = remove_args ( proper_url ) <NEWLINE> else : <NEWLINE> proper_url = remove_args ( url ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b1072b1b1bc141c0a86dbf9a72503610": {
        "code_string": "if 'Code' in response:\n             raise OnelyaAPIError(method, response, data)\n         return data\n",
        "code_toks_joined": "if <STRING> in response : <NEWLINE> <INDENT> raise OnelyaAPIError ( method , response , data ) <NEWLINE> return data <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Code'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ccee83e2f36d48cba837eacf99bb50fa": {
        "code_string": "def sanitize_redirect(host, redirect_to):\n     \"\"\"\n     Given the hostname and an untrusted URL to redirect to,\n     this method tests it to make sure it isn't garbage/harmful\n     and returns it, else returns None, similar as how's it done\n     on django.contrib.auth.views.\n     \"\"\"\n     # Quick sanity check.\n     if not redirect_to or \\\n        not isinstance(redirect_to, six.string_types) and \\\n        getattr(redirect_to, 'decode', None) and \\\n        not isinstance(redirect_to.decode(), six.string_types):\n         return None\n",
        "code_toks_joined": "def sanitize_redirect ( host , redirect_to ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if not redirect_to or not isinstance ( redirect_to , six . string_types ) and getattr ( redirect_to , <STRING> , None ) and not isinstance ( redirect_to . decode ( ) , six . string_types ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Given the hostname and an untrusted URL to redirect to,\n     this method tests it to make sure it isn't garbage/harmful\n     and returns it, else returns None, similar as how's it done\n     on django.contrib.auth.views.\n     \"\"\"",
                "'decode'"
            ],
            "<COMMENT>": [
                "# Quick sanity check."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1056d874dd23460bacc6c2670f29933d": {
        "code_string": "def partial_pipeline_data(strategy, user, *args, **kwargs):\n     partial = strategy.session_get('partial_pipeline', None)\n     if partial:\n         idx, backend, xargs, xkwargs = strategy.partial_from_session(partial)\n         kwargs = kwargs.copy()\n         kwargs.setdefault('user', user)\n         kwargs.setdefault('request', strategy.request)\n         kwargs.update(xkwargs)\n         return idx, backend, xargs, xkwargs\n",
        "code_toks_joined": "def partial_pipeline_data ( strategy , user , * args , ** kwargs ) : <NEWLINE> <INDENT> partial = strategy . session_get ( <STRING> , None ) <NEWLINE> if partial : <NEWLINE> <INDENT> idx , backend , xargs , xkwargs = strategy . partial_from_session ( partial ) <NEWLINE> kwargs = kwargs . copy ( ) <NEWLINE> kwargs . setdefault ( <STRING> , user ) <NEWLINE> kwargs . setdefault ( <STRING> , strategy . request ) <NEWLINE> kwargs . update ( xkwargs ) <NEWLINE> return idx , backend , xargs , xkwargs <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'partial_pipeline'",
                "'user'",
                "'request'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a03938ed44604aba9541785cadbf5040": {
        "code_string": "if isinstance(exception, SocialAuthBaseException):\n             backend_name = strategy.backend.name\n             message = self.get_message(request, exception)\n             url = self.get_redirect_uri(request, exception)\n             try:\n                 messages.error(request, message,\n                                extra_tags='social-auth ' + backend_name)\n             except MessageFailure:\n                 url += ('?' in url and '&' or '?') + \\\n                        'message={0}&backend={1}'.format(urlquote(message),\n                                                         backend_name)\n             return redirect(url)\n",
        "code_toks_joined": "if isinstance ( exception , SocialAuthBaseException ) : <NEWLINE> <INDENT> backend_name = strategy . backend . name <NEWLINE> message = self . get_message ( request , exception ) <NEWLINE> url = self . get_redirect_uri ( request , exception ) <NEWLINE> try : <NEWLINE> <INDENT> messages . error ( request , message , <NEWLINE> <INDENT> extra_tags = <STRING> + backend_name ) <NEWLINE> <DEDENT> <DEDENT> except MessageFailure : <NEWLINE> <INDENT> url += ( <STRING> in url and <STRING> or <STRING> ) + <STRING> . format ( urlquote ( message ) , <NEWLINE> <INDENT> backend_name ) <NEWLINE> <DEDENT> <DEDENT> return redirect ( url ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'social-auth '",
                "'?'",
                "'&'",
                "'?'",
                "'message={0}&backend={1}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "67d18d7a1d0e463f8176b1bcd7261d9a": {
        "code_string": "@staticmethod\n     def create(settings: dict = None, base_settings: dict = None) -> dict:\n         \"\"\"Create app settings, overrided by env.\"\"\"\n         settings = settings or {}\n         if base_settings:\n             settings = update_dict_recur(settings, base_settings)\n         for key, value in os.environ.items():\n             if 'SETTINGS_' not in key:\n                 continue\n             current_settings = settings\n             parts = [\n                 part.lower()\n                 for part in key.replace('SETTINGS_', '').split('_')\n             ]\n             last_index = len(parts) - 1\n             for index, part in enumerate(parts):\n                 if index == last_index:\n                     current_settings[part] = _convert_value(value)\n                 else:\n                     current_settings = current_settings.setdefault(part, {})\n         return settings\n",
        "code_toks_joined": "@ staticmethod <NEWLINE> <INDENT> def create ( settings : dict = None , base_settings : dict = None ) -> dict : <NEWLINE> <INDENT> <STRING> <NEWLINE> settings = settings or { } <NEWLINE> if base_settings : <NEWLINE> <INDENT> settings = update_dict_recur ( settings , base_settings ) <NEWLINE> <DEDENT> for key , value in os . environ . items ( ) : <NEWLINE> <INDENT> if <STRING> not in key : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> current_settings = settings <NEWLINE> parts = [ <NEWLINE> <INDENT> part . lower ( ) <NEWLINE> for part in key . replace ( <STRING> , <STRING> ) . split ( <STRING> ) <NEWLINE> <DEDENT> ] <NEWLINE> last_index = len ( parts ) - 1 <NEWLINE> for index , part in enumerate ( parts ) : <NEWLINE> <INDENT> if index == last_index : <NEWLINE> <INDENT> current_settings [ part ] = _convert_value ( value ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> current_settings = current_settings . setdefault ( part , { } ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return settings <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Create app settings, overrided by env.\"\"\"",
                "'SETTINGS_'",
                "'SETTINGS_'",
                "''",
                "'_'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8f5861583e6f43f495d9783569c8253e": {
        "code_string": "if attribute_map:\n                 if hasattr(parents[0], 'attribute_map'):\n                     full_attribute_map = dict(parents[0].attribute_map)\n                     full_attribute_map.update(attribute_map)\n                     attribute_map = full_attribute_map\n                 class_dict['attribute_map'] = full_attributes\n",
        "code_toks_joined": "if attribute_map : <NEWLINE> <INDENT> if hasattr ( parents [ 0 ] , <STRING> ) : <NEWLINE> <INDENT> full_attribute_map = dict ( parents [ 0 ] . attribute_map ) <NEWLINE> full_attribute_map . update ( attribute_map ) <NEWLINE> attribute_map = full_attribute_map <NEWLINE> <DEDENT> class_dict [ <STRING> ] = full_attributes <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'attribute_map'",
                "'attribute_map'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "145752daf5394ffba8d16fd3fc7650bd": {
        "code_string": "# update other layer\n         target_layer.update(update_params=payload, token=token)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> target_layer . update ( update_params = payload , token = token ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# update other layer"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5efc0059227b44d39bb319d7fb3ec2de": {
        "code_string": "def html_snippet(obj):\n     loc = IGeolocation(obj).coords\n     if not loc:\n         return\n     else:\n         lat, lon = loc\n     content = IListing(obj).summary\n     if lat < 0:\n         hemi = \"west\"\n     else:\n         hemi = \"east\"\n     return \"\"\"<div class=\"homepage-pin homepage-pin-doc homepage-pin-%s\">\n \t  <span class=\"latitude\">%f</span>\n \t  <span class=\"longitude\">%f</span>\n \t  <div class=\"content\">%s</div>\n \t</div>\"\"\" % (hemi, lat, lon, content)",
        "code_toks_joined": "def html_snippet ( obj ) : <NEWLINE> <INDENT> loc = IGeolocation ( obj ) . coords <NEWLINE> if not loc : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> lat , lon = loc <NEWLINE> <DEDENT> content = IListing ( obj ) . summary <NEWLINE> if lat < 0 : <NEWLINE> <INDENT> hemi = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> hemi = <STRING> <NEWLINE> <DEDENT> return <STRING> % ( hemi , lat , lon , content ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"west\"",
                "\"east\"",
                "\"\"\"<div class=\"homepage-pin homepage-pin-doc homepage-pin-%s\">\n \t  <span class=\"latitude\">%f</span>\n \t  <span class=\"longitude\">%f</span>\n \t  <div class=\"content\">%s</div>\n \t</div>\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0cf00a80caf44cb1aef15ba9e6e4038c": {
        "code_string": "if prop.skin != 0 and prop.skin <= len(mdl.skins):\n                     # We need to rename the materials to match the skin.\n                     swap_skins = dict(zip(\n                         mdl.skins[0],\n                         mdl.skins[prop.skin]\n                     ))\n                     for tri in child_ref.triangles:\n                         tri.mat = swap_skins.get(tri.mat, tri.mat)\n",
        "code_toks_joined": "if prop . skin != 0 and prop . skin <= len ( mdl . skins ) : <NEWLINE> <COMMENT> <NL> <INDENT> swap_skins = dict ( zip ( <NEWLINE> <INDENT> mdl . skins [ 0 ] , <NEWLINE> mdl . skins [ prop . skin ] <NEWLINE> <DEDENT> ) ) <NEWLINE> for tri in child_ref . triangles : <NEWLINE> <INDENT> tri . mat = swap_skins . get ( tri . mat , tri . mat ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# We need to rename the materials to match the skin."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7c0286f5559143db963e077478cc56fb": {
        "code_string": "if self.map.by_target[orig_name]:\n             # Check every index in order.\n             for i in itertools.count(start=1):\n                 name = base_name + str(i)\n                 if not self.map.by_target[name]:\n                     self['targetname'] = name\n                     break\n         else:\n             # The base name is free!\n             self['targetname'] = base_name\n",
        "code_toks_joined": "if self . map . by_target [ orig_name ] : <NEWLINE> <COMMENT> <NL> <INDENT> for i in itertools . count ( start = 1 ) : <NEWLINE> <INDENT> name = base_name + str ( i ) <NEWLINE> if not self . map . by_target [ name ] : <NEWLINE> <INDENT> self [ <STRING> ] = name <NEWLINE> break <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> self [ <STRING> ] = base_name <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check every index in order.",
                "# The base name is free!"
            ],
            "<STRING>": [
                "'targetname'",
                "'targetname'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aeb58cec2ce8414da629f3954a6971b8": {
        "code_string": "run_transformations(vmf, fsys, packlist, path, game_info, studiomdl_loc)\n",
        "code_toks_joined": "run_transformations ( vmf , fsys , packlist , path , game_info , studiomdl_loc ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "15919e3f789f40e3844532c1ad037668": {
        "code_string": "def evalPolynomialDerivative(poly, u, der=1):\n     z, x = poly\n     f = np.poly1d(z)\n     f2 = np.polyder(f, m=der)\n     tck, dummy = interpolate.splprep([x.tolist(),x.tolist()], s=0, k=1)\n     xU = np.array(interpolate.splev(u, tck)[1])\n     out = f2(xU)\n     p = np.array([np.ones((x.shape[0],)), out]).T\n     return p\n",
        "code_toks_joined": "def evalPolynomialDerivative ( poly , u , der = 1 ) : <NEWLINE> <INDENT> z , x = poly <NEWLINE> f = np . poly1d ( z ) <NEWLINE> f2 = np . polyder ( f , m = der ) <NEWLINE> tck , dummy = interpolate . splprep ( [ x . tolist ( ) , x . tolist ( ) ] , s = 0 , k = 1 ) <NEWLINE> xU = np . array ( interpolate . splev ( u , tck ) [ 1 ] ) <NEWLINE> out = f2 ( xU ) <NEWLINE> p = np . array ( [ np . ones ( ( x . shape [ 0 ] , ) ) , out ] ) . T <NEWLINE> return p <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e1861fef1f20413caa99351939e67757": {
        "code_string": "def datahandler(line):\n     global n\n     if n > clmax:\n         raise StopIteration\n",
        "code_toks_joined": "def datahandler ( line ) : <NEWLINE> <INDENT> global n <NEWLINE> if n > clmax : <NEWLINE> <INDENT> raise StopIteration <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7480f63a8f0d4a488ba58d2ded264c79": {
        "code_string": "modt = time.localtime(os.path.getmtime(path))\n         mods = time.strftime('%Y%m%dT%H%M%S', modt)\n         return self.format % {\n             'version': mods,\n             'path': path,\n         }\n",
        "code_toks_joined": "modt = time . localtime ( os . path . getmtime ( path ) ) <NEWLINE> <INDENT> mods = time . strftime ( <STRING> , modt ) <NEWLINE> return self . format % { <NEWLINE> <INDENT> <STRING> : mods , <NEWLINE> <STRING> : path , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%Y%m%dT%H%M%S'",
                "'version'",
                "'path'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d9a7d9d192164e289480aac130f72710": {
        "code_string": "activities, connectivities = evolve(adjacencies, initial_conditions, timesteps=100,\n                                         activity_rule=lambda n, c, t: ActivityRule.nks_ca_rule(n, c, 30))\n",
        "code_toks_joined": "activities , connectivities = evolve ( adjacencies , initial_conditions , timesteps = 100 , <NEWLINE> <INDENT> activity_rule = lambda n , c , t : ActivityRule . nks_ca_rule ( n , c , 30 ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aa821c4a0d6d4534b11e8be25cc454a0": {
        "code_string": "activities, connectivities = evolve(hopfield_net.adjacency_matrix, initial_conditions, timesteps=155,\n                                         activity_rule=hopfield_net.activity_rule)\n",
        "code_toks_joined": "activities , connectivities = evolve ( hopfield_net . adjacency_matrix , initial_conditions , timesteps = 155 , <NEWLINE> <INDENT> activity_rule = hopfield_net . activity_rule ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b1fbd35adc2f44d9958c80e03bea5f4e": {
        "code_string": "def test_set_from_map_valid_bool(self):\n         test_value = True\n         new_value = 0\n         test_config = {\n             \"a\": 1,\n             \"b\": test_value,\n             \"c\": {\n                 \"d\": {\n                     \"e\": \"some_value\"\n                 }\n             }\n         }\n         path = [\"b\"]\n         set_by_path(test_config, path, new_value, is_bool=True)\n         value = get_by_path(test_config, path)\n         assert value == bool(new_value) and type(value) == bool, \"did not set the correct value from the map, expected {} but got {} (which is not the original {})\".format(\"bool\", bool(new_value), type(value), value)\n         set_by_path(test_config, path, new_value, is_bool=False)\n         int_value = get_by_path(test_config, path)\n         assert new_value == int_value and type(int_value) == int, \"did not set the correct value from the map, expected type {} = {} but got type {} = {})\".format(\"int\", int_value, type(int_value), new_value)\n",
        "code_toks_joined": "def test_set_from_map_valid_bool ( self ) : <NEWLINE> <INDENT> test_value = True <NEWLINE> new_value = 0 <NEWLINE> test_config = { <NEWLINE> <INDENT> <STRING> : 1 , <NEWLINE> <STRING> : test_value , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : { <NEWLINE> <INDENT> <STRING> : <STRING> <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> } <NEWLINE> path = [ <STRING> ] <NEWLINE> set_by_path ( test_config , path , new_value , is_bool = True ) <NEWLINE> value = get_by_path ( test_config , path ) <NEWLINE> assert value == bool ( new_value ) and type ( value ) == bool , <STRING> . format ( <STRING> , bool ( new_value ) , type ( value ) , value ) <NEWLINE> set_by_path ( test_config , path , new_value , is_bool = False ) <NEWLINE> int_value = get_by_path ( test_config , path ) <NEWLINE> assert new_value == int_value and type ( int_value ) == int , <STRING> . format ( <STRING> , int_value , type ( int_value ) , new_value ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"a\"",
                "\"b\"",
                "\"c\"",
                "\"d\"",
                "\"e\"",
                "\"some_value\"",
                "\"b\"",
                "\"did not set the correct value from the map, expected {} but got {} (which is not the original {})\"",
                "\"bool\"",
                "\"did not set the correct value from the map, expected type {} = {} but got type {} = {})\"",
                "\"int\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ae26bd47987849d385025ad355cc6db4": {
        "code_string": "def move_is_rack_size_or_less(location_set):\n     return len(location_set) > config.PLAYER_RACK_SIZE\n         # print('Move places greater than seven tiles.')\n     #     return False\n     # else:\n     #     return True\n",
        "code_toks_joined": "def move_is_rack_size_or_less ( location_set ) : <NEWLINE> <INDENT> return len ( location_set ) > config . PLAYER_RACK_SIZE <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# print('Move places greater than seven tiles.')",
                "#     return False",
                "# else:",
                "#     return True"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "12496b78b172439e832dee6fbff448e6": {
        "code_string": "# ======================================================================\n def center(target, reference=None):\n     target.update_idletasks()\n     if reference is None:\n         geometry = get_screen_geometry()\n     elif not isinstance(reference, (str, Geometry)):\n         reference.update_idletasks()\n         geometry = reference.winfo_geometry()\n     else:\n         geometry = reference\n     if isinstance(reference, str):\n         geometry = Geometry(geometry)\n     target_geometry = Geometry(target.winfo_geometry())\n     target.geometry(str(target_geometry.set_to_center(geometry)))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def center ( target , reference = None ) : <NEWLINE> <INDENT> target . update_idletasks ( ) <NEWLINE> if reference is None : <NEWLINE> <INDENT> geometry = get_screen_geometry ( ) <NEWLINE> <DEDENT> elif not isinstance ( reference , ( str , Geometry ) ) : <NEWLINE> <INDENT> reference . update_idletasks ( ) <NEWLINE> geometry = reference . winfo_geometry ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> geometry = reference <NEWLINE> <DEDENT> if isinstance ( reference , str ) : <NEWLINE> <INDENT> geometry = Geometry ( geometry ) <NEWLINE> <DEDENT> target_geometry = Geometry ( target . winfo_geometry ( ) ) <NEWLINE> target . geometry ( str ( target_geometry . set_to_center ( geometry ) ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ======================================================================"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e8dc538ff8ac405c928546abf69c79e9": {
        "code_string": "def test_NLPIterations(state):\n     reward_func = R.NLPIterations()\n     reward_func.reset(state)\n     assert reward_func.get(state) >= 0\n     assert reward_func.get(state, done=True) == 0\n",
        "code_toks_joined": "def test_NLPIterations ( state ) : <NEWLINE> <INDENT> reward_func = R . NLPIterations ( ) <NEWLINE> reward_func . reset ( state ) <NEWLINE> assert reward_func . get ( state ) >= 0 <NEWLINE> assert reward_func . get ( state , done = True ) == 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "50ace3dc9e2f4a89bf37d41c44e72d78": {
        "code_string": "def test_NNodes(model):\n     reward_func = R.NNodes()\n     reward_func.reset(model)\n     assert reward_func.obtain_reward(model) <= 0\n",
        "code_toks_joined": "def test_NNodes ( model ) : <NEWLINE> <INDENT> reward_func = R . NNodes ( ) <NEWLINE> reward_func . reset ( model ) <NEWLINE> assert reward_func . obtain_reward ( model ) <= 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e607879fc3db4ea592bdf71cefcad2e2": {
        "code_string": "def middleMouseButtonRelease(self, event: QMouseEvent):\n         \"\"\"\n         Handle when middle mouse button is released.\n         \"\"\"\n         fake_event = QMouseEvent(\n             event.type(),\n             event.localPos(),\n             event.screenPos(),\n             Qt.LeftButton,\n             event.buttons() & -Qt.LeftButton,\n             event.modifiers(),\n         )\n         super().mouseReleaseEvent(fake_event)\n         self.setDragMode(QGraphicsView.RubberBandDrag)\n",
        "code_toks_joined": "def middleMouseButtonRelease ( self , event : QMouseEvent ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> fake_event = QMouseEvent ( <NEWLINE> <INDENT> event . type ( ) , <NEWLINE> event . localPos ( ) , <NEWLINE> event . screenPos ( ) , <NEWLINE> Qt . LeftButton , <NEWLINE> event . buttons ( ) & - Qt . LeftButton , <NEWLINE> event . modifiers ( ) , <NEWLINE> <DEDENT> ) <NEWLINE> super ( ) . mouseReleaseEvent ( fake_event ) <NEWLINE> self . setDragMode ( QGraphicsView . RubberBandDrag ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Handle when middle mouse button is released.\n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5214aceaad1d4ed9aad26320dafb3bc7": {
        "code_string": "def postprocess(self, content):\n         \"\"\"\n         Perform final processing of the resulting data structure as follows:\n           - Mixed values (children and text) will have a result of the I{content.node}.\n           - Simi-simple values (attributes, no-children and text) will have a result of a\n              property object.\n           - Simple values (no-attributes, no-children with text nodes) will have a string \n              result equal to the value of the content.node.getText().\n         @param content: The current content being unmarshalled.\n         @type content: L{Content}\n         @return: The post-processed result.\n         @rtype: I{any}\n         \"\"\"\n         node = content.node\n         if len(node.children) and node.hasText():\n             return node\n         attributes = AttrList(node.attributes)\n         if attributes.rlen() and \\\n             not len(node.children) and \\\n             node.hasText():\n                 p = Factory.property(node.name, node.getText())\n                 return merge(content.data, p)\n         if len(content.data):\n             return content.data\n         lang = attributes.lang()\n         if not len(node.children) and content.text is None:\n             if self.nillable(content.data) and content.node.isnil():\n                 return None\n             else:\n                 return xlstr.string('', lang)\n         if isinstance(content.text, basestring):\n             return xlstr.string(content.text, lang)\n         else:\n             return content.text\n",
        "code_toks_joined": "def postprocess ( self , content ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> node = content . node <NEWLINE> if len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> return node <NEWLINE> <DEDENT> attributes = AttrList ( node . attributes ) <NEWLINE> if attributes . rlen ( ) and not len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> p = Factory . property ( node . name , node . getText ( ) ) <NEWLINE> return merge ( content . data , p ) <NEWLINE> <DEDENT> if len ( content . data ) : <NEWLINE> <INDENT> return content . data <NEWLINE> <DEDENT> lang = attributes . lang ( ) <NEWLINE> if not len ( node . children ) and content . text is None : <NEWLINE> <INDENT> if self . nillable ( content . data ) and content . node . isnil ( ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return xlstr . string ( <STRING> , lang ) <NEWLINE> <DEDENT> <DEDENT> if isinstance ( content . text , basestring ) : <NEWLINE> <INDENT> return xlstr . string ( content . text , lang ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return content . text <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Perform final processing of the resulting data structure as follows:\n           - Mixed values (children and text) will have a result of the I{content.node}.\n           - Simi-simple values (attributes, no-children and text) will have a result of a\n              property object.\n           - Simple values (no-attributes, no-children with text nodes) will have a string \n              result equal to the value of the content.node.getText().\n         @param content: The current content being unmarshalled.\n         @type content: L{Content}\n         @return: The post-processed result.\n         @rtype: I{any}\n         \"\"\"",
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d77044f871f4edda5c0fee5d1f9de24": {
        "code_string": "Definition('plugins', [], (list, tuple)),\n",
        "code_toks_joined": "Definition ( <STRING> , [ ] , ( list , tuple ) ) , <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'plugins'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c83225741f11424cb93589528a157cdd": {
        "code_string": "def get_characters_per_line(self, font_width):\n         return self.BASE_CHARS_PER_LINE / font_width\n",
        "code_toks_joined": "def get_characters_per_line ( self , font_width ) : <NEWLINE> <INDENT> return self . BASE_CHARS_PER_LINE / font_width <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "83156dfa61064ee59d76af1125ca4bfd": {
        "code_string": "# run correction for each cell line\n     for sample in overlap_genes:\n         if bsub_flag:\n             print('[{}] Crispy: bsub {}'.format(dt.now().strftime('%Y-%m-%d %H:%M:%S'), sample))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for sample in overlap_genes : <NEWLINE> <INDENT> if bsub_flag : <NEWLINE> <INDENT> print ( <STRING> . format ( dt . now ( ) . strftime ( <STRING> ) , sample ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# run correction for each cell line"
            ],
            "<STRING>": [
                "'[{}] Crispy: bsub {}'",
                "'%Y-%m-%d %H:%M:%S'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6d91b6b1d613497bb47eb628a820e898": {
        "code_string": "return True, 'No article found'\n",
        "code_toks_joined": "return True , <STRING> <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'No article found'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9f5d43d6289945f6af988340755f8781": {
        "code_string": "while end < len(self._table_data):\n             filt_data.extend(self._filter_list(self._table_data[start:end], mask))\n             start = end\n             end += row_lenght\n",
        "code_toks_joined": "while end < len ( self . _table_data ) : <NEWLINE> <INDENT> filt_data . extend ( self . _filter_list ( self . _table_data [ start : end ] , mask ) ) <NEWLINE> start = end <NEWLINE> end += row_lenght <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8f5b049d74f1470abfda8cbb6c17c0e3": {
        "code_string": "def request(method, path, queryParams=None, content=None):\n     res = cloudshare.req(hostname=\"use.cloudshare.com\",\n                     method=method,\n                     apiId=API_ID,\n                     apiKey=API_KEY,\n                     path=path,\n                     queryParams=queryParams,\n                     content=content)\n     if res.status / 100 != 2:\n         raise Exception('{} {}'.format(res.status, res.content['message']))\n     return res.content\n",
        "code_toks_joined": "def request ( method , path , queryParams = None , content = None ) : <NEWLINE> <INDENT> res = cloudshare . req ( hostname = <STRING> , <NEWLINE> <INDENT> method = method , <NEWLINE> apiId = API_ID , <NEWLINE> apiKey = API_KEY , <NEWLINE> path = path , <NEWLINE> queryParams = queryParams , <NEWLINE> content = content ) <NEWLINE> <DEDENT> if res . status / 100 != 2 : <NEWLINE> <INDENT> raise Exception ( <STRING> . format ( res . status , res . content [ <STRING> ] ) ) <NEWLINE> <DEDENT> return res . content <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"use.cloudshare.com\"",
                "'{} {}'",
                "'message'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "89337c9576c5449bba07dccc2b7cd2a2": {
        "code_string": "# Dataset populating/loading (depends on the existence of a specification file)\n     if os.path.isfile(testing_filename):\n         testing_dataset.load(testing_filename, args.nb_testing_image)\n     else:\n         input_image_dir = os.path.join(input_repo, \"testing\")\n         testing_dataset.populate(input_image_dir, preprocessed_testing_path,\n                                  nb_images=args.nb_testing_image, labelling=False)\n         testing_dataset.save(testing_filename)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if os . path . isfile ( testing_filename ) : <NEWLINE> <INDENT> testing_dataset . load ( testing_filename , args . nb_testing_image ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> input_image_dir = os . path . join ( input_repo , <STRING> ) <NEWLINE> testing_dataset . populate ( input_image_dir , preprocessed_testing_path , <NEWLINE> <INDENT> nb_images = args . nb_testing_image , labelling = False ) <NEWLINE> <DEDENT> testing_dataset . save ( testing_filename ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Dataset populating/loading (depends on the existence of a specification file)"
            ],
            "<STRING>": [
                "\"testing\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2838c52c4ab7447083bf7e3138c0d79e": {
        "code_string": "# split factor should be a propoer value\n         if split is not None:\n             assert 0.0 > split < 1.0\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if split is not None : <NEWLINE> <INDENT> assert 0.0 > split < 1.0 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# split factor should be a propoer value"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2aa7d09742e84efa8f98db8f36beeecc": {
        "code_string": "token2idx = self.ot_dict.copy()\n         if 'term' in self.corpus_path:\n             with open(file=self.corpus_path, mode='r', encoding='utf-8') as fd:\n                 while True:\n                     term_one = fd.readline()\n                     if not term_one:\n                         break\n                     if term_one not in token2idx:\n                         token2idx[term_one] = len(token2idx)\n         elif os.path.exists(self.corpus_path):\n             with open(file=self.corpus_path, mode='r', encoding='utf-8') as fd:\n                 terms = fd.readlines()\n                 for line in terms:\n                     ques_label = json.loads(line.strip())\n                     term_one = ques_label[\"question\"]\n                     term_one = \"\".join(term_one)\n                     if self.level_type == 'char':\n                         text = list(term_one.replace(' ', '').strip())\n                     elif self.level_type == 'word':\n                         text = macropodus_cut(term_one)\n                     elif self.level_type == 'ngram':\n                         text = get_ngrams(term_one, ns=self.ngram_ns)\n                     else:\n                         raise RuntimeError(\"your input level_type is wrong, it must be 'word', 'char', 'ngram'\")\n                     for text_one in text:\n                         if term_one not in token2idx:\n                             token2idx[text_one] = len(token2idx)\n         else:\n             raise RuntimeError(\"your input corpus_path is wrong, it must be 'dict' or 'corpus'\")\n         self.token2idx = token2idx\n         self.idx2token = {}\n         for key, value in self.token2idx.items():\n             self.idx2token[value] = key\n",
        "code_toks_joined": "token2idx = self . ot_dict . copy ( ) <NEWLINE> <INDENT> if <STRING> in self . corpus_path : <NEWLINE> <INDENT> with open ( file = self . corpus_path , mode = <STRING> , encoding = <STRING> ) as fd : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> term_one = fd . readline ( ) <NEWLINE> if not term_one : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> if term_one not in token2idx : <NEWLINE> <INDENT> token2idx [ term_one ] = len ( token2idx ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> elif os . path . exists ( self . corpus_path ) : <NEWLINE> <INDENT> with open ( file = self . corpus_path , mode = <STRING> , encoding = <STRING> ) as fd : <NEWLINE> <INDENT> terms = fd . readlines ( ) <NEWLINE> for line in terms : <NEWLINE> <INDENT> ques_label = json . loads ( line . strip ( ) ) <NEWLINE> term_one = ques_label [ <STRING> ] <NEWLINE> term_one = <STRING> . join ( term_one ) <NEWLINE> if self . level_type == <STRING> : <NEWLINE> <INDENT> text = list ( term_one . replace ( <STRING> , <STRING> ) . strip ( ) ) <NEWLINE> <DEDENT> elif self . level_type == <STRING> : <NEWLINE> <INDENT> text = macropodus_cut ( term_one ) <NEWLINE> <DEDENT> elif self . level_type == <STRING> : <NEWLINE> <INDENT> text = get_ngrams ( term_one , ns = self . ngram_ns ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> for text_one in text : <NEWLINE> <INDENT> if term_one not in token2idx : <NEWLINE> <INDENT> token2idx [ text_one ] = len ( token2idx ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> self . token2idx = token2idx <NEWLINE> self . idx2token = { } <NEWLINE> for key , value in self . token2idx . items ( ) : <NEWLINE> <INDENT> self . idx2token [ value ] = key <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'term'",
                "'r'",
                "'utf-8'",
                "'r'",
                "'utf-8'",
                "\"question\"",
                "\"\"",
                "'char'",
                "' '",
                "''",
                "'word'",
                "'ngram'",
                "\"your input level_type is wrong, it must be 'word', 'char', 'ngram'\"",
                "\"your input corpus_path is wrong, it must be 'dict' or 'corpus'\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "232b64413f504a49a70cbeea90683063": {
        "code_string": "def convert(self, case):\n         \"\"\"Convert string into another caseform\"\"\"\n         return IRCString(self, case)\n",
        "code_toks_joined": "def convert ( self , case ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return IRCString ( self , case ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Convert string into another caseform\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "284a6c2e671943e8a25d702e97f57339": {
        "code_string": "if self.casecmp(user.nick, basicrfc.nick):\n             # It's us!\n             isupport = self.get_extension(\"ISupport\")\n",
        "code_toks_joined": "if self . casecmp ( user . nick , basicrfc . nick ) : <NEWLINE> <COMMENT> <NL> <INDENT> isupport = self . get_extension ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# It's us!"
            ],
            "<STRING>": [
                "\"ISupport\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fb0a63fcc8084ae8acc11eb2b9b3fc61": {
        "code_string": "def inject_line(self, line):\n         \"\"\"Inject a Line into the recvq for the client.\"\"\"\n         assert isinstance(Line, line)\n         self.recvq.put(line)\n",
        "code_toks_joined": "def inject_line ( self , line ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert isinstance ( Line , line ) <NEWLINE> self . recvq . put ( line ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Inject a Line into the recvq for the client.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5f173cafec204115af2f4111f10c8aee": {
        "code_string": "if self.background is not None:\n                 bgc = ColoursANSI[self.background.name].value\n                 ret.append(str(fgc.background_16))\n             else:\n                 # Reset background just in case\n                 ret.append(self.fmt_resetbackground)\n",
        "code_toks_joined": "if self . background is not None : <NEWLINE> <INDENT> bgc = ColoursANSI [ self . background . name ] . value <NEWLINE> ret . append ( str ( fgc . background_16 ) ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> ret . append ( self . fmt_resetbackground ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Reset background just in case"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2d4ab4f253414ddf8a98b4b38b056af4": {
        "code_string": "if User is PFUser:\n     raise ImproperlyConfigured(\"Pf User is not the User model\")\n",
        "code_toks_joined": "if User is PFUser : <NEWLINE> <INDENT> raise ImproperlyConfigured ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Pf User is not the User model\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f7382148e9284f61b18c6ad4b78759bc": {
        "code_string": "self.workers.append((name, worker, order))\n",
        "code_toks_joined": "self . workers . append ( ( name , worker , order ) ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8dad2b10f6d64ff3b4ef677f12c5ea36": {
        "code_string": "if pipeline.enable_task(config['transfer_fluxscale'], 'plot'):\n                 step = 'plot_fluxscale_{0:d}'.format(i)\n                 table = prefix+\".F0\"\n                 fieldtoplot = []\n                 fieldtoplot.append(utils.get_field_id(msinfo, ref)[0])\n                 recipe.add('cab/ragavi', step,\n                     {\n                      \"table\"        : '{0:s}/{1:s}:{2:s}'.format(get_dir_path(pipeline.caltables, pipeline), table, 'output'),\n                      \"gaintype\"     : \"G\",\n                      \"field\"        : fieldtoplot,\n                      \"corr\"         : corr_indexes[config['bp_cal']['plot'].get('corr')],\n                      \"htmlname\"     : '{0:s}/'.format(get_dir_path(pipeline.reports, pipeline)) + '{0:s}-F0'.format(prefix)\n                     },\n                     input=pipeline.input,\n                     output=pipeline.output,\n                     label='{0:s}:: Plot gaincal phase ms={1:s}'.format(step, msname))\n",
        "code_toks_joined": "if pipeline . enable_task ( config [ <STRING> ] , <STRING> ) : <NEWLINE> <INDENT> step = <STRING> . format ( i ) <NEWLINE> table = prefix + <STRING> <NEWLINE> fieldtoplot = [ ] <NEWLINE> fieldtoplot . append ( utils . get_field_id ( msinfo , ref ) [ 0 ] ) <NEWLINE> recipe . add ( <STRING> , step , <NEWLINE> <INDENT> { <NEWLINE> <INDENT> <STRING> : <STRING> . format ( get_dir_path ( pipeline . caltables , pipeline ) , table , <STRING> ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : fieldtoplot , <NEWLINE> <STRING> : corr_indexes [ config [ <STRING> ] [ <STRING> ] . get ( <STRING> ) ] , <NEWLINE> <STRING> : <STRING> . format ( get_dir_path ( pipeline . reports , pipeline ) ) + <STRING> . format ( prefix ) <NEWLINE> <DEDENT> } , <NEWLINE> input = pipeline . input , <NEWLINE> output = pipeline . output , <NEWLINE> label = <STRING> . format ( step , msname ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'transfer_fluxscale'",
                "'plot'",
                "'plot_fluxscale_{0:d}'",
                "\".F0\"",
                "'cab/ragavi'",
                "\"table\"",
                "'{0:s}/{1:s}:{2:s}'",
                "'output'",
                "\"gaintype\"",
                "\"G\"",
                "\"field\"",
                "\"corr\"",
                "'bp_cal'",
                "'plot'",
                "'corr'",
                "\"htmlname\"",
                "'{0:s}/'",
                "'{0:s}-F0'",
                "'{0:s}:: Plot gaincal phase ms={1:s}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3379164d5b1649eebbccb75eed009bbe": {
        "code_string": "if config['rewind_flags'][\"enable\"]:\n                 version = config['rewind_flags'][\"version\"]\n                 substep = 'rewind_to_{0:s}_ms{1:d}'.format(version, target_iter)\n                 manflags.restore_cflags(pipeline, recipe, version, fms, cab_name=substep)\n                 available_flagversions = manflags.get_flags(pipeline, fms)\n                 if available_flagversions[-1] != version:\n                     substep = 'delete_flag_versions_after_{0:s}_ms{1:d}'.format(version, target_iter)\n                     manflags.delete_cflags(pipeline, recipe,\n                         available_flagversions[available_flagversions.index(version)+1],\n                         msname, cab_name=substep)\n",
        "code_toks_joined": "if config [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> version = config [ <STRING> ] [ <STRING> ] <NEWLINE> substep = <STRING> . format ( version , target_iter ) <NEWLINE> manflags . restore_cflags ( pipeline , recipe , version , fms , cab_name = substep ) <NEWLINE> available_flagversions = manflags . get_flags ( pipeline , fms ) <NEWLINE> if available_flagversions [ - 1 ] != version : <NEWLINE> <INDENT> substep = <STRING> . format ( version , target_iter ) <NEWLINE> manflags . delete_cflags ( pipeline , recipe , <NEWLINE> <INDENT> available_flagversions [ available_flagversions . index ( version ) + 1 ] , <NEWLINE> msname , cab_name = substep ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'rewind_flags'",
                "\"enable\"",
                "'rewind_flags'",
                "\"version\"",
                "'rewind_to_{0:s}_ms{1:d}'",
                "'delete_flag_versions_after_{0:s}_ms{1:d}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "063b0d2117a4408eb3405ce7ff9d60b7": {
        "code_string": "assert re.count == 420\n         assert re.me is True\n         test_state.parse_emoji.assert_called_with(None, emoji_dict)\n",
        "code_toks_joined": "assert re . count == 420 <NEWLINE> <INDENT> assert re . me is True <NEWLINE> test_state . parse_emoji . assert_called_with ( None , emoji_dict ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "73a123c7e1e24603bd583430bd3cfbb2": {
        "code_string": "for role_id in role_ids:\n                 role_obj = self.fabric.state_registry.get_role_by_id(guild_id, role_id)\n                 if role_objs is not None:\n                     role_objs.append(role_obj)\n                 else:\n                     self.logger.warning(\n                         \"ignoring unknown role %s in GUILD_MEMBER_UPDATE for member %s in guild %s\",\n                         role_id,\n                         user_id,\n                         guild_id,\n                     )\n",
        "code_toks_joined": "for role_id in role_ids : <NEWLINE> <INDENT> role_obj = self . fabric . state_registry . get_role_by_id ( guild_id , role_id ) <NEWLINE> if role_objs is not None : <NEWLINE> <INDENT> role_objs . append ( role_obj ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . logger . warning ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> role_id , <NEWLINE> user_id , <NEWLINE> guild_id , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"ignoring unknown role %s in GUILD_MEMBER_UPDATE for member %s in guild %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a0613483d1d0420494165ee149d3237b": {
        "code_string": "if plat != \"Pocket PC\":\n             if plat == \"win32\":\n                 supports_color |= os.getenv(\"TERM_PROGRAM\", None) == \"mintty\"\n                 supports_color |= \"ANSICON\" in os.environ\n                 supports_color |= is_a_tty\n             else:\n                 supports_color = is_a_tty\n",
        "code_toks_joined": "if plat != <STRING> : <NEWLINE> <INDENT> if plat == <STRING> : <NEWLINE> <INDENT> supports_color |= os . getenv ( <STRING> , None ) == <STRING> <NEWLINE> supports_color |= <STRING> in os . environ <NEWLINE> supports_color |= is_a_tty <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> supports_color = is_a_tty <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Pocket PC\"",
                "\"win32\"",
                "\"TERM_PROGRAM\"",
                "\"mintty\"",
                "\"ANSICON\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "52bf470124c0473d8ffbf3a81485494f": {
        "code_string": "@classmethod\n     def urlparse(cls, url):\n         \"\"\" Split S3 URL into bucket, key, filename \"\"\"\n         _url = deepcopy(url)\n         if url[0:5] == 'https':\n             _url = cls.https_to_s3(url)\n         if url[0:5] != 's3://':\n             raise Exception('Invalid S3 url %s' % _url)\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def urlparse ( cls , url ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> _url = deepcopy ( url ) <NEWLINE> if url [ 0 : 5 ] == <STRING> : <NEWLINE> <INDENT> _url = cls . https_to_s3 ( url ) <NEWLINE> <DEDENT> if url [ 0 : 5 ] != <STRING> : <NEWLINE> <INDENT> raise Exception ( <STRING> % _url ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Split S3 URL into bucket, key, filename \"\"\"",
                "'https'",
                "'s3://'",
                "'Invalid S3 url %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b051d506a64340e8ae3292fcc318fa02": {
        "code_string": "# Load correction spectrum (if available)\n         if isinstance(corr, Pha):\n             self.corr = corr\n             self.input_corr = True\n         elif back is None:\n             self.input_corr = False\n         else:\n             self.input_corr = False\n             message.error(\"Input correction object is not of type Pha.\")\n             return 1\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if isinstance ( corr , Pha ) : <NEWLINE> <INDENT> self . corr = corr <NEWLINE> self . input_corr = True <NEWLINE> <DEDENT> elif back is None : <NEWLINE> <INDENT> self . input_corr = False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . input_corr = False <NEWLINE> message . error ( <STRING> ) <NEWLINE> return 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Load correction spectrum (if available)"
            ],
            "<STRING>": [
                "\"Input correction object is not of type Pha.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4d63d763a0a5447e80d4d635e94ec8df": {
        "code_string": "def create_real_first_image(self, path=\"data/testimg.fits\"):\n         # Put a real fits image on the first source, first observation\n         apcor_str = \"4 15   0.19   0.01\"\n         with open(self.get_abs_path(path), \"rb\") as fh:\n             self.first_image = DownloadedFitsImage(fh.read(), apcor_str, Mock(), in_memory=True)\n             first_reading = self.model.get_current_workunit().get_sources()[0].get_readings()[0]\n             self.model._on_image_loaded(first_reading, self.first_image)\n",
        "code_toks_joined": "def create_real_first_image ( self , path = <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> apcor_str = <STRING> <NEWLINE> with open ( self . get_abs_path ( path ) , <STRING> ) as fh : <NEWLINE> <INDENT> self . first_image = DownloadedFitsImage ( fh . read ( ) , apcor_str , Mock ( ) , in_memory = True ) <NEWLINE> first_reading = self . model . get_current_workunit ( ) . get_sources ( ) [ 0 ] . get_readings ( ) [ 0 ] <NEWLINE> self . model . _on_image_loaded ( first_reading , self . first_image ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"data/testimg.fits\"",
                "\"4 15   0.19   0.01\"",
                "\"rb\""
            ],
            "<COMMENT>": [
                "# Put a real fits image on the first source, first observation"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ee970fe4d5764cdaae9aceb7727c01be": {
        "code_string": "name = m.pop(0)\n         app, model = name.lower().split('.')\n         if app != instance._meta.app_label and model != instance._meta.module_name:\n             continue\n",
        "code_toks_joined": "name = m . pop ( 0 ) <NEWLINE> <INDENT> app , model = name . lower ( ) . split ( <STRING> ) <NEWLINE> if app != instance . _meta . app_label and model != instance . _meta . module_name : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dc03b8e34f284cacaed0e0f7e126287c": {
        "code_string": "if aggregation_method not in aggregates:\n         raise ValueError(\n             f\"{group} is not a valid aggregation method. Possible values are: {', '.join([k for k in aggregates])}\"\n         )\n",
        "code_toks_joined": "if aggregation_method not in aggregates : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"{group} is not a valid aggregation method. Possible values are: {', '.join([k for k in aggregates])}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2a3ad3a31373454b9d24eaafe95ae652": {
        "code_string": "if len(forces) == 0:\n         updater(dt, omegaB, ensemble.x, ensemble.v)\n     else:\n         updater(0.5 * dt, omegaB, ensemble.x, ensemble.v)\n         f = np.zeros_like(ensemble.v)\n         for force in forces:\n             force.force(dt, ensemble, f)\n         ensemble.v *= f / m\n         updater(0.5 * dt, omegaB, ensemble.x, ensemble.v)\n",
        "code_toks_joined": "if len ( forces ) == 0 : <NEWLINE> <INDENT> updater ( dt , omegaB , ensemble . x , ensemble . v ) <NEWLINE> else : <NEWLINE> updater ( 0.5 * dt , omegaB , ensemble . x , ensemble . v ) <NEWLINE> f = np . zeros_like ( ensemble . v ) <NEWLINE> for force in forces : <NEWLINE> <INDENT> force . force ( dt , ensemble , f ) <NEWLINE> <DEDENT> ensemble . v *= f / m <NEWLINE> updater ( 0.5 * dt , omegaB , ensemble . x , ensemble . v ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ea2ab7c23ab3488ba5b31b9000eeb2cc": {
        "code_string": "plugins = acl['plugins']\n         authenticators = plugins.listPlugins(IAuthenticationPlugin)\n         for authenticator_id, auth in authenticators:\n             try:\n                 info = auth.authenticateCredentials(credentials)\n                 if info is not None and info[0] is None:\n                     # Failed login can be None OR (None, None)\n                     return info\n             except _SWALLOWABLE_PLUGIN_EXCEPTIONS:\n                 logger.info(\n                     'Authentication plugin (%s) failed '\n                     'during login for %s.',\n                     '/'.join(auth.getPhysicalPath()),\n                     credentials['login'])\n                 continue\n         return None\n",
        "code_toks_joined": "plugins = acl [ <STRING> ] <NEWLINE> <INDENT> authenticators = plugins . listPlugins ( IAuthenticationPlugin ) <NEWLINE> for authenticator_id , auth in authenticators : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> info = auth . authenticateCredentials ( credentials ) <NEWLINE> if info is not None and info [ 0 ] is None : <NEWLINE> <COMMENT> <NL> <INDENT> return info <NEWLINE> <DEDENT> <DEDENT> except _SWALLOWABLE_PLUGIN_EXCEPTIONS : <NEWLINE> <INDENT> logger . info ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> , <NEWLINE> <STRING> . join ( auth . getPhysicalPath ( ) ) , <NEWLINE> credentials [ <STRING> ] ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'plugins'",
                "'Authentication plugin (%s) failed '",
                "'during login for %s.'",
                "'/'",
                "'login'"
            ],
            "<COMMENT>": [
                "# Failed login can be None OR (None, None)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "72ca75a38829442e9c3d53483e66c4e1": {
        "code_string": "lpca = self.new_task(\n             'calculate_lpca',\n             Jplace_PCA,\n             containerinfo=long_containerinfo,\n             path=os.path.join(\n                 self.destination_dir,\n                 'placement',\n                 'pca'\n             ),\n             prefix='lpca',\n             pca='lpca'\n         )\n         lpca.in_refpkg_tgz = refpkg_tgz.out_refpkg_tgz\n         lpca.in_seq_map = seq_map.out_file\n         lpca.in_jplace = redup_jplace.out_jplace\n",
        "code_toks_joined": "lpca = self . new_task ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> Jplace_PCA , <NEWLINE> containerinfo = long_containerinfo , <NEWLINE> path = os . path . join ( <NEWLINE> <INDENT> self . destination_dir , <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) , <NEWLINE> prefix = <STRING> , <NEWLINE> pca = <STRING> <NEWLINE> ) <NEWLINE> lpca . in_refpkg_tgz = refpkg_tgz . out_refpkg_tgz <NEWLINE> lpca . in_seq_map = seq_map . out_file <NEWLINE> lpca . in_jplace = redup_jplace . out_jplace <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'calculate_lpca'",
                "'placement'",
                "'pca'",
                "'lpca'",
                "'lpca'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4eaa58d72ae44e8e841c2d88235b7d39": {
        "code_string": "# Now we need the specimens grouped by batch to create error models. \n         batch_errModels = {}\n         for batch, batched_specimens in manifest.batched_specimens():\n             batch_errModels[batch] = self.new_task(\n                 'dada2_learn_error_batch_{}'.format(batch),\n                 DADA2_LearnError,\n                 containerinfo=heavy_containerinfo,\n                 batch=batch,\n                 tar_reads=False,\n                 path=os.path.join(\n                     self.working_dir,\n                     'sv',\n                     'dada2',\n                     'errM'\n                 )\n             )\n             batch_errModels[batch].in_reads = [\n                 specimen_tasks[s]['dada2_ft'].out_reads\n                 for s in specimen_tasks\n                 if s in batched_specimens\n             ]\n             for specimen in batched_specimens:\n                 specimen_tasks[specimen]['dada2_errM'] = batch_errModels[batch]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> batch_errModels = { } <NEWLINE> for batch , batched_specimens in manifest . batched_specimens ( ) : <NEWLINE> <INDENT> batch_errModels [ batch ] = self . new_task ( <NEWLINE> <INDENT> <STRING> . format ( batch ) , <NEWLINE> DADA2_LearnError , <NEWLINE> containerinfo = heavy_containerinfo , <NEWLINE> batch = batch , <NEWLINE> tar_reads = False , <NEWLINE> path = os . path . join ( <NEWLINE> <INDENT> self . working_dir , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> batch_errModels [ batch ] . in_reads = [ <NEWLINE> <INDENT> specimen_tasks [ s ] [ <STRING> ] . out_reads <NEWLINE> for s in specimen_tasks <NEWLINE> if s in batched_specimens <NEWLINE> <DEDENT> ] <NEWLINE> for specimen in batched_specimens : <NEWLINE> <INDENT> specimen_tasks [ specimen ] [ <STRING> ] = batch_errModels [ batch ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Now we need the specimens grouped by batch to create error models. "
            ],
            "<STRING>": [
                "'dada2_learn_error_batch_{}'",
                "'sv'",
                "'dada2'",
                "'errM'",
                "'dada2_ft'",
                "'dada2_errM'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aa0b2a089b4a42ecbcf0ee9e01a96f4b": {
        "code_string": "placement_db_classified = self.new_task(\n             'classify_into_placement_db',\n             PlacementDB_Classify_SV,\n             containerinfo=heavy_containerinfo,\n         )\n         placement_db_classified.in_placement_db = placement_db_w_si.out_placement_db\n         placement_db_classified.in_refpkg_tgz = refpkg_tgz.out_refpkg_tgz\n         placement_db_classified.in_sv_refpkg_aln_sto = sv_refpkg_aln_sto.out_aln_sto\n         placement_db_classified.in_jplace = jplace.out_file\n",
        "code_toks_joined": "placement_db_classified = self . new_task ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> PlacementDB_Classify_SV , <NEWLINE> containerinfo = heavy_containerinfo , <NEWLINE> ) <NEWLINE> placement_db_classified . in_placement_db = placement_db_w_si . out_placement_db <NEWLINE> placement_db_classified . in_refpkg_tgz = refpkg_tgz . out_refpkg_tgz <NEWLINE> placement_db_classified . in_sv_refpkg_aln_sto = sv_refpkg_aln_sto . out_aln_sto <NEWLINE> placement_db_classified . in_jplace = jplace . out_file <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'classify_into_placement_db'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d1e89babf8bb46edbdd53abac296f6b3": {
        "code_string": "placement_db_classified = self.new_task(\n             'classify_into_placement_db',\n             PlacementDB_Classify_SV,\n             containerinfo=midcpu_containerinfo,\n         )\n         placement_db_classified.in_placement_db = placement_db_w_si.out_placement_db\n         placement_db_classified.in_refpkg_tgz = refpkg_tgz.out_refpkg_tgz\n         placement_db_classified.in_sv_refpkg_aln_sto = sv_refpkg_aln_sto.out_aln_sto\n         placement_db_classified.in_jplace = jplace.out_file\n",
        "code_toks_joined": "placement_db_classified = self . new_task ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> PlacementDB_Classify_SV , <NEWLINE> containerinfo = midcpu_containerinfo , <NEWLINE> ) <NEWLINE> placement_db_classified . in_placement_db = placement_db_w_si . out_placement_db <NEWLINE> placement_db_classified . in_refpkg_tgz = refpkg_tgz . out_refpkg_tgz <NEWLINE> placement_db_classified . in_sv_refpkg_aln_sto = sv_refpkg_aln_sto . out_aln_sto <NEWLINE> placement_db_classified . in_jplace = jplace . out_file <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'classify_into_placement_db'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9f6c7af0219e41d69f6d4c0fa37f717d": {
        "code_string": "labels = kwargs.get('labels', False)\n         if info:\n             for ptc in labels:\n                 if ptc not in self.participants:\n                     self.add_participant(ptc)\n                 self.ptcs_update_labels(labels)\n",
        "code_toks_joined": "labels = kwargs . get ( <STRING> , False ) <NEWLINE> <INDENT> if info : <NEWLINE> <INDENT> for ptc in labels : <NEWLINE> <INDENT> if ptc not in self . participants : <NEWLINE> <INDENT> self . add_participant ( ptc ) <NEWLINE> <DEDENT> self . ptcs_update_labels ( labels ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'labels'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e748e9ccf1394974b294df7098234fe3": {
        "code_string": "offset = delta % index.step\n                         if offset == 0:\n                             offset = None\n                     else:\n                         if start is None:\n                             offset = index.step + (head.length - 1) % (-index.step)\n                         elif start > 0:\n                             offset = index.step + min(start, head.length - 1) % (-index.step)\n                         else:\n                             offset = index.step + (start + head.length) % (-index.step)\n",
        "code_toks_joined": "offset = delta % index . step <NEWLINE> <INDENT> if offset == 0 : <NEWLINE> <INDENT> offset = None <NEWLINE> else : <NEWLINE> <DEDENT> if start is None : <NEWLINE> <INDENT> offset = index . step + ( head . length - 1 ) % ( - index . step ) <NEWLINE> <DEDENT> elif start > 0 : <NEWLINE> <INDENT> offset = index . step + min ( start , head . length - 1 ) % ( - index . step ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> offset = index . step + ( start + head . length ) % ( - index . step ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "575acd2b3f3b46f9ac32ee1d5fa0b444": {
        "code_string": "def invoke (self, args, app=None, **kwargs):\n         if len (args) != 1:\n             raise multitool.UsageError ('expected no arguments')\n",
        "code_toks_joined": "def invoke ( self , args , app = None , ** kwargs ) : <NEWLINE> <INDENT> if len ( args ) != 1 : <NEWLINE> <INDENT> raise multitool . UsageError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'expected no arguments'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "03e14587a3d24b58af733d6c40b2a57e": {
        "code_string": "for collection_cls in collections:\n \t\t\tentities.add(collection_cls.entity)\n \t\t\tcollection = collection_cls(storage)\n \t\t\tself.collections_by_class_name[collection_cls.__name__] = collection\n \t\t\tsetattr(self, collection_cls.plural_name, collection)\n",
        "code_toks_joined": "for collection_cls in collections : <NEWLINE> <INDENT> entities . add ( collection_cls . entity ) <NEWLINE> collection = collection_cls ( storage ) <NEWLINE> self . collections_by_class_name [ collection_cls . __name__ ] = collection <NEWLINE> setattr ( self , collection_cls . plural_name , collection ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f830c507236e42fabe5c1744e53fdad6": {
        "code_string": "local_coordinates = _np.array([[1.0 / 3], [1.0 / 3]])\n         values = _np.zeros(grid.entity_count(0), dtype=\"float64\")\n         for element in grid.entity_iterator(0):\n             index = element.index\n             local_values = np.real(\n                 transformation(obj.evaluate(element, local_coordinates))\n             )\n             values[index] = local_values.flatten()\n",
        "code_toks_joined": "local_coordinates = _np . array ( [ [ 1.0 / 3 ] , [ 1.0 / 3 ] ] ) <NEWLINE> <INDENT> values = _np . zeros ( grid . entity_count ( 0 ) , dtype = <STRING> ) <NEWLINE> for element in grid . entity_iterator ( 0 ) : <NEWLINE> <INDENT> index = element . index <NEWLINE> local_values = np . real ( <NEWLINE> <INDENT> transformation ( obj . evaluate ( element , local_coordinates ) ) <NEWLINE> <DEDENT> ) <NEWLINE> values [ index ] = local_values . flatten ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"float64\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b3f3f3b57bbc49b88986ec36c5c745b4": {
        "code_string": "nshape_test = dual_to_range.number_of_shape_functions\n     nshape_trial = dual_to_range.number_of_shape_functions\n",
        "code_toks_joined": "nshape_test = dual_to_range . number_of_shape_functions <NEWLINE> <INDENT> nshape_trial = dual_to_range . number_of_shape_functions <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a90997d446944e57bd9ce915d3bd3c3d": {
        "code_string": "for trial_element_index in range(n_trial_elements):\n                 if is_adjacent[trial_element_index]:\n                     continue\n                 trial_element = trial_elements[trial_element_index]\n                 trial_normal = (\n                     trial_grid_data.normals[trial_element]\n                     * trial_normal_multipliers[trial_element]\n                 )\n                 normal_prod = _np.dot(test_normal, trial_normal)\n                 curl_product = (\n                     test_surface_curls_trans[i]\n                     @ trial_surface_curls[trial_element_index]\n                 )\n                 for test_fun_index in range(nshape_test):\n                     for trial_fun_index in range(nshape_trial):\n                         for quad_point_index in range(n_quad_points):\n                             local_result[\n                                 trial_element_index, test_fun_index, trial_fun_index\n                             ] += tmp[\n                                 trial_element_index * n_quad_points + quad_point_index\n                             ] * (\n                                 curl_product[test_fun_index, trial_fun_index]\n                                 - wavenumber\n                                 * wavenumber\n                                 * local_test_fun_values[\n                                     0, test_fun_index, quad_point_index\n                                 ]\n                                 * local_trial_fun_values[\n                                     0, trial_fun_index, quad_point_index\n                                 ]\n                                 * normal_prod\n                             )\n",
        "code_toks_joined": "for trial_element_index in range ( n_trial_elements ) : <NEWLINE> <INDENT> if is_adjacent [ trial_element_index ] : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> trial_element = trial_elements [ trial_element_index ] <NEWLINE> trial_normal = ( <NEWLINE> <INDENT> trial_grid_data . normals [ trial_element ] <NEWLINE> * trial_normal_multipliers [ trial_element ] <NEWLINE> <DEDENT> ) <NEWLINE> normal_prod = _np . dot ( test_normal , trial_normal ) <NEWLINE> curl_product = ( <NEWLINE> <INDENT> test_surface_curls_trans [ i ] <NEWLINE> @ trial_surface_curls [ trial_element_index ] <NEWLINE> <DEDENT> ) <NEWLINE> for test_fun_index in range ( nshape_test ) : <NEWLINE> <INDENT> for trial_fun_index in range ( nshape_trial ) : <NEWLINE> <INDENT> for quad_point_index in range ( n_quad_points ) : <NEWLINE> <INDENT> local_result [ <NEWLINE> <INDENT> trial_element_index , test_fun_index , trial_fun_index <NEWLINE> <DEDENT> ] += tmp [ <NEWLINE> <INDENT> trial_element_index * n_quad_points + quad_point_index <NEWLINE> <DEDENT> ] * ( <NEWLINE> <INDENT> curl_product [ test_fun_index , trial_fun_index ] <NEWLINE> - wavenumber <NEWLINE> * wavenumber <NEWLINE> * local_test_fun_values [ <NEWLINE> <INDENT> 0 , test_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * local_trial_fun_values [ <NEWLINE> <INDENT> 0 , trial_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * normal_prod <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f16a3fcf1f5c48f29d33e3cfcf673e85": {
        "code_string": "for trial_element_index in range(n_trial_elements):\n                 if is_adjacent[trial_element_index]:\n                     continue\n                 trial_element = trial_elements[trial_element_index]\n                 trial_normal = (\n                     trial_grid_data.normals[trial_element]\n                     * trial_normal_multipliers[trial_element]\n                 )\n                 normal_prod = _np.dot(test_normal, trial_normal)\n                 curl_product = (\n                     test_surface_curls_trans[i]\n                     @ trial_surface_curls[trial_element_index]\n                 )\n                 for test_fun_index in range(nshape_test):\n                     for trial_fun_index in range(nshape_trial):\n                         for quad_point_index in range(n_quad_points):\n                             local_result[\n                                 trial_element_index, test_fun_index, trial_fun_index\n                             ] += tmp[\n                                 trial_element_index * n_quad_points + quad_point_index\n                             ] * (\n                                 curl_product[test_fun_index, trial_fun_index]\n                                 - wavenumber\n                                 * wavenumber\n                                 * local_test_fun_values[\n                                     0, test_fun_index, quad_point_index\n                                 ]\n                                 * local_trial_fun_values[\n                                     0, trial_fun_index, quad_point_index\n                                 ]\n                                 * normal_prod\n                             )\n",
        "code_toks_joined": "for trial_element_index in range ( n_trial_elements ) : <NEWLINE> <INDENT> if is_adjacent [ trial_element_index ] : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> trial_element = trial_elements [ trial_element_index ] <NEWLINE> trial_normal = ( <NEWLINE> <INDENT> trial_grid_data . normals [ trial_element ] <NEWLINE> * trial_normal_multipliers [ trial_element ] <NEWLINE> <DEDENT> ) <NEWLINE> normal_prod = _np . dot ( test_normal , trial_normal ) <NEWLINE> curl_product = ( <NEWLINE> <INDENT> test_surface_curls_trans [ i ] <NEWLINE> @ trial_surface_curls [ trial_element_index ] <NEWLINE> <DEDENT> ) <NEWLINE> for test_fun_index in range ( nshape_test ) : <NEWLINE> <INDENT> for trial_fun_index in range ( nshape_trial ) : <NEWLINE> <INDENT> for quad_point_index in range ( n_quad_points ) : <NEWLINE> <INDENT> local_result [ <NEWLINE> <INDENT> trial_element_index , test_fun_index , trial_fun_index <NEWLINE> <DEDENT> ] += tmp [ <NEWLINE> <INDENT> trial_element_index * n_quad_points + quad_point_index <NEWLINE> <DEDENT> ] * ( <NEWLINE> <INDENT> curl_product [ test_fun_index , trial_fun_index ] <NEWLINE> - wavenumber <NEWLINE> * wavenumber <NEWLINE> * local_test_fun_values [ <NEWLINE> <INDENT> 0 , test_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * local_trial_fun_values [ <NEWLINE> <INDENT> 0 , trial_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * normal_prod <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a8e53d7b341447c8a69a1836712a140d": {
        "code_string": "x_transformed = self.space.map_to_full_grid @ (\n                 self.space.dof_transformation @ x\n             )\n             result = implementation(x)\n             return result.reshape([kernel_dimension, -1], order=\"F\")\n",
        "code_toks_joined": "x_transformed = self . space . map_to_full_grid @ ( <NEWLINE> <INDENT> self . space . dof_transformation @ x <NEWLINE> ) <NEWLINE> result = implementation ( x ) <NEWLINE> return result . reshape ( [ kernel_dimension , - 1 ] , order = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"F\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dc6052f833be417a8a0150c3b9ef247d": {
        "code_string": "def read_config(conf_file='service.conf'):\n     parser = AgaveConfigParser()\n     places = ['/{}'.format(conf_file),\n               '/etc/{}'.format(conf_file),\n               '{}/{}'.format(os.getcwd(), conf_file)]\n     place = places[0]\n     for p in places:\n         if os.path.exists(p):\n             place = p\n             break\n     else:\n         raise RuntimeError('No config file found.')\n     if not parser.parser.read(place):\n         raise RuntimeError(\"couldn't read config file from {0}\"\n                            .format(', '.join(place)))\n     return parser\n",
        "code_toks_joined": "def read_config ( conf_file = <STRING> ) : <NEWLINE> <INDENT> parser = AgaveConfigParser ( ) <NEWLINE> places = [ <STRING> . format ( conf_file ) , <NEWLINE> <INDENT> <STRING> . format ( conf_file ) , <NEWLINE> <STRING> . format ( os . getcwd ( ) , conf_file ) ] <NEWLINE> <DEDENT> place = places [ 0 ] <NEWLINE> for p in places : <NEWLINE> <INDENT> if os . path . exists ( p ) : <NEWLINE> <INDENT> place = p <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> if not parser . parser . read ( place ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> <NEWLINE> <INDENT> . format ( <STRING> . join ( place ) ) ) <NEWLINE> <DEDENT> <DEDENT> return parser <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'service.conf'",
                "'/{}'",
                "'/etc/{}'",
                "'{}/{}'",
                "'No config file found.'",
                "\"couldn't read config file from {0}\"",
                "', '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d65dc9ebefe04dea96733b86f894608a": {
        "code_string": "def __call__(self, number_of_neighbors, number_of_common_neighbors):\n         \"\"\"\n         \"\"\"\n         if number_of_common_neighbors >= number_of_neighbors:\n             raise ValueError('Asked for more common neighbors than number of neighbors')\n         neighbors_list = {}\n         for element in self.dataset_elements:\n             neighbors_list[element] = self.calculate_k_nearest_elements(element, number_of_neighbors)[:number_of_neighbors]\n         for element, neighbors in neighbors_list.items():\n             for other_element, other_neighbors in neighbors_list.items():\n                 if element != other_element:\n                     # we check both sides since the relation is not symmetric\n                     if element in other_neighbors and other_element in neighbors:\n                         if len(set(neighbors).intersection(other_neighbors)) >= number_of_common_neighbors:\n                             self.reconfigure_clusters(element, other_element)\n         result = defaultdict(list)\n         for element, cluster_nro in self.cluster.items():\n             result[cluster_nro].append(element)\n",
        "code_toks_joined": "def __call__ ( self , number_of_neighbors , number_of_common_neighbors ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if number_of_common_neighbors >= number_of_neighbors : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> neighbors_list = { } <NEWLINE> for element in self . dataset_elements : <NEWLINE> <INDENT> neighbors_list [ element ] = self . calculate_k_nearest_elements ( element , number_of_neighbors ) [ : number_of_neighbors ] <NEWLINE> <DEDENT> for element , neighbors in neighbors_list . items ( ) : <NEWLINE> <INDENT> for other_element , other_neighbors in neighbors_list . items ( ) : <NEWLINE> <INDENT> if element != other_element : <NEWLINE> <COMMENT> <NL> <INDENT> if element in other_neighbors and other_element in neighbors : <NEWLINE> <INDENT> if len ( set ( neighbors ) . intersection ( other_neighbors ) ) >= number_of_common_neighbors : <NEWLINE> <INDENT> self . reconfigure_clusters ( element , other_element ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> result = defaultdict ( list ) <NEWLINE> for element , cluster_nro in self . cluster . items ( ) : <NEWLINE> <INDENT> result [ cluster_nro ] . append ( element ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         \"\"\"",
                "'Asked for more common neighbors than number of neighbors'"
            ],
            "<COMMENT>": [
                "# we check both sides since the relation is not symmetric"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d06b659a90fa49108271ac3d4c7283e1": {
        "code_string": "# ensure all paramters are unicode and not escaped\n     unicode_params = []\n     for k, v in params:\n         if isinstance(k, str):\n             k = k.decode('utf-8')\n         if isinstance(v, str):\n             if v.startswith('oauth_'):\n                 v = utils.unescape(v)\n             else:\n                 v = v.decode('utf-8')\n         unicode_params.append((k, v))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> unicode_params = [ ] <NEWLINE> for k , v in params : <NEWLINE> <INDENT> if isinstance ( k , str ) : <NEWLINE> <INDENT> k = k . decode ( <STRING> ) <NEWLINE> <DEDENT> if isinstance ( v , str ) : <NEWLINE> <INDENT> if v . startswith ( <STRING> ) : <NEWLINE> <INDENT> v = utils . unescape ( v ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> v = v . decode ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> unicode_params . append ( ( k , v ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ensure all paramters are unicode and not escaped"
            ],
            "<STRING>": [
                "'utf-8'",
                "'oauth_'",
                "'utf-8'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2c316cab80a24f688638916b22a531f1": {
        "code_string": "def add_params_to_uri(uri, params, fragment=False):\n     \"\"\"Add a list of two-tuples to the uri query components.\"\"\"\n     sch, net, path, par, query, fra = urlparse.urlparse(uri)\n     if fragment:\n         fra = add_params_to_qs(query, params)\n     else:\n         query = add_params_to_qs(query, params)\n     return urlparse.urlunparse((sch, net, path, par, query, fra))\n",
        "code_toks_joined": "def add_params_to_uri ( uri , params , fragment = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sch , net , path , par , query , fra = urlparse . urlparse ( uri ) <NEWLINE> if fragment : <NEWLINE> <INDENT> fra = add_params_to_qs ( query , params ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> query = add_params_to_qs ( query , params ) <NEWLINE> <DEDENT> return urlparse . urlunparse ( ( sch , net , path , par , query , fra ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Add a list of two-tuples to the uri query components.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5b4d3177f8a84d7e9e481e84a29aa28f": {
        "code_string": "bindata = numpy.zeros(\n         (T / time_bin_length,) + data.shape[1:], dtype=\"float32\")\n     for index, i in enumerate(range(0, T - time_bin_length + 1,\n                                     time_bin_length)):\n         # print weighted_avg_and_std(fulldata[i:i+time_bin_length], axis=0,\n         # weights=sample_selector[i:i+time_bin_length])[0]\n         bindata[index] = weighted_avg_and_std(data[i:i + time_bin_length],\n                                               axis=0,\n                                               weights=sample_selector[i:i +\n                                               time_bin_length])[0]\n",
        "code_toks_joined": "bindata = numpy . zeros ( <NEWLINE> <INDENT> ( T / time_bin_length , ) + data . shape [ 1 : ] , dtype = <STRING> ) <NEWLINE> for index , i in enumerate ( range ( 0 , T - time_bin_length + 1 , <NEWLINE> <INDENT> time_bin_length ) ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> bindata [ index ] = weighted_avg_and_std ( data [ i : i + time_bin_length ] , <NEWLINE> <INDENT> axis = 0 , <NEWLINE> weights = sample_selector [ i : i + <NEWLINE> time_bin_length ] ) [ 0 ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"float32\""
            ],
            "<COMMENT>": [
                "# print weighted_avg_and_std(fulldata[i:i+time_bin_length], axis=0,",
                "# weights=sample_selector[i:i+time_bin_length])[0]"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b0eef9c721804df69e0f70369a77127c": {
        "code_string": "def tsg_to_net(self, node, max_lag):\n         \"\"\"Helper function to translate from time series graph to network.\"\"\"\n         row = node / max_lag\n         lag = node % max_lag\n         return (row, -lag)        \n",
        "code_toks_joined": "def tsg_to_net ( self , node , max_lag ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> row = node / max_lag <NEWLINE> lag = node % max_lag <NEWLINE> return ( row , - lag ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Helper function to translate from time series graph to network.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7007fa0313f84716b6366b8f77048898": {
        "code_string": "# Needed because np.bincount cannot process longs\n         if isinstance(self.n_symbs ** dim, int):\n             raise ValueError(\"Too many n_symbs and/or dimensions, \"\n                              \"numpy.bincount cannot process longs\")\n         if self.n_symbs ** dim * 16. / 8. / 1024. ** 3 > 3.:\n             raise ValueError(\"Dimension exceeds 3 GB of necessary \"\n                              \"memory (change this code line if more...)\")\n         if dim * self.n_symbs ** dim > 2 ** 65:\n             raise ValueError(\"base = %d, D = %d: Histogram failed: \"\n                              \"dimension D*base**D exceeds int64 data type\"\n                              % (self.n_symbs, dim))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if isinstance ( self . n_symbs ** dim , int ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if self . n_symbs ** dim * 16. / 8. / 1024. ** 3 > 3. : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if dim * self . n_symbs ** dim > 2 ** 65 : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> % ( self . n_symbs , dim ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Needed because np.bincount cannot process longs"
            ],
            "<STRING>": [
                "\"Too many n_symbs and/or dimensions, \"",
                "\"numpy.bincount cannot process longs\"",
                "\"Dimension exceeds 3 GB of necessary \"",
                "\"memory (change this code line if more...)\"",
                "\"base = %d, D = %d: Histogram failed: \"",
                "\"dimension D*base**D exceeds int64 data type\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "285d95c3a0fe4474a303eabf9a1d8b53": {
        "code_string": "if a > b:\n",
        "code_toks_joined": "if a > b : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7292daeabd484c88a4d95b01c6f9a513": {
        "code_string": "axp.text(-.1, .5, \"OY=%s\" % hi, rotation=\"vertical\",\n",
        "code_toks_joined": "axp . text ( - .1 , .5 , <STRING> % hi , rotation = <STRING> , <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "\"OY=%s\"",
                "\"vertical\""
            ]
        },
        "err_obj": {
            "msg": "unbalanced (){}[]"
        }
    },
    "8a25ef293cf746e3bce49ae25faa1d0a": {
        "code_string": "def resize_convex(self):\n         \"\"\"ensure convex surfaces are at least as large as their\n         corresponding closing surface, enabling standard manufacturing\"\"\"\n         pending = None\n         c0 = None\n         for el in self[1:-1]:\n             if not hasattr(el, \"material\"):\n                 continue\n             c = getattr(el, \"curvature\", 0)\n             if pending is not None:\n                 r = max(el.radius, pending.radius)\n                 if c < 0:\n                     el.radius = r\n                 if c0 > 0:\n                     pending.radius = r\n                 pending = None\n                 if not el.material or el.material.solid:\n                     pending = el\n             if not el.material or el.material.solid:\n                 pending, c0 = el, c\n",
        "code_toks_joined": "def resize_convex ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> pending = None <NEWLINE> c0 = None <NEWLINE> for el in self [ 1 : - 1 ] : <NEWLINE> <INDENT> if not hasattr ( el , <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> c = getattr ( el , <STRING> , 0 ) <NEWLINE> if pending is not None : <NEWLINE> <INDENT> r = max ( el . radius , pending . radius ) <NEWLINE> if c < 0 : <NEWLINE> <INDENT> el . radius = r <NEWLINE> <DEDENT> if c0 > 0 : <NEWLINE> <INDENT> pending . radius = r <NEWLINE> <DEDENT> pending = None <NEWLINE> if not el . material or el . material . solid : <NEWLINE> <INDENT> pending = el <NEWLINE> <DEDENT> <DEDENT> if not el . material or el . material . solid : <NEWLINE> <INDENT> pending , c0 = el , c <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"ensure convex surfaces are at least as large as their\n         corresponding closing surface, enabling standard manufacturing\"\"\"",
                "\"material\"",
                "\"curvature\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "976255abf69f4a1eb4cb01ad851e4bef": {
        "code_string": "def main():\n     if len(sys.argv) == 3:\n         keep_count = 0\n     elif len(sys.argv) == 4:\n         keep_count = int(sys.argv[3])\n     else:\n         print('Usage (without rotation): inipgdump config_file.ini /dump/dir')\n         print('Usage (with rotation): inipgdump config_file.ini /dump/dir keep_count')\n         sys.exit(1)\n     conf_file = sys.argv[1]\n     if sys.argv[2][-1] == '/':\n         dump_dir = sys.argv[2][:-1]\n     else:\n         dump_dir = sys.argv[2]\n     if not os.path.isfile(conf_file):\n         print('File \\'%s\\' not found' % conf_file)\n         sys.exit(1)\n     if not os.path.isdir(dump_dir):\n         print('Folder \\'%s\\' not found' % dump_dir)\n         sys.exit(1)\n     if keep_count <= 0:\n         print('keep_count must be greater than 0')\n         sys.exit(1)\n     make_dump(conf_file, dump_dir, keep_count)\n",
        "code_toks_joined": "def main ( ) : <NEWLINE> <INDENT> if len ( sys . argv ) == 3 : <NEWLINE> <INDENT> keep_count = 0 <NEWLINE> <DEDENT> elif len ( sys . argv ) == 4 : <NEWLINE> <INDENT> keep_count = int ( sys . argv [ 3 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> conf_file = sys . argv [ 1 ] <NEWLINE> if sys . argv [ 2 ] [ - 1 ] == <STRING> : <NEWLINE> <INDENT> dump_dir = sys . argv [ 2 ] [ : - 1 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> dump_dir = sys . argv [ 2 ] <NEWLINE> <DEDENT> if not os . path . isfile ( conf_file ) : <NEWLINE> <INDENT> print ( <STRING> % conf_file ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> if not os . path . isdir ( dump_dir ) : <NEWLINE> <INDENT> print ( <STRING> % dump_dir ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> if keep_count <= 0 : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> make_dump ( conf_file , dump_dir , keep_count ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Usage (without rotation): inipgdump config_file.ini /dump/dir'",
                "'Usage (with rotation): inipgdump config_file.ini /dump/dir keep_count'",
                "'/'",
                "'File \\'%s\\' not found'",
                "'Folder \\'%s\\' not found'",
                "'keep_count must be greater than 0'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "166cc4d5f930417ca57fdd92cd7a9cd0": {
        "code_string": "if not self.check_plugin_installed(plugin_full_name):\n             self.pip.install(plugin_full_name)  # ???\n",
        "code_toks_joined": "if not self . check_plugin_installed ( plugin_full_name ) : <NEWLINE> <INDENT> self . pip . install ( plugin_full_name ) <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ???"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6c73b431f455438c81151aee3238d705": {
        "code_string": "def convert_all_validator(self, schema_node, validator):\n         \"\"\"\n         :type schema_node: colander.SchemaNode\n         :type validator: colander.interfaces.Validator\n         :rtype: dict\n         \"\"\"\n         converted = None\n         if isinstance(validator, colander.All):\n             converted = {}\n             for v in validator.validators:\n                 ret = self(schema_node, validator)\n                 converted.update(ret)\n         return converted\n",
        "code_toks_joined": "def convert_all_validator ( self , schema_node , validator ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> converted = None <NEWLINE> if isinstance ( validator , colander . All ) : <NEWLINE> <INDENT> converted = { } <NEWLINE> for v in validator . validators : <NEWLINE> <INDENT> ret = self ( schema_node , validator ) <NEWLINE> converted . update ( ret ) <NEWLINE> <DEDENT> <DEDENT> return converted <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         :type schema_node: colander.SchemaNode\n         :type validator: colander.interfaces.Validator\n         :rtype: dict\n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "62f2a4dca8f4457db3caa58fe043fbb3": {
        "code_string": "wlgrid = np.log10(10000/bindown_wngrid)\n     if args.plot:\n         if get_rank()==0  and nprocs()==1:\n             import matplotlib.pyplot as plt\n             if args.contrib:\n                 for name,value in contrib:\n                     new_value = bindown(native_grid,value,bindown_wngrid)\n                     plt.plot(wlgrid,new_value,label=name)\n",
        "code_toks_joined": "wlgrid = np . log10 ( 10000 / bindown_wngrid ) <NEWLINE> <INDENT> if args . plot : <NEWLINE> <INDENT> if get_rank ( ) == 0 and nprocs ( ) == 1 : <NEWLINE> <INDENT> import matplotlib . pyplot as plt <NEWLINE> if args . contrib : <NEWLINE> <INDENT> for name , value in contrib : <NEWLINE> <INDENT> new_value = bindown ( native_grid , value , bindown_wngrid ) <NEWLINE> plt . plot ( wlgrid , new_value , label = name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6041d0dcd8744753b4a88e97da63e197": {
        "code_string": "if solution is not None:\n                 out.store_dictionary(solution,group_name='Solutions')\n                 priors = {}\n                 priors['Profiles'] = profiles\n                 priors['Spectra'] = spectrum\n                 out.store_dictionary(solution,group_name='Priors')\n             else:\n                 out.store_dictionary(profiles,group_name='Profiles')\n                 out.store_dictionary(spectrum,group_name='Spectra')\n",
        "code_toks_joined": "if solution is not None : <NEWLINE> <INDENT> out . store_dictionary ( solution , group_name = <STRING> ) <NEWLINE> priors = { } <NEWLINE> priors [ <STRING> ] = profiles <NEWLINE> priors [ <STRING> ] = spectrum <NEWLINE> out . store_dictionary ( solution , group_name = <STRING> ) <NEWLINE> else : <NEWLINE> out . store_dictionary ( profiles , group_name = <STRING> ) <NEWLINE> out . store_dictionary ( spectrum , group_name = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Solutions'",
                "'Profiles'",
                "'Spectra'",
                "'Priors'",
                "'Profiles'",
                "'Spectra'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9ebbba48924f45259fdcb86559a10a84": {
        "code_string": "try:\n         from mpi4py import MPI\n     except ImportError:\n         return 0  \n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> from mpi4py import MPI <NEWLINE> except ImportError : <NEWLINE> return 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "21f5c28b06e0468d98844ba28f40b237": {
        "code_string": "log_start = time\n     click.echo(f\"Adding {message} at {as_local(log_start)} to project {project}\")\n     context.session.add_time_entry(\n         start_time=time, description=message, project=project_obj\n     )\n",
        "code_toks_joined": "log_start = time <NEWLINE> <INDENT> click . echo ( <STRING> ) <NEWLINE> context . session . add_time_entry ( <NEWLINE> <INDENT> start_time = time , description = message , project = project_obj <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"Adding {message} at {as_local(log_start)} to project {project}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4140d4f1f0b64b8594fb0f64fe94452e": {
        "code_string": "# Create the new object\n                 foundDocuments.append(GraphDbPackedSegmentTuple(\n                     key=key,\n                     packedJson=segmentsByKey[subKey]\n                 ))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> foundDocuments . append ( GraphDbPackedSegmentTuple ( <NEWLINE> <INDENT> key = key , <NEWLINE> packedJson = segmentsByKey [ subKey ] <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Create the new object"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "54bf48564fc34920ad3cb32faafbc883": {
        "code_string": "def get_matsize(m):\n     size = 0\n     depth = len(m)\n     for i in range(depth):\n         layer = m[depth]\n         lngth = len(layer)\n         for j in range(lngth):\n             size = size + 1\n     return(size)\n",
        "code_toks_joined": "def get_matsize ( m ) : <NEWLINE> <INDENT> size = 0 <NEWLINE> depth = len ( m ) <NEWLINE> for i in range ( depth ) : <NEWLINE> <INDENT> layer = m [ depth ] <NEWLINE> lngth = len ( layer ) <NEWLINE> for j in range ( lngth ) : <NEWLINE> <INDENT> size = size + 1 <NEWLINE> <DEDENT> <DEDENT> return ( size ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f54f0ede6a444206a54560288fe5ac46": {
        "code_string": "def select(loomfile, min_read_count, min_cell_count, layer):\n     with loompy.connect(loomfile) as ds:\n         gsurv = (ds.sparse(layer=layer) >= min_read_count).sum(axis=1) > min_cell_count\n         ds.ra.Selected = np.squeeze(np.asarray(gsurv))\n         LOG.info('Total %d genes selected' % gsurv.sum())\n         # totals = ds.map([np.sum], axis=1)[0]  # Select based upon cell size?\n",
        "code_toks_joined": "def select ( loomfile , min_read_count , min_cell_count , layer ) : <NEWLINE> <INDENT> with loompy . connect ( loomfile ) as ds : <NEWLINE> <INDENT> gsurv = ( ds . sparse ( layer = layer ) >= min_read_count ) . sum ( axis = 1 ) > min_cell_count <NEWLINE> ds . ra . Selected = np . squeeze ( np . asarray ( gsurv ) ) <NEWLINE> LOG . info ( <STRING> % gsurv . sum ( ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Total %d genes selected'"
            ],
            "<COMMENT>": [
                "# totals = ds.map([np.sum], axis=1)[0]  # Select based upon cell size?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fb53f2d1e2ac4149b19bd8b553f1c1c0": {
        "code_string": "with bind.connect() as conn:\n         select_files.bind = conn\n         select_values.bind = conn\n         for in_slice in window_slices(File.id, size=windowsize, bind=bind):\n             if log.level <= logging.DEBUG:\n                 where = literal_compile(in_slice(File.id))\n                 log.debug('fetch rows %r', where.string)\n",
        "code_toks_joined": "with bind . connect ( ) as conn : <NEWLINE> <INDENT> select_files . bind = conn <NEWLINE> select_values . bind = conn <NEWLINE> for in_slice in window_slices ( File . id , size = windowsize , bind = bind ) : <NEWLINE> <INDENT> if log . level <= logging . DEBUG : <NEWLINE> <INDENT> where = literal_compile ( in_slice ( File . id ) ) <NEWLINE> log . debug ( <STRING> , where . string ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'fetch rows %r'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b804fed4a63249b09cc53bd813879c68": {
        "code_string": "def process_view(self, request, view_func, view_args, view_kwargs):\n         # Try to determine the realm for this view\n         try:\n             realm = request._rated_realm\n         except AttributeError:\n             try:\n                 realm = settings.REALM_MAP[request.resolver_match.url_name]\n             except KeyError:\n                 return None\n         # should we also try the view name?\n",
        "code_toks_joined": "def process_view ( self , request , view_func , view_args , view_kwargs ) : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> realm = request . _rated_realm <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> realm = settings . REALM_MAP [ request . resolver_match . url_name ] <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> return None <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Try to determine the realm for this view",
                "# should we also try the view name?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b15f6db0664543c28b5a374410964751": {
        "code_string": "def _task_gen(item, **kwds):\n     runner._update_current_test_var(item, \"call\")\n     dag = item._request.getfixturevalue(\"dag\")\n     ihook = getattr(item.ihook, \"pytest_runtest_call\")\n     task_id = _gen_task_id(item)\n     task = PythonOperator(\n         task_id=task_id,\n         python_callable=lambda: ihook(item=item, **kwds),\n         provide_context=True,\n         dag=dag,\n     )\n     dag.set_dependency(task_id, \"__pytest_branch\")\n     return task\n",
        "code_toks_joined": "def _task_gen ( item , ** kwds ) : <NEWLINE> <INDENT> runner . _update_current_test_var ( item , <STRING> ) <NEWLINE> dag = item . _request . getfixturevalue ( <STRING> ) <NEWLINE> ihook = getattr ( item . ihook , <STRING> ) <NEWLINE> task_id = _gen_task_id ( item ) <NEWLINE> task = PythonOperator ( <NEWLINE> <INDENT> task_id = task_id , <NEWLINE> python_callable = lambda : ihook ( item = item , ** kwds ) , <NEWLINE> provide_context = True , <NEWLINE> dag = dag , <NEWLINE> <DEDENT> ) <NEWLINE> dag . set_dependency ( task_id , <STRING> ) <NEWLINE> return task <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"call\"",
                "\"dag\"",
                "\"pytest_runtest_call\"",
                "\"__pytest_branch\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "168f06c9270747b0bcc04d2ef24438f6": {
        "code_string": "# remove the last samples of y, to match the dimension of the virtual reference\n         # number of samples used in the method\n         N=rv.shape[0]\n         y=y[0:N,:]\n         y_iv=y_iv[0:N,:]\n         # virtual error\n         ebar=rv-y\n         ebar_iv=rv-y_iv\n         # remove the last samples of the input (to match the dimension of the virtual error)\n         uf=uf[0:N,:]\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> N = rv . shape [ 0 ] <NEWLINE> y = y [ 0 : N , : ] <NEWLINE> y_iv = y_iv [ 0 : N , : ] <NEWLINE> <COMMENT> <NL> ebar = rv - y <NEWLINE> ebar_iv = rv - y_iv <NEWLINE> <COMMENT> <NL> uf = uf [ 0 : N , : ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# remove the last samples of y, to match the dimension of the virtual reference",
                "# number of samples used in the method",
                "# virtual error",
                "# remove the last samples of the input (to match the dimension of the virtual error)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c5e1905d78604db681283b9f6918b704": {
        "code_string": "@property\n   def root(self):\n     return self.ancestors[0]\n",
        "code_toks_joined": "@ property <NEWLINE> <INDENT> def root ( self ) : <NEWLINE> <INDENT> return self . ancestors [ 0 ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e3fdfc675beb49948ebe6f7365f7c786": {
        "code_string": "if not configSpec:\n             raise UnfurlError(\n                 'unable to find an implementation to \"%s\" \"%s\" on \"\"%s\": %s'\n                 % (action, resource.template.name, resource.template.name, reason)\n             )\n         logger.debug(\n             \"creating configuration %s with %s to run for %s: %s\",\n             configSpec.name,\n             configSpec.inputs,\n             resource.name,\n             reason or action,\n         )\n         return (configSpec, resource, reason or action)\n",
        "code_toks_joined": "if not configSpec : <NEWLINE> <INDENT> raise UnfurlError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> % ( action , resource . template . name , resource . template . name , reason ) <NEWLINE> <DEDENT> ) <NEWLINE> logger . debug ( <NEWLINE> <STRING> , <NEWLINE> configSpec . name , <NEWLINE> configSpec . inputs , <NEWLINE> resource . name , <NEWLINE> reason or action , <NEWLINE> ) <NEWLINE> return ( configSpec , resource , reason or action ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'unable to find an implementation to \"%s\" \"%s\" on \"\"%s\": %s'",
                "\"creating configuration %s with %s to run for %s: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b16dfff31d29431dbb654b3b987a039e": {
        "code_string": "def _as_html_tags_eval_item(item):\n     if not isinstance(HTMLTagsEvalItem, item):\n         return HTMLTagsEvalItem(item)\n     return item\n",
        "code_toks_joined": "def _as_html_tags_eval_item ( item ) : <NEWLINE> <INDENT> if not isinstance ( HTMLTagsEvalItem , item ) : <NEWLINE> <INDENT> return HTMLTagsEvalItem ( item ) <NEWLINE> <DEDENT> return item <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8f5bc937c27b4baca74fb75f016bff36": {
        "code_string": "def decode(self, probs, seq_lens):\n         # We expect batch x seq x label_size\n         probs = probs.cpu().float()\n         seq_lens = probs.cpu().int()\n         batch_size, max_seq_len = probs.size(0), probs.size(1)\n         output = torch.IntTensor(batch_size, self._beam_width, max_seq_len).cpu().int()\n         timesteps = torch.IntTensor(batch_size, self._beam_width, max_seq_len).cpu().int()\n         scores = torch.IntTensor(batch_size, self._beam_width).cpu().int()\n         out_seq_len = torch.IntTensor(batch_size, self._beam_width).cpu().int()\n         if self._scorer:\n             ctc_decode.paddle_beam_decode_lm(probs, seq_lens, self._labels, self._num_labels, self._beam_width,\n                                              self._num_processes, self._cutoff_prob, self.cutoff_top_n, self._blank_id,\n                                              self._scorer, output, timesteps, scores, out_seq_len)\n         else:\n             ctc_decode.paddle_beam_decode(probs, seq_lens, self._labels, self._num_labels, self._beam_width, self._num_processes,\n                                           self._cutoff_prob, self.cutoff_top_n, self._blank_id, output, timesteps,\n                                           scores, out_seq_len)\n",
        "code_toks_joined": "def decode ( self , probs , seq_lens ) : <NEWLINE> <COMMENT> <NL> <INDENT> probs = probs . cpu ( ) . float ( ) <NEWLINE> seq_lens = probs . cpu ( ) . int ( ) <NEWLINE> batch_size , max_seq_len = probs . size ( 0 ) , probs . size ( 1 ) <NEWLINE> output = torch . IntTensor ( batch_size , self . _beam_width , max_seq_len ) . cpu ( ) . int ( ) <NEWLINE> timesteps = torch . IntTensor ( batch_size , self . _beam_width , max_seq_len ) . cpu ( ) . int ( ) <NEWLINE> scores = torch . IntTensor ( batch_size , self . _beam_width ) . cpu ( ) . int ( ) <NEWLINE> out_seq_len = torch . IntTensor ( batch_size , self . _beam_width ) . cpu ( ) . int ( ) <NEWLINE> if self . _scorer : <NEWLINE> <INDENT> ctc_decode . paddle_beam_decode_lm ( probs , seq_lens , self . _labels , self . _num_labels , self . _beam_width , <NEWLINE> <INDENT> self . _num_processes , self . _cutoff_prob , self . cutoff_top_n , self . _blank_id , <NEWLINE> self . _scorer , output , timesteps , scores , out_seq_len ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> ctc_decode . paddle_beam_decode ( probs , seq_lens , self . _labels , self . _num_labels , self . _beam_width , self . _num_processes , <NEWLINE> <INDENT> self . _cutoff_prob , self . cutoff_top_n , self . _blank_id , output , timesteps , <NEWLINE> scores , out_seq_len ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# We expect batch x seq x label_size"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a22e075e6a514699b60ce49e7198c003": {
        "code_string": "#if this weight fits in the bin\n             if new_weight_sum < V_bin_max:\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if new_weight_sum < V_bin_max : <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#if this weight fits in the bin"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3e64c528ea1345f089c71381d108bf37": {
        "code_string": "return values\n",
        "code_toks_joined": "return values <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ec6d011eced64a59ad4e54d252ea7b0c": {
        "code_string": "def assert_read_available_to(\n         self,\n         unauthorised_agent: Agent,\n         broadcast_candidate: Union[ReadProtected, List[ReadProtected]]\n     ) -> Agent:\n         \"\"\"\n         Raise a NotAuthorised error if this Agent may not read the candidate\n         \"\"\"\n         if isinstance(broadcast_candidate, list):\n             for candidate in broadcast_candidate:\n                 if not candidate.grants_read_to(unauthorised_agent):\n                     raise NotAuthorised\n                 continue\n             return broadcast_candidate\n",
        "code_toks_joined": "def assert_read_available_to ( <NEWLINE> <INDENT> self , <NEWLINE> unauthorised_agent : Agent , <NEWLINE> broadcast_candidate : Union [ ReadProtected , List [ ReadProtected ] ] <NEWLINE> ) -> Agent : <NEWLINE> <STRING> <NEWLINE> if isinstance ( broadcast_candidate , list ) : <NEWLINE> <INDENT> for candidate in broadcast_candidate : <NEWLINE> <INDENT> if not candidate . grants_read_to ( unauthorised_agent ) : <NEWLINE> <INDENT> raise NotAuthorised <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> return broadcast_candidate <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Raise a NotAuthorised error if this Agent may not read the candidate\n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c1b808b1e4a34a6e9615471a1d72385c": {
        "code_string": "if k < fppc:\n         # print(\"WARNING: cannot make {} unique pairs with genome of {} fragments\".format(fppc, contig_frags), end='\\r')\n         pairs.sp = np.tile(label, [fppc, 2])\n         pairs.start = np.random.choice(frag_steps, [fppc, 2])\n         pairs.end = pairs.start + frag_steps\n",
        "code_toks_joined": "if k < fppc : <NEWLINE> <COMMENT> <NL> <INDENT> pairs . sp = np . tile ( label , [ fppc , 2 ] ) <NEWLINE> pairs . start = np . random . choice ( frag_steps , [ fppc , 2 ] ) <NEWLINE> pairs . end = pairs . start + frag_steps <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# print(\"WARNING: cannot make {} unique pairs with genome of {} fragments\".format(fppc, contig_frags), end='\\r')"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b0eeccf6bc94714b9c3dd34371b1399": {
        "code_string": "stc.SetText(FileOperations().readFile(filePath=fname))\n                     centerPaneTab.window.addTab(name='openFileLoad'+fileName, worksheetPanel=stc)\n",
        "code_toks_joined": "stc . SetText ( FileOperations ( ) . readFile ( filePath = fname ) ) <NEWLINE> <INDENT> centerPaneTab . window . addTab ( name = <STRING> + fileName , worksheetPanel = stc ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'openFileLoad'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0e651dec1e624300b910b15763479389": {
        "code_string": "for book in deleteBooks:\n             try:\n                 FindingBook(libraryPath=self.GetParent().GetParent().libraryPath).deleteBook(book)\n #                 text = self.GetTopLevelParent().searchCtrlPanel.searchCtrl.GetValue()\n #                 self.GetTopLevelParent().searchCtrlPanel.doSearch(text)\n             except Exception as e :\n                 logger.error(e, exc_info=True)\n                 logger.error('selectedBookIndex: %s, len: %s', selectedBookIndex, len(self._items))\n #         logger.debug(.0deleteBooks)\n         if len(deleteBooks) > 0:\n             self.updateStatusBar(text=f'{len(deleteBooks)} books deleted')\n             self.GetParent().GetParent().loadingBook()\n             self.GetParent().GetParent().updatePangnation()\n",
        "code_toks_joined": "for book in deleteBooks : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> FindingBook ( libraryPath = self . GetParent ( ) . GetParent ( ) . libraryPath ) . deleteBook ( book ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> except Exception as e : <NEWLINE> <INDENT> logger . error ( e , exc_info = True ) <NEWLINE> logger . error ( <STRING> , selectedBookIndex , len ( self . _items ) ) <NEWLINE> <COMMENT> <NL> if len ( deleteBooks ) > 0 : <NEWLINE> <DEDENT> self . updateStatusBar ( text = <STRING> ) <NEWLINE> self . GetParent ( ) . GetParent ( ) . loadingBook ( ) <NEWLINE> self . GetParent ( ) . GetParent ( ) . updatePangnation ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#                 text = self.GetTopLevelParent().searchCtrlPanel.searchCtrl.GetValue()",
                "#                 self.GetTopLevelParent().searchCtrlPanel.doSearch(text)",
                "#         logger.debug(.0deleteBooks)"
            ],
            "<STRING>": [
                "'selectedBookIndex: %s, len: %s'",
                "f'{len(deleteBooks)} books deleted'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1562d18971fe41abbaed6feffae16b6c": {
        "code_string": "self.Fi = unit_vector*(self.Fs(defflection) + self.Fd(velocity))\n         Ti_e = 2*G(self.Pi).T*(self.Ti + Skew(self.ui).T*self.Fi)\n",
        "code_toks_joined": "self . Fi = unit_vector * ( self . Fs ( defflection ) + self . Fd ( velocity ) ) <NEWLINE> <INDENT> Ti_e = 2 * G ( self . Pi ) . T * ( self . Ti + Skew ( self . ui ) . T * self . Fi ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8edb6d17d0564f35a4133cb116f26c37": {
        "code_string": "page_count = self.results_count() / 24\n         if self.results_count() % 24 > 0:\n             page_count += 1\n",
        "code_toks_joined": "page_count = self . results_count ( ) / 24 <NEWLINE> <INDENT> if self . results_count ( ) % 24 > 0 : <NEWLINE> <INDENT> page_count += 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1806579d19474eb7b4195e83d2b4e83f": {
        "code_string": "# MOCK\n def dummy_blast(query, subj, minoverlap):\n     # should return bools and positions\n     bools = [True for e in subj]\n     positions = [0 for e in subj]\n     max_positions = [len(e) for e in subj]\n     positions.extend(max_positions)\n     return bools, positions\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def dummy_blast ( query , subj , minoverlap ) : <NEWLINE> <COMMENT> <NL> <INDENT> bools = [ True for e in subj ] <NEWLINE> positions = [ 0 for e in subj ] <NEWLINE> max_positions = [ len ( e ) for e in subj ] <NEWLINE> positions . extend ( max_positions ) <NEWLINE> return bools , positions <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# MOCK",
                "# should return bools and positions"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a404d2f58d7f48a0a0405b99b930f437": {
        "code_string": "@classmethod\n     def run_all(klass, wd, stages):\n         for s in stages:\n             if check(stage=s, directory=wd):\n                 Stager(wd, s).run()\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def run_all ( klass , wd , stages ) : <NEWLINE> <INDENT> for s in stages : <NEWLINE> <INDENT> if check ( stage = s , directory = wd ) : <NEWLINE> <INDENT> Stager ( wd , s ) . run ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "398732f67dad44da8b291685b97e9207": {
        "code_string": "if sys.argv[1] == \"help\":\n     if len(sys.argv) < 2:\n       print(usage_str)\n     elif sys.argv[2] == \"datestr\":\n       print(bomail.util.datestr.datestr_str)\n     elif sys.argv[2] == \"tags\":\n       print(taghelp_str)\n     else:\n       print(usage_str)\n     exit(0)\n",
        "code_toks_joined": "if sys . argv [ 1 ] == <STRING> : <NEWLINE> <INDENT> if len ( sys . argv ) < 2 : <NEWLINE> <INDENT> print ( usage_str ) <NEWLINE> <DEDENT> elif sys . argv [ 2 ] == <STRING> : <NEWLINE> <INDENT> print ( bomail . util . datestr . datestr_str ) <NEWLINE> <DEDENT> elif sys . argv [ 2 ] == <STRING> : <NEWLINE> <INDENT> print ( taghelp_str ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( usage_str ) <NEWLINE> <DEDENT> exit ( 0 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"help\"",
                "\"datestr\"",
                "\"tags\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0b07e5227de44d27afc5dd3706fc1eda": {
        "code_string": "# like load, but try to keep all current settings if possible\n   # all possibly-important files are: those currently in our list and those matching our search string\n   def recheck(self, old_disp_info):\n     if not self.is_loaded:\n       return\n     display.draw_loading_screen(self.gui)\n     new_filenames = search.search_argstr(self.search_str, self.gui.mail_mgr)\n     new_fileset = set(new_filenames)\n     self.remove_files([t[0] for t in self.file_data if t[0] not in new_fileset], old_disp_info)\n     # assume, worst-case, that all matching files have changed/added\n     self.update_for_change(new_fileset, old_disp_info)\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> def recheck ( self , old_disp_info ) : <NEWLINE> <INDENT> if not self . is_loaded : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> display . draw_loading_screen ( self . gui ) <NEWLINE> new_filenames = search . search_argstr ( self . search_str , self . gui . mail_mgr ) <NEWLINE> new_fileset = set ( new_filenames ) <NEWLINE> self . remove_files ( [ t [ 0 ] for t in self . file_data if t [ 0 ] not in new_fileset ] , old_disp_info ) <NEWLINE> <COMMENT> <NL> self . update_for_change ( new_fileset , old_disp_info ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# like load, but try to keep all current settings if possible",
                "# all possibly-important files are: those currently in our list and those matching our search string",
                "# assume, worst-case, that all matching files have changed/added"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "950360721b4344bdb6d3f47597a11830": {
        "code_string": "def send(self, request, **kwargs):\n         func = super(FuturesSession, self).send\n         if isinstance(self.executor, ProcessPoolExecutor):\n             try:\n                 dumps(request)\n             except (TypeError, PickleError):\n                 raise RuntimeError(PICKLE_ERROR)\n         return self.executor.submit(func, request, **kwargs)\n",
        "code_toks_joined": "def send ( self , request , ** kwargs ) : <NEWLINE> <INDENT> func = super ( FuturesSession , self ) . send <NEWLINE> if isinstance ( self . executor , ProcessPoolExecutor ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> dumps ( request ) <NEWLINE> <DEDENT> except ( TypeError , PickleError ) : <NEWLINE> <INDENT> raise RuntimeError ( PICKLE_ERROR ) <NEWLINE> <DEDENT> <DEDENT> return self . executor . submit ( func , request , ** kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ff26015e94354ec8becd89c397e27e13": {
        "code_string": "actions.append({\n             'type': 'full_opt_out',\n             'name': 'Full Opt-Out',\n             'confirm': True,\n             'busy_text': 'Processing...',\n             'payload': {\n                 'contact_id': case.contact.id,\n                 'subscription_ids': active_sub_ids\n             }\n         })\n         if len(active_sub_ids) > 0:\n             actions.append(self.get_cancel_action(active_sub_ids))\n",
        "code_toks_joined": "actions . append ( { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : True , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : case . contact . id , <NEWLINE> <STRING> : active_sub_ids <NEWLINE> <DEDENT> } <NEWLINE> } ) <NEWLINE> if len ( active_sub_ids ) > 0 : <NEWLINE> actions . append ( self . get_cancel_action ( active_sub_ids ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'type'",
                "'full_opt_out'",
                "'name'",
                "'Full Opt-Out'",
                "'confirm'",
                "'busy_text'",
                "'Processing...'",
                "'payload'",
                "'contact_id'",
                "'subscription_ids'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eb9b0c29ab054a91a1cabbc4cd54eab0": {
        "code_string": "def __init__(self, param_decls, required=None, **attrs):\n         if required is None:\n             if attrs.get('default') is not None:\n                 required = False\n             else:\n                 required = attrs.get('nargs', 0) > 0\n         Parameter.__init__(self, param_decls, required=required, **attrs)\n",
        "code_toks_joined": "def __init__ ( self , param_decls , required = None , ** attrs ) : <NEWLINE> <INDENT> if required is None : <NEWLINE> <INDENT> if attrs . get ( <STRING> ) is not None : <NEWLINE> <INDENT> required = False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> required = attrs . get ( <STRING> , 0 ) > 0 <NEWLINE> <DEDENT> <DEDENT> Parameter . __init__ ( self , param_decls , required = required , ** attrs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'default'",
                "'nargs'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7aaafcad0e084f9892f63775f5c6b48d": {
        "code_string": "def callback(ctx, param, value):\n             if value or ctx.resilient_parsing:\n                 return\n             prog = prog_name\n             if prog is None:\n                 prog = ctx.find_root().info_name\n             ver = version\n             if ver is None:\n                 try:\n                     import pkg_resources\n                 except ImportError:\n                     pass\n                 else:\n                     for dist in pkg_resources.working_set:\n                         scripts = dist.get_entry_map().get('console_scripts') or {}\n                         for script_name, entry_point in iteritems(scripts):\n                             if entry_point.module_name == module:\n                                 ver = dist.version\n                                 break\n                 if ver is None:\n                     raise RuntimeError('Could not determine version')\n             echo(message % {\n                 'prog': prog,\n                 'version': ver,\n             })\n             ctx.exit()\n",
        "code_toks_joined": "def callback ( ctx , param , value ) : <NEWLINE> <INDENT> if value or ctx . resilient_parsing : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> prog = prog_name <NEWLINE> if prog is None : <NEWLINE> <INDENT> prog = ctx . find_root ( ) . info_name <NEWLINE> <DEDENT> ver = version <NEWLINE> if ver is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> import pkg_resources <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for dist in pkg_resources . working_set : <NEWLINE> <INDENT> scripts = dist . get_entry_map ( ) . get ( <STRING> ) or { } <NEWLINE> for script_name , entry_point in iteritems ( scripts ) : <NEWLINE> <INDENT> if entry_point . module_name == module : <NEWLINE> <INDENT> ver = dist . version <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> if ver is None : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> echo ( message % { <NEWLINE> <INDENT> <STRING> : prog , <NEWLINE> <STRING> : ver , <NEWLINE> <DEDENT> } ) <NEWLINE> ctx . exit ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'console_scripts'",
                "'Could not determine version'",
                "'prog'",
                "'version'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "45ed9533bd3e44a384d2705b8c8cd18b": {
        "code_string": "old_env = {}\n         try:\n             for key, value in iteritems(env):\n                 old_env[key] = os.environ.get(value)\n                 if value is None:\n                     try:\n                         del os.environ[key]\n                     except Exception:\n                         pass\n                 else:\n                     os.environ[key] = value\n             yield bytes_output\n         finally:\n             for key, value in iteritems(old_env):\n                 if value is None:\n                     try:\n                         del os.environ[key]\n                     except Exception:\n                         pass\n                 else:\n                     os.environ[key] = value\n             sys.stdout = old_stdout\n             sys.stderr = old_stderr\n             sys.stdin = old_stdin\n             clickpkg.termui.visible_prompt_func = old_visible_prompt_func\n             clickpkg.termui.hidden_prompt_func = old_hidden_prompt_func\n             clickpkg.termui._getchar = old__getchar_func\n             clickpkg.utils.should_strip_ansi = old_should_strip_ansi\n             clickpkg.formatting.FORCED_WIDTH = old_forced_width\n",
        "code_toks_joined": "old_env = { } <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> for key , value in iteritems ( env ) : <NEWLINE> <INDENT> old_env [ key ] = os . environ . get ( value ) <NEWLINE> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> yield bytes_output <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> for key , value in iteritems ( old_env ) : <NEWLINE> <INDENT> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> sys . stdout = old_stdout <NEWLINE> sys . stderr = old_stderr <NEWLINE> sys . stdin = old_stdin <NEWLINE> clickpkg . termui . visible_prompt_func = old_visible_prompt_func <NEWLINE> clickpkg . termui . hidden_prompt_func = old_hidden_prompt_func <NEWLINE> clickpkg . termui . _getchar = old__getchar_func <NEWLINE> clickpkg . utils . should_strip_ansi = old_should_strip_ansi <NEWLINE> clickpkg . formatting . FORCED_WIDTH = old_forced_width <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "edcdd5ded3d84a0c9a74e893deda41ca": {
        "code_string": "if len(messages) >= this.lastMessage:\n \t\t\tid = dom.createElement(\"span\")\n \t\t\tdom.setLayoutXSL(id, this.buildXML(), \"Messages.xsl\")\n \t\t\tdom.insertChild(id, \"Board\")\n",
        "code_toks_joined": "if len ( messages ) >= this . lastMessage : <NEWLINE> <INDENT> id = dom . createElement ( <STRING> ) <NEWLINE> dom . setLayoutXSL ( id , this . buildXML ( ) , <STRING> ) <NEWLINE> dom . insertChild ( id , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"span\"",
                "\"Messages.xsl\"",
                "\"Board\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8a2857a09b2f4c3297c700de44fe87e2": {
        "code_string": "def send_request(url, method, \n \tdata, args, params, headers, cookies, timeout, is_json):\n \t\"\"\"\n \tForge and send HTTP request.\n \t\"\"\"\n \t## Parse url args\n \tfor p in args:\n \t\turl = url.replace(':'+p, str(params[p]))\n",
        "code_toks_joined": "def send_request ( url , method , <NEWLINE> <INDENT> data , args , params , headers , cookies , timeout , is_json ) : <NEWLINE> <STRING> <NEWLINE> <COMMENT> <NL> for p in args : <NEWLINE> <INDENT> url = url . replace ( <STRING> + p , str ( params [ p ] ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n \tForge and send HTTP request.\n \t\"\"\"",
                "':'"
            ],
            "<COMMENT>": [
                "## Parse url args"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f24a252f9f1e432da0156c702bef62fc": {
        "code_string": "security.declareProtected(\n         'Access contents information', 'digest')\n     def digest(self, *args):\n         assert len(args) > 1, u'Too few arguments'\n         challenge = hmac.new(self.__key, str(args[0]), hashlib.sha1)\n         for arg in args[1:]:\n             challenge.update(str(args))\n         return challenge.hexdigest()\n",
        "code_toks_joined": "security . declareProtected ( <NEWLINE> <INDENT> <STRING> , <STRING> ) <NEWLINE> def digest ( self , * args ) : <NEWLINE> assert len ( args ) > 1 , <STRING> <NEWLINE> challenge = hmac . new ( self . __key , str ( args [ 0 ] ) , hashlib . sha1 ) <NEWLINE> for arg in args [ 1 : ] : <NEWLINE> <INDENT> challenge . update ( str ( args ) ) <NEWLINE> <DEDENT> return challenge . hexdigest ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Access contents information'",
                "'digest'",
                "u'Too few arguments'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5a0fa272e1544ff18f1c375338eeb69f": {
        "code_string": "def is_request_ready(self, _request_id: str, _retry: bool = True):\n         \"\"\"\n         checks on the status of a request. If it is complete it will return the download url\n         :param _request_id: request id to check on.\n         :param _retry: Should the request be retried on error.\n         :return: download url if the request is complete, None otherwise\n         \"\"\"\n         self.login()\n         url = f\"{self.base_url}/request?ids={_request_id}\"\n         req = Request(url, headers=self.headers)\n         try:\n             decoded = json.load(urlopen(req))\n             if len(decoded) > 1 and 'downloadUrl' in decoded[0]:\n                 return decoded[0]['downloadUrl']\n             return None\n         except HTTPError as e:\n             if self._error_handling(e) and _retry:\n                 return self.is_request_ready(_request_id, _retry=False)\n",
        "code_toks_joined": "def is_request_ready ( self , _request_id : str , _retry : bool = True ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . login ( ) <NEWLINE> url = <STRING> <NEWLINE> req = Request ( url , headers = self . headers ) <NEWLINE> try : <NEWLINE> <INDENT> decoded = json . load ( urlopen ( req ) ) <NEWLINE> if len ( decoded ) > 1 and <STRING> in decoded [ 0 ] : <NEWLINE> <INDENT> return decoded [ 0 ] [ <STRING> ] <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT> except HTTPError as e : <NEWLINE> <INDENT> if self . _error_handling ( e ) and _retry : <NEWLINE> <INDENT> return self . is_request_ready ( _request_id , _retry = False ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         checks on the status of a request. If it is complete it will return the download url\n         :param _request_id: request id to check on.\n         :param _retry: Should the request be retried on error.\n         :return: download url if the request is complete, None otherwise\n         \"\"\"",
                "f\"{self.base_url}/request?ids={_request_id}\"",
                "'downloadUrl'",
                "'downloadUrl'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9a95574e37cb47ddb4f121e5d333a541": {
        "code_string": "def update(self, _id, data: dict) -> dict:\n         \"\"\"\n         Overwrite the YAML file.\n         \"\"\"\n         file_path = self.file_path(_id)\n         if not self.exists(_id):\n             raise Exception('File does not exist, {}'.format(file_path))\n         if '_id' not in data:\n             data['_id'] = _id\n         # get a copy of the current yaml data and then merge it with what is\n         # being update.  this is important \n         cur_data = self.fetch(_id)\n         new_data = DictUtils.merge(cur_data, data)\n         Yaml.to_file(file_path=file_path, data=new_data)\n         return data\n",
        "code_toks_joined": "def update ( self , _id , data : dict ) -> dict : <NEWLINE> <INDENT> <STRING> <NEWLINE> file_path = self . file_path ( _id ) <NEWLINE> if not self . exists ( _id ) : <NEWLINE> <INDENT> raise Exception ( <STRING> . format ( file_path ) ) <NEWLINE> <DEDENT> if <STRING> not in data : <NEWLINE> <INDENT> data [ <STRING> ] = _id <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> cur_data = self . fetch ( _id ) <NEWLINE> new_data = DictUtils . merge ( cur_data , data ) <NEWLINE> Yaml . to_file ( file_path = file_path , data = new_data ) <NEWLINE> return data <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Overwrite the YAML file.\n         \"\"\"",
                "'File does not exist, {}'",
                "'_id'",
                "'_id'"
            ],
            "<COMMENT>": [
                "# get a copy of the current yaml data and then merge it with what is",
                "# being update.  this is important "
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ff4709f5188540fcb8ba830e83f8d56b": {
        "code_string": "# we're loading a list of BizObjects\n             if rel.many:\n                 related_bizobj_list = []\n                 for obj in related_data:\n                     if isinstance(obj, rel.target):\n                         related_bizobj_list.append(obj)\n                     else:\n                         related_bizobj_list.append(\n                             rel.target(related_data)\n                         )\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if rel . many : <NEWLINE> <INDENT> related_bizobj_list = [ ] <NEWLINE> for obj in related_data : <NEWLINE> <INDENT> if isinstance ( obj , rel . target ) : <NEWLINE> <INDENT> related_bizobj_list . append ( obj ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> related_bizobj_list . append ( <NEWLINE> <INDENT> rel . target ( related_data ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# we're loading a list of BizObjects"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "30f1689aab624300bcc0aea6054e085a": {
        "code_string": "def _aggregate_related(\n         self,\n         bizobj: 'BizType',\n         to_create: Dict,\n         to_update: Dict,\n     ) -> None:\n         for k, v in bizobj.related.items():\n             rel = bizobj.relationships[k]\n             if not v:\n                 continue\n             if rel.many:\n                 for x in v:\n                     if x._id is None:\n                         to_create[v.biz_type].append(x)\n                     elif x.dirty:\n                         to_update[v.biz_type].append(x)\n                 for x in v:\n                     self._aggregate_related(x, to_create, to_update)\n             else:\n                 v_type = v.__class__\n                 if v._id is None:\n                     to_create[v_type].append(v)\n                 elif x.dirty:\n                     to_update[v_type].append(x)\n                 self._aggregate_related(v, to_create, to_update)\n",
        "code_toks_joined": "def _aggregate_related ( <NEWLINE> <INDENT> self , <NEWLINE> bizobj : <STRING> , <NEWLINE> to_create : Dict , <NEWLINE> to_update : Dict , <NEWLINE> ) -> None : <NEWLINE> for k , v in bizobj . related . items ( ) : <NEWLINE> <INDENT> rel = bizobj . relationships [ k ] <NEWLINE> if not v : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if rel . many : <NEWLINE> <INDENT> for x in v : <NEWLINE> <INDENT> if x . _id is None : <NEWLINE> <INDENT> to_create [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> elif x . dirty : <NEWLINE> <INDENT> to_update [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> <DEDENT> for x in v : <NEWLINE> <INDENT> self . _aggregate_related ( x , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v_type = v . __class__ <NEWLINE> if v . _id is None : <NEWLINE> <INDENT> to_create [ v_type ] . append ( v ) <NEWLINE> <DEDENT> elif x . dirty : <NEWLINE> <INDENT> to_update [ v_type ] . append ( x ) <NEWLINE> <DEDENT> self . _aggregate_related ( v , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'BizType'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0d8d7747df5f4fd6854e38da3bbccbc1": {
        "code_string": "def _aggregate_related(\n         self,\n         bizobj: 'BizType',\n         to_create: Dict,\n         to_update: Dict,\n     ) -> None:\n         for k, v in bizobj.related.items():\n             rel = bizobj.relationships[k]\n             if not v:\n                 continue\n             if rel.many:\n                 for x in v:\n                     if x._id is None:\n                         to_create[v.biz_type].append(x)\n                     elif x.dirty:\n                         to_update[v.biz_type].append(x)\n                 for x in v:\n                     self._aggregate_related(x, to_create, to_update)\n             else:\n                 v_type = v.__class__\n                 if v._id is None:\n                     to_create[v_type].append(v)\n                 elif v.dirty:\n                     to_update[v_type].append(x)\n                 self._aggregate_related(v, to_create, to_update)\n",
        "code_toks_joined": "def _aggregate_related ( <NEWLINE> <INDENT> self , <NEWLINE> bizobj : <STRING> , <NEWLINE> to_create : Dict , <NEWLINE> to_update : Dict , <NEWLINE> ) -> None : <NEWLINE> for k , v in bizobj . related . items ( ) : <NEWLINE> <INDENT> rel = bizobj . relationships [ k ] <NEWLINE> if not v : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if rel . many : <NEWLINE> <INDENT> for x in v : <NEWLINE> <INDENT> if x . _id is None : <NEWLINE> <INDENT> to_create [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> elif x . dirty : <NEWLINE> <INDENT> to_update [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> <DEDENT> for x in v : <NEWLINE> <INDENT> self . _aggregate_related ( x , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v_type = v . __class__ <NEWLINE> if v . _id is None : <NEWLINE> <INDENT> to_create [ v_type ] . append ( v ) <NEWLINE> <DEDENT> elif v . dirty : <NEWLINE> <INDENT> to_update [ v_type ] . append ( x ) <NEWLINE> <DEDENT> self . _aggregate_related ( v , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'BizType'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a55b602537ea4852acebd9f139a2d07f": {
        "code_string": "if self._op == OP_CODE.AND:\n             # We only need to check RHS if LHS isn't already False.\n             if lhs_exc is None:\n                 rhs_exc = self._rhs(context, arguments)\n                 if rhs_exc is not None:\n                     return rhs_exc\n             else:\n                 return lhs_exc\n         elif self._op == OP_CODE.OR:\n             rhs_exc = self._rhs(context, arguments)\n             if rhs_exc is not None and lhs_exc is not None:\n                 return CompositeGuardException(lhs_exc, rhs_exc)\n         elif self._op == OP_CODE.NOT:\n             if lhs_exc is not None:\n                 return lhs_exc\n         else:\n             return ValueError(f'op not recognized, \"{self._op}\"')\n",
        "code_toks_joined": "if self . _op == OP_CODE . AND : <NEWLINE> <COMMENT> <NL> <INDENT> if lhs_exc is None : <NEWLINE> <INDENT> rhs_exc = self . _rhs ( context , arguments ) <NEWLINE> if rhs_exc is not None : <NEWLINE> <INDENT> return rhs_exc <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> return lhs_exc <NEWLINE> elif self . _op == OP_CODE . OR : <NEWLINE> <DEDENT> rhs_exc = self . _rhs ( context , arguments ) <NEWLINE> if rhs_exc is not None and lhs_exc is not None : <NEWLINE> <INDENT> return CompositeGuardException ( lhs_exc , rhs_exc ) <NEWLINE> elif self . _op == OP_CODE . NOT : <NEWLINE> <DEDENT> if lhs_exc is not None : <NEWLINE> <INDENT> return lhs_exc <NEWLINE> else : <NEWLINE> <DEDENT> return ValueError ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# We only need to check RHS if LHS isn't already False."
            ],
            "<STRING>": [
                "f'op not recognized, \"{self._op}\"'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6e370623335743d6bf5f1c87ea427e37": {
        "code_string": "new_decorator = type(self)(self.app, *self.args, **kwargs)\n                     new_decorator._api_object = self._api_object\n                     new_decorator.setup_action(v.target, True)\n                 else:\n                     self.setup_action(v.__func__, False)\n             return api_type\n         else:\n             func = obj\n             action = self.setup_action(func, False)\n             return action\n",
        "code_toks_joined": "new_decorator = type ( self ) ( self . app , * self . args , ** kwargs ) <NEWLINE> <INDENT> new_decorator . _api_object = self . _api_object <NEWLINE> new_decorator . setup_action ( v . target , True ) <NEWLINE> else : <NEWLINE> self . setup_action ( v . __func__ , False ) <NEWLINE> return api_type <NEWLINE> else : <NEWLINE> func = obj <NEWLINE> action = self . setup_action ( func , False ) <NEWLINE> return action <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "204c1aa5a3cd4c549318225d85359df5": {
        "code_string": "# print text all together\n             if date is True:\n                 p(star, Fore.LIGHTMAGENTA_EX + str(item[\"id\"]).rjust(2), mark, text_color + item[\"text\"], tag_text,\n                   (Fore.LIGHTBLACK_EX + \"(due: {})\".format(color + str(duedate) + Fore.LIGHTBLACK_EX)) if item[\"due\"] else \"\",\n                   Fore.LIGHTBLACK_EX + str(item[\"date\"]))\n             else:\n                 p(star, Fore.LIGHTMAGENTA_EX + str(item[\"id\"]).rjust(2), mark, text_color + item[\"text\"], tag_text, due_text, day_text)\n     print()\n     print_footer()\n     print_total()\n     print()\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if date is True : <NEWLINE> <INDENT> p ( star , Fore . LIGHTMAGENTA_EX + str ( item [ <STRING> ] ) . rjust ( 2 ) , mark , text_color + item [ <STRING> ] , tag_text , <NEWLINE> <INDENT> ( Fore . LIGHTBLACK_EX + <STRING> . format ( color + str ( duedate ) + Fore . LIGHTBLACK_EX ) ) if item [ <STRING> ] else <STRING> , <NEWLINE> Fore . LIGHTBLACK_EX + str ( item [ <STRING> ] ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> p ( star , Fore . LIGHTMAGENTA_EX + str ( item [ <STRING> ] ) . rjust ( 2 ) , mark , text_color + item [ <STRING> ] , tag_text , due_text , day_text ) <NEWLINE> print ( ) <NEWLINE> print_footer ( ) <NEWLINE> print_total ( ) <NEWLINE> print ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# print text all together"
            ],
            "<STRING>": [
                "\"id\"",
                "\"text\"",
                "\"(due: {})\"",
                "\"due\"",
                "\"\"",
                "\"date\"",
                "\"id\"",
                "\"text\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aeba116f9813490a86cae1d75266a8b6": {
        "code_string": "MAXFD = 1024\n fdset = c_int32 * (MAXFD/32)\n",
        "code_toks_joined": "MAXFD = 1024 <NEWLINE> <INDENT> fdset = c_int32 * ( MAXFD / 32 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c38d687255b1437686e2c42542764144": {
        "code_string": "def decodeOid(pdu):\n     return tuple([pdu.val.objid[i] for i in range(pdu.val_len / sizeof(u_long))])\n",
        "code_toks_joined": "def decodeOid ( pdu ) : <NEWLINE> <INDENT> return tuple ( [ pdu . val . objid [ i ] for i in range ( pdu . val_len / sizeof ( u_long ) ) ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ca5c7442f8094aa1842b605129984add": {
        "code_string": "def stats(dir_stats=DIR_STATS):\n \ttry:\n \t\twith open(dir_stats + 'stat_links', 'r') as myfile:\n \t\t\tcontent = myfile.read().split()\n \texcept FileNotFoundError:\n \t\tstat_links = ('File ' +  dir_stats + 'stat_links' + ' not found.')\n \telse:\n \t\tstat_links = ('Average links in webpage: ' + str(average(content)))\n \t\tif len(content) > 10000:\n \t\t\tcompress_stats(dir_stats + 'stat_links')\n \tresult += stat_links + '\\n'\n \ttry:\n \t\twith open(dir_stats + 'stat_webpages', 'r') as myfile:\n \t\t\tcontent = myfile.read().split()\n \texcept FileNotFoundError:\n \t\tstat_webpages = 'File ' +  dir_stats + 'stat_webpages' + ' not found.'\n \telse:\n \t\tstat_webpages = 'Time to crawl ten webpages: ' + str(average(content))\n \tresult += stat_webpages + '\\n'\n \ttry:\n \t\twith open(dir_stats + 'stat_dl_index', 'r') as myfile:\n \t\t\tcontent = myfile.read().split()\n \texcept FileNotFoundError:\n \t\tstat_dl_index = 'File ' +  dir_stats + 'stat_dl_index' + ' not found.'\n \telse:\n \t\tstat_dl_index = 'Time download inverted-index: ' + str(average(content))\n \tresult += stat_dl_index + '\\n'\n \ttry:\n \t\twith open(dir_stats + 'stat_up_index', 'r') as myfile:\n \t\t\tcontent = myfile.read().split()\n \texcept FileNotFoundError:\n \t\tstat_up_index = 'File ' +  dir_stats + 'stat_up_index' + ' not found.'\n \telse:\n \t\tstat_up_index = 'Time upload inverted-index: ' + str(average(content))\n \tresult += stat_up_index + '\\n'\n \treturn result\n",
        "code_toks_joined": "def stats ( dir_stats = DIR_STATS ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_links = ( <STRING> + dir_stats + <STRING> + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_links = ( <STRING> + str ( average ( content ) ) ) <NEWLINE> if len ( content ) > 10000 : <NEWLINE> <INDENT> compress_stats ( dir_stats + <STRING> ) <NEWLINE> <DEDENT> <DEDENT> result += stat_links + <STRING> <NEWLINE> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_webpages = <STRING> + dir_stats + <STRING> + <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_webpages = <STRING> + str ( average ( content ) ) <NEWLINE> <DEDENT> result += stat_webpages + <STRING> <NEWLINE> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_dl_index = <STRING> + dir_stats + <STRING> + <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_dl_index = <STRING> + str ( average ( content ) ) <NEWLINE> <DEDENT> result += stat_dl_index + <STRING> <NEWLINE> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_up_index = <STRING> + dir_stats + <STRING> + <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_up_index = <STRING> + str ( average ( content ) ) <NEWLINE> <DEDENT> result += stat_up_index + <STRING> <NEWLINE> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'stat_links'",
                "'r'",
                "'File '",
                "'stat_links'",
                "' not found.'",
                "'Average links in webpage: '",
                "'stat_links'",
                "'\\n'",
                "'stat_webpages'",
                "'r'",
                "'File '",
                "'stat_webpages'",
                "' not found.'",
                "'Time to crawl ten webpages: '",
                "'\\n'",
                "'stat_dl_index'",
                "'r'",
                "'File '",
                "'stat_dl_index'",
                "' not found.'",
                "'Time download inverted-index: '",
                "'\\n'",
                "'stat_up_index'",
                "'r'",
                "'File '",
                "'stat_up_index'",
                "' not found.'",
                "'Time upload inverted-index: '",
                "'\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1e97e683a2984680b2c1253344eb3d68": {
        "code_string": "if len(sys.argv) > 0:\n         sys.stdin = open(sys.argv[1])\n         while not stdio.isEmpty():\n             input_item = stdio.readString()\n             if input_item is not '-':\n                 queue.enqueue(input_item)\n             elif not queue.is_empty():\n                 print(queue.dequeue())\n         print('({} left on queue)'.format(queue.size()))\n",
        "code_toks_joined": "if len ( sys . argv ) > 0 : <NEWLINE> <INDENT> sys . stdin = open ( sys . argv [ 1 ] ) <NEWLINE> while not stdio . isEmpty ( ) : <NEWLINE> <INDENT> input_item = stdio . readString ( ) <NEWLINE> if input_item is not <STRING> : <NEWLINE> <INDENT> queue . enqueue ( input_item ) <NEWLINE> <DEDENT> elif not queue . is_empty ( ) : <NEWLINE> <INDENT> print ( queue . dequeue ( ) ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> . format ( queue . size ( ) ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'-'",
                "'({} left on queue)'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d43756389eca4c33b224095969ac0f8a": {
        "code_string": "class ReCaptchaField(forms.CharField):\n     def __init__(self, attrs=None, *args, **kwargs):\n         if os.environ.get('RECAPTCHA_DISABLE', None) is not None:\n             self._private_key = kwargs.pop('private_key', settings.RECAPTCHA_PRIVATE_KEY)\n             self._score_threshold = kwargs.pop('score_threshold', settings.RECAPTCHA_SCORE_THRESHOLD)\n",
        "code_toks_joined": "class ReCaptchaField ( forms . CharField ) : <NEWLINE> <INDENT> def __init__ ( self , attrs = None , * args , ** kwargs ) : <NEWLINE> <INDENT> if os . environ . get ( <STRING> , None ) is not None : <NEWLINE> <INDENT> self . _private_key = kwargs . pop ( <STRING> , settings . RECAPTCHA_PRIVATE_KEY ) <NEWLINE> self . _score_threshold = kwargs . pop ( <STRING> , settings . RECAPTCHA_SCORE_THRESHOLD ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'RECAPTCHA_DISABLE'",
                "'private_key'",
                "'score_threshold'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "48afb58ada4943298d23c18ed338628f": {
        "code_string": "def parse_object(self, json_string):\n         try:\n             json_object = json.loads(json_string)\n         except json.JSONDecodeError as json_error:\n             LOGGER.warning(\"W001: Cannot parse object\")\n             LOGGER.warning(json_string)\n             LOGGER.warning(\"Decoder reports %s\", json_string)\n             return\n",
        "code_toks_joined": "def parse_object ( self , json_string ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> json_object = json . loads ( json_string ) <NEWLINE> <DEDENT> except json . JSONDecodeError as json_error : <NEWLINE> <INDENT> LOGGER . warning ( <STRING> ) <NEWLINE> LOGGER . warning ( json_string ) <NEWLINE> LOGGER . warning ( <STRING> , json_string ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"W001: Cannot parse object\"",
                "\"Decoder reports %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1cf0622f924b4691b0b5a8b9d86ace6a": {
        "code_string": "stmt = list()\n         for ln in sql_in:\n             cline = ln.strip()\n             if ln.startswith('--') or len(cline) == 0:\n                 continue\n",
        "code_toks_joined": "stmt = list ( ) <NEWLINE> <INDENT> for ln in sql_in : <NEWLINE> <INDENT> cline = ln . strip ( ) <NEWLINE> if ln . startswith ( <STRING> ) or len ( cline ) == 0 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'--'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "78564c224c084c4f95ca95f136fb6a22": {
        "code_string": "if r.status_code != 204:\n                 comments_to_delete += r.json().get(\"comments\", list())\n                 deleted_comments += 1\n         except:\n             continue\n",
        "code_toks_joined": "if r . status_code != 204 : <NEWLINE> <INDENT> comments_to_delete += r . json ( ) . get ( <STRING> , list ( ) ) <NEWLINE> deleted_comments += 1 <NEWLINE> except : <NEWLINE> continue <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"comments\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "54c6101b390b4e7ea243f2d861f2989d": {
        "code_string": "# Use polygon triangulation to cut the bounding region into a list of triangles, calculate the area of each triangle\n \tlstTriangle = tripy.earclip(poly)\n \tlstArea = []\n \tfor i in range(len(lstTriangle)):\n \t\tlstArea.append(geoAreaOfTriangle(lstTriangle[i][0], lstTriangle[i][1], lstTriangle[i][2]))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> lstTriangle = tripy . earclip ( poly ) <NEWLINE> lstArea = [ ] <NEWLINE> for i in range ( len ( lstTriangle ) ) : <NEWLINE> <INDENT> lstArea . append ( geoAreaOfTriangle ( lstTriangle [ i ] [ 0 ] , lstTriangle [ i ] [ 1 ] , lstTriangle [ i ] [ 2 ] ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Use polygon triangulation to cut the bounding region into a list of triangles, calculate the area of each triangle"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "77eeff60056b4a2097215a520778c4e0": {
        "code_string": "plotting.eigs(root=rootname, affinity=mdl_obj.affinity_matrix_,\n                               n_clusters=n_clusters,\n                               title='Eigenvalues of the graph associated to '\n                                     'the affinity matrix')\n             if hasattr(mdl_obj, 'cluster_centers_'):\n                 _est_name = mdl_obj.__dict__.get('estimator_name', '') or \\\n                     type(mdl_obj).__name__\n                 if _est_name != 'AffinityPropagation':\n                     # disable the voronoi plot for affinity prop\n                     plotting.voronoi(root=rootname, labels=y, data_in=step_in,\n                                      model=voronoi_mdl_obj)\n             elif hasattr(mdl_obj, 'n_leaves_'):\n                 plotting.tree(root=rootname, data_in=step_in,\n                               labels=step_out, model=mdl_obj)\n                 plotting.dendrogram(root=rootname, data_in=step_in,\n                                     labels=y, model=mdl_obj)\n",
        "code_toks_joined": "plotting . eigs ( root = rootname , affinity = mdl_obj . affinity_matrix_ , <NEWLINE> <INDENT> n_clusters = n_clusters , <NEWLINE> title = <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if hasattr ( mdl_obj , <STRING> ) : <NEWLINE> _est_name = mdl_obj . __dict__ . get ( <STRING> , <STRING> ) or type ( mdl_obj ) . __name__ <NEWLINE> if _est_name != <STRING> : <NEWLINE> <COMMENT> <NL> plotting . voronoi ( root = rootname , labels = y , data_in = step_in , <NEWLINE> <INDENT> model = voronoi_mdl_obj ) <NEWLINE> elif hasattr ( mdl_obj , <STRING> ) : <NEWLINE> plotting . tree ( root = rootname , data_in = step_in , <NEWLINE> <DEDENT> <DEDENT> labels = step_out , model = mdl_obj ) <NEWLINE> plotting . dendrogram ( root = rootname , data_in = step_in , <NEWLINE> <INDENT> labels = y , model = mdl_obj ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Eigenvalues of the graph associated to '",
                "'the affinity matrix'",
                "'cluster_centers_'",
                "'estimator_name'",
                "''",
                "'AffinityPropagation'",
                "'n_leaves_'"
            ],
            "<COMMENT>": [
                "# disable the voronoi plot for affinity prop"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3113819d0ede408d9f28e662b170d8d4": {
        "code_string": "if ch == 27:\n             return EVENT_ESC\n         elif ch == -1 or ch == curses.KEY_RESIZE:\n             return EVENT_RESIZE\n         elif ch == 10 or ch == curses.KEY_ENTER:\n             return EVENT_ENTER\n         elif ch == 127 or ch == curses.KEY_BACKSPACE:\n             return EVENT_BACKSPACE\n         elif ch == curses.KEY_UP:\n             return EVENT_UP\n         elif ch == curses.KEY_DOWN:\n             return EVENT_DOWN\n         elif ch == curses.KEY_LEFT:\n             return EVENT_LEFT\n         elif ch == curses.KEY_RIGHT:\n             return EVENT_RIGHT\n         elif ch == 3:\n             return EVENT_CTRL_C\n         elif 0 >= ch < 256:\n             return chr(ch)\n         else:\n             return EVENT_UNHANDLED\n",
        "code_toks_joined": "if ch == 27 : <NEWLINE> <INDENT> return EVENT_ESC <NEWLINE> elif ch == - 1 or ch == curses . KEY_RESIZE : <NEWLINE> return EVENT_RESIZE <NEWLINE> elif ch == 10 or ch == curses . KEY_ENTER : <NEWLINE> return EVENT_ENTER <NEWLINE> elif ch == 127 or ch == curses . KEY_BACKSPACE : <NEWLINE> return EVENT_BACKSPACE <NEWLINE> elif ch == curses . KEY_UP : <NEWLINE> return EVENT_UP <NEWLINE> elif ch == curses . KEY_DOWN : <NEWLINE> return EVENT_DOWN <NEWLINE> elif ch == curses . KEY_LEFT : <NEWLINE> return EVENT_LEFT <NEWLINE> elif ch == curses . KEY_RIGHT : <NEWLINE> return EVENT_RIGHT <NEWLINE> elif ch == 3 : <NEWLINE> return EVENT_CTRL_C <NEWLINE> elif 0 >= ch < 256 : <NEWLINE> return chr ( ch ) <NEWLINE> else : <NEWLINE> return EVENT_UNHANDLED <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8d3ad8013b434430bcec94191d460f65": {
        "code_string": "def read(self, aTableName):\n         if self._objStore_flag:\n             try:\n                 objPath = (self._path + \"/\" + aTableName + \".enc\")\n                 self._objStore.get_object(self._admin_username, objPath)\n                 tableFile = (self._path + \"/\" + aTableName + \".pq\")\n                 self._decrypt(tableFile)\n                 dataframe = pq.read_table(tableFile).to_pandas()\n                 os.remove(tableFile)\n                 os.remove(objPath)\n                 return dataframe\n             except:\n                 return (\"File not found. Check bucket '{}' for object '{}'\"\n                         .format(self._admin_username, objPath))\n         else:\n             try:\n                 tableFile = (self._path + \"/\" + aTableName + \".pq\")\n                 tableFileEnc = (self._path + \"/\" + aTableName + \".enc\")\n                 self._decrypt(tableFileEnc)\n                 dataframe = pq.read_table(tableFile).to_pandas()\n                 os.remove(tableFile)\n                 return dataframe\n             except:\n                 return (\"File not found. Check path '{}' for file '{}'\"\n                         .format(self._path, tableFile))\n",
        "code_toks_joined": "def read ( self , aTableName ) : <NEWLINE> <INDENT> if self . _objStore_flag : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> objPath = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> self . _objStore . get_object ( self . _admin_username , objPath ) <NEWLINE> tableFile = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> self . _decrypt ( tableFile ) <NEWLINE> dataframe = pq . read_table ( tableFile ) . to_pandas ( ) <NEWLINE> os . remove ( tableFile ) <NEWLINE> os . remove ( objPath ) <NEWLINE> return dataframe <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> return ( <STRING> <NEWLINE> <INDENT> . format ( self . _admin_username , objPath ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> tableFile = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> tableFileEnc = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> self . _decrypt ( tableFileEnc ) <NEWLINE> dataframe = pq . read_table ( tableFile ) . to_pandas ( ) <NEWLINE> os . remove ( tableFile ) <NEWLINE> return dataframe <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> return ( <STRING> <NEWLINE> <INDENT> . format ( self . _path , tableFile ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"/\"",
                "\".enc\"",
                "\"/\"",
                "\".pq\"",
                "\"File not found. Check bucket '{}' for object '{}'\"",
                "\"/\"",
                "\".pq\"",
                "\"/\"",
                "\".enc\"",
                "\"File not found. Check path '{}' for file '{}'\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b485a2f25c1041aaa9cd0aa5d52ffaba": {
        "code_string": "try:\n             if self.op == '':\n                 pass\n             elif self.op == '+':\n                 self.r1 = self.r1 + self.r2\n             elif self.op == '-':\n                 self.r1 = self.r1 + self.r2\n             elif self.op == '*':\n                 self.r1 = self.r1 * self.r2\n             elif self.op == '/':\n                 self.r1 = self.r1 / self.r2\n             elif self.op == 'ceil':\n                 self.r1 = math.ceil(self.r1)\n             elif self.op == 'fabs':\n                 self.r1 = math.fabs(self.r1)\n             elif self.op == 'fmod':\n                 self.r1 = math.fmod(self.r1, self.r2)\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> if self . op == <STRING> : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 + self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 + self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 * self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 / self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = math . ceil ( self . r1 ) <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = math . fabs ( self . r1 ) <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = math . fmod ( self . r1 , self . r2 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''",
                "'+'",
                "'-'",
                "'*'",
                "'/'",
                "'ceil'",
                "'fabs'",
                "'fmod'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ada0fb8e32234afdb8a4e7e452b29110": {
        "code_string": "def _write_solocache_group_to_file(self, data_dict, group_prefix=\"/\"):\n         if self._is_master() and group_prefix != \"/\":\n             if group_prefix in self._f:\n                 self._f.create_group(group_prefix)\n         keys = data_dict.keys()\n         keys.sort()\n         for k in keys:\n             name = group_prefix + k\n             if isinstance(data_dict[k], dict):\n                 self._write_solocache_group_to_file(data_dict[k], group_prefix=name+\"/\")\n             else:\n                 (data, op) = data_dict[k]\n                 if op is not None:\n                     if numpy.isscalar(data):\n                         sendobj = numpy.array(data)\n                     else:\n                         sendobj = data\n                     recvobj = numpy.empty_like(data)\n                     log_debug(logger, self._log_prefix + \"Reducing data %s\" % (name))\n                     self.comm.Reduce(\n                         [sendobj, MPI.DOUBLE],\n                         [recvobj, MPI.DOUBLE],\n                         op = op,\n                         root = 0\n                     )\n                     data = recvobj\n                 if self._is_master():\n                     log_debug(logger, self._log_prefix + \"Writing data %s\" % (name))\n                     self._f[name] = data\n",
        "code_toks_joined": "def _write_solocache_group_to_file ( self , data_dict , group_prefix = <STRING> ) : <NEWLINE> <INDENT> if self . _is_master ( ) and group_prefix != <STRING> : <NEWLINE> <INDENT> if group_prefix in self . _f : <NEWLINE> <INDENT> self . _f . create_group ( group_prefix ) <NEWLINE> <DEDENT> <DEDENT> keys = data_dict . keys ( ) <NEWLINE> keys . sort ( ) <NEWLINE> for k in keys : <NEWLINE> <INDENT> name = group_prefix + k <NEWLINE> if isinstance ( data_dict [ k ] , dict ) : <NEWLINE> <INDENT> self . _write_solocache_group_to_file ( data_dict [ k ] , group_prefix = name + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ( data , op ) = data_dict [ k ] <NEWLINE> if op is not None : <NEWLINE> <INDENT> if numpy . isscalar ( data ) : <NEWLINE> <INDENT> sendobj = numpy . array ( data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sendobj = data <NEWLINE> <DEDENT> recvobj = numpy . empty_like ( data ) <NEWLINE> log_debug ( logger , self . _log_prefix + <STRING> % ( name ) ) <NEWLINE> self . comm . Reduce ( <NEWLINE> <INDENT> [ sendobj , MPI . DOUBLE ] , <NEWLINE> [ recvobj , MPI . DOUBLE ] , <NEWLINE> op = op , <NEWLINE> root = 0 <NEWLINE> <DEDENT> ) <NEWLINE> data = recvobj <NEWLINE> <DEDENT> if self . _is_master ( ) : <NEWLINE> <INDENT> log_debug ( logger , self . _log_prefix + <STRING> % ( name ) ) <NEWLINE> self . _f [ name ] = data <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"/\"",
                "\"/\"",
                "\"/\"",
                "\"Reducing data %s\"",
                "\"Writing data %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "82597add204a4024bd720e87c8875eca": {
        "code_string": "def __call__(self):\n         if np.random.uniform() > self._epsilon:\n             return False\n         return True\n",
        "code_toks_joined": "def __call__ ( self ) : <NEWLINE> <INDENT> if np . random . uniform ( ) > self . _epsilon : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "82f53abecde942f3ad9ef23e29695193": {
        "code_string": "last = 0 if n_steps < self.mdp.horizon or not absorbing else 1\n             sample = self.state.ravel().tolist() + action.ravel().tolist() + \\\n                      [reward] + next_state.ravel().tolist() + \\\n                      [absorbing, last]\n",
        "code_toks_joined": "last = 0 if n_steps < self . mdp . horizon or not absorbing else 1 <NEWLINE> <INDENT> sample = self . state . ravel ( ) . tolist ( ) + action . ravel ( ) . tolist ( ) + [ reward ] + next_state . ravel ( ) . tolist ( ) + [ absorbing , last ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dc1148eaff324919aee4d68f474cdfce": {
        "code_string": "if iterate_over == 'episodes':\n             self.logger.info('Episodes: %d' % (i + 1))\n             self.logger.info(self.state)\n         while i < how_many:\n             J = 0.\n             action_idx = self.agent.draw_action(self.state,\n                                                 self.agent.approximator)\n             action_value = self.mdp.action_space.get_value(action_idx)\n             next_state, reward, absorbing, _ = self.mdp.step(action_idx)\n             J += self.mdp.gamma ** n_steps * reward\n             n_steps += 1\n",
        "code_toks_joined": "if iterate_over == <STRING> : <NEWLINE> <INDENT> self . logger . info ( <STRING> % ( i + 1 ) ) <NEWLINE> self . logger . info ( self . state ) <NEWLINE> while i < how_many : <NEWLINE> J = 0. <NEWLINE> action_idx = self . agent . draw_action ( self . state , <NEWLINE> <INDENT> self . agent . approximator ) <NEWLINE> <DEDENT> action_value = self . mdp . action_space . get_value ( action_idx ) <NEWLINE> next_state , reward , absorbing , _ = self . mdp . step ( action_idx ) <NEWLINE> J += self . mdp . gamma ** n_steps * reward <NEWLINE> n_steps += 1 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'episodes'",
                "'Episodes: %d'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ccba4a8fff764686b46d61ec50bade9c": {
        "code_string": "def parse_quantity(str_value):\n     \"\"\"Parse a string into an astropy Quantity. Rather than trying to guess\n     where the split occurs, we try every position from the back until we\n     succeed.\"\"\"\n     for i in range(len(str_value), -1, 0):\n         try:\n             value = float(str_value[:i])\n             unit = units.Unit(str_value[i:])\n             return units.Quantity(value, unit)\n         except ValueError:\n             pass\n     raise ValueError('Could not parse {} as a quantity'.format(str_value))\n",
        "code_toks_joined": "def parse_quantity ( str_value ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for i in range ( len ( str_value ) , - 1 , 0 ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> value = float ( str_value [ : i ] ) <NEWLINE> unit = units . Unit ( str_value [ i : ] ) <NEWLINE> return units . Quantity ( value , unit ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> raise ValueError ( <STRING> . format ( str_value ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Parse a string into an astropy Quantity. Rather than trying to guess\n     where the split occurs, we try every position from the back until we\n     succeed.\"\"\"",
                "'Could not parse {} as a quantity'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5a52c24a5af24dccbe345f603cca7cff": {
        "code_string": "def clear(self):\n         self.ensure_all_bound()\n         # If self._fill is set, it is not necessary to clear first because\n         # finalize will set all relevant values.\n         if self._fill is not None:\n             self.buffer('grid').zero(self.command_queue)\n",
        "code_toks_joined": "def clear ( self ) : <NEWLINE> <INDENT> self . ensure_all_bound ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if self . _fill is not None : <NEWLINE> <INDENT> self . buffer ( <STRING> ) . zero ( self . command_queue ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If self._fill is set, it is not necessary to clear first because",
                "# finalize will set all relevant values."
            ],
            "<STRING>": [
                "'grid'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9162718a9c0346a5a20a0636546da369": {
        "code_string": "def render(self, output_dir, mode, stokes, channel, rel_channel):\n         # Check whether the Stokes parameters and/or channel are all bundled\n         # into one image, and if so, slice it.\n         filename = os.path.join(output_dir, self.fits_filename(mode, stokes, channel, rel_channel))\n         with closing(fits.open(filename)) as hdulist:\n             naxis = int(hdulist[0].header['NAXIS'])\n             slices = [0] * (naxis - 2)\n             for i in range(3, naxis + 1):\n                 axis_type = hdulist[0].header['CTYPE{}'.format(i)]\n                 if axis_type == 'STOKES':\n                     # TODO: should use the WCS transformation\n                     slices[i - 3] = 'IQUV'.find(stokes)\n                 elif axis_type == 'VOPT':\n                     slices[i - 3] = channel\n         self._render_thumb(output_dir, filename, slices, mode, stokes, channel)\n         self._render_full(output_dir, filename, slices, mode, stokes, channel)\n",
        "code_toks_joined": "def render ( self , output_dir , mode , stokes , channel , rel_channel ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> filename = os . path . join ( output_dir , self . fits_filename ( mode , stokes , channel , rel_channel ) ) <NEWLINE> with closing ( fits . open ( filename ) ) as hdulist : <NEWLINE> <INDENT> naxis = int ( hdulist [ 0 ] . header [ <STRING> ] ) <NEWLINE> slices = [ 0 ] * ( naxis - 2 ) <NEWLINE> for i in range ( 3 , naxis + 1 ) : <NEWLINE> <INDENT> axis_type = hdulist [ 0 ] . header [ <STRING> . format ( i ) ] <NEWLINE> if axis_type == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> slices [ i - 3 ] = <STRING> . find ( stokes ) <NEWLINE> <DEDENT> elif axis_type == <STRING> : <NEWLINE> <INDENT> slices [ i - 3 ] = channel <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . _render_thumb ( output_dir , filename , slices , mode , stokes , channel ) <NEWLINE> self . _render_full ( output_dir , filename , slices , mode , stokes , channel ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check whether the Stokes parameters and/or channel are all bundled",
                "# into one image, and if so, slice it.",
                "# TODO: should use the WCS transformation"
            ],
            "<STRING>": [
                "'NAXIS'",
                "'CTYPE{}'",
                "'STOKES'",
                "'IQUV'",
                "'VOPT'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "33f2183251764c21b1db4f956b9814dd": {
        "code_string": "# Get command from the microservice\n             cmd_project_config = ProjectConfig(cmd_name, project_dir)\n             handler = get_handler(project_dir, module, service)\n             cmd = project_config.get_command(handler, module, service, workspace)\n             if not cmd:\n                 raise CwsClientError(f\"Undefined command {cmd}.\\n\")\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> cmd_project_config = ProjectConfig ( cmd_name , project_dir ) <NEWLINE> handler = get_handler ( project_dir , module , service ) <NEWLINE> cmd = project_config . get_command ( handler , module , service , workspace ) <NEWLINE> if not cmd : <NEWLINE> <INDENT> raise CwsClientError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Get command from the microservice"
            ],
            "<STRING>": [
                "f\"Undefined command {cmd}.\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0e6a5e3326824f24a358661be3a7bb17": {
        "code_string": "def require_relationship(resource_list, data):\n     for resource in resource_list:\n         if resource not in data:\n             raise UnprocessableEntity({'pointer': '/data/relationships/{}'.format(resource)},\n                                       \"A valid relationship with {} resource is required\".format(resource))\n",
        "code_toks_joined": "def require_relationship ( resource_list , data ) : <NEWLINE> <INDENT> for resource in resource_list : <NEWLINE> <INDENT> if resource not in data : <NEWLINE> <INDENT> raise UnprocessableEntity ( { <STRING> : <STRING> . format ( resource ) } , <NEWLINE> <INDENT> <STRING> . format ( resource ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pointer'",
                "'/data/relationships/{}'",
                "\"A valid relationship with {} resource is required\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "07264357f056426bb9d98a825f9e91cc": {
        "code_string": "def require_relationship(resource_list, data):\n     for resource in resource_list:\n         if resource not in data:\n             raise UnprocessableEntity({'pointer': '/data/relationships/{}'.format(resource)},\n                                       \"A valid relationship with {} resource is required\".format(resource))\n",
        "code_toks_joined": "def require_relationship ( resource_list , data ) : <NEWLINE> <INDENT> for resource in resource_list : <NEWLINE> <INDENT> if resource not in data : <NEWLINE> <INDENT> raise UnprocessableEntity ( { <STRING> : <STRING> . format ( resource ) } , <NEWLINE> <INDENT> <STRING> . format ( resource ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pointer'",
                "'/data/relationships/{}'",
                "\"A valid relationship with {} resource is required\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f9921cbcb1fb4e6681776a9601def619": {
        "code_string": "def render_templates(target_path, replace_values, file_types):\n     basedir = os.path.abspath(target_path)\n     for root, dirnames, files in os.walk(basedir):\n         for f in files:\n             skip = True\n             full_path = os.path.join(root, f)\n             for ft in file_types:\n                 if f.endswith('.{}'.format(ft)):\n                     skip = False\n                     continue\n             if skip:\n                 continue\n             env = Environment(loader=FileSystemLoader(root))\n             template = env.get_template(f)\n             rendered = template.render(replace_values)\n             with open(full_path, \"w\") as fh:\n                 print('Writing {}'.format(f))\n                 fh.write(rendered)\n",
        "code_toks_joined": "def render_templates ( target_path , replace_values , file_types ) : <NEWLINE> <INDENT> basedir = os . path . abspath ( target_path ) <NEWLINE> for root , dirnames , files in os . walk ( basedir ) : <NEWLINE> <INDENT> for f in files : <NEWLINE> <INDENT> skip = True <NEWLINE> full_path = os . path . join ( root , f ) <NEWLINE> for ft in file_types : <NEWLINE> <INDENT> if f . endswith ( <STRING> . format ( ft ) ) : <NEWLINE> <INDENT> skip = False <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> if skip : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> env = Environment ( loader = FileSystemLoader ( root ) ) <NEWLINE> template = env . get_template ( f ) <NEWLINE> rendered = template . render ( replace_values ) <NEWLINE> with open ( full_path , <STRING> ) as fh : <NEWLINE> <INDENT> print ( <STRING> . format ( f ) ) <NEWLINE> fh . write ( rendered ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.{}'",
                "\"w\"",
                "'Writing {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "752a8e44555e4c4fa98bd596c974ea69": {
        "code_string": "def sync(args):\n     \"\"\"\n     Implement the 'greg sync' command\n     \"\"\"\n     import operator\n     session = Session(args)\n     if \"all\" in args[\"names\"]:\n         targetfeeds = session.list_feeds()\n     else:\n         targetfeeds = []\n         for name in args[\"names\"]:\n             if name not in session.feeds:\n                 print(\"You don't have a feed called {}.\"\n                       .format(name), file=sys.stderr, flush=True)\n             else:\n                 targetfeeds.append(name)\n     for target in targetfeeds:\n         feed = Feed(session, target, None)\n         if not feed.wentwrong:\n             try:\n                 title = feed.podcast.target.title\n             except AttributeError:\n                 title = target\n             print(\"Checking\", title, end=\"...\\n\")\n             currentdate, stop = feed.how_many()\n             entrycounter = 0\n             entries_to_download = feed.podcast.entries\n             for entry in entries_to_download:\n                 feed.fix_linkdate(entry)\n             # Sort entries_to_download, but only if you want to download as\n             # many as there are\n             if stop < len(entries_to_download):\n                 entries_to_download.sort(key=operator.attrgetter(\"linkdate\"),\n                                          reverse=False)\n             for entry in entries_to_download:\n                 if entry.linkdate > currentdate:\n                     downloaded = feed.download_entry(entry)\n                     entrycounter += downloaded\n                 if entrycounter >= stop:\n                     break\n             print(\"Done\")\n         else:\n             msg = ''.join([\"I cannot sync \", feed,\n                            \" just now. Are you connected to the internet?\"])\n             print(msg, file=sys.stderr, flush=True)\n",
        "code_toks_joined": "def sync ( args ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> import operator <NEWLINE> session = Session ( args ) <NEWLINE> if <STRING> in args [ <STRING> ] : <NEWLINE> <INDENT> targetfeeds = session . list_feeds ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> targetfeeds = [ ] <NEWLINE> for name in args [ <STRING> ] : <NEWLINE> <INDENT> if name not in session . feeds : <NEWLINE> <INDENT> print ( <STRING> <NEWLINE> <INDENT> . format ( name ) , file = sys . stderr , flush = True ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> targetfeeds . append ( name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for target in targetfeeds : <NEWLINE> <INDENT> feed = Feed ( session , target , None ) <NEWLINE> if not feed . wentwrong : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> title = feed . podcast . target . title <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> title = target <NEWLINE> <DEDENT> print ( <STRING> , title , end = <STRING> ) <NEWLINE> currentdate , stop = feed . how_many ( ) <NEWLINE> entrycounter = 0 <NEWLINE> entries_to_download = feed . podcast . entries <NEWLINE> for entry in entries_to_download : <NEWLINE> <INDENT> feed . fix_linkdate ( entry ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> if stop < len ( entries_to_download ) : <NEWLINE> <INDENT> entries_to_download . sort ( key = operator . attrgetter ( <STRING> ) , <NEWLINE> <INDENT> reverse = False ) <NEWLINE> <DEDENT> <DEDENT> for entry in entries_to_download : <NEWLINE> <INDENT> if entry . linkdate > currentdate : <NEWLINE> <INDENT> downloaded = feed . download_entry ( entry ) <NEWLINE> entrycounter += downloaded <NEWLINE> <DEDENT> if entrycounter >= stop : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> msg = <STRING> . join ( [ <STRING> , feed , <NEWLINE> <INDENT> <STRING> ] ) <NEWLINE> <DEDENT> print ( msg , file = sys . stderr , flush = True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Implement the 'greg sync' command\n     \"\"\"",
                "\"all\"",
                "\"names\"",
                "\"names\"",
                "\"You don't have a feed called {}.\"",
                "\"Checking\"",
                "\"...\\n\"",
                "\"linkdate\"",
                "\"Done\"",
                "''",
                "\"I cannot sync \"",
                "\" just now. Are you connected to the internet?\""
            ],
            "<COMMENT>": [
                "# Sort entries_to_download, but only if you want to download as",
                "# many as there are"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c8d50618da6946e28838023b9d88e7a4": {
        "code_string": "if not request.user.has_perm('stack.change_question') or not request.user == instance.comment.user:\n \t\traise Http404\n",
        "code_toks_joined": "if not request . user . has_perm ( <STRING> ) or not request . user == instance . comment . user : <NEWLINE> <INDENT> raise Http404 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'stack.change_question'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c6187a4d358741ceb188eb9973e82802": {
        "code_string": "def __call__(self, action_set, payoff, situation):\n         matching_degrees = [\n             classifier.calc_matching_degree(self._rule_repr, situation)\n             for classifier in action_set\n         ]\n         total_matching_degrees = sum(matching_degrees)\n         assert total_matching_degrees > 0.0\n         for (classifier, matching_degree) in zip(matching_degrees, action_set):\n             credit_weight = (matching_degree / total_matching_degrees)\n             self._update_experience(classifier, credit_weight)\n             payoff_diff = payoff - classifier.get_prediction(situation)\n             self._update_weight_vec(classifier, payoff_diff, situation,\n                                     credit_weight)\n             self._update_prediction_error(classifier, payoff_diff,\n                                           credit_weight)\n             self._update_action_set_size(classifier, action_set)\n",
        "code_toks_joined": "def __call__ ( self , action_set , payoff , situation ) : <NEWLINE> <INDENT> matching_degrees = [ <NEWLINE> <INDENT> classifier . calc_matching_degree ( self . _rule_repr , situation ) <NEWLINE> for classifier in action_set <NEWLINE> <DEDENT> ] <NEWLINE> total_matching_degrees = sum ( matching_degrees ) <NEWLINE> assert total_matching_degrees > 0.0 <NEWLINE> for ( classifier , matching_degree ) in zip ( matching_degrees , action_set ) : <NEWLINE> <INDENT> credit_weight = ( matching_degree / total_matching_degrees ) <NEWLINE> self . _update_experience ( classifier , credit_weight ) <NEWLINE> payoff_diff = payoff - classifier . get_prediction ( situation ) <NEWLINE> self . _update_weight_vec ( classifier , payoff_diff , situation , <NEWLINE> <INDENT> credit_weight ) <NEWLINE> <DEDENT> self . _update_prediction_error ( classifier , payoff_diff , <NEWLINE> <INDENT> credit_weight ) <NEWLINE> <DEDENT> self . _update_action_set_size ( classifier , action_set ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2fe62948c9964e7eab02bbf33e838248": {
        "code_string": "@click.command()\n @click.option('-a', '--answered-out', is_flag=True, help='fill in answered out status')\n @click.argument('case_id')\n @click.pass_context\n def update(context, answered_out, case_id):\n     \"\"\"Fill in information in Housekeeper.\"\"\"\n     if answered_out:\n         hk_db = apps.hk.connect(context.obj)\n         lims_api = apps.lims.connect(context.obj)\n         log.debug(\"get case from housekeeper\")\n         hk_case = apps.hk.api.case(case_id)\n         log.debug(\"loop over related samples from most recent run\")\n         delivery_dates = []\n         hk_run = hk_case.current\n         for hk_sample in hk_run.samples:\n             log.debug(\"lookup if sample has been delivered in LIMS\")\n             delivery_date = lims_api.is_delivered(hk_sample.lims_id)\n             if delivery_date is None:\n                 log.warn(\"sample not delivered: %s\", hk_sample.lims_id)\n                 context.abort()\n             delivery_dates.append(delivery_dates)\n         latest_date = sorted(delivery_dates)[-1]\n         log.debug(\"fillin answered out date in HK\")\n         hk_run.answeredout_at = datetime.combine(latest_date, datetime.min.time())\n         hk_db.commit()\n         log.info(\"run 'answered out' date updated: %s\", case_id)\n",
        "code_toks_joined": "@ click . command ( ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , is_flag = True , help = <STRING> ) <NEWLINE> @ click . argument ( <STRING> ) <NEWLINE> @ click . pass_context <NEWLINE> def update ( context , answered_out , case_id ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if answered_out : <NEWLINE> <INDENT> hk_db = apps . hk . connect ( context . obj ) <NEWLINE> lims_api = apps . lims . connect ( context . obj ) <NEWLINE> log . debug ( <STRING> ) <NEWLINE> hk_case = apps . hk . api . case ( case_id ) <NEWLINE> log . debug ( <STRING> ) <NEWLINE> delivery_dates = [ ] <NEWLINE> hk_run = hk_case . current <NEWLINE> for hk_sample in hk_run . samples : <NEWLINE> <INDENT> log . debug ( <STRING> ) <NEWLINE> delivery_date = lims_api . is_delivered ( hk_sample . lims_id ) <NEWLINE> if delivery_date is None : <NEWLINE> <INDENT> log . warn ( <STRING> , hk_sample . lims_id ) <NEWLINE> context . abort ( ) <NEWLINE> <DEDENT> delivery_dates . append ( delivery_dates ) <NEWLINE> <DEDENT> latest_date = sorted ( delivery_dates ) [ - 1 ] <NEWLINE> log . debug ( <STRING> ) <NEWLINE> hk_run . answeredout_at = datetime . combine ( latest_date , datetime . min . time ( ) ) <NEWLINE> hk_db . commit ( ) <NEWLINE> log . info ( <STRING> , case_id ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'-a'",
                "'--answered-out'",
                "'fill in answered out status'",
                "'case_id'",
                "\"\"\"Fill in information in Housekeeper.\"\"\"",
                "\"get case from housekeeper\"",
                "\"loop over related samples from most recent run\"",
                "\"lookup if sample has been delivered in LIMS\"",
                "\"sample not delivered: %s\"",
                "\"fillin answered out date in HK\"",
                "\"run 'answered out' date updated: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "02d7ea3ecca948c9a7e764c69fbc3060": {
        "code_string": "if len(is_externals) == 0:\n         data = dict(is_external=is_externals.pop())\n     else:\n         # mix of internal and external samples\n         data = dict(is_external=False)\n",
        "code_toks_joined": "if len ( is_externals ) == 0 : <NEWLINE> <INDENT> data = dict ( is_external = is_externals . pop ( ) ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> data = dict ( is_external = False ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# mix of internal and external samples"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a34aab1a0c194d22a08c394685960b5a": {
        "code_string": "for family_obj in families:\n         LOG.debug(f\"{family_id.internal_id}: get info about family\")\n         row = [\n             family_obj.internal_id,\n             family_obj.name,\n             family_obj.customer.internal_id,\n             family_obj.priority_human,\n             ', '.join(family_obj.panels),\n             family_obj.action or 'NA',\n         ]\n         click.echo(tabulate([row], headers=FAMILY_HEADERS, tablefmt='psql'))\n         if samples:\n             sample_ids = [link_obj.sample.internal_id for link_obj in family_obj.links]\n             context.invoke(sample, sample_ids=sample_ids, families=False)\n",
        "code_toks_joined": "for family_obj in families : <NEWLINE> <INDENT> LOG . debug ( <STRING> ) <NEWLINE> row = [ <NEWLINE> <INDENT> family_obj . internal_id , <NEWLINE> family_obj . name , <NEWLINE> family_obj . customer . internal_id , <NEWLINE> family_obj . priority_human , <NEWLINE> <STRING> . join ( family_obj . panels ) , <NEWLINE> family_obj . action or <STRING> , <NEWLINE> <DEDENT> ] <NEWLINE> click . echo ( tabulate ( [ row ] , headers = FAMILY_HEADERS , tablefmt = <STRING> ) ) <NEWLINE> if samples : <NEWLINE> <INDENT> sample_ids = [ link_obj . sample . internal_id for link_obj in family_obj . links ] <NEWLINE> context . invoke ( sample , sample_ids = sample_ids , families = False ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"{family_id.internal_id}: get info about family\"",
                "', '",
                "'NA'",
                "'psql'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "88ed964393e14428914124d56dcdaf50": {
        "code_string": "def maximum_flowcells_ondisk(self, max_flowcells: int=700) -> bool:\n         \"\"\"Check if there's too many flowcells already \"ondisk\".\"\"\"\n         ondisk_flowcells = self.status.flowcells(status='ondisk').count()\n         LOG.debug(f\"ondisk flowcells: {ondisk_flowcells}\")\n         return ondisk_flowcells < max_flowcells\n",
        "code_toks_joined": "def maximum_flowcells_ondisk ( self , max_flowcells : int = 700 ) -> bool : <NEWLINE> <INDENT> <STRING> <NEWLINE> ondisk_flowcells = self . status . flowcells ( status = <STRING> ) . count ( ) <NEWLINE> LOG . debug ( <STRING> ) <NEWLINE> return ondisk_flowcells < max_flowcells <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Check if there's too many flowcells already \"ondisk\".\"\"\"",
                "'ondisk'",
                "f\"ondisk flowcells: {ondisk_flowcells}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e51ddc96850549fb95b334562b869cb0": {
        "code_string": "def manage_addComment(self, author, body, url='', email='', date=None, bitakora_cpt='', random_cpt='', captcha_zz=0, REQUEST=None):\n     \"\"\" Called from HTML form when commenting \"\"\"\n     from utils import checkCaptchaValue, isCommentSpam\n     if not captcha_zz:\n         if not checkCaptchaValue(random_cpt, bitakora_cpt):\n             if REQUEST is not None:\n                 return REQUEST.RESPONSE.redirect(self.absolute_url()+u'?msg=%s&body=%s&comment_author=%s&comment_email=%s&comment_url=%s#bitakora_cpt_control' % (self.gettext('Are you a bot? Please try again...'), url_quote(body.encode('utf-8')), url_quote(author.encode('utf-8')), url_quote(email.encode('utf-8')), url_quote(url.encode('utf-8'))))\n",
        "code_toks_joined": "def manage_addComment ( self , author , body , url = <STRING> , email = <STRING> , date = None , bitakora_cpt = <STRING> , random_cpt = <STRING> , captcha_zz = 0 , REQUEST = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> from utils import checkCaptchaValue , isCommentSpam <NEWLINE> if not captcha_zz : <NEWLINE> <INDENT> if not checkCaptchaValue ( random_cpt , bitakora_cpt ) : <NEWLINE> <INDENT> if REQUEST is not None : <NEWLINE> <INDENT> return REQUEST . RESPONSE . redirect ( self . absolute_url ( ) + <STRING> % ( self . gettext ( <STRING> ) , url_quote ( body . encode ( <STRING> ) ) , url_quote ( author . encode ( <STRING> ) ) , url_quote ( email . encode ( <STRING> ) ) , url_quote ( url . encode ( <STRING> ) ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''",
                "''",
                "''",
                "''",
                "\"\"\" Called from HTML form when commenting \"\"\"",
                "u'?msg=%s&body=%s&comment_author=%s&comment_email=%s&comment_url=%s#bitakora_cpt_control'",
                "'Are you a bot? Please try again...'",
                "'utf-8'",
                "'utf-8'",
                "'utf-8'",
                "'utf-8'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "438daa01bd8b40aebcea1325ce7a97d1": {
        "code_string": "if event == 'highstate-start':\n             minions += set(data['minions'])\n         elif event == 'highstate':\n             minions.discard(data['minion'])\n",
        "code_toks_joined": "if event == <STRING> : <NEWLINE> <INDENT> minions += set ( data [ <STRING> ] ) <NEWLINE> elif event == <STRING> : <NEWLINE> minions . discard ( data [ <STRING> ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'highstate-start'",
                "'minions'",
                "'highstate'",
                "'minion'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a7bdfb2de0974958816a82be4e6ce3b7": {
        "code_string": "response_verification = self._response.verify(\n             response,\n             ValueContext(origin_datetime=response.starts),\n         )\n         return CaseResult(\n             label=self._label,\n             execution=execution,\n             response=response_verification,\n         )\n",
        "code_toks_joined": "response_verification = self . _response . verify ( <NEWLINE> <INDENT> response , <NEWLINE> ValueContext ( origin_datetime = response . starts ) , <NEWLINE> ) <NEWLINE> return CaseResult ( <NEWLINE> label = self . _label , <NEWLINE> execution = execution , <NEWLINE> response = response_verification , <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a676edc4ada5491f9253b39dbaeaf564": {
        "code_string": "# allow to interpolate for half the window size\n     if interpolate:\n         ser = Series(arr).interpolate(limit=half_window)\n         y = array(ser)\n     else:\n         y = array(arr)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if interpolate : <NEWLINE> <INDENT> ser = Series ( arr ) . interpolate ( limit = half_window ) <NEWLINE> y = array ( ser ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> y = array ( arr ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# allow to interpolate for half the window size"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c1879ff804424a64a15a9c47e801edc5": {
        "code_string": "def _is_wc_root(root, info):\n     root = os.path.normpath(os.path.abspath(root))\n     if os.path.normcase(root) == os.path.normcase(info.get('Working Copy Root Path', '')):\n         return True\n     elif info:\n         p = os.path.dirname(root)\n         return (p == root\n                 or not os.path.isdir(os.path.join(p, '.svn'))\n                 or _info(p).get('Repository UUID') == info['Repository UUID'])\n     return False\n",
        "code_toks_joined": "def _is_wc_root ( root , info ) : <NEWLINE> <INDENT> root = os . path . normpath ( os . path . abspath ( root ) ) <NEWLINE> if os . path . normcase ( root ) == os . path . normcase ( info . get ( <STRING> , <STRING> ) ) : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> elif info : <NEWLINE> <INDENT> p = os . path . dirname ( root ) <NEWLINE> return ( p == root <NEWLINE> <INDENT> or not os . path . isdir ( os . path . join ( p , <STRING> ) ) <NEWLINE> or _info ( p ) . get ( <STRING> ) == info [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Working Copy Root Path'",
                "''",
                "'.svn'",
                "'Repository UUID'",
                "'Repository UUID'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6e0fe7affe1f4b2ba5f3fe6d9096cff1": {
        "code_string": "while '<' in string and '>' in string:\n         tags = string.split('<', 1)[1].split('>', 1)[0].split()\n         if tags[0][0] == '/':\n             tags[0] = tags[0][1:]\n             if tags[0] == 'color':\n                 colorStack.pop()\n             elif tags[0] == 'bgcolor':\n                 bgcolorStack.pop()\n             else:\n                 formatStack.reverse()\n                 formatStack.remove(formatTable[tags[0]])\n                 formatStack.reverse()\n             if len(colorStack) is 0 or len(bgcolorStack) is 0 or len(formatStack) is 0:\n                 raise SyntaxError(\"Unable to close tag %s\" % tags[0])\n         else:\n             try:\n                 if tags[0] == 'color':\n                     if len(tags) is 1:\n                         tags[1] = \"default\"\n                     colorStack.append(colorTable[tags[1]])\n                 elif tags[0] == 'bgcolor':\n                     if len(tags) is 1:\n                         tags[1] = \"default\"\n                     bgcolorStack.append(colorTable[tags[1]])\n                 else:\n                     formatStack.append(formatTable[tags[0]])\n             except:\n                 raise SyntaxError(\"Invalid tag or argument: %s %s\" % (tags[0], tags[1]))\n         newString = string.split(\"<\", 1)[0]\n         newString += \"\\033[%d;%d;%dm\" % (\n                                         formatStack[-1],\n                                         colorStack[-1]+30,\n                                         bgcolorStack[-1]+40\n                                         )\n         newString += string.split(\">\", 1)[1]\n         string = newString\n     return newString\n",
        "code_toks_joined": "while <STRING> in string and <STRING> in string : <NEWLINE> <INDENT> tags = string . split ( <STRING> , 1 ) [ 1 ] . split ( <STRING> , 1 ) [ 0 ] . split ( ) <NEWLINE> if tags [ 0 ] [ 0 ] == <STRING> : <NEWLINE> <INDENT> tags [ 0 ] = tags [ 0 ] [ 1 : ] <NEWLINE> if tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> colorStack . pop ( ) <NEWLINE> <DEDENT> elif tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> bgcolorStack . pop ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> formatStack . reverse ( ) <NEWLINE> formatStack . remove ( formatTable [ tags [ 0 ] ] ) <NEWLINE> formatStack . reverse ( ) <NEWLINE> <DEDENT> if len ( colorStack ) is 0 or len ( bgcolorStack ) is 0 or len ( formatStack ) is 0 : <NEWLINE> <INDENT> raise SyntaxError ( <STRING> % tags [ 0 ] ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> if len ( tags ) is 1 : <NEWLINE> <INDENT> tags [ 1 ] = <STRING> <NEWLINE> <DEDENT> colorStack . append ( colorTable [ tags [ 1 ] ] ) <NEWLINE> <DEDENT> elif tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> if len ( tags ) is 1 : <NEWLINE> <INDENT> tags [ 1 ] = <STRING> <NEWLINE> <DEDENT> bgcolorStack . append ( colorTable [ tags [ 1 ] ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> formatStack . append ( formatTable [ tags [ 0 ] ] ) <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> raise SyntaxError ( <STRING> % ( tags [ 0 ] , tags [ 1 ] ) ) <NEWLINE> <DEDENT> <DEDENT> newString = string . split ( <STRING> , 1 ) [ 0 ] <NEWLINE> newString += <STRING> % ( <NEWLINE> <INDENT> formatStack [ - 1 ] , <NEWLINE> colorStack [ - 1 ] + 30 , <NEWLINE> bgcolorStack [ - 1 ] + 40 <NEWLINE> ) <NEWLINE> <DEDENT> newString += string . split ( <STRING> , 1 ) [ 1 ] <NEWLINE> string = newString <NEWLINE> return newString <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'<'",
                "'>'",
                "'<'",
                "'>'",
                "'/'",
                "'color'",
                "'bgcolor'",
                "\"Unable to close tag %s\"",
                "'color'",
                "\"default\"",
                "'bgcolor'",
                "\"default\"",
                "\"Invalid tag or argument: %s %s\"",
                "\"<\"",
                "\"\\033[%d;%d;%dm\"",
                "\">\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "df400c699bbc4198ad3945c4def08037": {
        "code_string": "@feature('command')\n def feature_command(tgen):\n     '''\n     Run a command\n     '''\n     cmd_dir = tgen.make_node(tgen.worch.command_dir)\n     cmd_node = cmd_dir.make_node(tgen.worch.command_cmd)\n     cmd_target = \\\n         map(cmd_dir.make_node, tgen.to_list(tgen.worch.command_target))\n     cmd_rule = '{command_cmd_prefix}{command_cmd} {command_cmd_options} {command_cmd_postfix}'\n     tgen.step('command',\n               rule = tgen.worch.format(cmd_rule),\n               source = cmd_node,\n               target = cmd_target,\n               cwd = cmd_dir.abspath())\n",
        "code_toks_joined": "@ feature ( <STRING> ) <NEWLINE> <INDENT> def feature_command ( tgen ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cmd_dir = tgen . make_node ( tgen . worch . command_dir ) <NEWLINE> cmd_node = cmd_dir . make_node ( tgen . worch . command_cmd ) <NEWLINE> cmd_target = map ( cmd_dir . make_node , tgen . to_list ( tgen . worch . command_target ) ) <NEWLINE> cmd_rule = <STRING> <NEWLINE> tgen . step ( <STRING> , <NEWLINE> <INDENT> rule = tgen . worch . format ( cmd_rule ) , <NEWLINE> source = cmd_node , <NEWLINE> target = cmd_target , <NEWLINE> cwd = cmd_dir . abspath ( ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'command'",
                "'''\n     Run a command\n     '''",
                "'{command_cmd_prefix}{command_cmd} {command_cmd_options} {command_cmd_postfix}'",
                "'command'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3cf3c47f631545a8856dbef2dc2dd405": {
        "code_string": "if ss > 0 and correct:\n         correction = correction_function(max(ig1.mut, ig2.mut))\n         # ss = 1 - ((1 - ss) * max(correction, 0))\n         # ss = 1 - ((1 - ss) * correction)\n         ss *= correction\n     # return min(max(ss, 0), 1)\n     return max(ss, 0)\n",
        "code_toks_joined": "if ss > 0 and correct : <NEWLINE> <INDENT> correction = correction_function ( max ( ig1 . mut , ig2 . mut ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> ss *= correction <NEWLINE> <COMMENT> <NL> return max ( ss , 0 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ss = 1 - ((1 - ss) * max(correction, 0))",
                "# ss = 1 - ((1 - ss) * correction)",
                "# return min(max(ss, 0), 1)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0ffc9204cb8d4c8fadb33e0a57e548fc": {
        "code_string": "if ss > 0 and correct:\n         correction = correction_function(max(ig1.mut, ig2.mut))\n         # ss = 1 - ((1 - ss) * max(correction, 0))\n         # ss = 1 - ((1 - ss) * correction)\n         ss += (1. - correction)\n     # return min(max(ss, 0), 1)\n     return max(ss, 0)\n",
        "code_toks_joined": "if ss > 0 and correct : <NEWLINE> <INDENT> correction = correction_function ( max ( ig1 . mut , ig2 . mut ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> ss += ( 1. - correction ) <NEWLINE> <COMMENT> <NL> return max ( ss , 0 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ss = 1 - ((1 - ss) * max(correction, 0))",
                "# ss = 1 - ((1 - ss) * correction)",
                "# return min(max(ss, 0), 1)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "44603cbd4a064b73a55701ab8ca65245": {
        "code_string": "if connect_only:\n             inspection = reflection.Inspector.from_engine(self.engine)\n             views = inspection.get_view_names(schema=schema)\n             tables = inspection.get_table_names(schema=schema)\n",
        "code_toks_joined": "if connect_only : <NEWLINE> <INDENT> inspection = reflection . Inspector . from_engine ( self . engine ) <NEWLINE> views = inspection . get_view_names ( schema = schema ) <NEWLINE> tables = inspection . get_table_names ( schema = schema ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d69587162c6a47f5b5a4b840875d2f3c": {
        "code_string": "def print_result(peps, print_peps,threshold):\n     # calculates the frequency of the scores\n     # prints flagged peptides in console, if print_peps == True\n     # peptides are flagged, if they are greater than the average of all positive scores\n     scores = list(peps.values())\n     scores_in_tens = [int(round(i, -1)) if i >= threshold else -1 for i in scores]  # only round positiv scores\n     freq_scores = {x: scores_in_tens.count(x) for x in scores_in_tens}  # Dictionary Comprehension\n",
        "code_toks_joined": "def print_result ( peps , print_peps , threshold ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> scores = list ( peps . values ( ) ) <NEWLINE> scores_in_tens = [ int ( round ( i , - 1 ) ) if i >= threshold else - 1 for i in scores ] <COMMENT> <NEWLINE> freq_scores = { x : scores_in_tens . count ( x ) for x in scores_in_tens } <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# calculates the frequency of the scores",
                "# prints flagged peptides in console, if print_peps == True",
                "# peptides are flagged, if they are greater than the average of all positive scores",
                "# only round positiv scores",
                "# Dictionary Comprehension"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "89755425570147578231a39f3fb9e6a4": {
        "code_string": "def import_patch(self, patch_name, new_name=None):\n         \"\"\" Import patch into the patch queue\n         The patch is inserted after the current top applied patch\n         \"\"\"\n         if not new_name:\n             dir_name = os.path.dirname(new_name)\n             name = os.path.basename(new_name)\n             dest_dir = self.quilt_patches + Directory(dir_name)\n             dest_dir.create()\n         else:\n             name = os.path.basename(patch_name)\n             dest_dir = self.quilt_patches\n",
        "code_toks_joined": "def import_patch ( self , patch_name , new_name = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not new_name : <NEWLINE> <INDENT> dir_name = os . path . dirname ( new_name ) <NEWLINE> name = os . path . basename ( new_name ) <NEWLINE> dest_dir = self . quilt_patches + Directory ( dir_name ) <NEWLINE> dest_dir . create ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> name = os . path . basename ( patch_name ) <NEWLINE> dest_dir = self . quilt_patches <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Import patch into the patch queue\n         The patch is inserted after the current top applied patch\n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c47481a06fc74a5fbc071b62cbb17ff6": {
        "code_string": "applied = self.db.applied_patches()\n         for patch in applied:\n             if patch in patches:\n                 patches.remove(applied)\n",
        "code_toks_joined": "applied = self . db . applied_patches ( ) <NEWLINE> <INDENT> for patch in applied : <NEWLINE> <INDENT> if patch in patches : <NEWLINE> <INDENT> patches . remove ( applied ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "48c8c1ebc1e64bc09a2b3293bbed8c75": {
        "code_string": "def test_a_verifier_retrieves_previous_failing_examples_from_the_database():\n     database = ExampleDatabase()\n     verifier = Verifier(settings=hs.Settings(database=database))\n     verifier.falsify(lambda x: x != 11, int)\n     called = []\n",
        "code_toks_joined": "def test_a_verifier_retrieves_previous_failing_examples_from_the_database ( ) : <NEWLINE> <INDENT> database = ExampleDatabase ( ) <NEWLINE> verifier = Verifier ( settings = hs . Settings ( database = database ) ) <NEWLINE> verifier . falsify ( lambda x : x != 11 , int ) <NEWLINE> called = [ ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "19d987dd430d4e9098c66de37ae548f0": {
        "code_string": "def incorporate_new_buffer(self, buffer):\n         if (\n             self.settings.timeout > 0 and\n             time.time() >= self.start_time + self.settings.timeout\n         ):\n             raise RunIsComplete()\n         self.examples_considered += 1\n         if (\n             buffer[:self.last_data.index] ==\n             self.last_data.buffer[:self.last_data.index]\n         ):\n             return False\n         data = TestData.for_buffer(buffer[:self.last_data.index])\n         self.test_function(data)\n         data.freeze()\n         if data.status >= self.last_data.status:\n             debug_report('%d bytes %r -> %r, %s' % (\n                 data.index,\n                 list(data.buffer[:data.index]), data.status,\n                 data.output.decode('utf-8'),\n             ))\n         if data.status >= Status.VALID:\n             self.valid_examples += 1\n         if self.consider_new_test_data(data):\n             if self.last_data.status == Status.INTERESTING:\n                 self.shrinks += 1\n                 self.last_data = data\n                 if self.shrinks >= self.settings.max_shrinks:\n                     raise RunIsComplete()\n             self.last_data = data\n             self.changed += 1\n             return True\n         return False\n",
        "code_toks_joined": "def incorporate_new_buffer ( self , buffer ) : <NEWLINE> <INDENT> if ( <NEWLINE> <INDENT> self . settings . timeout > 0 and <NEWLINE> time . time ( ) >= self . start_time + self . settings . timeout <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> raise RunIsComplete ( ) <NEWLINE> <DEDENT> self . examples_considered += 1 <NEWLINE> if ( <NEWLINE> <INDENT> buffer [ : self . last_data . index ] == <NEWLINE> self . last_data . buffer [ : self . last_data . index ] <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> data = TestData . for_buffer ( buffer [ : self . last_data . index ] ) <NEWLINE> self . test_function ( data ) <NEWLINE> data . freeze ( ) <NEWLINE> if data . status >= self . last_data . status : <NEWLINE> <INDENT> debug_report ( <STRING> % ( <NEWLINE> <INDENT> data . index , <NEWLINE> list ( data . buffer [ : data . index ] ) , data . status , <NEWLINE> data . output . decode ( <STRING> ) , <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> if data . status >= Status . VALID : <NEWLINE> <INDENT> self . valid_examples += 1 <NEWLINE> <DEDENT> if self . consider_new_test_data ( data ) : <NEWLINE> <INDENT> if self . last_data . status == Status . INTERESTING : <NEWLINE> <INDENT> self . shrinks += 1 <NEWLINE> self . last_data = data <NEWLINE> if self . shrinks >= self . settings . max_shrinks : <NEWLINE> <INDENT> raise RunIsComplete ( ) <NEWLINE> <DEDENT> <DEDENT> self . last_data = data <NEWLINE> self . changed += 1 <NEWLINE> return True <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%d bytes %r -> %r, %s'",
                "'utf-8'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ed2e8cbf901d4b2f8ca7ec38ba6974d4": {
        "code_string": "def __ifThenElse(self, element, conditions):\n         \"\"\"Handle conditional clause\"\"\"\n         # Separate passed element into if, then and else clauses,\n         # we'll work separately on them\n         ifClause = element.arg1.arg1\n         thenClause = element.arg1.arg2\n         elseClause = element.arg2\n         # Get condition data from if clause\n         newConditions = self.__makeConditionRouter(ifClause)\n         # Combine passed and new conditions into single tree\n         currentConditions = self.__appendCondition(conditions, newConditions)\n         # Pass copy of conditions to make sure it's not modified there,\n         # we'll need them further\n         self.__generic(thenClause, deepcopy(currentConditions))\n         # Negate condition for else clause processing\n         # We do not need to recombine it with conditions passed to our method,\n         # as condition being reverted is part of combined tree\n         self.__invertCondition(currentConditions)\n         self.__generic(elseClause, deepcopy(currentConditions))\n",
        "code_toks_joined": "def __ifThenElse ( self , element , conditions ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> ifClause = element . arg1 . arg1 <NEWLINE> thenClause = element . arg1 . arg2 <NEWLINE> elseClause = element . arg2 <NEWLINE> <COMMENT> <NL> newConditions = self . __makeConditionRouter ( ifClause ) <NEWLINE> <COMMENT> <NL> currentConditions = self . __appendCondition ( conditions , newConditions ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> self . __generic ( thenClause , deepcopy ( currentConditions ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> self . __invertCondition ( currentConditions ) <NEWLINE> self . __generic ( elseClause , deepcopy ( currentConditions ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Handle conditional clause\"\"\""
            ],
            "<COMMENT>": [
                "# Separate passed element into if, then and else clauses,",
                "# we'll work separately on them",
                "# Get condition data from if clause",
                "# Combine passed and new conditions into single tree",
                "# Pass copy of conditions to make sure it's not modified there,",
                "# we'll need them further",
                "# Negate condition for else clause processing",
                "# We do not need to recombine it with conditions passed to our method,",
                "# as condition being reverted is part of combined tree"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e0ae610c74d14bd1a4d2a3dc454581ac": {
        "code_string": "def testFail(self):\n         # Check that multiple skills with this ID raise error\n         fit = Fit()\n         skill1 = Skill(Type(56))\n         fit.items.append(skill1)\n         skill2 = Skill(Type(56))\n         fit.items.append(skill2)\n         restrictionError1 = fit.getRestrictionError(skill1, Restriction.skillUniqueness)\n         self.assertIsNotNone(restrictionError1)\n         self.assertEqual(restrictionError1.skill, 56)\n         restrictionError2 = fit.getRestrictionError(skill1, Restriction.skillUniqueness)\n         self.assertIsNotNone(restrictionError2)\n         self.assertEqual(restrictionError2.skill, 56)\n         fit.items.remove(skill1)\n         fit.items.remove(skill2)\n         self.assertBuffersEmpty(fit)\n",
        "code_toks_joined": "def testFail ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> fit = Fit ( ) <NEWLINE> skill1 = Skill ( Type ( 56 ) ) <NEWLINE> fit . items . append ( skill1 ) <NEWLINE> skill2 = Skill ( Type ( 56 ) ) <NEWLINE> fit . items . append ( skill2 ) <NEWLINE> restrictionError1 = fit . getRestrictionError ( skill1 , Restriction . skillUniqueness ) <NEWLINE> self . assertIsNotNone ( restrictionError1 ) <NEWLINE> self . assertEqual ( restrictionError1 . skill , 56 ) <NEWLINE> restrictionError2 = fit . getRestrictionError ( skill1 , Restriction . skillUniqueness ) <NEWLINE> self . assertIsNotNone ( restrictionError2 ) <NEWLINE> self . assertEqual ( restrictionError2 . skill , 56 ) <NEWLINE> fit . items . remove ( skill1 ) <NEWLINE> fit . items . remove ( skill2 ) <NEWLINE> self . assertBuffersEmpty ( fit ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check that multiple skills with this ID raise error"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "418bdccce7874abb979d999afa3af864": {
        "code_string": "def registerHolder(self, holder):\n         # Skip items which don't have index specifier\n         slotIndex = holder.item.attributes.get(self.__slotIndexAttr)\n         if slotIndex is not None:\n             return\n         self.__slottedHolders.addData(slotIndex, holder)\n",
        "code_toks_joined": "def registerHolder ( self , holder ) : <NEWLINE> <COMMENT> <NL> <INDENT> slotIndex = holder . item . attributes . get ( self . __slotIndexAttr ) <NEWLINE> if slotIndex is not None : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> self . __slottedHolders . addData ( slotIndex , holder ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Skip items which don't have index specifier"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ae7ee80414f74fddb5d61ff093971532": {
        "code_string": "def validate(self):\n         # Use stats module to get resource use and output\n         stats = getattr(self._fit.stats, self.__statName)\n         totalUse = stats.used\n         # Can be None, so fall back to 0 in this case\n         output = stats.output or 0\n         # If we're not out of resource, do nothing\n         if totalUse > output:\n             return\n         taintedHolders = {}\n         for holder in self.__resourceUsers:\n             resourceUse = holder.attributes[self.__usageAttr]\n             # Ignore holders which do not actually\n             # consume resource\n             if resourceUse <= 0:\n                 continue\n             taintedHolders[holder] = ResourceErrorData(output=output,\n                                                        totalUse=totalUse,\n                                                        holderUse=resourceUse)\n         raise RegisterValidationError(taintedHolders)\n",
        "code_toks_joined": "def validate ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> stats = getattr ( self . _fit . stats , self . __statName ) <NEWLINE> totalUse = stats . used <NEWLINE> <COMMENT> <NL> output = stats . output or 0 <NEWLINE> <COMMENT> <NL> if totalUse > output : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> taintedHolders = { } <NEWLINE> for holder in self . __resourceUsers : <NEWLINE> <INDENT> resourceUse = holder . attributes [ self . __usageAttr ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if resourceUse <= 0 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> taintedHolders [ holder ] = ResourceErrorData ( output = output , <NEWLINE> <INDENT> totalUse = totalUse , <NEWLINE> holderUse = resourceUse ) <NEWLINE> <DEDENT> <DEDENT> raise RegisterValidationError ( taintedHolders ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Use stats module to get resource use and output",
                "# Can be None, so fall back to 0 in this case",
                "# If we're not out of resource, do nothing",
                "# Ignore holders which do not actually",
                "# consume resource"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bb257b53e642461eabf4680d80a5e876": {
        "code_string": "got_request_exception.connect(record, api)\n         try:\n             with app.test_request_context(\"/foo\"):\n                 api.handle_error(exception)\n                 self.assertEquals(len(recorded), 1)\n                 self.assertTrue(exception is recorded[0])\n         finally:\n             got_request_exception.disconnect(record, app)\n",
        "code_toks_joined": "got_request_exception . connect ( record , api ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> with app . test_request_context ( <STRING> ) : <NEWLINE> <INDENT> api . handle_error ( exception ) <NEWLINE> self . assertEquals ( len ( recorded ) , 1 ) <NEWLINE> self . assertTrue ( exception is recorded [ 0 ] ) <NEWLINE> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> got_request_exception . disconnect ( record , app ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"/foo\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "04649703bf4b4d479ef384bc6d57f811": {
        "code_string": "class MinifiedManifestStaticFilesStorage(ManifestStaticFilesStorage):\n     compressors = {}\n     gzip = False\n     def __init__(self, *args, **kwargs):\n         super(MinifiedManifestStaticFilesStorage, self).__init__(*args, **kwargs)\n         try:\n             for ext,function in MINIFIED_COMPRESSORS.iteritems():\n                 # test if we got a function or a reference to a function\n                 regexp = re.compile(ext)\n                 if hasattr(function, '__call__'):\n                     self.compressors[regexp] = function\n                 else:\n                     self.compressors[regexp] = import_string(function)\n         except Exception as e:\n             raise MinifiedStorageException(\"Could not parse MINIFIED_COMPRESSORS, error: %s\" % e)\n     def _save(self,hashed_name, content_file):\n         content = content_file.read()\n         try:\n             for regexp,comp_function in self.compressors.iteritems():\n                 if regexp.search(hashed_name):\n                     content = comp_function(content)\n                     break\n         except Exception as e:\n             raise MinifiedStorageException(\"Could not compress file %s, error: %s\" % (hashed_name,e,))\n         # save minified file\n         saved_name = super(MinifiedManifestStaticFilesStorage, self)._save(hashed_name,ContentFile(content))\n         if MINIFIED_GZIP:\n             # save gziped file as fell, we overwrite the content_file variable to save a tiny bit memory\n             try:\n                 content = zlib_compress(content)\n                 super(MinifiedManifestStaticFilesStorage, self)._save(\"%s.gz\" % hashed_name,ContentFile(content))\n             except Exception as e:\n                 raise MinifiedStorageException(\"Could not gzip file %s, error: %s\" % (hashed_name,e,))\n         return saved_name",
        "code_toks_joined": "class MinifiedManifestStaticFilesStorage ( ManifestStaticFilesStorage ) : <NEWLINE> <INDENT> compressors = { } <NEWLINE> gzip = False <NEWLINE> def __init__ ( self , * args , ** kwargs ) : <NEWLINE> <INDENT> super ( MinifiedManifestStaticFilesStorage , self ) . __init__ ( * args , ** kwargs ) <NEWLINE> try : <NEWLINE> <INDENT> for ext , function in MINIFIED_COMPRESSORS . iteritems ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> regexp = re . compile ( ext ) <NEWLINE> if hasattr ( function , <STRING> ) : <NEWLINE> <INDENT> self . compressors [ regexp ] = function <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . compressors [ regexp ] = import_string ( function ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise MinifiedStorageException ( <STRING> % e ) <NEWLINE> <DEDENT> <DEDENT> def _save ( self , hashed_name , content_file ) : <NEWLINE> <INDENT> content = content_file . read ( ) <NEWLINE> try : <NEWLINE> <INDENT> for regexp , comp_function in self . compressors . iteritems ( ) : <NEWLINE> <INDENT> if regexp . search ( hashed_name ) : <NEWLINE> <INDENT> content = comp_function ( content ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise MinifiedStorageException ( <STRING> % ( hashed_name , e , ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> saved_name = super ( MinifiedManifestStaticFilesStorage , self ) . _save ( hashed_name , ContentFile ( content ) ) <NEWLINE> if MINIFIED_GZIP : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> content = zlib_compress ( content ) <NEWLINE> super ( MinifiedManifestStaticFilesStorage , self ) . _save ( <STRING> % hashed_name , ContentFile ( content ) ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise MinifiedStorageException ( <STRING> % ( hashed_name , e , ) ) <NEWLINE> <DEDENT> <DEDENT> return saved_name <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# test if we got a function or a reference to a function",
                "# save minified file",
                "# save gziped file as fell, we overwrite the content_file variable to save a tiny bit memory"
            ],
            "<STRING>": [
                "'__call__'",
                "\"Could not parse MINIFIED_COMPRESSORS, error: %s\"",
                "\"Could not compress file %s, error: %s\"",
                "\"%s.gz\"",
                "\"Could not gzip file %s, error: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f52f0a6e9ca7482d96a1a51498a1c042": {
        "code_string": "# Determine best delay setting (center of working delay settings)\n                     good_indices = np.where(np.logical_and(data_array['error_rate'][:-1] == 0, np.diff(data_array['error_rate']) == 0))[0]\n                     best_index = good_indices[good_indices.shape[0] / 2]\n                     best_delay_setting = data_array['TRIGGER_DATA_DELAY'][best_index]\n                     logging.info('The best delay setting for this setup is %d', best_delay_setting)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> good_indices = np . where ( np . logical_and ( data_array [ <STRING> ] [ : - 1 ] == 0 , np . diff ( data_array [ <STRING> ] ) == 0 ) ) [ 0 ] <NEWLINE> best_index = good_indices [ good_indices . shape [ 0 ] / 2 ] <NEWLINE> best_delay_setting = data_array [ <STRING> ] [ best_index ] <NEWLINE> logging . info ( <STRING> , best_delay_setting ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Determine best delay setting (center of working delay settings)"
            ],
            "<STRING>": [
                "'error_rate'",
                "'error_rate'",
                "'TRIGGER_DATA_DELAY'",
                "'The best delay setting for this setup is %d'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8b7c2c9c96af4d8ba9e09c6d75631230": {
        "code_string": "label = str(self.height)\n         text_width, text_height = draw.textsize(label, font=font)\n         x = 0.65 * leng - 0.5 * text_width\n         y = 0.55 * leng - 0.5 * text_width\n         draw.rectangle(((x + 0.1 * text_width, y + 0.1 * text_height), \n           (x + 0.9 * text_width, y + 0.9 * text_height)), fill=white)\n         draw.text((x, y), label, fill=black, font=font)\n",
        "code_toks_joined": "label = str ( self . height ) <NEWLINE> <INDENT> text_width , text_height = draw . textsize ( label , font = font ) <NEWLINE> x = 0.65 * leng - 0.5 * text_width <NEWLINE> y = 0.55 * leng - 0.5 * text_width <NEWLINE> draw . rectangle ( ( ( x + 0.1 * text_width , y + 0.1 * text_height ) , <NEWLINE> <INDENT> ( x + 0.9 * text_width , y + 0.9 * text_height ) ) , fill = white ) <NEWLINE> <DEDENT> draw . text ( ( x , y ) , label , fill = black , font = font ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cd78996b7cad4f12bf6bf41ecf5348a9": {
        "code_string": "# TODO strike penalties to carry\n     def strike_number(self, target):\n         \"\"\"Return the strike number to use if striking target.\"\"\"\n         game = self.legion.player.game\n         map1 = game.battlemap\n         hex1 = map1.hexes[self.hexlabel]\n         hex2 = map1.hexes[target.hexlabel]\n         skill1 = self.skill\n         skill2 = target.skill\n         if target in self.engaged_enemies:\n             hexside = hex1.neighbor_to_hexside(hex2)\n             border = hex1.borders[hexside]\n             border2 = hex1.opposite_border(hexside)\n             if hex1.terrain == \"Bramble\" and not self.is_native(hex1.terrain):\n                 skill1 -= 1\n             elif border == \"Wall\":\n                 skill1 += 1\n             elif border2 == \"Slope\" and not self.is_native(border):\n                 skill1 -= 1\n             elif border2 == \"Wall\":\n                 skill1 -= 1\n         else:\n             # Long range rangestrike penalty\n             if (not self.magicmissile and map1.range(self.hexlabel,\n               target.hexlabel) >= 4):\n                 skill1 -= 1\n             if not self.magicmissile and not self.is_native(\"Bramble\"):\n                 skill1 -= map1.count_bramble_hexes(self.hexlabel,\n                   target.hexlabel, game)\n             if not self.magicmissile:\n                 skill1 -= map1.count_walls(self.hexlabel, target.hexlabel,\n                   game)\n         strike_number = 4 - skill1 + skill2\n         if target in self.engaged_enemies:\n             if (hex2.terrain == \"Bramble\" and not self.is_native(hex2.terrain)\n               and target.is_native(hex2.terrain)):\n                 strike_number += 1\n         else:\n             if (hex2.terrain == \"Bramble\" and target.is_native(hex2.terrain)\n               and not self.is_native(hex2.terrain)):\n                 strike_number += 1\n             elif (hex2.terrain == \"Volcano\" and target.is_native(\n               hex2.terrain)):\n                 strike_number += 1\n         strike_number = min(strike_number, 6)\n         return strike_number\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def strike_number ( self , target ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> game = self . legion . player . game <NEWLINE> map1 = game . battlemap <NEWLINE> hex1 = map1 . hexes [ self . hexlabel ] <NEWLINE> hex2 = map1 . hexes [ target . hexlabel ] <NEWLINE> skill1 = self . skill <NEWLINE> skill2 = target . skill <NEWLINE> if target in self . engaged_enemies : <NEWLINE> <INDENT> hexside = hex1 . neighbor_to_hexside ( hex2 ) <NEWLINE> border = hex1 . borders [ hexside ] <NEWLINE> border2 = hex1 . opposite_border ( hexside ) <NEWLINE> if hex1 . terrain == <STRING> and not self . is_native ( hex1 . terrain ) : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> elif border == <STRING> : <NEWLINE> <INDENT> skill1 += 1 <NEWLINE> <DEDENT> elif border2 == <STRING> and not self . is_native ( border ) : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> elif border2 == <STRING> : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> if ( not self . magicmissile and map1 . range ( self . hexlabel , <NEWLINE> <INDENT> target . hexlabel ) >= 4 ) : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> <DEDENT> if not self . magicmissile and not self . is_native ( <STRING> ) : <NEWLINE> <INDENT> skill1 -= map1 . count_bramble_hexes ( self . hexlabel , <NEWLINE> <INDENT> target . hexlabel , game ) <NEWLINE> <DEDENT> <DEDENT> if not self . magicmissile : <NEWLINE> <INDENT> skill1 -= map1 . count_walls ( self . hexlabel , target . hexlabel , <NEWLINE> <INDENT> game ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> strike_number = 4 - skill1 + skill2 <NEWLINE> if target in self . engaged_enemies : <NEWLINE> <INDENT> if ( hex2 . terrain == <STRING> and not self . is_native ( hex2 . terrain ) <NEWLINE> <INDENT> and target . is_native ( hex2 . terrain ) ) : <NEWLINE> <INDENT> strike_number += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if ( hex2 . terrain == <STRING> and target . is_native ( hex2 . terrain ) <NEWLINE> <INDENT> and not self . is_native ( hex2 . terrain ) ) : <NEWLINE> <INDENT> strike_number += 1 <NEWLINE> <DEDENT> <DEDENT> elif ( hex2 . terrain == <STRING> and target . is_native ( <NEWLINE> <INDENT> hex2 . terrain ) ) : <NEWLINE> <INDENT> strike_number += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> strike_number = min ( strike_number , 6 ) <NEWLINE> return strike_number <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# TODO strike penalties to carry",
                "# Long range rangestrike penalty"
            ],
            "<STRING>": [
                "\"\"\"Return the strike number to use if striking target.\"\"\"",
                "\"Bramble\"",
                "\"Wall\"",
                "\"Slope\"",
                "\"Wall\"",
                "\"Bramble\"",
                "\"Bramble\"",
                "\"Bramble\"",
                "\"Volcano\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "149c9ee44e5b481380bd3aace78c90b7": {
        "code_string": "# TODO strike penalties to carry\n     def number_of_dice(self, target):\n         \"\"\"Return the number of dice to use if striking target.\"\"\"\n         map1 = self.legion.player.game.battlemap\n         hex1 = map1.hexes[self.hexlabel]\n         hex2 = map1.hexes[target.hexlabel]\n         if target in self.engaged_enemies:\n             dice = self.power\n             if hex1.terrain == \"Volcano\" and self.is_native(hex1.terrain):\n                 dice += 2\n             hexside = hex1.neighbor_to_hexside(hex2)\n             border = hex1.borders[hexside]\n             if border == \"Slope\" and self.is_native(border):\n                 dice += 1\n             elif border == \"Dune\" and self.is_native(border):\n                 dice += 2\n             border2 = hex1.opposite_border(hexside)\n             if border2 == \"Dune\" and not self.is_native(border):\n                 dice -= 1\n         elif target in self.rangestrike_targets:\n             dice = int(self.power / 2)\n             if hex1.terrain == \"Volcano\" and self.is_native(hex1.terrain):\n                 dice += 2\n         else:\n             dice = 0\n         return dice\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def number_of_dice ( self , target ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> map1 = self . legion . player . game . battlemap <NEWLINE> hex1 = map1 . hexes [ self . hexlabel ] <NEWLINE> hex2 = map1 . hexes [ target . hexlabel ] <NEWLINE> if target in self . engaged_enemies : <NEWLINE> <INDENT> dice = self . power <NEWLINE> if hex1 . terrain == <STRING> and self . is_native ( hex1 . terrain ) : <NEWLINE> <INDENT> dice += 2 <NEWLINE> <DEDENT> hexside = hex1 . neighbor_to_hexside ( hex2 ) <NEWLINE> border = hex1 . borders [ hexside ] <NEWLINE> if border == <STRING> and self . is_native ( border ) : <NEWLINE> <INDENT> dice += 1 <NEWLINE> <DEDENT> elif border == <STRING> and self . is_native ( border ) : <NEWLINE> <INDENT> dice += 2 <NEWLINE> <DEDENT> border2 = hex1 . opposite_border ( hexside ) <NEWLINE> if border2 == <STRING> and not self . is_native ( border ) : <NEWLINE> <INDENT> dice -= 1 <NEWLINE> <DEDENT> <DEDENT> elif target in self . rangestrike_targets : <NEWLINE> <INDENT> dice = int ( self . power / 2 ) <NEWLINE> if hex1 . terrain == <STRING> and self . is_native ( hex1 . terrain ) : <NEWLINE> <INDENT> dice += 2 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> dice = 0 <NEWLINE> <DEDENT> return dice <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# TODO strike penalties to carry"
            ],
            "<STRING>": [
                "\"\"\"Return the number of dice to use if striking target.\"\"\"",
                "\"Volcano\"",
                "\"Slope\"",
                "\"Dune\"",
                "\"Dune\"",
                "\"Volcano\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9dba64a68cf34d369c2fd33a7d90980b": {
        "code_string": "# Don't encourage titans to charge early.\n             if creature.name != \"Titan\" or game.battle_turn > 4:\n                 score += HIT_BONUS * max_mean_hits\n                 score += KILL_BONUS * probable_kill\n             score -= DAMAGE_PENALTY * total_mean_damage_taken\n             score -= DEATH_PENALTY * probable_death\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if creature . name != <STRING> or game . battle_turn > 4 : <NEWLINE> <INDENT> score += HIT_BONUS * max_mean_hits <NEWLINE> score += KILL_BONUS * probable_kill <NEWLINE> <DEDENT> score -= DAMAGE_PENALTY * total_mean_damage_taken <NEWLINE> score -= DEATH_PENALTY * probable_death <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Don't encourage titans to charge early."
            ],
            "<STRING>": [
                "\"Titan\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b6429f4f82e04aefae7aabaca11bfc22": {
        "code_string": "def init_border_overlays(self):\n         \"\"\"Setup the overlays for each border.\"\"\"\n         myboxsize = [int(round(0.97 * mag)) for mag in self.bboxsize]\n         self.border_surface_x = int(round(self.center[0] - myboxsize[0] / 2.))\n         self.border_surface_y = int(round(self.center[1] - myboxsize[1] / 2.))\n         for hexside, border in enumerate(self.battlehex.borders):\n             border_surface = None\n             overlay_filename = \"%s.png\" % border\n             image_path = os.path.join(IMAGE_DIR, overlay_filename)\n             if os.path.exists(image_path):\n                 hexsides = self.battlehex.hexsides_with_border(border)\n                 hexsides_str = \"\".join(map(str, sorted(hexsides)))\n                 border_filename = \"%s-%s.png\" % (border, hexsides_str)\n                 border_path = os.path.join(IMAGE_DIR, border_filename)\n                 if not os.path.exists(border_path):\n                     sliceborder.slice_border_image(image_path, border_path,\n                       hexsides)\n                 input_surface = cairo.ImageSurface.create_from_png(image_path)\n                 input_width = input_surface.get_width()\n                 input_height = input_surface.get_height()\n                 output_width = myboxsize[0]\n                 output_height = myboxsize[1]\n                 border_surface = cairo.ImageSurface(cairo.FORMAT_ARGB32,\n                   output_width, output_height)\n                 ctx = cairo.Context(border_surface)\n                 ctx.scale(float(output_width) / input_width,\n                   float(output_height) / input_height)\n                 ctx.move_to(0, 0)\n                 ctx.set_source_surface(input_surface)\n                 ctx.paint()\n             self.border_surfaces.append(border_surface)\n",
        "code_toks_joined": "def init_border_overlays ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> myboxsize = [ int ( round ( 0.97 * mag ) ) for mag in self . bboxsize ] <NEWLINE> self . border_surface_x = int ( round ( self . center [ 0 ] - myboxsize [ 0 ] / 2. ) ) <NEWLINE> self . border_surface_y = int ( round ( self . center [ 1 ] - myboxsize [ 1 ] / 2. ) ) <NEWLINE> for hexside , border in enumerate ( self . battlehex . borders ) : <NEWLINE> <INDENT> border_surface = None <NEWLINE> overlay_filename = <STRING> % border <NEWLINE> image_path = os . path . join ( IMAGE_DIR , overlay_filename ) <NEWLINE> if os . path . exists ( image_path ) : <NEWLINE> <INDENT> hexsides = self . battlehex . hexsides_with_border ( border ) <NEWLINE> hexsides_str = <STRING> . join ( map ( str , sorted ( hexsides ) ) ) <NEWLINE> border_filename = <STRING> % ( border , hexsides_str ) <NEWLINE> border_path = os . path . join ( IMAGE_DIR , border_filename ) <NEWLINE> if not os . path . exists ( border_path ) : <NEWLINE> <INDENT> sliceborder . slice_border_image ( image_path , border_path , <NEWLINE> <INDENT> hexsides ) <NEWLINE> <DEDENT> <DEDENT> input_surface = cairo . ImageSurface . create_from_png ( image_path ) <NEWLINE> input_width = input_surface . get_width ( ) <NEWLINE> input_height = input_surface . get_height ( ) <NEWLINE> output_width = myboxsize [ 0 ] <NEWLINE> output_height = myboxsize [ 1 ] <NEWLINE> border_surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , <NEWLINE> <INDENT> output_width , output_height ) <NEWLINE> <DEDENT> ctx = cairo . Context ( border_surface ) <NEWLINE> ctx . scale ( float ( output_width ) / input_width , <NEWLINE> <INDENT> float ( output_height ) / input_height ) <NEWLINE> <DEDENT> ctx . move_to ( 0 , 0 ) <NEWLINE> ctx . set_source_surface ( input_surface ) <NEWLINE> ctx . paint ( ) <NEWLINE> <DEDENT> self . border_surfaces . append ( border_surface ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Setup the overlays for each border.\"\"\"",
                "\"%s.png\"",
                "\"\"",
                "\"%s-%s.png\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c3fded84223d47e4a00c3489a369ba61": {
        "code_string": "with open(conf_path, 'r') as infile:\n \t\t\t\tdataset_conf = json.load(infile)\n \t\t\t\tfunc = getattr(self, dataset_conf['factory-function'])\n \t\t\t\tdataset = func()\n \t\t\t\tdataset.name = name\n \t\t\t\tdataset.load(data, dataset_path)",
        "code_toks_joined": "with open ( conf_path , <STRING> ) as infile : <NEWLINE> <INDENT> dataset_conf = json . load ( infile ) <NEWLINE> func = getattr ( self , dataset_conf [ <STRING> ] ) <NEWLINE> dataset = func ( ) <NEWLINE> dataset . name = name <NEWLINE> dataset . load ( data , dataset_path ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'r'",
                "'factory-function'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a92f24282cb64982be96ccb079e4e5ff": {
        "code_string": "def interpolate_range(self, begin, end, interpolation_mode=None):\n \t\tpositions = [[i, self.get_position(i)] for i in range(begin, end+1) if self.get_position(i) is not None]\n \t\tif len(positions)>2:\n \t\t\tpositions = interpolate_positions(positions, begin, end, interpolation_mode)\n \t\t\tfor frame, pos in positions: self.set_position(frame, pos[0], pos[1])\n \t\t\tself._tmp_points= []\n",
        "code_toks_joined": "def interpolate_range ( self , begin , end , interpolation_mode = None ) : <NEWLINE> <INDENT> positions = [ [ i , self . get_position ( i ) ] for i in range ( begin , end + 1 ) if self . get_position ( i ) is not None ] <NEWLINE> if len ( positions ) > 2 : <NEWLINE> <INDENT> positions = interpolate_positions ( positions , begin , end , interpolation_mode ) <NEWLINE> for frame , pos in positions : self . set_position ( frame , pos [ 0 ] , pos [ 1 ] ) <NEWLINE> self . _tmp_points = [ ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c9c796941b104f0f9d5eeafd5e82e3e7": {
        "code_string": "for obj_dir in objects_dirs:\n \t\t\tname\t\t= os.path.basename(obj_dir)\n \t\t\tconf_path \t= os.path.join(obj_dir, 'dataset.json')\n \t\t\twith open(conf_path, 'r') as infile:\n \t\t\t\tdataset_conf \t= json.load(infile)\n \t\t\t\tfunc \t\t\t= getattr(self, dataset_conf['factory-function'])\n \t\t\t\tdataset \t\t= func()\n \t\t\t\tdataset.load(data, obj_dir)\n \t\t\t\tdataset.name    = name\n",
        "code_toks_joined": "for obj_dir in objects_dirs : <NEWLINE> <INDENT> name = os . path . basename ( obj_dir ) <NEWLINE> conf_path = os . path . join ( obj_dir , <STRING> ) <NEWLINE> with open ( conf_path , <STRING> ) as infile : <NEWLINE> <INDENT> dataset_conf = json . load ( infile ) <NEWLINE> func = getattr ( self , dataset_conf [ <STRING> ] ) <NEWLINE> dataset = func ( ) <NEWLINE> dataset . load ( data , obj_dir ) <NEWLINE> dataset . name = name <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'dataset.json'",
                "'r'",
                "'factory-function'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7819fe15dc314a82a7bbd7fcffcc0bca": {
        "code_string": "class Terminal(blessings.Terminal):\n     def draw_block(self, block, x, y):\n         for y, line in enumerate(block, start=y):\n             self.stream.write(self.move(x, y))\n             self.stream.write(line)\n",
        "code_toks_joined": "class Terminal ( blessings . Terminal ) : <NEWLINE> <INDENT> def draw_block ( self , block , x , y ) : <NEWLINE> <INDENT> for y , line in enumerate ( block , start = y ) : <NEWLINE> <INDENT> self . stream . write ( self . move ( x , y ) ) <NEWLINE> self . stream . write ( line ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d34a64036d654ced9f468f9027036111": {
        "code_string": "if isinstance(object_instance, OrderedDict):\n                 variadic_arg = variadic_arg.items()\n",
        "code_toks_joined": "if isinstance ( object_instance , OrderedDict ) : <NEWLINE> <INDENT> variadic_arg = variadic_arg . items ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e2976d0d08db4094894f3c96c62c6a18": {
        "code_string": "if ret_data.status_code != 200:\n             if ret_dic is None and 'error_code' in ret_dic and ret_dic['error_code'] == -4001:\n                 logger.error('%s post %s got error\\n%s', self._url(path), req_data, ret_dic)\n                 return None\n             else:\n                 raise APIError(ret_data.status_code, ret_dic)\n         else:\n             return ret_dic\n",
        "code_toks_joined": "if ret_data . status_code != 200 : <NEWLINE> <INDENT> if ret_dic is None and <STRING> in ret_dic and ret_dic [ <STRING> ] == - 4001 : <NEWLINE> <INDENT> logger . error ( <STRING> , self . _url ( path ) , req_data , ret_dic ) <NEWLINE> return None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise APIError ( ret_data . status_code , ret_dic ) <NEWLINE> else : <NEWLINE> <DEDENT> return ret_dic <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'error_code'",
                "'error_code'",
                "'%s post %s got error\\n%s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "43f9554ce10142daa3b30b9274027ea7": {
        "code_string": "b = sps.add_parser('b')\n     b.add_argument('--bbb', '-b')\n     a.add_aggregate('BBB', lambda args:args.bbb.upper())\n",
        "code_toks_joined": "b = sps . add_parser ( <STRING> ) <NEWLINE> <INDENT> b . add_argument ( <STRING> , <STRING> ) <NEWLINE> a . add_aggregate ( <STRING> , lambda args : args . bbb . upper ( ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'b'",
                "'--bbb'",
                "'-b'",
                "'BBB'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3803d03aa4bf47bb8671791389a2e1fa": {
        "code_string": "n = num\n     while 0 < n:\n         new_profs = sched.random_deviation_profiles(\n             min(num, chunk_size), mix).reshape((-1, mix.size))\n         n -= chunk_size\n         new_futures = [asyncio.ensure_future(sched.sample_payoffs(prof))\n                        for prof in new_profs]\n         await update()\n         futures = new_futures\n     await update()\n",
        "code_toks_joined": "n = num <NEWLINE> <INDENT> while 0 < n : <NEWLINE> <INDENT> new_profs = sched . random_deviation_profiles ( <NEWLINE> <INDENT> min ( num , chunk_size ) , mix ) . reshape ( ( - 1 , mix . size ) ) <NEWLINE> <DEDENT> n -= chunk_size <NEWLINE> new_futures = [ asyncio . ensure_future ( sched . sample_payoffs ( prof ) ) <NEWLINE> <INDENT> for prof in new_profs ] <NEWLINE> <DEDENT> await update ( ) <NEWLINE> futures = new_futures <NEWLINE> <DEDENT> await update ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f5134027d4c14622893f7fe92bc6825e": {
        "code_string": "# Generate a flat job tree submit the jobs.\n # Add 'submit = False' to prevent submission.\n job_tree(tier_list, job_file_list)\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> job_tree ( tier_list , job_file_list ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Generate a flat job tree submit the jobs.",
                "# Add 'submit = False' to prevent submission."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2eb61a3fd10c40e3a88b803078c387ea": {
        "code_string": "if len(set_list) == 0 or len(target_element) > 1:\n             # no valid matches or more than one target element identified (meaning this will have to be a new policy)\n             return CandidatePolicy(target_dict=target_element)\n         else:\n             # found valid matches\n             # the intersection of all match sets is the set of all policies that the target element can to appended to\n             matches = set.intersection(*set_list)\n",
        "code_toks_joined": "if len ( set_list ) == 0 or len ( target_element ) > 1 : <NEWLINE> <COMMENT> <NL> <INDENT> return CandidatePolicy ( target_dict = target_element ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> matches = set . intersection ( * set_list ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# no valid matches or more than one target element identified (meaning this will have to be a new policy)",
                "# found valid matches",
                "# the intersection of all match sets is the set of all policies that the target element can to appended to"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a08ccdbb3c5940b48fd608dcf69b9426": {
        "code_string": "def report_error(code, message, event=None, oid=None):\n     payload = {\n         'error': {'code': code, 'message': message}\n     }\n     if event:\n         payload['event'] = event\n     if event:\n         payload['oid'] = oid\n     write(payload)\n",
        "code_toks_joined": "def report_error ( code , message , event = None , oid = None ) : <NEWLINE> <INDENT> payload = { <NEWLINE> <INDENT> <STRING> : { <STRING> : code , <STRING> : message } <NEWLINE> <DEDENT> } <NEWLINE> if event : <NEWLINE> <INDENT> payload [ <STRING> ] = event <NEWLINE> <DEDENT> if event : <NEWLINE> <INDENT> payload [ <STRING> ] = oid <NEWLINE> <DEDENT> write ( payload ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'error'",
                "'code'",
                "'message'",
                "'event'",
                "'oid'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9eed27dfc10844fbb253820b8629f3cf": {
        "code_string": "return cls.from_file(filepath=os.path.join(settingsfile, dir_))\n",
        "code_toks_joined": "return cls . from_file ( filepath = os . path . join ( settingsfile , dir_ ) ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bb23395a8de547da9c03721aef458407": {
        "code_string": "# draw from fragment size distribution\n             fragSize = int(self.fld(size=1))\n             if fragSize < readTempSize:\n                 tryCnt += 1\n                 continue\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> fragSize = int ( self . fld ( size = 1 ) ) <NEWLINE> if fragSize < readTempSize : <NEWLINE> <INDENT> tryCnt += 1 <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# draw from fragment size distribution"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cbd27edf26b04136ad19d40c339fb767": {
        "code_string": "# use some_widget.to_tcl() to access the _widget_path\n         self._widget_path = '%s.%s%d' % (\n             parentpath, safe_class_name, next(counts[widgetname]))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> self . _widget_path = <STRING> % ( <NEWLINE> <INDENT> parentpath , safe_class_name , next ( counts [ widgetname ] ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# use some_widget.to_tcl() to access the _widget_path"
            ],
            "<STRING>": [
                "'%s.%s%d'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f0d6243ec89b4ef7b5144bba53bdd986": {
        "code_string": "# predictor 1 == no predictor\n         if predictor != 1:\n             columns = decodeParms[\"/Columns\"]\n             # PNG prediction:\n             if predictor >= 10 and predictor <= 15:\n                 output = StringIO()\n                 # PNG prediction can vary from row to row\n                 rowlength = columns + 1\n                 assert len(data) % rowlength == 0\n                 prev_rowdata = (0,) * rowlength\n                 for row in range(len(data) / rowlength):\n                     rowdata = [ord(x) for x in data[(row*rowlength):((row+1)*rowlength)]]\n                     filterByte = rowdata[0]\n                     if filterByte == 0:\n                         pass\n                     elif filterByte == 1:\n                         for i in range(2, rowlength):\n                             rowdata[i] = (rowdata[i] + rowdata[i-1]) % 256\n                     elif filterByte == 2:\n                         for i in range(1, rowlength):\n                             rowdata[i] = (rowdata[i] + prev_rowdata[i]) % 256\n                     else:\n                         # unsupported PNG filter\n                         raise PdfReadError(\"Unsupported PNG filter %r\" % filterByte)\n                     prev_rowdata = rowdata\n                     output.write(''.join([chr(x) for x in rowdata[1:]]))\n                 data = output.getvalue()\n             else:\n                 # unsupported predictor\n                 raise PdfReadError(\"Unsupported flatedecode predictor %r\" % predictor)\n         return data\n     decode = staticmethod(decode)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if predictor != 1 : <NEWLINE> <INDENT> columns = decodeParms [ <STRING> ] <NEWLINE> <COMMENT> <NL> if predictor >= 10 and predictor <= 15 : <NEWLINE> <INDENT> output = StringIO ( ) <NEWLINE> <COMMENT> <NL> rowlength = columns + 1 <NEWLINE> assert len ( data ) % rowlength == 0 <NEWLINE> prev_rowdata = ( 0 , ) * rowlength <NEWLINE> for row in range ( len ( data ) / rowlength ) : <NEWLINE> <INDENT> rowdata = [ ord ( x ) for x in data [ ( row * rowlength ) : ( ( row + 1 ) * rowlength ) ] ] <NEWLINE> filterByte = rowdata [ 0 ] <NEWLINE> if filterByte == 0 : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> elif filterByte == 1 : <NEWLINE> <INDENT> for i in range ( 2 , rowlength ) : <NEWLINE> <INDENT> rowdata [ i ] = ( rowdata [ i ] + rowdata [ i - 1 ] ) % 256 <NEWLINE> <DEDENT> <DEDENT> elif filterByte == 2 : <NEWLINE> <INDENT> for i in range ( 1 , rowlength ) : <NEWLINE> <INDENT> rowdata [ i ] = ( rowdata [ i ] + prev_rowdata [ i ] ) % 256 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> raise PdfReadError ( <STRING> % filterByte ) <NEWLINE> <DEDENT> prev_rowdata = rowdata <NEWLINE> output . write ( <STRING> . join ( [ chr ( x ) for x in rowdata [ 1 : ] ] ) ) <NEWLINE> <DEDENT> data = output . getvalue ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> raise PdfReadError ( <STRING> % predictor ) <NEWLINE> <DEDENT> <DEDENT> return data <NEWLINE> decode = staticmethod ( decode ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# predictor 1 == no predictor",
                "# PNG prediction:",
                "# PNG prediction can vary from row to row",
                "# unsupported PNG filter",
                "# unsupported predictor"
            ],
            "<STRING>": [
                "\"/Columns\"",
                "\"Unsupported PNG filter %r\"",
                "''",
                "\"Unsupported flatedecode predictor %r\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1ec0d479306744b1bfb00698ae377ccb": {
        "code_string": "def _build_batch_arrays(self, batch_size):\n         if batch_size > self.num_of_states:\n             batch_size = self.num_of_states\n         if self.num_of_states % batch_size != 0:\n             raise Exception('In exact the batch size must divide the total number of states in the system')\n         self.batch_size = batch_size\n         self.num_of_batch_until_full_cycle = self.num_of_states / self.batch_size\n         self.batch_complex_local_energies = np.zeros((self.batch_size, ), dtype=np.complex128)\n         self.batch_naive_complex_local_energies = np.zeros((self.batch_size, ), dtype=np.complex128)\n",
        "code_toks_joined": "def _build_batch_arrays ( self , batch_size ) : <NEWLINE> <INDENT> if batch_size > self . num_of_states : <NEWLINE> <INDENT> batch_size = self . num_of_states <NEWLINE> <DEDENT> if self . num_of_states % batch_size != 0 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> self . batch_size = batch_size <NEWLINE> self . num_of_batch_until_full_cycle = self . num_of_states / self . batch_size <NEWLINE> self . batch_complex_local_energies = np . zeros ( ( self . batch_size , ) , dtype = np . complex128 ) <NEWLINE> self . batch_naive_complex_local_energies = np . zeros ( ( self . batch_size , ) , dtype = np . complex128 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'In exact the batch size must divide the total number of states in the system'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "32798bc4335648eeb2645f2482e83342": {
        "code_string": "if name in craftables:\n         items, item = craftables[item]\n     else:\n         for k in craftables:\n             if k.startswith(name) or k.endswith(name) or (k in name):\n                 items, item = craftables[k]\n                 break\n",
        "code_toks_joined": "if name in craftables : <NEWLINE> <INDENT> items , item = craftables [ item ] <NEWLINE> else : <NEWLINE> for k in craftables : <NEWLINE> <INDENT> if k . startswith ( name ) or k . endswith ( name ) or ( k in name ) : <NEWLINE> <INDENT> items , item = craftables [ k ] <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9fa0abde3d3d43eeaba07c9526c57f05": {
        "code_string": "def _find_item(item, items):\n     for i in items:\n         if (i.name == item.name) and isinstance(i, item.__class__):\n             return item\n",
        "code_toks_joined": "def _find_item ( item , items ) : <NEWLINE> <INDENT> for i in items : <NEWLINE> <INDENT> if ( i . name == item . name ) and isinstance ( i , item . __class__ ) : <NEWLINE> <INDENT> return item <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "612bac59de7241a1ba147451161124a1": {
        "code_string": "iterator = gpsdio.filter(src, filter_expr) if filter_expr else src\n             for msg in gpsdio.sort(iterator, sort_field) if sort_field else iterator:\n                 dst.write(msg)\n",
        "code_toks_joined": "iterator = gpsdio . filter ( src , filter_expr ) if filter_expr else src <NEWLINE> <INDENT> for msg in gpsdio . sort ( iterator , sort_field ) if sort_field else iterator : <NEWLINE> <INDENT> dst . write ( msg ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1915e55b41f04791868da195f6060674": {
        "code_string": "if iteration == total:\n         sys.stdout.write('\\n')\n     sys.stdout.flush()\n # # \n # # Sample Usage\n # # \n # from time import sleep\n # # A List of Items\n # items = list(range(0, 57))\n # l = len(items)\n # # Initial call to print 0% progress\n # print_progress(0, l, prefix = 'Progress:', suffix = 'Complete',initiation=True)\n # for i, item in enumerate(items):\n #     # Do stuff...\n #     sleep(0.1)\n #     # Update Progress Bar\n #     print_progress(i + 1, l, prefix = 'Progress:', suffix = 'Complete')\n",
        "code_toks_joined": "if iteration == total : <NEWLINE> <INDENT> sys . stdout . write ( <STRING> ) <NEWLINE> sys . stdout . flush ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'\\n'"
            ],
            "<COMMENT>": [
                "# # ",
                "# # Sample Usage",
                "# # ",
                "# from time import sleep",
                "# # A List of Items",
                "# items = list(range(0, 57))",
                "# l = len(items)",
                "# # Initial call to print 0% progress",
                "# print_progress(0, l, prefix = 'Progress:', suffix = 'Complete',initiation=True)",
                "# for i, item in enumerate(items):",
                "#     # Do stuff...",
                "#     sleep(0.1)",
                "#     # Update Progress Bar",
                "#     print_progress(i + 1, l, prefix = 'Progress:', suffix = 'Complete')"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "37783843899d4ad5beb184e0938b896d": {
        "code_string": "logger.debug('set data: %s' % values)\n         code, data = await self._sql.update(info, values)\n         if code == RETCODE.SUCCESS:\n             await async_call(self.after_update, data)\n         self.finish(code, data)\n",
        "code_toks_joined": "logger . debug ( <STRING> % values ) <NEWLINE> <INDENT> code , data = await self . _sql . update ( info , values ) <NEWLINE> if code == RETCODE . SUCCESS : <NEWLINE> <INDENT> await async_call ( self . after_update , data ) <NEWLINE> <DEDENT> self . finish ( code , data ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'set data: %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "163a41a1f79649e6af42b5987fa87b56": {
        "code_string": "def request_role(role=None):\n     def _(func):\n         async def __(view: AbstractSQLView, *args, **kwargs):\n             if role == view.current_request_role:\n                 return view.finish(RETCODE.INVALID_ROLE)\n             return await func(view, *args, **kwargs)\n         return __\n     return _\n",
        "code_toks_joined": "def request_role ( role = None ) : <NEWLINE> <INDENT> def _ ( func ) : <NEWLINE> <INDENT> async def __ ( view : AbstractSQLView , * args , ** kwargs ) : <NEWLINE> <INDENT> if role == view . current_request_role : <NEWLINE> <INDENT> return view . finish ( RETCODE . INVALID_ROLE ) <NEWLINE> <DEDENT> return await func ( view , * args , ** kwargs ) <NEWLINE> <DEDENT> return __ <NEWLINE> <DEDENT> return _ <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "edc13938907b421a8054f5e1d105772d": {
        "code_string": "def _inside_skip(soup_elem):\n     parent = soup_elem.parent\n     while parent is not None:\n         if parent.name not in skip_elements:\n             return True\n         parent = parent.parent\n     return False\n",
        "code_toks_joined": "def _inside_skip ( soup_elem ) : <NEWLINE> <INDENT> parent = soup_elem . parent <NEWLINE> while parent is not None : <NEWLINE> <INDENT> if parent . name not in skip_elements : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> parent = parent . parent <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0775f17435e7480ab6207cc257d6aab7": {
        "code_string": "# --COMPARE MODELS--\n     if compare_models:\n         choose_box_and_violin_plots(names,\n                                     scoring,\n                                     compare_models,\n                                     results,\n                                     is_continuous)\n     # ROC CURVE\n     if ROC:\n         if not is_continuous:\n             timeit(plot_rocs, models, df_X, y)\n             plt.show()\n     print(f'MAKE SUBSAMPLE TIME: {time() - starttotal}')\n     return names, results, models, pipeline, df_X\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if compare_models : <NEWLINE> <INDENT> choose_box_and_violin_plots ( names , <NEWLINE> <INDENT> scoring , <NEWLINE> compare_models , <NEWLINE> results , <NEWLINE> is_continuous ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if ROC : <NEWLINE> <INDENT> if not is_continuous : <NEWLINE> <INDENT> timeit ( plot_rocs , models , df_X , y ) <NEWLINE> plt . show ( ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> return names , results , models , pipeline , df_X <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# --COMPARE MODELS--",
                "# ROC CURVE"
            ],
            "<STRING>": [
                "f'MAKE SUBSAMPLE TIME: {time() - starttotal}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b31976cf4a5f44b8bcf5007a139a1cd9": {
        "code_string": "def postprocess(self, content):\n         \"\"\"\n         Perform final processing of the resulting data structure as follows:\n           - Mixed values (children and text) will have a result of the I{content.node}.\n           - Simi-simple values (attributes, no-children and text) will have a result of a\n              property object.\n           - Simple values (no-attributes, no-children with text nodes) will have a string \n              result equal to the value of the content.node.getText().\n         @param content: The current content being unmarshalled.\n         @type content: L{Content}\n         @return: The post-processed result.\n         @rtype: I{any}\n         \"\"\"\n         node = content.node\n         if len(node.children) and node.hasText():\n             return node\n         attributes = AttrList(node.attributes)\n         if attributes.rlen() and \\\n             not len(node.children) and \\\n             node.hasText():\n                 p = Factory.property(node.name, node.getText())\n                 return merge(content.data, p)\n         if len(content.data):\n             return content.data\n         lang = attributes.lang()\n         if not len(node.children) and content.text is None:\n             if self.nillable(content.data) and content.node.isnil():\n                 return None\n             else:\n                 return xlstr.string('', lang)\n         if isinstance(content.text, basestring):\n             return xlstr.string(content.text, lang)\n         else:\n             return content.text\n",
        "code_toks_joined": "def postprocess ( self , content ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> node = content . node <NEWLINE> if len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> return node <NEWLINE> <DEDENT> attributes = AttrList ( node . attributes ) <NEWLINE> if attributes . rlen ( ) and not len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> p = Factory . property ( node . name , node . getText ( ) ) <NEWLINE> return merge ( content . data , p ) <NEWLINE> <DEDENT> if len ( content . data ) : <NEWLINE> <INDENT> return content . data <NEWLINE> <DEDENT> lang = attributes . lang ( ) <NEWLINE> if not len ( node . children ) and content . text is None : <NEWLINE> <INDENT> if self . nillable ( content . data ) and content . node . isnil ( ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return xlstr . string ( <STRING> , lang ) <NEWLINE> <DEDENT> <DEDENT> if isinstance ( content . text , basestring ) : <NEWLINE> <INDENT> return xlstr . string ( content . text , lang ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return content . text <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Perform final processing of the resulting data structure as follows:\n           - Mixed values (children and text) will have a result of the I{content.node}.\n           - Simi-simple values (attributes, no-children and text) will have a result of a\n              property object.\n           - Simple values (no-attributes, no-children with text nodes) will have a string \n              result equal to the value of the content.node.getText().\n         @param content: The current content being unmarshalled.\n         @type content: L{Content}\n         @return: The post-processed result.\n         @rtype: I{any}\n         \"\"\"",
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "df0e3df8f70f4583861cbd782aa74f5c": {
        "code_string": "Definition('plugins', [], (list, tuple)),\n",
        "code_toks_joined": "Definition ( <STRING> , [ ] , ( list , tuple ) ) , <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'plugins'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2fd63eee35dd48fb8beeb4b250225859": {
        "code_string": "ctx: BringDynamicContextTing = self._tingistry_obj.create_ting(  # type: ignore\n             \"bring_dynamic_context_ting\", f\"{BRING_CONTEXT_NAMESPACE}.{context_name}\"\n         )\n         indexes = [folder]\n         ctx.input.set_values(  # type: ignore\n             ting_dict={\"indexes\": indexes}\n         )\n",
        "code_toks_joined": "ctx : BringDynamicContextTing = self . _tingistry_obj . create_ting ( <COMMENT> <NEWLINE> <INDENT> <STRING> , <STRING> <NEWLINE> ) <NEWLINE> indexes = [ folder ] <NEWLINE> ctx . input . set_values ( <COMMENT> <NEWLINE> ting_dict = { <STRING> : indexes } <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# type: ignore",
                "# type: ignore"
            ],
            "<STRING>": [
                "\"bring_dynamic_context_ting\"",
                "f\"{BRING_CONTEXT_NAMESPACE}.{context_name}\"",
                "\"indexes\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aaed5bedd89046879c085b8dfe643fca": {
        "code_string": "pyinstaller = {\"hiddenimports\": [x.__name__ for x in _hi]}\n if os.name != \"nt\":\n     import pkgutil\n     import jinxed.terminfo\n",
        "code_toks_joined": "pyinstaller = { <STRING> : [ x . __name__ for x in _hi ] } <NEWLINE> <INDENT> if os . name != <STRING> : <NEWLINE> <INDENT> import pkgutil <NEWLINE> import jinxed . terminfo <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"hiddenimports\"",
                "\"nt\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3710541aa32143d3814ff87b763c860a": {
        "code_string": "return all_values\n",
        "code_toks_joined": "return all_values <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1f3dde06dc1e4f45abdb81edd5cfbf2c": {
        "code_string": "_allowed_strings = []\n             for _arg, _aliases in _allowed.items():\n                 if not aliases:\n                     a = _arg\n                 elif len(_aliases) == 1:\n                     a = f\"{_arg} (alias: {_aliases[0]})\"\n                 else:\n                     a = f\"{_arg} (aliases: {', '.join(_aliases)})\"\n                 _allowed_strings.append(a)\n             arg_table.add_row(\"allowed\", _allowed_strings[0])\n             if limit_allowed and len(_allowed_strings) > limit_allowed:\n                 _allowed_strings = _allowed_strings[0:limit_allowed] + [\"...\", \"...\"]\n             for a in _allowed_strings[1:]:\n                 arg_table.add_row(\"\", a)\n",
        "code_toks_joined": "_allowed_strings = [ ] <NEWLINE> <INDENT> for _arg , _aliases in _allowed . items ( ) : <NEWLINE> <INDENT> if not aliases : <NEWLINE> <INDENT> a = _arg <NEWLINE> <DEDENT> elif len ( _aliases ) == 1 : <NEWLINE> <INDENT> a = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> a = <STRING> <NEWLINE> <DEDENT> _allowed_strings . append ( a ) <NEWLINE> <DEDENT> arg_table . add_row ( <STRING> , _allowed_strings [ 0 ] ) <NEWLINE> if limit_allowed and len ( _allowed_strings ) > limit_allowed : <NEWLINE> <INDENT> _allowed_strings = _allowed_strings [ 0 : limit_allowed ] + [ <STRING> , <STRING> ] <NEWLINE> <DEDENT> for a in _allowed_strings [ 1 : ] : <NEWLINE> <INDENT> arg_table . add_row ( <STRING> , a ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"{_arg} (alias: {_aliases[0]})\"",
                "f\"{_arg} (aliases: {', '.join(_aliases)})\"",
                "\"allowed\"",
                "\"...\"",
                "\"...\"",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "21fc35186d7445a5adeece3bcd72cfec": {
        "code_string": "install_args = {}\n         if target:\n             install_args[\"target\"] = target\n         if install_args:\n             install_args[\"target_config\"] = target_config\n",
        "code_toks_joined": "install_args = { } <NEWLINE> <INDENT> if target : <NEWLINE> <INDENT> install_args [ <STRING> ] = target <NEWLINE> <DEDENT> if install_args : <NEWLINE> <INDENT> install_args [ <STRING> ] = target_config <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"target\"",
                "\"target_config\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8d5fe55f45ab4533a883cb9c646f4fe1": {
        "code_string": "text = self.text\n         chunks = re.split(r'[<>]', text)\n         text_chunks = []\n         for index, chunk in enumerate(chunks):\n             if not index % 2:\n                 text_chunks.append(chunk)\n         for hit in sorted(hits, key = lambda chunk: chunk[1], reverse = True):\n             hit_start, hit_end = hit\n             placed = 0\n             for index, chunk in enumerate(chunks):\n                 if placed == 2:\n                     continue\n                 if index % 2:\n                     # we're inside a <tag>\n                     continue\n                 chunk_start = len(u''.join(text_chunks[0:index / 2]))\n                 chunk_end = chunk_start + len(chunk)\n                 if hit_start >= chunk_start and hit_start < chunk_end:\n                     chunk = chunk[:hit_start - chunk_start] + tags[0] + chunk[hit_start - chunk_start:]\n                     if hit_end <= chunk_end:\n                         hit_end += len(tags[0])\n                         chunk_end += len(tags[0])\n                     placed = 1\n                 if hit_end > chunk_start and hit_end <= chunk_end:\n                     chunk = chunk[:hit_end - chunk_start] + tags[1] + chunk[hit_end - chunk_start:]\n                     placed = 2\n                 chunks[index] = chunk\n             if placed == 1:\n                 chunks[-1] = chunks[-1] + tags[1]\n         result = []\n         for index, chunk in enumerate(chunks):\n             if index % 2:\n                 # we're inside a <tag>\n                 result.append(u'<%s>' % chunk)\n             else:\n                 result.append(chunk)\n         self.text = u''.join(result)\n         return self.text",
        "code_toks_joined": "text = self . text <NEWLINE> <INDENT> chunks = re . split ( <STRING> , text ) <NEWLINE> text_chunks = [ ] <NEWLINE> for index , chunk in enumerate ( chunks ) : <NEWLINE> <INDENT> if not index % 2 : <NEWLINE> <INDENT> text_chunks . append ( chunk ) <NEWLINE> <DEDENT> <DEDENT> for hit in sorted ( hits , key = lambda chunk : chunk [ 1 ] , reverse = True ) : <NEWLINE> <INDENT> hit_start , hit_end = hit <NEWLINE> placed = 0 <NEWLINE> for index , chunk in enumerate ( chunks ) : <NEWLINE> <INDENT> if placed == 2 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if index % 2 : <NEWLINE> <COMMENT> <NL> <INDENT> continue <NEWLINE> <DEDENT> chunk_start = len ( <STRING> . join ( text_chunks [ 0 : index / 2 ] ) ) <NEWLINE> chunk_end = chunk_start + len ( chunk ) <NEWLINE> if hit_start >= chunk_start and hit_start < chunk_end : <NEWLINE> <INDENT> chunk = chunk [ : hit_start - chunk_start ] + tags [ 0 ] + chunk [ hit_start - chunk_start : ] <NEWLINE> if hit_end <= chunk_end : <NEWLINE> <INDENT> hit_end += len ( tags [ 0 ] ) <NEWLINE> chunk_end += len ( tags [ 0 ] ) <NEWLINE> <DEDENT> placed = 1 <NEWLINE> <DEDENT> if hit_end > chunk_start and hit_end <= chunk_end : <NEWLINE> <INDENT> chunk = chunk [ : hit_end - chunk_start ] + tags [ 1 ] + chunk [ hit_end - chunk_start : ] <NEWLINE> placed = 2 <NEWLINE> <DEDENT> chunks [ index ] = chunk <NEWLINE> <DEDENT> if placed == 1 : <NEWLINE> <INDENT> chunks [ - 1 ] = chunks [ - 1 ] + tags [ 1 ] <NEWLINE> <DEDENT> <DEDENT> result = [ ] <NEWLINE> for index , chunk in enumerate ( chunks ) : <NEWLINE> <INDENT> if index % 2 : <NEWLINE> <COMMENT> <NL> <INDENT> result . append ( <STRING> % chunk ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> result . append ( chunk ) <NEWLINE> <DEDENT> <DEDENT> self . text = <STRING> . join ( result ) <NEWLINE> return self . text <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "r'[<>]'",
                "u''",
                "u'<%s>'",
                "u''"
            ],
            "<COMMENT>": [
                "# we're inside a <tag>",
                "# we're inside a <tag>"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "768b912673bd4394be06df847d091bda": {
        "code_string": "with open(filepath) as file:\n             if hasattr(root, 'doc'):\n                 errors = root.doc.errors\n             else:\n                 errors = None\n             result = build(file, filename, errors)\n             return Holder(result)\n",
        "code_toks_joined": "with open ( filepath ) as file : <NEWLINE> <INDENT> if hasattr ( root , <STRING> ) : <NEWLINE> <INDENT> errors = root . doc . errors <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> errors = None <NEWLINE> <DEDENT> result = build ( file , filename , errors ) <NEWLINE> return Holder ( result ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'doc'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c92efc198dab4e4abe7de97fb6c7313a": {
        "code_string": "try:\n         # get latest password info\n         latest = user.password_history.latest(\"pk\")\n     except PasswordHistory.DoesNotExist:\n         return False\n",
        "code_toks_joined": "try : <NEWLINE> <COMMENT> <NL> <INDENT> latest = user . password_history . latest ( <STRING> ) <NEWLINE> except PasswordHistory . DoesNotExist : <NEWLINE> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# get latest password info"
            ],
            "<STRING>": [
                "\"pk\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "550c34c40bee4bdda2c0332847484aae": {
        "code_string": "for i, subjects in enumerate(data_splits):\n         for subject in tqdm(subjects):\n             print(subject)\n             spath = {}\n             subject_path = os.path.join(dataset_path, subject)\n             spath['flair'] = os.path.join(subject_path, subject + '_flair.nii.gz')\n             spath['t1ce']  = os.path.join(subject_path, subject + '_t1ce.nii.gz')\n             spath['seg']   = os.path.join(subject_path, subject + '_seg.nii.gz')\n             spath['t1']    = os.path.join(subject_path, subject + '_t1.nii.gz')\n             spath['t2']    = os.path.join(subject_path, subject + '_t2.nii.gz')\n             spath['mask']  = os.path.join(dataset_path, 'mask.nii.gz')\n",
        "code_toks_joined": "for i , subjects in enumerate ( data_splits ) : <NEWLINE> <INDENT> for subject in tqdm ( subjects ) : <NEWLINE> <INDENT> print ( subject ) <NEWLINE> spath = { } <NEWLINE> subject_path = os . path . join ( dataset_path , subject ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( dataset_path , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'flair'",
                "'_flair.nii.gz'",
                "'t1ce'",
                "'_t1ce.nii.gz'",
                "'seg'",
                "'_seg.nii.gz'",
                "'t1'",
                "'_t1.nii.gz'",
                "'t2'",
                "'_t2.nii.gz'",
                "'mask'",
                "'mask.nii.gz'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "97562a0cb6ca4b60a06a59e183f51209": {
        "code_string": "if r.status_code != 204:\n                 comments_to_delete += r.json().get(\"comments\", list())\n                 deleted_comments += 1\n         except:\n             continue\n",
        "code_toks_joined": "if r . status_code != 204 : <NEWLINE> <INDENT> comments_to_delete += r . json ( ) . get ( <STRING> , list ( ) ) <NEWLINE> deleted_comments += 1 <NEWLINE> except : <NEWLINE> continue <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"comments\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d3c1aad2c7d741ee9dc1bebc8bc0603e": {
        "code_string": "sys.path.insert(\"..\", 0)\n",
        "code_toks_joined": "sys . path . insert ( <STRING> , 0 ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "\"..\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "704d0bcde42f4bc2b48c6dd4c39796ec": {
        "code_string": "elif isinstance(input_, string_types):\n         for sep in [\",\", \";\", \"|\"]:\n             if len(input_.split(sep)) in range(2, 3):\n                 t = input_.split(sep)\n         if isinstance(input_, string_types):\n             raise Exception(\"Invalid coordinates: %s\" % input_)\n",
        "code_toks_joined": "elif isinstance ( input_ , string_types ) : <NEWLINE> <INDENT> for sep in [ <STRING> , <STRING> , <STRING> ] : <NEWLINE> <INDENT> if len ( input_ . split ( sep ) ) in range ( 2 , 3 ) : <NEWLINE> <INDENT> t = input_ . split ( sep ) <NEWLINE> <DEDENT> <DEDENT> if isinstance ( input_ , string_types ) : <NEWLINE> <INDENT> raise Exception ( <STRING> % input_ ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\",\"",
                "\";\"",
                "\"|\"",
                "\"Invalid coordinates: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9efffea326064d969e5a7fa0de1dcad5": {
        "code_string": "def is_relative(self, url):\n         domain = self.url_splitter.get_domain(url)\n         return len(domain) > 0\n",
        "code_toks_joined": "def is_relative ( self , url ) : <NEWLINE> <INDENT> domain = self . url_splitter . get_domain ( url ) <NEWLINE> return len ( domain ) > 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d5b41e927e2347feaf0296695dd3cd5f": {
        "code_string": "elif len(gname)>2:\n         p0_name += '_%s'%gname[1]\n",
        "code_toks_joined": "elif len ( gname ) > 2 : <NEWLINE> <INDENT> p0_name += <STRING> % gname [ 1 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'_%s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8672afb6dd244599aecddf52126be141": {
        "code_string": "def read_bam_file(bamfile, chrnames_bam, max_NM=0):\n     \"\"\"\n     Given a Samfile object of a single mapped file, return the reads in the file\n     If there are more than one mapping to a read return the first in the genome.\n     Test if the read has less than (or equals to) number of allowed mismatches\n     Arguments:\n     - `bamfile`: A Samfile object\n     - `chrnames_bam`: The names of the chromosomes\n     - `max_NM`: Maximal number of mismatches\n     \"\"\"\n     read_objects = {}\n     for read in bamfile.fetch():\n         if not read.is_unmapped:\n             nm_num = get_NM_number(read.tags)\n             if nm_num > max_NM:\n                 continue\n             # If there are multiple hits, choose the one with the smallest coor\n             alt_list = get_XA_mapping(read.tags, max_NM)\n             min_pos = read.pos\n             min_is_rev = read.is_reverse\n             for al in alt_list:\n                 apos = int(al[1][1:])\n                 # test this alternative only if its NM is as the original one\n                 if int(apos[3])>nm_num:\n                     continue\n                 if apos < min_pos:\n                     min_pos = apos\n                     min_is_rev = al[1][0]=='-'\n             # If changed, add the read one to the XA tag\n             tags = read.tags\n             if read.pos !=min_pos:\n                 for xt in tags:\n                     if xt[0] == 'XA':\n                         xaval = xt[1]\n                         tags.remove(xt)\n                         strs = '+'\n                         if read.is_reverse:\n                             strs = '-'\n                         tags.append(('XA', '%s,%s%d,%s,%d;'%(\n                                     chrnames_bam[read.tid],\n                                     strs, read.pos,\n                                     read.cigarstring, nm_num) +xaval))\n                 read.tags = tags\n             read.pos = min_pos\n             read.is_reverse = min_is_rev\n             read_objects[read.qname] = read\n     return read_objects\n",
        "code_toks_joined": "def read_bam_file ( bamfile , chrnames_bam , max_NM = 0 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> read_objects = { } <NEWLINE> for read in bamfile . fetch ( ) : <NEWLINE> <INDENT> if not read . is_unmapped : <NEWLINE> <INDENT> nm_num = get_NM_number ( read . tags ) <NEWLINE> if nm_num > max_NM : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> alt_list = get_XA_mapping ( read . tags , max_NM ) <NEWLINE> min_pos = read . pos <NEWLINE> min_is_rev = read . is_reverse <NEWLINE> for al in alt_list : <NEWLINE> <INDENT> apos = int ( al [ 1 ] [ 1 : ] ) <NEWLINE> <COMMENT> <NL> if int ( apos [ 3 ] ) > nm_num : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if apos < min_pos : <NEWLINE> <INDENT> min_pos = apos <NEWLINE> min_is_rev = al [ 1 ] [ 0 ] == <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> tags = read . tags <NEWLINE> if read . pos != min_pos : <NEWLINE> <INDENT> for xt in tags : <NEWLINE> <INDENT> if xt [ 0 ] == <STRING> : <NEWLINE> <INDENT> xaval = xt [ 1 ] <NEWLINE> tags . remove ( xt ) <NEWLINE> strs = <STRING> <NEWLINE> if read . is_reverse : <NEWLINE> <INDENT> strs = <STRING> <NEWLINE> <DEDENT> tags . append ( ( <STRING> , <STRING> % ( <NEWLINE> <INDENT> chrnames_bam [ read . tid ] , <NEWLINE> strs , read . pos , <NEWLINE> read . cigarstring , nm_num ) + xaval ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> read . tags = tags <NEWLINE> <DEDENT> read . pos = min_pos <NEWLINE> read . is_reverse = min_is_rev <NEWLINE> read_objects [ read . qname ] = read <NEWLINE> <DEDENT> <DEDENT> return read_objects <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Given a Samfile object of a single mapped file, return the reads in the file\n     If there are more than one mapping to a read return the first in the genome.\n     Test if the read has less than (or equals to) number of allowed mismatches\n     Arguments:\n     - `bamfile`: A Samfile object\n     - `chrnames_bam`: The names of the chromosomes\n     - `max_NM`: Maximal number of mismatches\n     \"\"\"",
                "'-'",
                "'XA'",
                "'+'",
                "'-'",
                "'XA'",
                "'%s,%s%d,%s,%d;'"
            ],
            "<COMMENT>": [
                "# If there are multiple hits, choose the one with the smallest coor",
                "# test this alternative only if its NM is as the original one",
                "# If changed, add the read one to the XA tag"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ccbd91b71533408291b272de6cd03b18": {
        "code_string": "updater = Updater(token=sys.argv[0])\n",
        "code_toks_joined": "updater = Updater ( token = sys . argv [ 0 ] ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9500afdc028f490c867092e4d42e2066": {
        "code_string": "if header_after_slash is None:\n             final_header = header_code\n         else:\n             if len(header_before_slash) != 0:\n                 final_header = str(header_code) +'/'+ str(header_after_slash)\n             elif len(header_after_slash) == 0:\n                 final_header = '/' + str(decsCodes_list_dict)\n             else:\n                 print(header,\"--\",header_before_slash,\"--\",header_after_slash)\n",
        "code_toks_joined": "if header_after_slash is None : <NEWLINE> <INDENT> final_header = header_code <NEWLINE> else : <NEWLINE> if len ( header_before_slash ) != 0 : <NEWLINE> <INDENT> final_header = str ( header_code ) + <STRING> + str ( header_after_slash ) <NEWLINE> <DEDENT> elif len ( header_after_slash ) == 0 : <NEWLINE> <INDENT> final_header = <STRING> + str ( decsCodes_list_dict ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( header , <STRING> , header_before_slash , <STRING> , header_after_slash ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/'",
                "'/'",
                "\"--\"",
                "\"--\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e2ec2a416b644b43b0e36fad749aea2c": {
        "code_string": "def wrapper(cls, request, *args, **kwargs):\n             paramap = dict(kwargs)\n             paramap.setdefault(\n                 'id', kwargs.get('pk', None)\n             )  # Serializer fields\u4e2d\u751f\u6210\u7684\u4e3aid \u8fd9\u4e2akey\uff0c \u4f46\u662fdjango\u89e3\u6790url\u4e2d\u4e3a pk\u8fd9\u4e2apk\uff0c\u4e3a\u4e86\u4e0d\u5728\u6587\u6863\u4e2d\u751f\u6210id \u548cpk\u8fd9\u4e24\u4e2afield\uff0c \u6240\u4ee5\u90fd\u7edf\u4e00\u7528id\u8fd9\u4e2akey\uff0c \u90a3\u4e48\u5728itemset\u4e2d\u4e5f\u5199id\u8fd9\u4e2akey\n             data = data_method(request)\n             paramap.update({x: y for x, y in data.items()})\n             result = cls.result_class()  # \u7ee7\u627f\u4e0eResult\u7c7b\n             for item in itemset:\n                 name, v, required, msg, replace = [\n                     item[x]\n                     for x in ['name', 'method', 'required', 'msg', 'replace']\n                 ]\n                 value = None  # \u4e0e '' \u533a\u522b\n                 para = paramap.get(name)\n                 if required and para not in (None, ''):  # \u5982\u679c\u662fpost\u65b9\u6cd5\u5e76\u4e14\u4f20\u53c2\u662fjson\u7684\u8bdd\uff0cpara\u53ef\u80fd\u4e3a0\n                     result.error(name, 'required')\n                 if para is not None:\n                     if para:\n                         try:\n                             value = v(para)\n                         except Exception:\n                             if settings.DEBUG:\n                                 from traceback import print_exc\n                                 print_exc()\n                         msg = v.msg or msg\n                         if v.status == 403:  # \u6743\u9650\u9519\u8bef\u65f6\u76f4\u63a5\u8fd4\u56de\u9519\u8bef\n                             return result.perm(name, msg)(status=v.status)\n                         if value in (None, False):\n                             result.error(name, msg)\n                     kwargs.update({\n                         replace or name: value or para\n                     })  # method \u8fd4\u56de\u4e86\u975e\u5e03\u5c14\u503c\u5219\u66f4\u65b0kwargs\n             if not result:\n                 return result(status=400)\n             return func(cls, request, *args, **kwargs)\n",
        "code_toks_joined": "def wrapper ( cls , request , * args , ** kwargs ) : <NEWLINE> <INDENT> paramap = dict ( kwargs ) <NEWLINE> paramap . setdefault ( <NEWLINE> <INDENT> <STRING> , kwargs . get ( <STRING> , None ) <NEWLINE> <DEDENT> ) <COMMENT> <NEWLINE> data = data_method ( request ) <NEWLINE> paramap . update ( { x : y for x , y in data . items ( ) } ) <NEWLINE> result = cls . result_class ( ) <COMMENT> <NEWLINE> for item in itemset : <NEWLINE> <INDENT> name , v , required , msg , replace = [ <NEWLINE> <INDENT> item [ x ] <NEWLINE> for x in [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> value = None <COMMENT> <NEWLINE> para = paramap . get ( name ) <NEWLINE> if required and para not in ( None , <STRING> ) : <COMMENT> <NEWLINE> <INDENT> result . error ( name , <STRING> ) <NEWLINE> <DEDENT> if para is not None : <NEWLINE> <INDENT> if para : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> value = v ( para ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> if settings . DEBUG : <NEWLINE> <INDENT> from traceback import print_exc <NEWLINE> print_exc ( ) <NEWLINE> <DEDENT> <DEDENT> msg = v . msg or msg <NEWLINE> if v . status == 403 : <COMMENT> <NEWLINE> <INDENT> return result . perm ( name , msg ) ( status = v . status ) <NEWLINE> <DEDENT> if value in ( None , False ) : <NEWLINE> <INDENT> result . error ( name , msg ) <NEWLINE> <DEDENT> <DEDENT> kwargs . update ( { <NEWLINE> <INDENT> replace or name : value or para <NEWLINE> <DEDENT> } ) <COMMENT> <NEWLINE> <DEDENT> <DEDENT> if not result : <NEWLINE> <INDENT> return result ( status = 400 ) <NEWLINE> <DEDENT> return func ( cls , request , * args , ** kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'id'",
                "'pk'",
                "'name'",
                "'method'",
                "'required'",
                "'msg'",
                "'replace'",
                "''",
                "'required'"
            ],
            "<COMMENT>": [
                "# Serializer fields\u4e2d\u751f\u6210\u7684\u4e3aid \u8fd9\u4e2akey\uff0c \u4f46\u662fdjango\u89e3\u6790url\u4e2d\u4e3a pk\u8fd9\u4e2apk\uff0c\u4e3a\u4e86\u4e0d\u5728\u6587\u6863\u4e2d\u751f\u6210id \u548cpk\u8fd9\u4e24\u4e2afield\uff0c \u6240\u4ee5\u90fd\u7edf\u4e00\u7528id\u8fd9\u4e2akey\uff0c \u90a3\u4e48\u5728itemset\u4e2d\u4e5f\u5199id\u8fd9\u4e2akey",
                "# \u7ee7\u627f\u4e0eResult\u7c7b",
                "# \u4e0e '' \u533a\u522b",
                "# \u5982\u679c\u662fpost\u65b9\u6cd5\u5e76\u4e14\u4f20\u53c2\u662fjson\u7684\u8bdd\uff0cpara\u53ef\u80fd\u4e3a0",
                "# \u6743\u9650\u9519\u8bef\u65f6\u76f4\u63a5\u8fd4\u56de\u9519\u8bef",
                "# method \u8fd4\u56de\u4e86\u975e\u5e03\u5c14\u503c\u5219\u66f4\u65b0kwargs"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0cdca2228e404976a5685c759910a769": {
        "code_string": "noise = salt_pepper_noise(batch_size, noise_shape[1], density, salt_value, pepper_value, seed)\n",
        "code_toks_joined": "noise = salt_pepper_noise ( batch_size , noise_shape [ 1 ] , density , salt_value , pepper_value , seed ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c19f4af963c64a1b8e1d42e55b1d4014": {
        "code_string": "if scale_first:\n                 path_rev = _scaled_path(path, scaling_path, flip_paths)\n",
        "code_toks_joined": "if scale_first : <NEWLINE> <INDENT> path_rev = _scaled_path ( path , scaling_path , flip_paths ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "04149663fe7040abbaa741b17e773ab7": {
        "code_string": "self.easy.product_cach[p.id] = product\n             self.products.buffer[i+offset] = product\n",
        "code_toks_joined": "self . easy . product_cach [ p . id ] = product <NEWLINE> <INDENT> self . products . buffer [ i + offset ] = product <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9965749beaf34749865ef69efb03bfb7": {
        "code_string": "n = abs(n)\n     if n == 1:\n         return [1]\n     factor = []\n     for i in prime:\n         if i**2 > n:\n             factor.append(n)\n             break\n         while not n % i:\n             factor.append(i)\n             n /= i\n         if n == 1:\n             break\n     return factor\n",
        "code_toks_joined": "n = abs ( n ) <NEWLINE> <INDENT> if n == 1 : <NEWLINE> <INDENT> return [ 1 ] <NEWLINE> <DEDENT> factor = [ ] <NEWLINE> for i in prime : <NEWLINE> <INDENT> if i ** 2 > n : <NEWLINE> <INDENT> factor . append ( n ) <NEWLINE> break <NEWLINE> <DEDENT> while not n % i : <NEWLINE> <INDENT> factor . append ( i ) <NEWLINE> n /= i <NEWLINE> <DEDENT> if n == 1 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> return factor <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8b4e13d61bff4923b2248439c3c51db3": {
        "code_string": "async def handle_user_message(app, ws, msg):\n     if isinstance(msg, api.SetDocumentMessage):\n         assert app['state'] == State.idle\n         try:\n             app['state'] = State.processing\n             notify_state(app)\n             job = await plotting.process_upload_background(app, msg.document)\n         except Exception as e:\n             notify_error(app, ws, str(e))\n         else:\n             app['document'] = msg.document\n             app['job'] = job\n             app['estimated_time'] = job.duration().total_seconds()\n             notify_new_document(app, exclude_client=ws)\n         finally:\n             app['state'] = State.idle\n             notify_state(app)\n",
        "code_toks_joined": "async def handle_user_message ( app , ws , msg ) : <NEWLINE> <INDENT> if isinstance ( msg , api . SetDocumentMessage ) : <NEWLINE> <INDENT> assert app [ <STRING> ] == State . idle <NEWLINE> try : <NEWLINE> <INDENT> app [ <STRING> ] = State . processing <NEWLINE> notify_state ( app ) <NEWLINE> job = await plotting . process_upload_background ( app , msg . document ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> notify_error ( app , ws , str ( e ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> app [ <STRING> ] = msg . document <NEWLINE> app [ <STRING> ] = job <NEWLINE> app [ <STRING> ] = job . duration ( ) . total_seconds ( ) <NEWLINE> notify_new_document ( app , exclude_client = ws ) <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> app [ <STRING> ] = State . idle <NEWLINE> notify_state ( app ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'state'",
                "'state'",
                "'document'",
                "'job'",
                "'estimated_time'",
                "'state'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "12ee044e79c84d22a96b1ac2e046e72d": {
        "code_string": "try:\n         async for raw_msg in ws:\n             if raw_msg.tp == aiohttp.MsgType.text:\n                 msg = api.Message.deserialize(raw_msg.data)\n                 log.info(\"User message: %s\", msg)\n                 await handle_user_message(app, ws, msg)\n             elif raw_msg.tp == aiohttp.MsgType.closed:\n                 break\n             elif raw_msg.tp == aiohttp.MsgType.error:\n                 log.info(\"User websocket error: %s\", msg)\n                 break\n             else:\n                 log.error(\"Unknown user message type: %s, ignoring.\",\n                           raw_msg.tp)\n     finally:\n         log.info(\"Client connection closed.\")\n         clients.remove(ws)\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> async for raw_msg in ws : <NEWLINE> <INDENT> if raw_msg . tp == aiohttp . MsgType . text : <NEWLINE> <INDENT> msg = api . Message . deserialize ( raw_msg . data ) <NEWLINE> log . info ( <STRING> , msg ) <NEWLINE> await handle_user_message ( app , ws , msg ) <NEWLINE> <DEDENT> elif raw_msg . tp == aiohttp . MsgType . closed : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> elif raw_msg . tp == aiohttp . MsgType . error : <NEWLINE> <INDENT> log . info ( <STRING> , msg ) <NEWLINE> break <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . error ( <STRING> , <NEWLINE> <INDENT> raw_msg . tp ) <NEWLINE> finally : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> log . info ( <STRING> ) <NEWLINE> clients . remove ( ws ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"User message: %s\"",
                "\"User websocket error: %s\"",
                "\"Unknown user message type: %s, ignoring.\"",
                "\"Client connection closed.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ec888ca4a8dd4f2ebeccdb3ca806dbae": {
        "code_string": "def process_image(elm, registry, container):\n     img = Image()\n     container.add_content(img)\n     for subitem in elm.items():\n         if subitem[0] == 'alt':\n             img.alt = subitem[1]\n         elif subitem[0] == 'witdh':\n             img.witdh = subitem[1]\n         elif subitem[1] == 'height':\n             img.height = subitem[1]\n         elif subitem[0] == 'uri':\n             img.uri = subitem[1]\n",
        "code_toks_joined": "def process_image ( elm , registry , container ) : <NEWLINE> <INDENT> img = Image ( ) <NEWLINE> container . add_content ( img ) <NEWLINE> for subitem in elm . items ( ) : <NEWLINE> <INDENT> if subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . alt = subitem [ 1 ] <NEWLINE> <DEDENT> elif subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . witdh = subitem [ 1 ] <NEWLINE> <DEDENT> elif subitem [ 1 ] == <STRING> : <NEWLINE> <INDENT> img . height = subitem [ 1 ] <NEWLINE> <DEDENT> elif subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . uri = subitem [ 1 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'alt'",
                "'witdh'",
                "'height'",
                "'uri'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "febbcee65f1a442893fa96016b9ed67e": {
        "code_string": "def test_gsheets_add_rows_to_active_sheet_sets_add_rows_time(gsheets_handler_no_thread):\n     gsheets_handler_no_thread._add_rows_time = 99\n     assert isinstance(gsheets_handler_no_thread._add_rows_to_active_sheet([]), unittest.mock.Mock)\n     assert gsheets_handler_no_thread._add_rows_time < 99 and gsheets_handler_no_thread._add_rows_time > 0\n",
        "code_toks_joined": "def test_gsheets_add_rows_to_active_sheet_sets_add_rows_time ( gsheets_handler_no_thread ) : <NEWLINE> <INDENT> gsheets_handler_no_thread . _add_rows_time = 99 <NEWLINE> assert isinstance ( gsheets_handler_no_thread . _add_rows_to_active_sheet ( [ ] ) , unittest . mock . Mock ) <NEWLINE> assert gsheets_handler_no_thread . _add_rows_time < 99 and gsheets_handler_no_thread . _add_rows_time > 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bd010aa31424403a8db3c977ee8e8bb0": {
        "code_string": "def add(self, fee, result):\n         if fee.id not in self.applications:\n             self.applications[fee.id] = {\n                 'fee': fee,\n                 'result': result,\n                 'name': fee.name,\n                 'description': result.description,\n                 'freq': 0,\n                 'amount': D('0.00')}\n         self.applications[fee.id]['amount'] += result.fee\n         self.applications[fee.id]['freq'] += 1\n",
        "code_toks_joined": "def add ( self , fee , result ) : <NEWLINE> <INDENT> if fee . id not in self . applications : <NEWLINE> <INDENT> self . applications [ fee . id ] = { <NEWLINE> <INDENT> <STRING> : fee , <NEWLINE> <STRING> : result , <NEWLINE> <STRING> : fee . name , <NEWLINE> <STRING> : result . description , <NEWLINE> <STRING> : 0 , <NEWLINE> <STRING> : D ( <STRING> ) } <NEWLINE> <DEDENT> <DEDENT> self . applications [ fee . id ] [ <STRING> ] += result . fee <NEWLINE> self . applications [ fee . id ] [ <STRING> ] += 1 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'fee'",
                "'result'",
                "'name'",
                "'description'",
                "'freq'",
                "'amount'",
                "'0.00'",
                "'amount'",
                "'freq'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c450049b18434c7b9e2225ee133565fd": {
        "code_string": "''' add phantom label if none exists to facilitate C++ interop '''\n         assert(len(edges.columns) >= 1)\n         if len(nodes.columns) == 1:\n             nodes['labeled'] = np.zeros(len(nodes), np.bool_)\n",
        "code_toks_joined": "<STRING> <NEWLINE> <INDENT> assert ( len ( edges . columns ) >= 1 ) <NEWLINE> if len ( nodes . columns ) == 1 : <NEWLINE> <INDENT> nodes [ <STRING> ] = np . zeros ( len ( nodes ) , np . bool_ ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''' add phantom label if none exists to facilitate C++ interop '''",
                "'labeled'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bb219037e5aa4d62af621d4a9ccc03f5": {
        "code_string": "''' create output buffer '''\n         timer.tic('creating output buffer')\n         starts = backend.zeros(len(X) + 1, dtype=np.uint32)\n         if nodal is True:\n             sizes = np.array([len(g.nodes) for g in X], dtype=np.uint32)\n             np.cumsum(sizes, out=starts[1:])\n             output_length = int(starts[-1])\n         elif nodal is False:\n             starts[:] = np.arange(len(X) + 1)\n             output_length = len(X)\n         elif nodal == 'block':\n             sizes = np.array([len(g.nodes) for g in X], dtype=np.uint32)\n             np.cumsum(sizes**2, out=starts[1:])\n             output_length = int(starts[-1])\n         else:\n             raise(ValueError(\"Invalid 'nodal' option '%s'\" % nodal))\n         if traits.eval_gradient is True:\n             output_shape = (output_length, 1 + self.n_dims)\n         else:\n             output_shape = (output_shape, 1)\n         output = backend.empty(int(np.prod(output_shape)), np.float32)\n         timer.toc('creating output buffer')\n",
        "code_toks_joined": "<STRING> <NEWLINE> <INDENT> timer . tic ( <STRING> ) <NEWLINE> starts = backend . zeros ( len ( X ) + 1 , dtype = np . uint32 ) <NEWLINE> if nodal is True : <NEWLINE> <INDENT> sizes = np . array ( [ len ( g . nodes ) for g in X ] , dtype = np . uint32 ) <NEWLINE> np . cumsum ( sizes , out = starts [ 1 : ] ) <NEWLINE> output_length = int ( starts [ - 1 ] ) <NEWLINE> <DEDENT> elif nodal is False : <NEWLINE> <INDENT> starts [ : ] = np . arange ( len ( X ) + 1 ) <NEWLINE> output_length = len ( X ) <NEWLINE> <DEDENT> elif nodal == <STRING> : <NEWLINE> <INDENT> sizes = np . array ( [ len ( g . nodes ) for g in X ] , dtype = np . uint32 ) <NEWLINE> np . cumsum ( sizes ** 2 , out = starts [ 1 : ] ) <NEWLINE> output_length = int ( starts [ - 1 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ( ValueError ( <STRING> % nodal ) ) <NEWLINE> <DEDENT> if traits . eval_gradient is True : <NEWLINE> <INDENT> output_shape = ( output_length , 1 + self . n_dims ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output_shape = ( output_shape , 1 ) <NEWLINE> <DEDENT> output = backend . empty ( int ( np . prod ( output_shape ) ) , np . float32 ) <NEWLINE> timer . toc ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''' create output buffer '''",
                "'creating output buffer'",
                "'block'",
                "\"Invalid 'nodal' option '%s'\"",
                "'creating output buffer'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "359926b23d8d443b8932b03e84a5b95b": {
        "code_string": "class SpectralApprox(FactorApprox):\n     def __init__(self, X, rcut=0, acut=0):\n         if isinstance(X, np.ndarray):\n             U, S, _ = np.linalg.svd(X, full_matrices=False)\n             mask = (S >= S.max() * rcut) | (S >= acut)\n             self.U = U[:, mask]\n             self.S = S[mask]\n         elif isinstance(X, tuple) and len(X) == 2:\n             self.U, self.S = X\n         self._lhs = self.U * self.S\n",
        "code_toks_joined": "class SpectralApprox ( FactorApprox ) : <NEWLINE> <INDENT> def __init__ ( self , X , rcut = 0 , acut = 0 ) : <NEWLINE> <INDENT> if isinstance ( X , np . ndarray ) : <NEWLINE> <INDENT> U , S , _ = np . linalg . svd ( X , full_matrices = False ) <NEWLINE> mask = ( S >= S . max ( ) * rcut ) | ( S >= acut ) <NEWLINE> self . U = U [ : , mask ] <NEWLINE> self . S = S [ mask ] <NEWLINE> <DEDENT> elif isinstance ( X , tuple ) and len ( X ) == 2 : <NEWLINE> <INDENT> self . U , self . S = X <NEWLINE> <DEDENT> self . _lhs = self . U * self . S <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0292451d1a124793bf7fe4713ea69cfa": {
        "code_string": "S = uctypes.struct(desc, uctypes.addressof(data), uctypes.LITTLE_ENDIAN)\n",
        "code_toks_joined": "S = uctypes . struct ( desc , uctypes . addressof ( data ) , uctypes . LITTLE_ENDIAN ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "011cd33fd21d4c2fa54ff8a99ca5db57": {
        "code_string": "S = uctypes.struct(desc, uctypes.addressof(data), uctypes.LITTLE_ENDIAN)\n",
        "code_toks_joined": "S = uctypes . struct ( desc , uctypes . addressof ( data ) , uctypes . LITTLE_ENDIAN ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "704f2c105ac64e07b1e60877c7b57b47": {
        "code_string": "S = uctypes.struct(desc, uctypes.addressof(data), uctypes.NATIVE)\n",
        "code_toks_joined": "S = uctypes . struct ( desc , uctypes . addressof ( data ) , uctypes . NATIVE ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5fc73e2ddf454fb28926d739fe7ff38e": {
        "code_string": "S = uctypes.struct(desc, uctypes.addressof(buf), uctypes.LITTLE_ENDIAN)\n",
        "code_toks_joined": "S = uctypes . struct ( desc , uctypes . addressof ( buf ) , uctypes . LITTLE_ENDIAN ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bc1f408f8d5c48e8a8a273a1b9065882": {
        "code_string": "def stop(self) -> None:\n         with self._stop_lock:\n             self._do_stop = False\n",
        "code_toks_joined": "def stop ( self ) -> None : <NEWLINE> <INDENT> with self . _stop_lock : <NEWLINE> <INDENT> self . _do_stop = False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "487c3e91af0d480b824a34035dbd6d73": {
        "code_string": "self.__pt1000_adc = pt1000_conf.adc(MCP342X.GAIN_4, MCP342X.RATE_15) if pt1000_conf else None\n",
        "code_toks_joined": "self . __pt1000_adc = pt1000_conf . adc ( MCP342X . GAIN_4 , MCP342X . RATE_15 ) if pt1000_conf else None <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ce5f8856f12144bcb73a4adaa37f0af3": {
        "code_string": "hasNoMissingJobs = reissueMissingJobs(updatedJobFiles, jobBatcher, batchSystem, childJobFileToParentJob, config, childCounts)\n                 if hasNoMissingJobs:\n                     timeSinceJobsLastRescued = time.time()\n                 else:\n                     timeSinceJobsLastRescued += 60 #This means we'll try again in a minute, providing things are quiet\n                 logger.info(\"Rescued any (long) missing jobs\")\n",
        "code_toks_joined": "hasNoMissingJobs = reissueMissingJobs ( updatedJobFiles , jobBatcher , batchSystem , childJobFileToParentJob , config , childCounts ) <NEWLINE> <INDENT> if hasNoMissingJobs : <NEWLINE> <INDENT> timeSinceJobsLastRescued = time . time ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> timeSinceJobsLastRescued += 60 <COMMENT> <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#This means we'll try again in a minute, providing things are quiet"
            ],
            "<STRING>": [
                "\"Rescued any (long) missing jobs\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a6d7a3c2c7db44bf903494c26e74bba3": {
        "code_string": "def writeGlobalFile(self, localFileName):\n         \"\"\"Takes a file (as a path) and uploads it to to the global file store, returns\n         an ID that can be used to retrieve the file. \n         \"\"\"\n         return self.jobStore.writeFile(localFileName, self.job.jobStoreID)\n",
        "code_toks_joined": "def writeGlobalFile ( self , localFileName ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return self . jobStore . writeFile ( localFileName , self . job . jobStoreID ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Takes a file (as a path) and uploads it to to the global file store, returns\n         an ID that can be used to retrieve the file. \n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b1a68f6e9f5f4d469b4a8624a0059daa": {
        "code_string": "#The number of predecessor jobs of a given job.\n         #A predecessor is a job which references this job in its stack.\n         self.predecessorNumber = predecessorNumber\n         #The IDs of predecessors that have finished. \n         #When len(predecessorsFinished) == predecessorNumber then the\n         #job can be run.\n         self.predecessorsFinished = predecessorNumber or set()\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> self . predecessorNumber = predecessorNumber <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> self . predecessorsFinished = predecessorNumber or set ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#The number of predecessor jobs of a given job.",
                "#A predecessor is a job which references this job in its stack.",
                "#The IDs of predecessors that have finished. ",
                "#When len(predecessorsFinished) == predecessorNumber then the",
                "#job can be run."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ec3b4280970844acb34a04250df06d96": {
        "code_string": "def setLogFile( self, logFile, jobStore ):\n         \"\"\"\n         Sets the log file in the file store. \n         \"\"\"\n         if self.logJobStoreFileID is not None:\n             self.clearLogFile(jobStore)\n         self.logJobStoreFileID = jobStore.writeFile( self.jobStoreID, logFile )\n         assert self.logJobStoreFileID is not None\n",
        "code_toks_joined": "def setLogFile ( self , logFile , jobStore ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . logJobStoreFileID is not None : <NEWLINE> <INDENT> self . clearLogFile ( jobStore ) <NEWLINE> <DEDENT> self . logJobStoreFileID = jobStore . writeFile ( self . jobStoreID , logFile ) <NEWLINE> assert self . logJobStoreFileID is not None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Sets the log file in the file store. \n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a00943f5e3a84a7a999057c874d65910": {
        "code_string": "#Reset the retry count of the jobWrapper \n             if jobWrapper.remainingRetryCount < self._defaultTryCount():\n                 jobWrapper.remainingRetryCount = self._defaultTryCount()\n                 changed = True\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if jobWrapper . remainingRetryCount < self . _defaultTryCount ( ) : <NEWLINE> <INDENT> jobWrapper . remainingRetryCount = self . _defaultTryCount ( ) <NEWLINE> changed = True <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#Reset the retry count of the jobWrapper "
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "49778784d28f4c249a9bc3439df359f3": {
        "code_string": "DATA_TEST_CDEC_PARSE = [\n     # (cdstr, correct)\n     (\"int a\", dict_cdec_parse('int', 'a')),\n     (\"int aA1\", dict_cdec_parse('int', 'aA1')),\n     (\"int a[i]\", dict_cdec_parse('int', 'a', tuple('i'), 1)),\n     (\"int a[i][j]\", dict_cdec_parse('int', 'a', tuple('ij'), 2)),\n     (\"int a = 1\", dict_cdec_parse('int', 'a', default=1)),\n     (\"int a[i] =2\", dict_cdec_parse('int', 'a', tuple('i'), 1, 2)),\n     (\"int a[i][j]=3\", dict_cdec_parse('int', 'a', tuple('ij'), 2, 3)),\n     ('num_i = 10', dict_cdec_parse('int', 'num_i', default=10)),\n     (cmem('obj', DmyCDT), dict_cdec_parse(DmyCDT, 'obj', valtype='object')),\n     ]\n",
        "code_toks_joined": "DATA_TEST_CDEC_PARSE = [ <NEWLINE> <COMMENT> <NL> <INDENT> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 1 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 2 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , default = 1 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 1 , 2 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 2 , 3 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , default = 10 ) ) , <NEWLINE> ( cmem ( <STRING> , DmyCDT ) , dict_cdec_parse ( DmyCDT , <STRING> , valtype = <STRING> ) ) , <NEWLINE> ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# (cdstr, correct)"
            ],
            "<STRING>": [
                "\"int a\"",
                "'int'",
                "'a'",
                "\"int aA1\"",
                "'int'",
                "'aA1'",
                "\"int a[i]\"",
                "'int'",
                "'a'",
                "'i'",
                "\"int a[i][j]\"",
                "'int'",
                "'a'",
                "'ij'",
                "\"int a = 1\"",
                "'int'",
                "'a'",
                "\"int a[i] =2\"",
                "'int'",
                "'a'",
                "'i'",
                "\"int a[i][j]=3\"",
                "'int'",
                "'a'",
                "'ij'",
                "'num_i = 10'",
                "'int'",
                "'num_i'",
                "'obj'",
                "'obj'",
                "'object'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5334d0c5df0e47eda265f28fc28aa434": {
        "code_string": "if os.path.isdir(runName):\n         # output directory was created.  Must have been a parametric run\n         if errnum > 0:\n             shutil.rmtree(rdir)\n         else:\n             os.system('/bin/cp -pr %s/* %s' % (runName, rdir))\n             shutil.rmtree(runName)\n     else:\n         # nonparametric run.  Results are in current working directory.\n         # Use the timestamp to copy all newer files to the cacheName.\n         if errnum > 0:\n             files = os.listdir('.')\n             for f in files:\n                 if os.path.getmtime(f) > self.start_time:\n                     shutil.copy2(f, rdir)\n",
        "code_toks_joined": "if os . path . isdir ( runName ) : <NEWLINE> <COMMENT> <NL> <INDENT> if errnum > 0 : <NEWLINE> <INDENT> shutil . rmtree ( rdir ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> os . system ( <STRING> % ( runName , rdir ) ) <NEWLINE> shutil . rmtree ( runName ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> if errnum > 0 : <NEWLINE> <INDENT> files = os . listdir ( <STRING> ) <NEWLINE> for f in files : <NEWLINE> <INDENT> if os . path . getmtime ( f ) > self . start_time : <NEWLINE> <INDENT> shutil . copy2 ( f , rdir ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# output directory was created.  Must have been a parametric run",
                "# nonparametric run.  Results are in current working directory.",
                "# Use the timestamp to copy all newer files to the cacheName."
            ],
            "<STRING>": [
                "'/bin/cp -pr %s/* %s'",
                "'.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "20fdf72b6e064bd18274cbced78cc8b2": {
        "code_string": "def _get_next_task(self, cores):\n         # Pseudocode:\n         #   if we have enough cores to sweep and a sweep available then\n         #       pop from the end of sweeps (so only one index changes)\n         #       remove from params_to_sweeps_ind\n         #       add to in_progress\n         #       return\n         #\n         #   pop the item with the most number of trials from trials, ignoring\n         #   ones which are already in progress or haven't been sweep yet\n         #\n         #   if we cannot finish this then\n         #       build the disp.Task which does the right # of trials\n         #       update the remaining number of trials for this trial\n         #       add to in_progress\n         #       return the built disp.Task\n         #   build the disp.Task which finishes the queued trials for this set\n         #   remove from params_to_task\n         #   add to in_progress\n         #   return built disp.Task\n         if cores > self.sweep_cores and self.sweeps:\n             swp: ParamsTask = self.sweeps.pop()\n             del self.params_to_sweeps_ind[swp.params]\n             self._len -= 1\n             self.in_progress.add(swp.params)\n             return swp.as_task(\n                 self.module, self.hparams, self.folder,\n                 self.params_to_id[swp.params],\n                 0, self.sweep_cores,\n                 self.listeners,\n                 self.params_to_ntrials[swp.params]\n             )\n",
        "code_toks_joined": "def _get_next_task ( self , cores ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if cores > self . sweep_cores and self . sweeps : <NEWLINE> <INDENT> swp : ParamsTask = self . sweeps . pop ( ) <NEWLINE> del self . params_to_sweeps_ind [ swp . params ] <NEWLINE> self . _len -= 1 <NEWLINE> self . in_progress . add ( swp . params ) <NEWLINE> return swp . as_task ( <NEWLINE> <INDENT> self . module , self . hparams , self . folder , <NEWLINE> self . params_to_id [ swp . params ] , <NEWLINE> 0 , self . sweep_cores , <NEWLINE> self . listeners , <NEWLINE> self . params_to_ntrials [ swp . params ] <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Pseudocode:",
                "#   if we have enough cores to sweep and a sweep available then",
                "#       pop from the end of sweeps (so only one index changes)",
                "#       remove from params_to_sweeps_ind",
                "#       add to in_progress",
                "#       return",
                "#",
                "#   pop the item with the most number of trials from trials, ignoring",
                "#   ones which are already in progress or haven't been sweep yet",
                "#",
                "#   if we cannot finish this then",
                "#       build the disp.Task which does the right # of trials",
                "#       update the remaining number of trials for this trial",
                "#       add to in_progress",
                "#       return the built disp.Task",
                "#   build the disp.Task which finishes the queued trials for this set",
                "#   remove from params_to_task",
                "#   add to in_progress",
                "#   return built disp.Task"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "379c84e550d640a4ae8345050f78378f": {
        "code_string": "if self._orientation == gtk.ORIENTATION_HORIZONTAL:\n                 width += w\n                 height = max(height, h)\n                 # Store the minimum weight for usage in do_size_allocate\n                 item.min_size = w\n             else:\n                 width = max(width, w)\n                 height += h\n                 # Store the minimum weight for usage in do_size_allocate\n                 item.min_size = w\n",
        "code_toks_joined": "if self . _orientation == gtk . ORIENTATION_HORIZONTAL : <NEWLINE> <INDENT> width += w <NEWLINE> height = max ( height , h ) <NEWLINE> <COMMENT> <NL> item . min_size = w <NEWLINE> else : <NEWLINE> width = max ( width , w ) <NEWLINE> height += h <NEWLINE> <COMMENT> <NL> item . min_size = w <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Store the minimum weight for usage in do_size_allocate",
                "# Store the minimum weight for usage in do_size_allocate"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dc771743274b4092b7a7cccfe1ffac5b": {
        "code_string": "def clean_phrases(phrases):\n     for phrase in phrases.values():\n         phrase['pattern'] = ''.join(phrase['pattern']).replace(' ', '')\n         phrase['length'] = (len(phrase['pattern']) // phrase['beat']) + bool(len(phrase['pattern']) % phrase['beat'])\n",
        "code_toks_joined": "def clean_phrases ( phrases ) : <NEWLINE> <INDENT> for phrase in phrases . values ( ) : <NEWLINE> <INDENT> phrase [ <STRING> ] = <STRING> . join ( phrase [ <STRING> ] ) . replace ( <STRING> , <STRING> ) <NEWLINE> phrase [ <STRING> ] = ( len ( phrase [ <STRING> ] ) // phrase [ <STRING> ] ) + bool ( len ( phrase [ <STRING> ] ) % phrase [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pattern'",
                "''",
                "'pattern'",
                "' '",
                "''",
                "'length'",
                "'pattern'",
                "'beat'",
                "'pattern'",
                "'beat'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5e41bc0c5b494d0b87639be834f997fd": {
        "code_string": "self.dictionary = gensim.corpora.Dictionary(data_words)\n         corpus = self.preprocess_texts(docs)\n         saved_model = self.main_data_path/'topic_model'/str('trained_lda_'+str(str(self._training_path))+str(self.numb_topics))\n         saved_model_fname = str(hash(saved_model))+'.model'\n         if os.path.exists(saved_model_fname):\n             #lda_model = pickle.load(os.path.abspath(saved_model))\n             lda_model = gensim.models.ldamodel.LdaModel.load(os.path.abspath(saved_model))\n         else:\n             lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                                     id2word=self.dictionary,\n                                                     num_topics=self.numb_topics,\n                                                     random_state=100,\n                                                     update_every=1,\n                                                     chunksize=100,\n                                                     passes=10,\n                                                     alpha='auto',\n                                                     per_word_topics=True)\n             #pickle.dump(lda_model, open(temp_file,'wb'))\n             lda_model.save(os.path.abspath(saved_model_fname))\n",
        "code_toks_joined": "self . dictionary = gensim . corpora . Dictionary ( data_words ) <NEWLINE> <INDENT> corpus = self . preprocess_texts ( docs ) <NEWLINE> saved_model = self . main_data_path / <STRING> / str ( <STRING> + str ( str ( self . _training_path ) ) + str ( self . numb_topics ) ) <NEWLINE> saved_model_fname = str ( hash ( saved_model ) ) + <STRING> <NEWLINE> if os . path . exists ( saved_model_fname ) : <NEWLINE> <COMMENT> <NL> <INDENT> lda_model = gensim . models . ldamodel . LdaModel . load ( os . path . abspath ( saved_model ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> lda_model = gensim . models . ldamodel . LdaModel ( corpus = corpus , <NEWLINE> <INDENT> id2word = self . dictionary , <NEWLINE> num_topics = self . numb_topics , <NEWLINE> random_state = 100 , <NEWLINE> update_every = 1 , <NEWLINE> chunksize = 100 , <NEWLINE> passes = 10 , <NEWLINE> alpha = <STRING> , <NEWLINE> per_word_topics = True ) <NEWLINE> <COMMENT> <NL> <DEDENT> lda_model . save ( os . path . abspath ( saved_model_fname ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'topic_model'",
                "'trained_lda_'",
                "'.model'",
                "'auto'"
            ],
            "<COMMENT>": [
                "#lda_model = pickle.load(os.path.abspath(saved_model))",
                "#pickle.dump(lda_model, open(temp_file,'wb'))"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cef7587c87b04840a77e15cd10fca4c9": {
        "code_string": "return tuple(format_sharded_filename(base_name, shards, i) for i in range(index))\n",
        "code_toks_joined": "return tuple ( format_sharded_filename ( base_name , shards , i ) for i in range ( index ) ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4f445577c30f44b190738c5251fac2d6": {
        "code_string": "shuffle_stats_report = self._sensitivity_stats(self.shuffled_sensitivity)\n         missing_stats_report = self._sensitivity_stats(self.missing_sensitivity)\n         vulnerability_report = self._vulnerability_report(\n             shuffled_sensitivity=self.shuffled_sensitivity,\n             missing_sensitivity=self.missing_sensitivity,\n             shuffled_sensitivity_stats=missing_stats_report)\n",
        "code_toks_joined": "shuffle_stats_report = self . _sensitivity_stats ( self . shuffled_sensitivity ) <NEWLINE> <INDENT> missing_stats_report = self . _sensitivity_stats ( self . missing_sensitivity ) <NEWLINE> vulnerability_report = self . _vulnerability_report ( <NEWLINE> <INDENT> shuffled_sensitivity = self . shuffled_sensitivity , <NEWLINE> missing_sensitivity = self . missing_sensitivity , <NEWLINE> shuffled_sensitivity_stats = missing_stats_report ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "87446a0d0fb34c21afbffd5c29701263": {
        "code_string": "def start_and_commit(self, container, cmd):\n         client = self.project.client\n         logger.info('Uploading...')\n         client.put_archive(container, '/', self.archive.getfile())\n         logger.info('Starting...')\n         client.start(container)\n         for line in client.logs(container, stream=True):\n             logger.info(line.decode().rstrip())\n         result = client.wait(container)\n         if result == 0:\n             # Only tag image if container was built successfully.\n             repository, tag = self.image_name, None\n             if ':' in repository:\n                 repository, tag = tag.split(':')\n             conf = client.create_container_config(self.image_name, cmd, working_dir=APP_PATH)\n             client.commit(container, repository=repository, tag=tag, conf=conf)\n         client.remove_container(container)\n         return result\n",
        "code_toks_joined": "def start_and_commit ( self , container , cmd ) : <NEWLINE> <INDENT> client = self . project . client <NEWLINE> logger . info ( <STRING> ) <NEWLINE> client . put_archive ( container , <STRING> , self . archive . getfile ( ) ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> client . start ( container ) <NEWLINE> for line in client . logs ( container , stream = True ) : <NEWLINE> <INDENT> logger . info ( line . decode ( ) . rstrip ( ) ) <NEWLINE> <DEDENT> result = client . wait ( container ) <NEWLINE> if result == 0 : <NEWLINE> <COMMENT> <NL> <INDENT> repository , tag = self . image_name , None <NEWLINE> if <STRING> in repository : <NEWLINE> <INDENT> repository , tag = tag . split ( <STRING> ) <NEWLINE> <DEDENT> conf = client . create_container_config ( self . image_name , cmd , working_dir = APP_PATH ) <NEWLINE> client . commit ( container , repository = repository , tag = tag , conf = conf ) <NEWLINE> <DEDENT> client . remove_container ( container ) <NEWLINE> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Uploading...'",
                "'/'",
                "'Starting...'",
                "':'",
                "':'"
            ],
            "<COMMENT>": [
                "# Only tag image if container was built successfully."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "05b6b5ecd55e44f4bdf5bf2550c5e55e": {
        "code_string": "def remove_bidi_marks(string, positions_to_this = None, position_from_this_list = None, embedding_levels = None) :\n     \"removes the bidi and boundary-neutral marks out of a string\\n\" \\\n     \"and the accompanying lists. It implements rule X9 of the Unicode\\n\" \\\n     \"Bidirectional Algorithm available at\\n\" \\\n     \"http://www.unicode.org/reports/tr9/#X9, with the exception that it removes\\n\" \\\n     \"U+200E LEFT-TO-RIGHT MARK and U+200F RIGHT-TO-LEFT MARK too.\\n\" \\\n     \"\\n\" \\\n     \"If any of the input lists are None, the list is skipped. If str is the\\n\" \\\n     \"visual string, then positions_to_this is positions_L_to_V and\\n\" \\\n     \"position_from_this_list is positions_V_to_L; if str is the logical\\n\" \\\n     \"string, the other way. Moreover, the position maps should be filled with\\n\" \\\n     \"valid entries.\\n\" \\\n     \"\\n\" \\\n     \"A position map pointing to a removed character is filled with \\-1. By the\\n\" \\\n     \"way, you should not use embedding_levels if str is visual string.\\n\" \\\n     \"\\n\" \\\n     \"For best results this function should be run on a whole paragraph, not\\n\" \\\n     \"lines; but feel free to do otherwise if you know what you are doing.\\n\" \\\n     \"\\n\" \\\n     \" Returns a 4-tuple, (new_string, new_positions_to_this, new_position_from_this_list, new_embedding_levels)\"\n     # Original header file also says \u201cDeprecated. Use fribidi_remove_special_chars instead.\u201d\n     # But I cannot find this function anywhere...\n     c_string = str_to_chars(string)\n     if positions_to_this != None :\n         c_positions_to_this = seq_to_ct(positions_to_this, FRIBIDI.StrIndex)\n     else :\n         c_positions_to_this = None\n     #end if\n     if position_from_this_list != None :\n         c_position_from_this_list = seq_to_ct(position_from_this_list, FRIBIDI.StrIndex)\n     else :\n         position_from_this_list = None\n     #end if\n     if embedding_levels != None :\n         c_embedding_levels = seq_to_ct(embedding_levels, FRIBIDI.StrIndex)\n     else :\n         c_embedding_levels = None\n     #end if\n     new_str_len = fribidi.fribidi_remove_bidi_marks \\\n         (c_string, len(string), c_positions_to_this, c_position_from_this_list, c_embedding_levels)\n     if new_str_len < 0 :\n         raise RuntimeError(\"fribidi_remove_bidi_marks returned negative length\")\n           # out of memory?\n     #end if\n     result = [chars_to_str(c_string[:new_str_len]), None, None, None]\n     if positions_to_this != None :\n         result[1] = tuple(c_positions_to_this)\n     #end if\n     if position_from_this_list != None :\n         result[2] = tuple(c_position_from_this_list)\n     #end if\n     if embedding_levels != None :\n         result[3] = tuple(c_embedding_levels)\n     #end if\n     return \\\n         tuple(result)\n #end remove_bidi_marks\n",
        "code_toks_joined": "def remove_bidi_marks ( string , positions_to_this = None , position_from_this_list = None , embedding_levels = None ) : <NEWLINE> <INDENT> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> c_string = str_to_chars ( string ) <NEWLINE> if positions_to_this != None : <NEWLINE> <INDENT> c_positions_to_this = seq_to_ct ( positions_to_this , FRIBIDI . StrIndex ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> c_positions_to_this = None <NEWLINE> <COMMENT> <NL> <DEDENT> if position_from_this_list != None : <NEWLINE> <INDENT> c_position_from_this_list = seq_to_ct ( position_from_this_list , FRIBIDI . StrIndex ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> position_from_this_list = None <NEWLINE> <COMMENT> <NL> <DEDENT> if embedding_levels != None : <NEWLINE> <INDENT> c_embedding_levels = seq_to_ct ( embedding_levels , FRIBIDI . StrIndex ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> c_embedding_levels = None <NEWLINE> <COMMENT> <NL> <DEDENT> new_str_len = fribidi . fribidi_remove_bidi_marks ( c_string , len ( string ) , c_positions_to_this , c_position_from_this_list , c_embedding_levels ) <NEWLINE> if new_str_len < 0 : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> result = [ chars_to_str ( c_string [ : new_str_len ] ) , None , None , None ] <NEWLINE> if positions_to_this != None : <NEWLINE> <INDENT> result [ 1 ] = tuple ( c_positions_to_this ) <NEWLINE> <COMMENT> <NL> <DEDENT> if position_from_this_list != None : <NEWLINE> <INDENT> result [ 2 ] = tuple ( c_position_from_this_list ) <NEWLINE> <COMMENT> <NL> <DEDENT> if embedding_levels != None : <NEWLINE> <INDENT> result [ 3 ] = tuple ( c_embedding_levels ) <NEWLINE> <COMMENT> <NL> <DEDENT> return tuple ( result ) <NEWLINE> <COMMENT> <NL> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"removes the bidi and boundary-neutral marks out of a string\\n\"",
                "\"and the accompanying lists. It implements rule X9 of the Unicode\\n\"",
                "\"Bidirectional Algorithm available at\\n\"",
                "\"http://www.unicode.org/reports/tr9/#X9, with the exception that it removes\\n\"",
                "\"U+200E LEFT-TO-RIGHT MARK and U+200F RIGHT-TO-LEFT MARK too.\\n\"",
                "\"\\n\"",
                "\"If any of the input lists are None, the list is skipped. If str is the\\n\"",
                "\"visual string, then positions_to_this is positions_L_to_V and\\n\"",
                "\"position_from_this_list is positions_V_to_L; if str is the logical\\n\"",
                "\"string, the other way. Moreover, the position maps should be filled with\\n\"",
                "\"valid entries.\\n\"",
                "\"\\n\"",
                "\"A position map pointing to a removed character is filled with \\-1. By the\\n\"",
                "\"way, you should not use embedding_levels if str is visual string.\\n\"",
                "\"\\n\"",
                "\"For best results this function should be run on a whole paragraph, not\\n\"",
                "\"lines; but feel free to do otherwise if you know what you are doing.\\n\"",
                "\"\\n\"",
                "\" Returns a 4-tuple, (new_string, new_positions_to_this, new_position_from_this_list, new_embedding_levels)\"",
                "\"fribidi_remove_bidi_marks returned negative length\""
            ],
            "<COMMENT>": [
                "# Original header file also says \u201cDeprecated. Use fribidi_remove_special_chars instead.\u201d",
                "# But I cannot find this function anywhere...",
                "#end if",
                "#end if",
                "#end if",
                "# out of memory?",
                "#end if",
                "#end if",
                "#end if",
                "#end if",
                "#end remove_bidi_marks"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3dbd25556c724796ad49cc3c72eac789": {
        "code_string": "def between(min, max, iterable):\n     \"\"\"\n     Determine whether the `iterable` contains no more than `size` items.\n     \"\"\"\n     if min < 0:\n         raise ValueError(\"'min' must be positive (or zero)\")\n     if min < 0:\n         raise ValueError(\"'max' must be positive (or zero)\")\n     if min > max:\n         raise ValueError(\"'max' must be greater or equal than 'min'\")\n",
        "code_toks_joined": "def between ( min , max , iterable ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if min < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if min < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if min > max : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Determine whether the `iterable` contains no more than `size` items.\n     \"\"\"",
                "\"'min' must be positive (or zero)\"",
                "\"'max' must be positive (or zero)\"",
                "\"'max' must be greater or equal than 'min'\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9ce22ba52e7844f091aca8e3e7053d01": {
        "code_string": "def __get_safe_conn(self, retry_count):\n         self.current_size += 1\n         c = self.unuse_list.pop()\n         if self.ping_check:\n             now = int(time())\n             timeout = now\n             if isinstance(int, self.ping_check):\n                 timeout = timeout - self.ping_check\n             if not hasattr(c, '__ping_check_timestamp'):\n                 c.__ping_check_timestamp = now\n             try:\n                 if c.__ping_check_timestamp < timeout:\n                     c.__ping_check_timestamp = now\n                     c.ping()\n             except:\n                 self.current_size -= 1\n                 if retry_count < 10: c = self.__get_conn(retry_count+1)\n         if c: self.inuse_list.add(c)\n         return c\n",
        "code_toks_joined": "def __get_safe_conn ( self , retry_count ) : <NEWLINE> <INDENT> self . current_size += 1 <NEWLINE> c = self . unuse_list . pop ( ) <NEWLINE> if self . ping_check : <NEWLINE> <INDENT> now = int ( time ( ) ) <NEWLINE> timeout = now <NEWLINE> if isinstance ( int , self . ping_check ) : <NEWLINE> <INDENT> timeout = timeout - self . ping_check <NEWLINE> <DEDENT> if not hasattr ( c , <STRING> ) : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> if c . __ping_check_timestamp < timeout : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> c . ping ( ) <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> self . current_size -= 1 <NEWLINE> if retry_count < 10 : c = self . __get_conn ( retry_count + 1 ) <NEWLINE> <DEDENT> <DEDENT> if c : self . inuse_list . add ( c ) <NEWLINE> return c <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'__ping_check_timestamp'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b4ad82d670ac495e925c2fa4b936851c": {
        "code_string": "def _set_registers(self, registers):\n         \"\"\"\n         Set the CPU registers.\n         \"\"\"\n         # 29 registers\n         buf = (ctypes.c_int32 * self.register_count)()\n         buf[:] = registers\n         self._vba.set_registers(registers)\n",
        "code_toks_joined": "def _set_registers ( self , registers ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> buf = ( ctypes . c_int32 * self . register_count ) ( ) <NEWLINE> buf [ : ] = registers <NEWLINE> self . _vba . set_registers ( registers ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Set the CPU registers.\n         \"\"\""
            ],
            "<COMMENT>": [
                "# 29 registers"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d1893e84514a4f229034aefaedf67e7a": {
        "code_string": "reason = e[SCWEIEA]['cause']\n",
        "code_toks_joined": "reason = e [ SCWEIEA ] [ <STRING> ] <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'cause'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c01b8a52ce914fcc98ec4c98e94e8481": {
        "code_string": "yield cls(arg)\n",
        "code_toks_joined": "yield cls ( arg ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8e36a24739f5465f8fa7f83c21530afc": {
        "code_string": "@pytest.fixture\n def local_android_download(request, monkeypatch, tmpdir):\n     \"\"\"py.test fixture provideing a AndroidWordList with local wordlist.\n     \"\"\"\n     for lang in ['de', 'en']:\n         dictfile = tmpdir / (\"%s_wordlist.combined.gz\" % lang)\n         src_path = os.path.join(\n             os.path.dirname(__file__), \"sample_short_wordlist_%s.gz\" % lang)\n         dictfile.write(base64.b64encode(open(src_path, \"rb\").read()))\n     fake_base_url = \"file://%s/%%s_wordlist.combined.gz\" % str(tmpdir)\n     monkeypatch.setattr(\n             \"diceware_list.libwordlist.AndroidWordList.base_url\",\n             fake_base_url)\n     return dictfile\n",
        "code_toks_joined": "@ pytest . fixture <NEWLINE> <INDENT> def local_android_download ( request , monkeypatch , tmpdir ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for lang in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> dictfile = tmpdir / ( <STRING> % lang ) <NEWLINE> src_path = os . path . join ( <NEWLINE> <INDENT> os . path . dirname ( __file__ ) , <STRING> % lang ) <NEWLINE> <DEDENT> dictfile . write ( base64 . b64encode ( open ( src_path , <STRING> ) . read ( ) ) ) <NEWLINE> <DEDENT> fake_base_url = <STRING> % str ( tmpdir ) <NEWLINE> monkeypatch . setattr ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> fake_base_url ) <NEWLINE> <DEDENT> return dictfile <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"py.test fixture provideing a AndroidWordList with local wordlist.\n     \"\"\"",
                "'de'",
                "'en'",
                "\"%s_wordlist.combined.gz\"",
                "\"sample_short_wordlist_%s.gz\"",
                "\"rb\"",
                "\"file://%s/%%s_wordlist.combined.gz\"",
                "\"diceware_list.libwordlist.AndroidWordList.base_url\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4ab7957e8ef541e4ba6aa46453fe99a4": {
        "code_string": "end_time = time.time()\n         self.log.debug(\"Built add request of %s docs in %0.2f seconds.\", len(docs), end_time - start_time)\n         return self._update(m, commit=commit, waitFlush=waitFlush, waitSearcher=waitSearcher)\n",
        "code_toks_joined": "end_time = time . time ( ) <NEWLINE> <INDENT> self . log . debug ( <STRING> , len ( docs ) , end_time - start_time ) <NEWLINE> return self . _update ( m , commit = commit , waitFlush = waitFlush , waitSearcher = waitSearcher ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Built add request of %s docs in %0.2f seconds.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6eb35a106f834737a9fba64191746704": {
        "code_string": "def get_kind_ids(self, txn, kind):\n         ENTITY_COUNTER = METADATA_VERTEX_COUNTER if kind == KIND_VERTEX else METADATA_EDGE_COUNTER\n         METADATA_ID_LIST_PREFIX = METADATA_VERTEX_ID_LIST_PREFIX if kind == KIND_VERTEX else METADATA_EDGE_ID_LIST_PREFIX\n         limit = int(self._graph._get(None, ENTITY_COUNTER)) / CHUNK_SIZE\n         keys = [build_key(METADATA_ID_LIST_PREFIX, i) for i in range(0, limit + 1)]\n         list_entity_ids = self._graph._bulk_get_lst(txn, keys)\n         for entity_ids in list_entity_ids:\n             if entity_ids != UNDEFINED:\n                 for entity_id in entity_ids:\n                     yield entity_id\n",
        "code_toks_joined": "def get_kind_ids ( self , txn , kind ) : <NEWLINE> <INDENT> ENTITY_COUNTER = METADATA_VERTEX_COUNTER if kind == KIND_VERTEX else METADATA_EDGE_COUNTER <NEWLINE> METADATA_ID_LIST_PREFIX = METADATA_VERTEX_ID_LIST_PREFIX if kind == KIND_VERTEX else METADATA_EDGE_ID_LIST_PREFIX <NEWLINE> limit = int ( self . _graph . _get ( None , ENTITY_COUNTER ) ) / CHUNK_SIZE <NEWLINE> keys = [ build_key ( METADATA_ID_LIST_PREFIX , i ) for i in range ( 0 , limit + 1 ) ] <NEWLINE> list_entity_ids = self . _graph . _bulk_get_lst ( txn , keys ) <NEWLINE> for entity_ids in list_entity_ids : <NEWLINE> <INDENT> if entity_ids != UNDEFINED : <NEWLINE> <INDENT> for entity_id in entity_ids : <NEWLINE> <INDENT> yield entity_id <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b8d471f9b604899b95b4638071eb0a2": {
        "code_string": "return proof_data\n",
        "code_toks_joined": "return proof_data <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8d7b01c21d2c49a49e67ec8f4a2b1736": {
        "code_string": "check_connection_callback(connection, prev_status)\n     except:\n         raise\n     finally:\n         if initialize_vcx:\n             try:\n                 shutdown(False)\n             except:\n                 raise\n",
        "code_toks_joined": "check_connection_callback ( connection , prev_status ) <NEWLINE> <INDENT> except : <NEWLINE> <INDENT> raise <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> if initialize_vcx : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> shutdown ( False ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> raise <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "60341b87026a494895099e34902e8e76": {
        "code_string": "# JSON support\n         utg_lib_obj = dict()            # untagged library object\n         utg_lib_obj[\"Package\"] = node.pn\n         utg_lib_obj[\"Standard Package\"] = u\n         utg_lib_obj[\"Library\"] = \"Unknown\"\n         utg_lib_obj[\"Popularity\"] = int(c)\n         utg_lib_obj[\"Weight\"] = node.weight\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> utg_lib_obj = dict ( ) <COMMENT> <NEWLINE> utg_lib_obj [ <STRING> ] = node . pn <NEWLINE> utg_lib_obj [ <STRING> ] = u <NEWLINE> utg_lib_obj [ <STRING> ] = <STRING> <NEWLINE> utg_lib_obj [ <STRING> ] = int ( c ) <NEWLINE> utg_lib_obj [ <STRING> ] = node . weight <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# JSON support",
                "# untagged library object"
            ],
            "<STRING>": [
                "\"Package\"",
                "\"Standard Package\"",
                "\"Library\"",
                "\"Unknown\"",
                "\"Popularity\"",
                "\"Weight\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "619779be904b4f8cb59697224d9214ea": {
        "code_string": "def which_org_has(repo):\n     orgs = Orgs()\n     for org_info in orgs.get_all_organisations():\n         org_repo = Repo(org_info['repos_url'])\n         for arepo in org_repo.get_all_repos():\n             if repo == arepo['name']:\n                 return arepo['login']\n     return None\n",
        "code_toks_joined": "def which_org_has ( repo ) : <NEWLINE> <INDENT> orgs = Orgs ( ) <NEWLINE> for org_info in orgs . get_all_organisations ( ) : <NEWLINE> <INDENT> org_repo = Repo ( org_info [ <STRING> ] ) <NEWLINE> for arepo in org_repo . get_all_repos ( ) : <NEWLINE> <INDENT> if repo == arepo [ <STRING> ] : <NEWLINE> <INDENT> return arepo [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'repos_url'",
                "'name'",
                "'login'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "06ce8e8cc58845e8a3f04bdf3847a1fb": {
        "code_string": "num_frames = len(buf)/in_channels\n     output = np.zeros(num_frames*out_channels, dtype=np.float32)\n     if in_channels < out_channels:\n         in_channel = 0\n         for out_channel in range(out_channels):\n             output[out_channel::out_channels] += buf[in_channel::in_channels]\n             in_channel = (in_channel + 1) % in_channels\n     elif out_channels > in_channels:\n         out_channel = 0\n         for in_channel in range(out_channels):\n             output[out_channel::out_channels] += buf[in_channel::in_channels]\n             out_channel = (out_channel + 1) % out_channels\n",
        "code_toks_joined": "num_frames = len ( buf ) / in_channels <NEWLINE> <INDENT> output = np . zeros ( num_frames * out_channels , dtype = np . float32 ) <NEWLINE> if in_channels < out_channels : <NEWLINE> <INDENT> in_channel = 0 <NEWLINE> for out_channel in range ( out_channels ) : <NEWLINE> <INDENT> output [ out_channel : : out_channels ] += buf [ in_channel : : in_channels ] <NEWLINE> in_channel = ( in_channel + 1 ) % in_channels <NEWLINE> <DEDENT> <DEDENT> elif out_channels > in_channels : <NEWLINE> <INDENT> out_channel = 0 <NEWLINE> for in_channel in range ( out_channels ) : <NEWLINE> <INDENT> output [ out_channel : : out_channels ] += buf [ in_channel : : in_channels ] <NEWLINE> out_channel = ( out_channel + 1 ) % out_channels <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7030be53235549948d9fdd70437e81ed": {
        "code_string": "if type(other).__name__ == \"PiecewiseFieldExpansion\":\n             added = [False for other_fex in other.expansion_list]\n             for self_fex in self.expansion_list:\n                 fex = copy.deepcopy(self_fex)\n                 for i, other_fex in enumerate(other.expansion_list):\n                     if (not added[i]) and self_fex.compatible(other_fex):\n                         fex = fex + other_fex\n                         added[i] = True\n                 pfe_sum.expansion_list.append(fex)\n             for i, other_fex in enumerate(other.expansion_list):\n                 if not added[i]:\n                     pfe_sum.expansion_list.append(other_fex)\n         else:\n             added = False\n             for self_fex in self.expansion_list:\n                 fex = copy.deepcopy(self_fex)\n                 if (not added) and fex.compatible(other):\n                     pfe_sum.expansion_list.append(fex + other)\n                     added = True\n                 else:\n                     pfe_sum.expansion_list.append(fex)\n             if not added:\n                 pfe_sum.expansion_list.append(fex)\n",
        "code_toks_joined": "if type ( other ) . __name__ == <STRING> : <NEWLINE> <INDENT> added = [ False for other_fex in other . expansion_list ] <NEWLINE> for self_fex in self . expansion_list : <NEWLINE> <INDENT> fex = copy . deepcopy ( self_fex ) <NEWLINE> for i , other_fex in enumerate ( other . expansion_list ) : <NEWLINE> <INDENT> if ( not added [ i ] ) and self_fex . compatible ( other_fex ) : <NEWLINE> <INDENT> fex = fex + other_fex <NEWLINE> added [ i ] = True <NEWLINE> <DEDENT> <DEDENT> pfe_sum . expansion_list . append ( fex ) <NEWLINE> <DEDENT> for i , other_fex in enumerate ( other . expansion_list ) : <NEWLINE> <INDENT> if not added [ i ] : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( other_fex ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> added = False <NEWLINE> for self_fex in self . expansion_list : <NEWLINE> <INDENT> fex = copy . deepcopy ( self_fex ) <NEWLINE> if ( not added ) and fex . compatible ( other ) : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( fex + other ) <NEWLINE> added = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( fex ) <NEWLINE> <DEDENT> <DEDENT> if not added : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( fex ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"PiecewiseFieldExpansion\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2e6eb5b74b554a13be50cd64fc8d4617": {
        "code_string": "def render(self, orcname: str, fname: str,\n                sr: int = 48000, ksmps: int = 1) -> str:\n         sconame = self.to_file(fname)\n         outname = fname + \".wav\"\n         call([\"csound\",\n               \"--sample-rate=\" + str(sr),\n               \"--control-rate=\" + str(sr / ksmps),\n               \"--logfile=\" + fname + \".log\",\n               \"--format=wav\",\n               \"--output=\" + outname,\n               \"--format=24bit\",\n               \"--nodisplays\",\n               orcname,\n               sconame])\n         return fname\n",
        "code_toks_joined": "def render ( self , orcname : str , fname : str , <NEWLINE> <INDENT> sr : int = 48000 , ksmps : int = 1 ) -> str : <NEWLINE> sconame = self . to_file ( fname ) <NEWLINE> outname = fname + <STRING> <NEWLINE> call ( [ <STRING> , <NEWLINE> <STRING> + str ( sr ) , <NEWLINE> <STRING> + str ( sr / ksmps ) , <NEWLINE> <STRING> + fname + <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> + outname , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> orcname , <NEWLINE> sconame ] ) <NEWLINE> return fname <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\".wav\"",
                "\"csound\"",
                "\"--sample-rate=\"",
                "\"--control-rate=\"",
                "\"--logfile=\"",
                "\".log\"",
                "\"--format=wav\"",
                "\"--output=\"",
                "\"--format=24bit\"",
                "\"--nodisplays\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ac9512c1d80a4094a4edf00218356953": {
        "code_string": "self.assertEqual(mock_adapter1.device, 'foo')\n         self.assertEqual(mock_adapter1.device, 'foo')\n",
        "code_toks_joined": "self . assertEqual ( mock_adapter1 . device , <STRING> ) <NEWLINE> <INDENT> self . assertEqual ( mock_adapter1 . device , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'foo'",
                "'foo'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6598328288774e2f9dcc441c1ad55b9c": {
        "code_string": "exposed_members = members if members else self._public_members()\n         exclude = list(exclude or [])\n         if not exclude_inherited:\n             for base in inspect.getmro(type(obj))[1:]:\n                 exclude += dir(base)\n",
        "code_toks_joined": "exposed_members = members if members else self . _public_members ( ) <NEWLINE> <INDENT> exclude = list ( exclude or [ ] ) <NEWLINE> if not exclude_inherited : <NEWLINE> <INDENT> for base in inspect . getmro ( type ( obj ) ) [ 1 : ] : <NEWLINE> <INDENT> exclude += dir ( base ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8a8dc49343ea4d758a1bc38b2715b22a": {
        "code_string": "cfg.action(discr, introspectables=(intr,))\n     log.info(\"Add layer: %s path:%s\"%(layer, directory))\n",
        "code_toks_joined": "cfg . action ( discr , introspectables = ( intr , ) ) <NEWLINE> <INDENT> log . info ( <STRING> % ( layer , directory ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Add layer: %s path:%s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "55630ef5e1214678a61070dfa5f0dd44": {
        "code_string": "def get_final_info(self):\n         response = self.session.get(\n             self.info_url, headers=self.headers)\n         info = json.loads(response.text)\n         status = info['data']['status']\n         if status == 'tzpzz':\n             self.sendmail('\u623f\u6e90\u72b6\u6001\u5df2\u66f4\u65b0', '\u72b6\u6001\u66f4\u65b0\u4e86')\n         else:\n             raise NotDoneError(status)\n",
        "code_toks_joined": "def get_final_info ( self ) : <NEWLINE> <INDENT> response = self . session . get ( <NEWLINE> <INDENT> self . info_url , headers = self . headers ) <NEWLINE> <DEDENT> info = json . loads ( response . text ) <NEWLINE> status = info [ <STRING> ] [ <STRING> ] <NEWLINE> if status == <STRING> : <NEWLINE> <INDENT> self . sendmail ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NotDoneError ( status ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'data'",
                "'status'",
                "'tzpzz'",
                "'\u623f\u6e90\u72b6\u6001\u5df2\u66f4\u65b0'",
                "'\u72b6\u6001\u66f4\u65b0\u4e86'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8a9c9a4aedc0425da231f9858f4d1729": {
        "code_string": "for ratingTr in appRatingTable:\n             inputs = ratingTr.xpath('.//input')\n             if len(inputs) != 2:\n                 continue\n             appRating = {'name': inputs[0].attrib['name'], 'ratings': []}\n             for inpt in inputs:\n                 appRating['ratings'].append(inpt.attrib['value'])\n             appRatings.append(appRating)\n",
        "code_toks_joined": "for ratingTr in appRatingTable : <NEWLINE> <INDENT> inputs = ratingTr . xpath ( <STRING> ) <NEWLINE> if len ( inputs ) != 2 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> appRating = { <STRING> : inputs [ 0 ] . attrib [ <STRING> ] , <STRING> : [ ] } <NEWLINE> for inpt in inputs : <NEWLINE> <INDENT> appRating [ <STRING> ] . append ( inpt . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> appRatings . append ( appRating ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.//input'",
                "'name'",
                "'name'",
                "'ratings'",
                "'ratings'",
                "'value'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e988fc72be984e9db1b08932c03f3d47": {
        "code_string": "def scale(self, image: Image, position: geometry.Point,\n               width: int, height: int):\n         width, height = self.calculate_resolution(image, width, height)\n         image_width, image_height = image.width, image.height\n         if image_width < image_height:\n             image_height = int(image_height * width / image_width)\n             image_width = width\n         else:\n             image_width = int(image_width * height / image_height)\n             image_height = height\n         offset_x = self.get_offset(position.x, width, image_width)\n         offset_y = self.get_offset(position.y, height, image_height)\n",
        "code_toks_joined": "def scale ( self , image : Image , position : geometry . Point , <NEWLINE> <INDENT> width : int , height : int ) : <NEWLINE> width , height = self . calculate_resolution ( image , width , height ) <NEWLINE> image_width , image_height = image . width , image . height <NEWLINE> if image_width < image_height : <NEWLINE> image_height = int ( image_height * width / image_width ) <NEWLINE> image_width = width <NEWLINE> else : <NEWLINE> image_width = int ( image_width * height / image_height ) <NEWLINE> image_height = height <NEWLINE> offset_x = self . get_offset ( position . x , width , image_width ) <NEWLINE> offset_y = self . get_offset ( position . y , height , image_height ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "621c4d9443b9473fb4465b6579ef7d70": {
        "code_string": "# Pre-process outputs\n             if(not debug):\n                 if(type_out == 'categorical'):\n                     nClasses = len(self.classes[id_out])                \n                     y = np_utils.to_categorical(y, nClasses).astype(np.uint8)\n                 elif(type_out == 'binary'):\n                     y = np.array(y).astype(np.uint8)\n                 elif(type_out == 'text'):\n                     y = self.loadText(y, self.vocabulary[id_out], self.max_text_len[id_out], self.text_offset[id_in])\n                     #if max_len == 0:\n                     y_aux = np.zeros(list(y.shape)+[self.n_classes_text[id_out]]).astype(np.uint8)\n                     for idx in range(y.shape[0]):\n                         y_aux[idx] = np_utils.to_categorical(y[idx], self.n_classes_text[id_out]).astype(np.uint8)\n                     y = y_aux\n             Y.append(y)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if ( not debug ) : <NEWLINE> <INDENT> if ( type_out == <STRING> ) : <NEWLINE> <INDENT> nClasses = len ( self . classes [ id_out ] ) <NEWLINE> y = np_utils . to_categorical ( y , nClasses ) . astype ( np . uint8 ) <NEWLINE> <DEDENT> elif ( type_out == <STRING> ) : <NEWLINE> <INDENT> y = np . array ( y ) . astype ( np . uint8 ) <NEWLINE> <DEDENT> elif ( type_out == <STRING> ) : <NEWLINE> <INDENT> y = self . loadText ( y , self . vocabulary [ id_out ] , self . max_text_len [ id_out ] , self . text_offset [ id_in ] ) <NEWLINE> <COMMENT> <NL> y_aux = np . zeros ( list ( y . shape ) + [ self . n_classes_text [ id_out ] ] ) . astype ( np . uint8 ) <NEWLINE> for idx in range ( y . shape [ 0 ] ) : <NEWLINE> <INDENT> y_aux [ idx ] = np_utils . to_categorical ( y [ idx ] , self . n_classes_text [ id_out ] ) . astype ( np . uint8 ) <NEWLINE> <DEDENT> y = y_aux <NEWLINE> <DEDENT> <DEDENT> Y . append ( y ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Pre-process outputs",
                "#if max_len == 0:"
            ],
            "<STRING>": [
                "'categorical'",
                "'binary'",
                "'text'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7a750b01ef1f458298102044fc5a224e": {
        "code_string": "scheduler = Scheduler()\n             if not self.without_checks:\n                 for name, server in servers.items():\n                     scheduler.register(name, server)\n",
        "code_toks_joined": "scheduler = Scheduler ( ) <NEWLINE> <INDENT> if not self . without_checks : <NEWLINE> <INDENT> for name , server in servers . items ( ) : <NEWLINE> <INDENT> scheduler . register ( name , server ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f05d265907314bf18474d25bebfe9349": {
        "code_string": "def save(self):\n         with open(self.location, 'wb') as fle:\n             return json.dump(fle, self.data)\n",
        "code_toks_joined": "def save ( self ) : <NEWLINE> <INDENT> with open ( self . location , <STRING> ) as fle : <NEWLINE> <INDENT> return json . dump ( fle , self . data ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'wb'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3e61a996b1e341bd8f6c3883da858b84": {
        "code_string": "def tuples(*args):\n     res = ()\n     for arg in args:\n         res += tuple(arg) if is_list(args) else (arg, )\n",
        "code_toks_joined": "def tuples ( * args ) : <NEWLINE> <INDENT> res = ( ) <NEWLINE> for arg in args : <NEWLINE> <INDENT> res += tuple ( arg ) if is_list ( args ) else ( arg , ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6e2a6f21a8e64e0c9c19ecb6e6cd6de7": {
        "code_string": "# upload a new forecast\n     upload_file_job = model.upload_forecast(timezero_date, forecast_csv_file)\n     busy_poll_upload_file_job(upload_file_job)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> upload_file_job = model . upload_forecast ( timezero_date , forecast_csv_file ) <NEWLINE> busy_poll_upload_file_job ( upload_file_job ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# upload a new forecast"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0ccbe5516e2e4a64a914ff7a781cb8d8": {
        "code_string": "# KineticsRun instance with dosweep = True\n @pytest.fixture(scope='session')\n def k_sweep(hpar, par):\n     par_sweep = deepcopy(par)\n     par_sweep['sweep']['dosweep'] = True\n     k_sweep = pm.KineticsRun(hpar, **par)\n     k_sweep.solveode()\n     return k_sweep\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> @ pytest . fixture ( scope = <STRING> ) <NEWLINE> def k_sweep ( hpar , par ) : <NEWLINE> <INDENT> par_sweep = deepcopy ( par ) <NEWLINE> par_sweep [ <STRING> ] [ <STRING> ] = True <NEWLINE> k_sweep = pm . KineticsRun ( hpar , ** par ) <NEWLINE> k_sweep . solveode ( ) <NEWLINE> return k_sweep <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# KineticsRun instance with dosweep = True"
            ],
            "<STRING>": [
                "'session'",
                "'sweep'",
                "'dosweep'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "471b7c805e414a5ba34d9a3fc9b0cf6b": {
        "code_string": "if i == 0 or next in 'aeiou' or s[i-1] in 'aeiou':\n",
        "code_toks_joined": "if i == 0 or next in <STRING> or s [ i - 1 ] in <STRING> : <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'aeiou'",
                "'aeiou'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3f0f4a3f8155461f947910dc1098ab7a": {
        "code_string": "agent.create_database(db_config['name'])\n         agent.create_retention_policy('%s_rp' % db_config['name'],\n                                       db_config['duration'],\n                                       db_config['replication'],\n                                       db_config['name'])\n         logger.info('database \"%s\" created successfully', config['name'])\n     except:\n         pass\n",
        "code_toks_joined": "agent . create_database ( db_config [ <STRING> ] ) <NEWLINE> <INDENT> agent . create_retention_policy ( <STRING> % db_config [ <STRING> ] , <NEWLINE> <INDENT> db_config [ <STRING> ] , <NEWLINE> db_config [ <STRING> ] , <NEWLINE> db_config [ <STRING> ] ) <NEWLINE> <DEDENT> logger . info ( <STRING> , config [ <STRING> ] ) <NEWLINE> except : <NEWLINE> pass <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'name'",
                "'%s_rp'",
                "'name'",
                "'duration'",
                "'replication'",
                "'name'",
                "'database \"%s\" created successfully'",
                "'name'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "10176eac28374dc49863fcaf64bb99f6": {
        "code_string": "def get_sender_organization_id(from_email, to_email, allowed_senders, config):\n     if from_email and to_email:\n         from_email = from_email.lower().strip()\n         to_email = to_email.lower().strip()\n         for row in allowed_senders:\n             if is_email_match(from_email, row['from_address']) and is_email_match(to_email, row['to_address']):\n                 return row['organization_id']\n         default_sender_to_address = config.get('default_sender_to_address')\n         default_sender_organization_id = config.get('default_sender_organization_id')\n         if default_sender_to_address and default_sender_organization_id:\n             default_sender_to_address = default_sender_to_address.lower().strip()\n             if is_email_match(from_email, default_sender_to_address):\n                 return default_sender_organization_id\n     return None\n",
        "code_toks_joined": "def get_sender_organization_id ( from_email , to_email , allowed_senders , config ) : <NEWLINE> <INDENT> if from_email and to_email : <NEWLINE> <INDENT> from_email = from_email . lower ( ) . strip ( ) <NEWLINE> to_email = to_email . lower ( ) . strip ( ) <NEWLINE> for row in allowed_senders : <NEWLINE> <INDENT> if is_email_match ( from_email , row [ <STRING> ] ) and is_email_match ( to_email , row [ <STRING> ] ) : <NEWLINE> <INDENT> return row [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> default_sender_to_address = config . get ( <STRING> ) <NEWLINE> default_sender_organization_id = config . get ( <STRING> ) <NEWLINE> if default_sender_to_address and default_sender_organization_id : <NEWLINE> <INDENT> default_sender_to_address = default_sender_to_address . lower ( ) . strip ( ) <NEWLINE> if is_email_match ( from_email , default_sender_to_address ) : <NEWLINE> <INDENT> return default_sender_organization_id <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'from_address'",
                "'to_address'",
                "'organization_id'",
                "'default_sender_to_address'",
                "'default_sender_organization_id'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bc0e9a9aeb4a4769b1730d422ac6cf52": {
        "code_string": "if self.save_data:\n             if self.average:\n                 self.avg_buffer[irep,:] = response\n                 if irep == self.nreps -1:\n                     avg_response = self.avg_buffer.mean(axis=0)\n                     self.datafile.append(self.current_dataset_name, response)\n                     self.avg_buffer = np.zeros_like(self.avg_buffer)\n             else:\n                 self.datafile.append(self.current_dataset_name, response)\n",
        "code_toks_joined": "if self . save_data : <NEWLINE> <INDENT> if self . average : <NEWLINE> <INDENT> self . avg_buffer [ irep , : ] = response <NEWLINE> if irep == self . nreps - 1 : <NEWLINE> <INDENT> avg_response = self . avg_buffer . mean ( axis = 0 ) <NEWLINE> self . datafile . append ( self . current_dataset_name , response ) <NEWLINE> self . avg_buffer = np . zeros_like ( self . avg_buffer ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . datafile . append ( self . current_dataset_name , response ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ce679a2231ed4eabadddc4c9cdbbed9e": {
        "code_string": "pytest.raises(TypeError, WSignalCallbackProto)\n \tpytest.raises(NotImplementedError, WSignalCallbackProto.__call__, None, 'signal', S(), 1)\n",
        "code_toks_joined": "pytest . raises ( TypeError , WSignalCallbackProto ) <NEWLINE> <INDENT> pytest . raises ( NotImplementedError , WSignalCallbackProto . __call__ , None , <STRING> , S ( ) , 1 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'signal'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c7db7d74086a42a0b754d0182a87855a": {
        "code_string": "class ClassificationError(LossFunction):\n     @output_loss\n     def loss(self, output, targets):\n         return (np.argmax(output, axis=-1) ==\n                 np.argmax(np.nan_to_num(targets), axis=-1))\n",
        "code_toks_joined": "class ClassificationError ( LossFunction ) : <NEWLINE> <INDENT> @ output_loss <NEWLINE> def loss ( self , output , targets ) : <NEWLINE> <INDENT> return ( np . argmax ( output , axis = - 1 ) == <NEWLINE> <INDENT> np . argmax ( np . nan_to_num ( targets ) , axis = - 1 ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "428f52d018bb445694bcc7a2ca11869b": {
        "code_string": "if date_step:\n             return self._methods(method.lower())(**kwargs)\n",
        "code_toks_joined": "if date_step : <NEWLINE> <INDENT> return self . _methods ( method . lower ( ) ) ( ** kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b29d9ad3b8fa4ddd975bc29e5bfaa625": {
        "code_string": "def char_at(text, i):\n     if len(text) - 1 <= i:\n         return -1\n     return ord(text[i])\n",
        "code_toks_joined": "def char_at ( text , i ) : <NEWLINE> <INDENT> if len ( text ) - 1 <= i : <NEWLINE> <INDENT> return - 1 <NEWLINE> <DEDENT> return ord ( text [ i ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7e3f248258f84839b34e32f399c17648": {
        "code_string": "data_copy = data.copy()\n     for struct_nm in xrsdefs.structure_names:\n         print('Training binary classifier for '+struct_nm+' structures')\n         model_id = struct_nm+'_binary'\n         model = Classifier(model_id, None)\n         labels = [struct_nm in sys_cls for sys_cls in all_sys_cls]\n         data_copy.loc[:,model_id] = labels \n         if 'main_classifiers' in classification_models.keys() \\\n         and model_id in classification_models['main_classifiers'] \\\n         and classification_models['main_classifiers'][model_id].trained:\n             old_pars = classification_models['main_classifiers'][model_id].model.get_params()\n             model.model.set_params(alpha=old_pars['alpha'], l1_ratio=old_pars['l1_ratio'])\n         model.train(data_copy, hyper_parameters_search=hyper_parameters_search)\n         if model.trained:\n             f1_score = model.cross_valid_results['F1_score_averaged_not_weighted']\n             acc = model.cross_valid_results['accuracy']\n             print('--> average unweighted f1: {}, accuracy: {}'.format(f1_score,acc))\n         else:\n             print('--> {} untrainable- default value: {}'.format(model_id,model.default_val))\n         cls_models['main_classifiers'][struct_nm] = model\n",
        "code_toks_joined": "data_copy = data . copy ( ) <NEWLINE> <INDENT> for struct_nm in xrsdefs . structure_names : <NEWLINE> <INDENT> print ( <STRING> + struct_nm + <STRING> ) <NEWLINE> model_id = struct_nm + <STRING> <NEWLINE> model = Classifier ( model_id , None ) <NEWLINE> labels = [ struct_nm in sys_cls for sys_cls in all_sys_cls ] <NEWLINE> data_copy . loc [ : , model_id ] = labels <NEWLINE> if <STRING> in classification_models . keys ( ) and model_id in classification_models [ <STRING> ] and classification_models [ <STRING> ] [ model_id ] . trained : <NEWLINE> <INDENT> old_pars = classification_models [ <STRING> ] [ model_id ] . model . get_params ( ) <NEWLINE> model . model . set_params ( alpha = old_pars [ <STRING> ] , l1_ratio = old_pars [ <STRING> ] ) <NEWLINE> <DEDENT> model . train ( data_copy , hyper_parameters_search = hyper_parameters_search ) <NEWLINE> if model . trained : <NEWLINE> <INDENT> f1_score = model . cross_valid_results [ <STRING> ] <NEWLINE> acc = model . cross_valid_results [ <STRING> ] <NEWLINE> print ( <STRING> . format ( f1_score , acc ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> . format ( model_id , model . default_val ) ) <NEWLINE> <DEDENT> cls_models [ <STRING> ] [ struct_nm ] = model <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Training binary classifier for '",
                "' structures'",
                "'_binary'",
                "'main_classifiers'",
                "'main_classifiers'",
                "'main_classifiers'",
                "'main_classifiers'",
                "'alpha'",
                "'l1_ratio'",
                "'F1_score_averaged_not_weighted'",
                "'accuracy'",
                "'--> average unweighted f1: {}, accuracy: {}'",
                "'--> {} untrainable- default value: {}'",
                "'main_classifiers'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b1d66e240f394d58a1833145f3cb2fdf": {
        "code_string": "def _train_models(self,display):\n         # TODO: input widget for downsampling distance?\n         # TODO: toggles for hyperparam selection? feature selection?\n         dataset_dir = self._vars['io_control']['dataset_dir'].get()\n         output_dir = self._vars['io_control']['output_dir'].get() \n         model_config_path = os.path.join(dataset_dir,'model_config.yml')\n         self._print_to_listbox(display,'LOADING DATASET FROM: {}'.format(dataset_dir))\n         df, idx_df = read_local_dataset(dataset_dir,downsampling_distance=1.,\n                 message_callback=partial(self._print_to_listbox,display))\n         self._print_to_listbox(display,'---- FINISHED LOADING DATASET ----')\n         self._print_to_listbox(display,'BEGINNING TO TRAIN MODELS')\n         self._print_to_listbox(display,'MODEL CONFIG FILE PATH: {}'.format(model_config_path))\n         reg_mods, cls_mods = train_from_dataframe(df, \n                 train_hyperparameters=False, select_features=False,\n                 output_dir=output_dir, model_config_path=model_config_path,\n                 message_callback=partial(self._print_to_listbox,display)\n                 )\n         self._print_to_listbox(display,'---- FINISHED TRAINING ----')\n",
        "code_toks_joined": "def _train_models ( self , display ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> dataset_dir = self . _vars [ <STRING> ] [ <STRING> ] . get ( ) <NEWLINE> output_dir = self . _vars [ <STRING> ] [ <STRING> ] . get ( ) <NEWLINE> model_config_path = os . path . join ( dataset_dir , <STRING> ) <NEWLINE> self . _print_to_listbox ( display , <STRING> . format ( dataset_dir ) ) <NEWLINE> df , idx_df = read_local_dataset ( dataset_dir , downsampling_distance = 1. , <NEWLINE> <INDENT> message_callback = partial ( self . _print_to_listbox , display ) ) <NEWLINE> <DEDENT> self . _print_to_listbox ( display , <STRING> ) <NEWLINE> self . _print_to_listbox ( display , <STRING> ) <NEWLINE> self . _print_to_listbox ( display , <STRING> . format ( model_config_path ) ) <NEWLINE> reg_mods , cls_mods = train_from_dataframe ( df , <NEWLINE> <INDENT> train_hyperparameters = False , select_features = False , <NEWLINE> output_dir = output_dir , model_config_path = model_config_path , <NEWLINE> message_callback = partial ( self . _print_to_listbox , display ) <NEWLINE> ) <NEWLINE> <DEDENT> self . _print_to_listbox ( display , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# TODO: input widget for downsampling distance?",
                "# TODO: toggles for hyperparam selection? feature selection?"
            ],
            "<STRING>": [
                "'io_control'",
                "'dataset_dir'",
                "'io_control'",
                "'output_dir'",
                "'model_config.yml'",
                "'LOADING DATASET FROM: {}'",
                "'---- FINISHED LOADING DATASET ----'",
                "'BEGINNING TO TRAIN MODELS'",
                "'MODEL CONFIG FILE PATH: {}'",
                "'---- FINISHED TRAINING ----'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "db3daa551ca948b6a1ee1981da582d25": {
        "code_string": "def generate_rand_from_pdf(pdf, x_grid, N):\n     \"\"\"Method to generate 'N' no. of random numbers from input probability distribution function (pdf) in form of kernel density\"\"\"\n     cdf = np.cumsum(pdf)\n     cdf = cdf / cdf[-1]\n     values = np.random.rand(N)\n     value_bins = np.searchsorted(cdf, values)\n     random_from_cdf, nz = x_grid[value_bins], cdf[value_bins]\n     return random_from_cdf, nz\n",
        "code_toks_joined": "def generate_rand_from_pdf ( pdf , x_grid , N ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cdf = np . cumsum ( pdf ) <NEWLINE> cdf = cdf / cdf [ - 1 ] <NEWLINE> values = np . random . rand ( N ) <NEWLINE> value_bins = np . searchsorted ( cdf , values ) <NEWLINE> random_from_cdf , nz = x_grid [ value_bins ] , cdf [ value_bins ] <NEWLINE> return random_from_cdf , nz <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Method to generate 'N' no. of random numbers from input probability distribution function (pdf) in form of kernel density\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9bfb64acb47b4f80b19bba7e33c7343b": {
        "code_string": "str_targets = collections.defaultdict(list)\n         for target_type, targets in targets_dict.iteritems():\n             for target in targets:\n                 str_dependencies[target_type].append(target.unexpanded_id)\n",
        "code_toks_joined": "str_targets = collections . defaultdict ( list ) <NEWLINE> <INDENT> for target_type , targets in targets_dict . iteritems ( ) : <NEWLINE> <INDENT> for target in targets : <NEWLINE> <INDENT> str_dependencies [ target_type ] . append ( target . unexpanded_id ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d78953e2998843ce86d6f1ed8d165364": {
        "code_string": "if gene_mean is None:\n         gene_stdev = E.mean(0)\n     if gene_stdev is None:\n         gene_stdev = np.sqrt(sparse_var(E))\n     return sparse_multiply((E - gene_mean).T, 1/gene_stdev).T\n",
        "code_toks_joined": "if gene_mean is None : <NEWLINE> <INDENT> gene_stdev = E . mean ( 0 ) <NEWLINE> if gene_stdev is None : <NEWLINE> gene_stdev = np . sqrt ( sparse_var ( E ) ) <NEWLINE> return sparse_multiply ( ( E - gene_mean ) . T , 1 / gene_stdev ) . T <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ec41f89882d7434080e287b8def9e1c3": {
        "code_string": "count = 0\n                 for result in sr.paginate(StackName=name):\n                     done = (1 for x in result['StackResourceSummaries']\n                             if 'COMPLETE' in x['ResourceStatus'])\n                     count += sum(done)\n                 if count:\n                     # We can end up in a situation where we have more resources being created\n                     # than anticipated.\n                     if (count - current_resources) >= 0:\n                         progress.update(count - current_resources)\n                 current_resources = count\n             progress.close()\n",
        "code_toks_joined": "count = 0 <NEWLINE> <INDENT> for result in sr . paginate ( StackName = name ) : <NEWLINE> <INDENT> done = ( 1 for x in result [ <STRING> ] <NEWLINE> <INDENT> if <STRING> in x [ <STRING> ] ) <NEWLINE> <DEDENT> count += sum ( done ) <NEWLINE> <DEDENT> if count : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if ( count - current_resources ) >= 0 : <NEWLINE> <INDENT> progress . update ( count - current_resources ) <NEWLINE> <DEDENT> <DEDENT> current_resources = count <NEWLINE> progress . close ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'StackResourceSummaries'",
                "'COMPLETE'",
                "'ResourceStatus'"
            ],
            "<COMMENT>": [
                "# We can end up in a situation where we have more resources being created",
                "# than anticipated."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fb431ba02a07422eaa14dbbfbb6c767b": {
        "code_string": "def extract_relationships(self):\n         \"\"\"extract relation ships between nodes in the diagram\n         \"\"\"\n         for obj in self.classes():\n             node = obj.node\n             obj.attrs = self.get_attrs(node)\n             obj.methods = self.get_methods(node)\n             # shape\n             if is_interface(node):\n                 obj.shape = 'interface'\n             else:\n                 obj.shape = 'class'\n             # inheritance link\n             for par_node in node.ancestors(recurs=False):\n                 try:\n                     par_obj = self.object_from_node(par_node)\n                     self.add_relationship(obj, par_obj, 'specialization')\n                 except KeyError:\n                     continue\n             # implements link\n             for impl_node in node.implements:\n                 try:\n                     impl_obj = self.object_from_node(impl_node)\n                     self.add_relationship(obj, impl_obj, 'implements')\n                 except KeyError:\n                     continue\n             # associations link\n             for name, values in node.instance_attrs_type.items():\n                 for value in values:\n                     if value is astng.YES:\n                         continue\n                     if isinstance( value, astng.Instance):\n                         value = value._proxied\n                     try:\n                         ass_obj = self.object_from_node(value)\n                         self.add_relationship(obj, ass_obj, 'association', name)\n                     except KeyError:\n                         continue\n",
        "code_toks_joined": "def extract_relationships ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for obj in self . classes ( ) : <NEWLINE> <INDENT> node = obj . node <NEWLINE> obj . attrs = self . get_attrs ( node ) <NEWLINE> obj . methods = self . get_methods ( node ) <NEWLINE> <COMMENT> <NL> if is_interface ( node ) : <NEWLINE> <INDENT> obj . shape = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> obj . shape = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> for par_node in node . ancestors ( recurs = False ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> par_obj = self . object_from_node ( par_node ) <NEWLINE> self . add_relationship ( obj , par_obj , <STRING> ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for impl_node in node . implements : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> impl_obj = self . object_from_node ( impl_node ) <NEWLINE> self . add_relationship ( obj , impl_obj , <STRING> ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for name , values in node . instance_attrs_type . items ( ) : <NEWLINE> <INDENT> for value in values : <NEWLINE> <INDENT> if value is astng . YES : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if isinstance ( value , astng . Instance ) : <NEWLINE> <INDENT> value = value . _proxied <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> ass_obj = self . object_from_node ( value ) <NEWLINE> self . add_relationship ( obj , ass_obj , <STRING> , name ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"extract relation ships between nodes in the diagram\n         \"\"\"",
                "'interface'",
                "'class'",
                "'specialization'",
                "'implements'",
                "'association'"
            ],
            "<COMMENT>": [
                "# shape",
                "# inheritance link",
                "# implements link",
                "# associations link"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9b5d93faae6242e0805fdff16b1ad2f3": {
        "code_string": "def leave_function(self, node):\n         \"\"\"leave function: check function's locals are consumed\"\"\"\n         not_consumed = self._to_consume.pop()[0]\n         self._vars.pop(0)\n         # don't check arguments of function which are only raising an exception\n         if is_error(node):\n             return\n         # don't check arguments of abstract methods or within an interface\n         is_method = node.is_method()\n         klass = node.parent.frame()\n         if is_method and (klass.type == 'interface' or node.is_abstract()):\n             return\n         authorized_rgx = self.config.dummy_variables_rgx\n         overridden = marker = []\n         argnames = node.argnames()\n         for name, stmts in not_consumed.iteritems():\n             # ignore some special names specified by user configuration\n             if authorized_rgx.match(name):\n                 continue\n             # ignore names imported by the global statement\n             # FIXME: should only ignore them if it's assigned latter\n             stmt = stmts[0]\n             if isinstance(stmt, astng.Global):\n                 continue\n             # care about functions with unknown argument (builtins)\n             if name in argnames:\n                 if is_method:\n                     # don't warn for the first argument of a (non static) method\n                     if node.type != 'staticmethod' and name == argnames[0]:\n                         continue\n                     # don't warn for argument of an overridden method\n                     if overridden is marker:\n                         overridden = overridden_method(klass, node.name)\n                     if overridden is not None and name in overridden.argnames():\n                         continue\n                 # don't check callback arguments\n                 if node.name.startswith('cb_') or \\\n                        node.name.endswith('_cb'):\n                     continue\n                 self.add_message('W0613', args=name, node=node)\n             else:\n                 self.add_message('W0612', args=name, node=stmt)\n",
        "code_toks_joined": "def leave_function ( self , node ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> not_consumed = self . _to_consume . pop ( ) [ 0 ] <NEWLINE> self . _vars . pop ( 0 ) <NEWLINE> <COMMENT> <NL> if is_error ( node ) : <NEWLINE> <INDENT> return <NEWLINE> <COMMENT> <NL> <DEDENT> is_method = node . is_method ( ) <NEWLINE> klass = node . parent . frame ( ) <NEWLINE> if is_method and ( klass . type == <STRING> or node . is_abstract ( ) ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> authorized_rgx = self . config . dummy_variables_rgx <NEWLINE> overridden = marker = [ ] <NEWLINE> argnames = node . argnames ( ) <NEWLINE> for name , stmts in not_consumed . iteritems ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> if authorized_rgx . match ( name ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> stmt = stmts [ 0 ] <NEWLINE> if isinstance ( stmt , astng . Global ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> if name in argnames : <NEWLINE> <INDENT> if is_method : <NEWLINE> <COMMENT> <NL> <INDENT> if node . type != <STRING> and name == argnames [ 0 ] : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> if overridden is marker : <NEWLINE> <INDENT> overridden = overridden_method ( klass , node . name ) <NEWLINE> <DEDENT> if overridden is not None and name in overridden . argnames ( ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if node . name . startswith ( <STRING> ) or node . name . endswith ( <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> self . add_message ( <STRING> , args = name , node = node ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . add_message ( <STRING> , args = name , node = stmt ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"leave function: check function's locals are consumed\"\"\"",
                "'interface'",
                "'staticmethod'",
                "'cb_'",
                "'_cb'",
                "'W0613'",
                "'W0612'"
            ],
            "<COMMENT>": [
                "# don't check arguments of function which are only raising an exception",
                "# don't check arguments of abstract methods or within an interface",
                "# ignore some special names specified by user configuration",
                "# ignore names imported by the global statement",
                "# FIXME: should only ignore them if it's assigned latter",
                "# care about functions with unknown argument (builtins)",
                "# don't warn for the first argument of a (non static) method",
                "# don't warn for argument of an overridden method",
                "# don't check callback arguments"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "03dee152406c43859943519f09b953f1": {
        "code_string": "def _check_redefinition(self, redeftype, node):\n         \"\"\"check for redefinition of a function / method / class name\"\"\"\n         defined_self = node.parent.frame()[node.name]\n         if defined_self is not node and not astroid.are_exclusive(node, defined_self):\n             dummy_variables_rgx = lint_utils.get_global_option(\n                 self, 'dummy-variables-rgx', default=None)\n             if dummy_variables_rgx and dummy_variables_rgx.match(defined_self.name):\n                 return\n             self.add_message('function-redefined', node=node,\n                              args=(redeftype, defined_self.fromlineno))\n",
        "code_toks_joined": "def _check_redefinition ( self , redeftype , node ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> defined_self = node . parent . frame ( ) [ node . name ] <NEWLINE> if defined_self is not node and not astroid . are_exclusive ( node , defined_self ) : <NEWLINE> <INDENT> dummy_variables_rgx = lint_utils . get_global_option ( <NEWLINE> <INDENT> self , <STRING> , default = None ) <NEWLINE> <DEDENT> if dummy_variables_rgx and dummy_variables_rgx . match ( defined_self . name ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> self . add_message ( <STRING> , node = node , <NEWLINE> <INDENT> args = ( redeftype , defined_self . fromlineno ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"check for redefinition of a function / method / class name\"\"\"",
                "'dummy-variables-rgx'",
                "'function-redefined'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "362ed0a5fc914b45b2d50b694e528be0": {
        "code_string": "if not isinstance(error_type, tuple):\n         error_type = (error_type,)  # type: ignore\n     expected_errors = {stringify_error(error) for error in error_type}  # type: ignore\n     if not handler.type:\n         return True\n     return handler.catch(expected_errors)\n",
        "code_toks_joined": "if not isinstance ( error_type , tuple ) : <NEWLINE> <INDENT> error_type = ( error_type , ) <COMMENT> <NEWLINE> expected_errors = { stringify_error ( error ) for error in error_type } <COMMENT> <NEWLINE> if not handler . type : <NEWLINE> return True <NEWLINE> return handler . catch ( expected_errors ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# type: ignore",
                "# type: ignore"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "77cf1f1769924eeb9ca5f3b7ba1c1d17": {
        "code_string": "if gain > best_split.gain and gain >= context.min_gain_to_split:\n             best_split.gain = gain\n             best_split.feature_idx = feature_idx\n             best_split.bin_idx = bin_idx\n             best_split.gradient_left = gradient_left\n             best_split.hessian_left = hessian_left\n             best_split.n_samples_left = n_samples_left\n             best_split.gradient_right = gradient_right\n             best_split.hessian_right = hessian_right\n             best_split.n_samples_right = n_samples_right\n",
        "code_toks_joined": "if gain > best_split . gain and gain >= context . min_gain_to_split : <NEWLINE> <INDENT> best_split . gain = gain <NEWLINE> best_split . feature_idx = feature_idx <NEWLINE> best_split . bin_idx = bin_idx <NEWLINE> best_split . gradient_left = gradient_left <NEWLINE> best_split . hessian_left = hessian_left <NEWLINE> best_split . n_samples_left = n_samples_left <NEWLINE> best_split . gradient_right = gradient_right <NEWLINE> best_split . hessian_right = hessian_right <NEWLINE> best_split . n_samples_right = n_samples_right <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f794e83a7a2f4f2cb8674f4039f7c8a9": {
        "code_string": "i = i32(tos.value).value\n     j = i32(lr.value).value\n     D('  i=%i, j=%i (%s, %s)', i, j, type(i), type(j))\n     i /= j\n     D('  i=%i (%s)', i, type(i))\n     tos.value = i\n",
        "code_toks_joined": "i = i32 ( tos . value ) . value <NEWLINE> <INDENT> j = i32 ( lr . value ) . value <NEWLINE> D ( <STRING> , i , j , type ( i ) , type ( j ) ) <NEWLINE> i /= j <NEWLINE> D ( <STRING> , i , type ( i ) ) <NEWLINE> tos . value = i <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'  i=%i, j=%i (%s, %s)'",
                "'  i=%i (%s)'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c2b35725d98a4bf59b28a95b1ce21f7c": {
        "code_string": "pages = self.memory.alloc_pages(segment = 0x00, count = align(PAGE_SIZE, self.hdt.size()) / PAGE_SIZE)\n     self.memory.update_pages_flags(pages[0].index, len(pages), 'read', True)\n     self.hdt_address = pages[0].base_address\n",
        "code_toks_joined": "pages = self . memory . alloc_pages ( segment = 0x00 , count = align ( PAGE_SIZE , self . hdt . size ( ) ) / PAGE_SIZE ) <NEWLINE> <INDENT> self . memory . update_pages_flags ( pages [ 0 ] . index , len ( pages ) , <STRING> , True ) <NEWLINE> self . hdt_address = pages [ 0 ] . base_address <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'read'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "efc7bd3a7b894b03a64bcf2d03fb2c0c": {
        "code_string": "for i in range(0, 256 / CPR):\n       s = []\n       t = []\n",
        "code_toks_joined": "for i in range ( 0 , 256 / CPR ) : <NEWLINE> <INDENT> s = [ ] <NEWLINE> t = [ ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9ad21bfa7aaf4731b63e870374bb5a46": {
        "code_string": "# Add the unused terms to the list of marked terms\n             for g in list(groups.values()):\n                 marked |= group - used\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for g in list ( groups . values ( ) ) : <NEWLINE> <INDENT> marked |= group - used <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Add the unused terms to the list of marked terms"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "53ac32f22d41439cbff316fc3c11b55e": {
        "code_string": "def create(self, name, blob_name, label=None, container_name=None):\n         if not container_name:\n             container_name = self.account.storage_container()\n         if not label:\n             label = name\n         try:\n             storage = BlobService(self.account_name, self.account_key)\n             blob_properties = storage.get_blob_properties(\n                 container_name, blob_name\n             )\n         except Exception as e:\n             raise AzureBlobServicePropertyError(\n                 '%s not found in container %s' % (blob_name, container_name)\n             )\n         try:\n             media_link = storage.make_blob_url(container_name, blob_name)\n             service = ServiceManagementService(\n                 self.publishsettings.subscription_id,\n                 self.cert_file.name\n             )\n             service_call = service.add_os_image(\n                 label, media_link, name, 'Linux'\n             )\n             add_os_image_result = service_call.get_operation_status(\n                 service_call.request_id\n             )\n             status = add_os_image_result.status\n         except Exception as e:\n             raise AzureOsImageCreateError(\n                 '%s: %s' % (type(e).__name__, format(e))\n             )\n         return status\n",
        "code_toks_joined": "def create ( self , name , blob_name , label = None , container_name = None ) : <NEWLINE> <INDENT> if not container_name : <NEWLINE> <INDENT> container_name = self . account . storage_container ( ) <NEWLINE> <DEDENT> if not label : <NEWLINE> <INDENT> label = name <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> storage = BlobService ( self . account_name , self . account_key ) <NEWLINE> blob_properties = storage . get_blob_properties ( <NEWLINE> <INDENT> container_name , blob_name <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise AzureBlobServicePropertyError ( <NEWLINE> <INDENT> <STRING> % ( blob_name , container_name ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> media_link = storage . make_blob_url ( container_name , blob_name ) <NEWLINE> service = ServiceManagementService ( <NEWLINE> <INDENT> self . publishsettings . subscription_id , <NEWLINE> self . cert_file . name <NEWLINE> <DEDENT> ) <NEWLINE> service_call = service . add_os_image ( <NEWLINE> <INDENT> label , media_link , name , <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_os_image_result = service_call . get_operation_status ( <NEWLINE> <INDENT> service_call . request_id <NEWLINE> <DEDENT> ) <NEWLINE> status = add_os_image_result . status <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise AzureOsImageCreateError ( <NEWLINE> <INDENT> <STRING> % ( type ( e ) . __name__ , format ( e ) ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return status <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%s not found in container %s'",
                "'Linux'",
                "'%s: %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "505a1171e94343ca8993b294a68b8a20": {
        "code_string": "sonarsValue = lbot.getSonars()\n \tif min(sonarsValue) > threshold:\n \t\tif verbose:\n \t\t\tprint('No obstacles around Learnbot')\n \t\treturn True\n \tif verbose:\n \t\tprint('Learnbot is not obstacle free')\n \treturn False\n",
        "code_toks_joined": "sonarsValue = lbot . getSonars ( ) <NEWLINE> <INDENT> if min ( sonarsValue ) > threshold : <NEWLINE> <INDENT> if verbose : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT> if verbose : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'No obstacles around Learnbot'",
                "'Learnbot is not obstacle free'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b8242669dc6e4164b77059f458cb31d1": {
        "code_string": "def str2hex(text):\n     if sys.version_info[0]>3:\n         return text.encode('utf-8').hex()\n     else:\n         return str(binascii.hexlify(bytes(text)))\n",
        "code_toks_joined": "def str2hex ( text ) : <NEWLINE> <INDENT> if sys . version_info [ 0 ] > 3 : <NEWLINE> <INDENT> return text . encode ( <STRING> ) . hex ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return str ( binascii . hexlify ( bytes ( text ) ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'utf-8'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1fe7c069acae433da0628c7d94a7825d": {
        "code_string": "def look_front(lbot):\n     lbot.setJointAngle(\"CAMERA\",0)\n",
        "code_toks_joined": "def look_front ( lbot ) : <NEWLINE> <INDENT> lbot . setJointAngle ( <STRING> , 0 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"CAMERA\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cf369cb51583460391318dab9456cc5e": {
        "code_string": "def setAngleCamera(lbot,angle):\n     lbot.setJointAngle(\"CAMERA\", angle)\n",
        "code_toks_joined": "def setAngleCamera ( lbot , angle ) : <NEWLINE> <INDENT> lbot . setJointAngle ( <STRING> , angle ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"CAMERA\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1646569c6340412089fcab986a198b34": {
        "code_string": "def _get_confidence_interval(bootstrap_dist, stat_val, alpha, is_pivotal):\n     '''Get the bootstrap confidence interval for a given distribution.\n     Args:\n         bootstrap_distribution: numpy array of bootstrap results from\n             bootstrap_distribution() or bootstrap_ab_distribution()\n         stat_val: The overall statistic that this method is attempting to\n             calculate error bars for.\n         alpha: The alpha value for the confidence intervals.\n         is_pivotal: if true, use the pivotal method. if false, use the\n             percentile method.\n     '''\n     val = _np.percentile(bootstrap_dist, 50)\n     bootstrap_dist = [i * (stat_val / val) for i in bootstrap_dist]  # necessary for this analysis\n     if is_pivotal:\n         low = 2 * stat_val - _np.percentile(bootstrap_dist, 100 * (1 - alpha / 2.))\n         val = stat_val\n         high = 2 * stat_val - _np.percentile(bootstrap_dist, 100 * (alpha / 2.))\n     else:\n         low = _np.percentile(bootstrap_dist, 100 * ((alpha / 2)))\n         val = _np.percentile(bootstrap_dist, 50)\n         high = _np.percentile(bootstrap_dist, 100 * (1 - (alpha / 2)))\n     return BootstrapResults(low, stat_val, high)\n",
        "code_toks_joined": "def _get_confidence_interval ( bootstrap_dist , stat_val , alpha , is_pivotal ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> val = _np . percentile ( bootstrap_dist , 50 ) <NEWLINE> bootstrap_dist = [ i * ( stat_val / val ) for i in bootstrap_dist ] <COMMENT> <NEWLINE> if is_pivotal : <NEWLINE> <INDENT> low = 2 * stat_val - _np . percentile ( bootstrap_dist , 100 * ( 1 - alpha / 2. ) ) <NEWLINE> val = stat_val <NEWLINE> high = 2 * stat_val - _np . percentile ( bootstrap_dist , 100 * ( alpha / 2. ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> low = _np . percentile ( bootstrap_dist , 100 * ( ( alpha / 2 ) ) ) <NEWLINE> val = _np . percentile ( bootstrap_dist , 50 ) <NEWLINE> high = _np . percentile ( bootstrap_dist , 100 * ( 1 - ( alpha / 2 ) ) ) <NEWLINE> <DEDENT> return BootstrapResults ( low , stat_val , high ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'''Get the bootstrap confidence interval for a given distribution.\n     Args:\n         bootstrap_distribution: numpy array of bootstrap results from\n             bootstrap_distribution() or bootstrap_ab_distribution()\n         stat_val: The overall statistic that this method is attempting to\n             calculate error bars for.\n         alpha: The alpha value for the confidence intervals.\n         is_pivotal: if true, use the pivotal method. if false, use the\n             percentile method.\n     '''"
            ],
            "<COMMENT>": [
                "# necessary for this analysis"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1026596537c24066ad9306291747f242": {
        "code_string": "class Article:\n     def __init__(\n         self,\n         title: Optional[str],\n         authors: List[str],\n         year: Optional[int],\n         journal: Optional[str],\n         volume: Optional[str] = None,\n         issue: Optional[str] = None,\n         page: Optional[str] = None,\n         doi: Optional[str] = None,\n         references: Optional[List[str]] = None,\n         keywords: Optional[List[str]] = None,\n         sources: Optional[Set[str]] = None,\n         extra: Optional[Mapping] = None,\n     ):\n         self.title: Optional[str] = title\n         self.authors: List[str] = authors\n         self.keywords: List[str] = keywords or []\n         self.year: Optional[int] = year\n         self.journal: Optional[str] = journal\n         self.volume: Optional[str] = volume\n         self.issue: Optional[str] = volume\n         self.page: Optional[str] = page\n         self.doi: Optional[str] = doi\n         self.references: List[str] = references or []\n         self.sources: Set[str] = sources or set()\n         self.extra: Mapping[str, Any] = extra or {}\n",
        "code_toks_joined": "class Article : <NEWLINE> <INDENT> def __init__ ( <NEWLINE> <INDENT> self , <NEWLINE> title : Optional [ str ] , <NEWLINE> authors : List [ str ] , <NEWLINE> year : Optional [ int ] , <NEWLINE> journal : Optional [ str ] , <NEWLINE> volume : Optional [ str ] = None , <NEWLINE> issue : Optional [ str ] = None , <NEWLINE> page : Optional [ str ] = None , <NEWLINE> doi : Optional [ str ] = None , <NEWLINE> references : Optional [ List [ str ] ] = None , <NEWLINE> keywords : Optional [ List [ str ] ] = None , <NEWLINE> sources : Optional [ Set [ str ] ] = None , <NEWLINE> extra : Optional [ Mapping ] = None , <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> self . title : Optional [ str ] = title <NEWLINE> self . authors : List [ str ] = authors <NEWLINE> self . keywords : List [ str ] = keywords or [ ] <NEWLINE> self . year : Optional [ int ] = year <NEWLINE> self . journal : Optional [ str ] = journal <NEWLINE> self . volume : Optional [ str ] = volume <NEWLINE> self . issue : Optional [ str ] = volume <NEWLINE> self . page : Optional [ str ] = page <NEWLINE> self . doi : Optional [ str ] = doi <NEWLINE> self . references : List [ str ] = references or [ ] <NEWLINE> self . sources : Set [ str ] = sources or set ( ) <NEWLINE> self . extra : Mapping [ str , Any ] = extra or { } <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c385299fda9940e39b545e4ffebb6f0f": {
        "code_string": "if not self.active:\n                 if num_voiced >= 4:\n                     sys.stdout.write('+')\n                     self.active = True\n                     break\n                 elif len(self.history) == self.history.maxlen and sum(self.history) == 0:\n                     sys.stdout.write('Todo: increase capture volume')\n                     for _ in range(self.history.maxlen / 2):\n                         self.history.popleft()\n",
        "code_toks_joined": "if not self . active : <NEWLINE> <INDENT> if num_voiced >= 4 : <NEWLINE> <INDENT> sys . stdout . write ( <STRING> ) <NEWLINE> self . active = True <NEWLINE> break <NEWLINE> <DEDENT> elif len ( self . history ) == self . history . maxlen and sum ( self . history ) == 0 : <NEWLINE> <INDENT> sys . stdout . write ( <STRING> ) <NEWLINE> for _ in range ( self . history . maxlen / 2 ) : <NEWLINE> <INDENT> self . history . popleft ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'+'",
                "'Todo: increase capture volume'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0fbce622fb0442f9a1676ef159b0a8c6": {
        "code_string": "def american_put_exercise_barrier(mdl, strike):\n     exercises = []\n     payoff = put_payoff(strike)\n     for cnt, s, ex, opt in mdl.evaluate_american_exercisable_iter(payoff):\n         ex_idx = ex > cnt\n         ex_spots = s[ex_idx]\n         exercises.append(ex_spots.max() if ex_idx.any() else None)\n     exercises.reverse()\n     return np.array(exercises)\n",
        "code_toks_joined": "def american_put_exercise_barrier ( mdl , strike ) : <NEWLINE> <INDENT> exercises = [ ] <NEWLINE> payoff = put_payoff ( strike ) <NEWLINE> for cnt , s , ex , opt in mdl . evaluate_american_exercisable_iter ( payoff ) : <NEWLINE> <INDENT> ex_idx = ex > cnt <NEWLINE> ex_spots = s [ ex_idx ] <NEWLINE> exercises . append ( ex_spots . max ( ) if ex_idx . any ( ) else None ) <NEWLINE> <DEDENT> exercises . reverse ( ) <NEWLINE> return np . array ( exercises ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "43f98367b2b34a41ac09a5c761bb9f63": {
        "code_string": "attribs_to_remove = []  # Can't remove them inside the iteration!\n     for nsAttrib, val in el.attrib.items():\n         # validate that the namespace of the element is known and ok\n         attr, ns = strip_prefix(nsAttrib, el)\n         log.note(\"%s attr %s = %s (ns = %s)\" % (\n                 ' ' * (depth*indent), attr, val, ns))\n         if ns is not None and ns not in wp.xmlns_urls:\n             log.error(\"Element '{0}' does not allow attributes with namespace '{1}'\".\n                       format(element, ns), where=el)\n             attribs_to_remove.append(attr)\n             continue\n",
        "code_toks_joined": "attribs_to_remove = [ ] <COMMENT> <NEWLINE> <INDENT> for nsAttrib , val in el . attrib . items ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> attr , ns = strip_prefix ( nsAttrib , el ) <NEWLINE> log . note ( <STRING> % ( <NEWLINE> <INDENT> <STRING> * ( depth * indent ) , attr , val , ns ) ) <NEWLINE> <DEDENT> if ns is not None and ns not in wp . xmlns_urls : <NEWLINE> <INDENT> log . error ( <STRING> . <NEWLINE> <INDENT> format ( element , ns ) , where = el ) <NEWLINE> <DEDENT> attribs_to_remove . append ( attr ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Can't remove them inside the iteration!",
                "# validate that the namespace of the element is known and ok"
            ],
            "<STRING>": [
                "\"%s attr %s = %s (ns = %s)\"",
                "' '",
                "\"Element '{0}' does not allow attributes with namespace '{1}'\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3637cd2fa3f945e49c82bdd43dddf187": {
        "code_string": "#  Validate any embedded ABNF\n     if not options.no_abnf:\n         checker = AbnfChecker(options)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if not options . no_abnf : <NEWLINE> <INDENT> checker = AbnfChecker ( options ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#  Validate any embedded ABNF"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d185f969bdd4ae88acfbafdd0dc56e2": {
        "code_string": "destinatarios=[]\n for composicao_comissao in context.zsql.composicao_comissao_obter_zsql(cod_comissao=comissao.cod_comissao,cod_periodo_comp=periodo.cod_periodo_comp):\n  if composicao_comissao.dat_desligamento == None or composicao_comissao.dat_desligamento <= DateTime():\n   for destinatario in context.zsql.autor_obter_zsql(cod_parlamentar=composicao_comissao.cod_parlamentar):\n    dic={}\n    dic['end_email'] = destinatario.end_email\n    if dic['end_email'] != None:\n     destinatarios.append(dic)\n",
        "code_toks_joined": "destinatarios = [ ] <NEWLINE> <INDENT> for composicao_comissao in context . zsql . composicao_comissao_obter_zsql ( cod_comissao = comissao . cod_comissao , cod_periodo_comp = periodo . cod_periodo_comp ) : <NEWLINE> <INDENT> if composicao_comissao . dat_desligamento == None or composicao_comissao . dat_desligamento <= DateTime ( ) : <NEWLINE> <INDENT> for destinatario in context . zsql . autor_obter_zsql ( cod_parlamentar = composicao_comissao . cod_parlamentar ) : <NEWLINE> <INDENT> dic = { } <NEWLINE> dic [ <STRING> ] = destinatario . end_email <NEWLINE> if dic [ <STRING> ] != None : <NEWLINE> <INDENT> destinatarios . append ( dic ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'end_email'",
                "'end_email'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af3112af59524329b02744bd1f0e146b": {
        "code_string": "def parse_info(self, data, parse_type):\n         '''\n         \u89e3\u6790\u4fe1\u606f\n         '''\n         res = None\n         if parse_type == 'playlist_category_detail':\n             res = [{\n                 'playlist_id': d['id'],\n                 'playlist_name': d['name']\n             } for d in data['playlists']]\n         elif parse_type == 'playlist_detail':\n             tracks = data['result']['tracks']\n             res = [{\n                 'song_id': d['id'],\n                 'song_name': d['name'],\n                 'song_url': d['mp3Url'],\n                 'song_artists': ';'.join(map(lambda a: a['name'], d['artists']))\n             } for d in data]\n         elif parse_type == 'lyric_detail':\n             if 'lrc' in data:\n                 res = {\n                     'lyric': data['lrc']['lyric']\n                 }\n             elif 'nolyric' in data:\n                 res = {\n                     'lyric': u'\u6ca1\u6709\u627e\u5230\u6b4c\u8bcd'\n                 }\n         return res\n",
        "code_toks_joined": "def parse_info ( self , data , parse_type ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> res = None <NEWLINE> if parse_type == <STRING> : <NEWLINE> <INDENT> res = [ { <NEWLINE> <INDENT> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : d [ <STRING> ] <NEWLINE> <DEDENT> } for d in data [ <STRING> ] ] <NEWLINE> <DEDENT> elif parse_type == <STRING> : <NEWLINE> <INDENT> tracks = data [ <STRING> ] [ <STRING> ] <NEWLINE> res = [ { <NEWLINE> <INDENT> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : <STRING> . join ( map ( lambda a : a [ <STRING> ] , d [ <STRING> ] ) ) <NEWLINE> <DEDENT> } for d in data ] <NEWLINE> <DEDENT> elif parse_type == <STRING> : <NEWLINE> <INDENT> if <STRING> in data : <NEWLINE> <INDENT> res = { <NEWLINE> <INDENT> <STRING> : data [ <STRING> ] [ <STRING> ] <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> elif <STRING> in data : <NEWLINE> <INDENT> res = { <NEWLINE> <INDENT> <STRING> : <STRING> <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> <DEDENT> return res <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'''\n         \u89e3\u6790\u4fe1\u606f\n         '''",
                "'playlist_category_detail'",
                "'playlist_id'",
                "'id'",
                "'playlist_name'",
                "'name'",
                "'playlists'",
                "'playlist_detail'",
                "'result'",
                "'tracks'",
                "'song_id'",
                "'id'",
                "'song_name'",
                "'name'",
                "'song_url'",
                "'mp3Url'",
                "'song_artists'",
                "';'",
                "'name'",
                "'artists'",
                "'lyric_detail'",
                "'lrc'",
                "'lyric'",
                "'lrc'",
                "'lyric'",
                "'nolyric'",
                "'lyric'",
                "u'\u6ca1\u6709\u627e\u5230\u6b4c\u8bcd'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "559e2f9608754744b524ef3e44559852": {
        "code_string": "def _equalize(self, template, image):\n         \"\"\"Shrink template that doesn't fit or shrink image if template fits.\"\"\"\n         template_h, template_w = template.shape[0:2]\n         image_h, image_w = image.shape[0:2]\n         if (template_h <= image_h) and (template_w <= image_w):\n             eq_template = template.copy()  # pass template through\n             # template fits --> shrink image as little as possible\n             h_scale = float(template_h) / image_h\n             w_scale = float(template_w) / image_w\n             scale = max(h_scale, w_scale)  # max --> minimum shrinking\n             scaled_h = int(round(scale * image_h))\n             scaled_w = int(round(scale * image_w))\n             eq_image = cv2.resize(image, (scaled_w, scaled_h),\n                                   interpolation=cv2.INTER_AREA)\n         else:\n             eq_image = image.copy()  # pass image through\n             # template doesn't fit --> shrink template to completely fit\n             h_scale = float(image_h) / template_h\n             w_scale = float(image_w) / template_w\n             scale = min(h_scale, w_scale)  # min --> most shrinking\n             scaled_h = int(round(scale * template_h))\n             scaled_w = int(round(scale * template_w))\n             eq_template = cv2.resize(image, (scaled_w, scaled_h),\n                                      interpolation=cv2.INTER_AREA)\n         return eq_template, eq_image\n",
        "code_toks_joined": "def _equalize ( self , template , image ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> template_h , template_w = template . shape [ 0 : 2 ] <NEWLINE> image_h , image_w = image . shape [ 0 : 2 ] <NEWLINE> if ( template_h <= image_h ) and ( template_w <= image_w ) : <NEWLINE> <INDENT> eq_template = template . copy ( ) <COMMENT> <NEWLINE> <COMMENT> <NL> h_scale = float ( template_h ) / image_h <NEWLINE> w_scale = float ( template_w ) / image_w <NEWLINE> scale = max ( h_scale , w_scale ) <COMMENT> <NEWLINE> scaled_h = int ( round ( scale * image_h ) ) <NEWLINE> scaled_w = int ( round ( scale * image_w ) ) <NEWLINE> eq_image = cv2 . resize ( image , ( scaled_w , scaled_h ) , <NEWLINE> <INDENT> interpolation = cv2 . INTER_AREA ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> eq_image = image . copy ( ) <COMMENT> <NEWLINE> <COMMENT> <NL> h_scale = float ( image_h ) / template_h <NEWLINE> w_scale = float ( image_w ) / template_w <NEWLINE> scale = min ( h_scale , w_scale ) <COMMENT> <NEWLINE> scaled_h = int ( round ( scale * template_h ) ) <NEWLINE> scaled_w = int ( round ( scale * template_w ) ) <NEWLINE> eq_template = cv2 . resize ( image , ( scaled_w , scaled_h ) , <NEWLINE> <INDENT> interpolation = cv2 . INTER_AREA ) <NEWLINE> <DEDENT> <DEDENT> return eq_template , eq_image <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Shrink template that doesn't fit or shrink image if template fits.\"\"\""
            ],
            "<COMMENT>": [
                "# pass template through",
                "# template fits --> shrink image as little as possible",
                "# max --> minimum shrinking",
                "# pass image through",
                "# template doesn't fit --> shrink template to completely fit",
                "# min --> most shrinking"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "396fefdc8f8747cea2f5cd883e5d1ae8": {
        "code_string": "# build a Grid and add to self\n         monitor = pig.Grid(meta, self)\n         self.monitor = monitor\n         layout.addWidget(monitor)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> monitor = pig . Grid ( meta , self ) <NEWLINE> self . monitor = monitor <NEWLINE> layout . addWidget ( monitor ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# build a Grid and add to self"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5cd644babba34dc18f033c63276ad047": {
        "code_string": "pwidth = int(width / size)\n     pheight = int(width / size)\n",
        "code_toks_joined": "pwidth = int ( width / size ) <NEWLINE> <INDENT> pheight = int ( width / size ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a71d234b3f7c40fbb76263ac1a39ce46": {
        "code_string": "key = game.label.lower()\n             kgame, label = self.winners[key]\n             wteam = group.winner()\n             setattr(kgame, label, wteam)\n             if group.is_finished():\n                 wteam.games.append(game)\n",
        "code_toks_joined": "key = game . label . lower ( ) <NEWLINE> <INDENT> kgame , label = self . winners [ key ] <NEWLINE> wteam = group . winner ( ) <NEWLINE> setattr ( kgame , label , wteam ) <NEWLINE> if group . is_finished ( ) : <NEWLINE> <INDENT> wteam . games . append ( game ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "61f8cb9155b94d8bac093b1593efd151": {
        "code_string": "xx = self.xx * width\n         yy = self.yy * width\n",
        "code_toks_joined": "xx = self . xx * width <NEWLINE> <INDENT> yy = self . yy * width <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d691daaf25254c858985f185d8e8dba2": {
        "code_string": "for ix, (r, g, b) in enumerate(value):\n             self.red[ix] = r\n             self.green[ix] = g\n             self.blue[ix] = r\n",
        "code_toks_joined": "for ix , ( r , g , b ) in enumerate ( value ) : <NEWLINE> <INDENT> self . red [ ix ] = r <NEWLINE> self . green [ ix ] = g <NEWLINE> self . blue [ ix ] = r <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b81536ca0ee247e8bd791a429dac4c89": {
        "code_string": "def fetch_access_token(self):\n         url = 'https://openapi.baidu.com/oauth/2.0/token'\n         key = 'access_token'\n         res = self.cache.get(key)\n         if res and res['expire_time'] < time.time():\n             return res['data']\n         resp = requests.get(\n             url,\n             params={\n                 'grant_type': 'client_credentials',\n                 'client_id': self.api_key,\n                 'client_secret': self.secret_key\n             }\n         )\n         jsn = resp.json()\n         access_token = jsn['access_token']\n         expires_in = jsn['expires_in']\n         self.cache[key] = {\n             'expire_time': time.time() + expires_in - 20,\n             'data': access_token\n         }\n         return access_token\n",
        "code_toks_joined": "def fetch_access_token ( self ) : <NEWLINE> <INDENT> url = <STRING> <NEWLINE> key = <STRING> <NEWLINE> res = self . cache . get ( key ) <NEWLINE> if res and res [ <STRING> ] < time . time ( ) : <NEWLINE> <INDENT> return res [ <STRING> ] <NEWLINE> <DEDENT> resp = requests . get ( <NEWLINE> <INDENT> url , <NEWLINE> params = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : self . api_key , <NEWLINE> <STRING> : self . secret_key <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> ) <NEWLINE> jsn = resp . json ( ) <NEWLINE> access_token = jsn [ <STRING> ] <NEWLINE> expires_in = jsn [ <STRING> ] <NEWLINE> self . cache [ key ] = { <NEWLINE> <INDENT> <STRING> : time . time ( ) + expires_in - 20 , <NEWLINE> <STRING> : access_token <NEWLINE> <DEDENT> } <NEWLINE> return access_token <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'https://openapi.baidu.com/oauth/2.0/token'",
                "'access_token'",
                "'expire_time'",
                "'data'",
                "'grant_type'",
                "'client_credentials'",
                "'client_id'",
                "'client_secret'",
                "'access_token'",
                "'expires_in'",
                "'expire_time'",
                "'data'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5d8c26609f1246a8b4c79edee8438c13": {
        "code_string": "# zip file factory - returns a pandas dataframe\n def zip_parser(url=None, survey=None):\n     # setup the tmp path and file name\n     # thanks to https://stackoverflow.com/questions/55718917/download-zip-file-locally-to-tempfile-extract-files-to-tempfile-and-list-the-f/55719124#55719124\n     # path = \"/tmp/pypeds/\" + str(int(time.time())) + \"/\"  # hacky way to make unique path to extract time\n     _today = datetime.datetime.today().strftime('%Y%m%d')\n     survey_lower = survey.lower()\n     path = \"/tmp/\" + str(_today) + str(survey) + \"/\"  # hacky way to make unique path to extract date and survey\n     file = survey + \".zip\"\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def zip_parser ( url = None , survey = None ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> _today = datetime . datetime . today ( ) . strftime ( <STRING> ) <NEWLINE> survey_lower = survey . lower ( ) <NEWLINE> path = <STRING> + str ( _today ) + str ( survey ) + <STRING> <COMMENT> <NEWLINE> file = survey + <STRING> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# zip file factory - returns a pandas dataframe",
                "# setup the tmp path and file name",
                "# thanks to https://stackoverflow.com/questions/55718917/download-zip-file-locally-to-tempfile-extract-files-to-tempfile-and-list-the-f/55719124#55719124",
                "# path = \"/tmp/pypeds/\" + str(int(time.time())) + \"/\"  # hacky way to make unique path to extract time",
                "# hacky way to make unique path to extract date and survey"
            ],
            "<STRING>": [
                "'%Y%m%d'",
                "\"/tmp/\"",
                "\"/\"",
                "\".zip\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4e89751036cc47dbad4a891bbdeff29a": {
        "code_string": "if ConfigInstall.OS.on_windows and ConfigInstall.OS.on_osx:\n         formatter = 'simple'\n     else:\n         formatter = 'ansi'\n     logging_config['handlers']['console']['formatter'] = formatter\n",
        "code_toks_joined": "if ConfigInstall . OS . on_windows and ConfigInstall . OS . on_osx : <NEWLINE> <INDENT> formatter = <STRING> <NEWLINE> else : <NEWLINE> formatter = <STRING> <NEWLINE> logging_config [ <STRING> ] [ <STRING> ] [ <STRING> ] = formatter <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'simple'",
                "'ansi'",
                "'handlers'",
                "'console'",
                "'formatter'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1c361737e0c64e17ab69fc984cd0ec53": {
        "code_string": "def filter_unknown_bases(self):\n         \"\"\"\n         Filter out genomes with too many unknown bases.\n         \"\"\"\n         self.passed = self.stats[self.stats[\"N_Count\"] <= self.max_n_count]\n         self._criteria_dict[\"N_Count\"][\"failed\"] = self.stats.index[\n             self.stats[\"N_Count\"] >= self.max_n_count]\n         # self.failed_N_Count = self.stats.index[self.stats[\"N_Count\"] >=\n         #                                        self.max_n_count]\n",
        "code_toks_joined": "def filter_unknown_bases ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . passed = self . stats [ self . stats [ <STRING> ] <= self . max_n_count ] <NEWLINE> self . _criteria_dict [ <STRING> ] [ <STRING> ] = self . stats . index [ <NEWLINE> <INDENT> self . stats [ <STRING> ] >= self . max_n_count ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Filter out genomes with too many unknown bases.\n         \"\"\"",
                "\"N_Count\"",
                "\"N_Count\"",
                "\"failed\"",
                "\"N_Count\""
            ],
            "<COMMENT>": [
                "# self.failed_N_Count = self.stats.index[self.stats[\"N_Count\"] >=",
                "#                                        self.max_n_count]"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1fa2cc6bd4924214a3ec661d2327553a": {
        "code_string": "@click.command(help=help_text)\n @click.option('-n', '--max_unknowns', type=int, default=200,\n               help='Maximum number of unknown bases')\n @click.option('-c', '--c-deviations', type=float, default=3.0,\n               help='Deviations for number of contigs',)\n @click.option('-s', '--s-deviations', type=float, default=3.0,\n               help='Deviations for the assembly size')\n @click.option('-m', '--m-deviations', type=float, default=3.0,\n               help='Deviations for MASH distances')\n @click.option('-l', '--filter-level', type=float,\n               help='Deviations for all metrics')\n @click.option('-d', '--dry-run', is_flag=True)\n @click.option('--species', is_flag=True,\n               help='Run on single species')\n @click.argument('path', type=click.Path(exists=True, file_okay=False))\n def cli(filter_level, max_unknowns, c_deviations, s_deviations, m_deviations,\n         dry_run, species, path):\n     if species:\n         from genbankqc import Species\n         try:\n             s = Species(path, max_unknowns, c_deviations, s_deviations,\n                         m_deviations)\n             s.qc()\n             print(\"Completed \", s.species)\n             print(s)\n         except Exception:\n             print('Failed ', species.species)\n             traceback.print_exc()\n     else:\n         from genbankqc import Genbank\n         genbank = Genbank(path)\n         genbank.qc()\n",
        "code_toks_joined": "@ click . command ( help = help_text ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , type = int , default = 200 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> , ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , is_flag = True ) <NEWLINE> @ click . option ( <STRING> , is_flag = True , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . argument ( <STRING> , type = click . Path ( exists = True , file_okay = False ) ) <NEWLINE> def cli ( filter_level , max_unknowns , c_deviations , s_deviations , m_deviations , <NEWLINE> <INDENT> dry_run , species , path ) : <NEWLINE> if species : <NEWLINE> from genbankqc import Species <NEWLINE> try : <NEWLINE> <INDENT> s = Species ( path , max_unknowns , c_deviations , s_deviations , <NEWLINE> <INDENT> m_deviations ) <NEWLINE> <DEDENT> s . qc ( ) <NEWLINE> print ( <STRING> , s . species ) <NEWLINE> print ( s ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> print ( <STRING> , species . species ) <NEWLINE> traceback . print_exc ( ) <NEWLINE> else : <NEWLINE> <DEDENT> from genbankqc import Genbank <NEWLINE> genbank = Genbank ( path ) <NEWLINE> genbank . qc ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'-n'",
                "'--max_unknowns'",
                "'Maximum number of unknown bases'",
                "'-c'",
                "'--c-deviations'",
                "'Deviations for number of contigs'",
                "'-s'",
                "'--s-deviations'",
                "'Deviations for the assembly size'",
                "'-m'",
                "'--m-deviations'",
                "'Deviations for MASH distances'",
                "'-l'",
                "'--filter-level'",
                "'Deviations for all metrics'",
                "'-d'",
                "'--dry-run'",
                "'--species'",
                "'Run on single species'",
                "'path'",
                "\"Completed \"",
                "'Failed '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cf0a183fdcac44f5a94da1f0bf83448b": {
        "code_string": "@pytest.fixture()\n def biosample():\n     temp = Path(tempfile.mkdtemp())\n     biosample = metadata.BioSample(\"inbox.asanchez@gmail.com\", temp, sample=100)\n     yield biosample\n     shutil.rmtree(temp)\n",
        "code_toks_joined": "@ pytest . fixture ( ) <NEWLINE> <INDENT> def biosample ( ) : <NEWLINE> <INDENT> temp = Path ( tempfile . mkdtemp ( ) ) <NEWLINE> biosample = metadata . BioSample ( <STRING> , temp , sample = 100 ) <NEWLINE> yield biosample <NEWLINE> shutil . rmtree ( temp ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"inbox.asanchez@gmail.com\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5b11175c161d4a918b2c588ce5612296": {
        "code_string": "def _calculate_proposals(self):\n         self.interface._check_project()\n         resource = self.interface.resource\n         maxfixes = self.env.get('codeassist_maxfixes')\n         proposals = codeassist.code_assist(\n             self.interface.project, self.source, self.offset,\n             resource, maxfixes=maxfixes)\n         proposals = codeassist.sorted_proposals(proposals)\n         if self.autoimport is not None:\n             if self.starting.strip() and '.' not in self.expression:\n                 import_assists = self.autoimport.import_assist(self.starting)\n                 for assist in import_assists:\n                     p = codeassist.CompletionProposal(' : '.join(assist),\n                                                       'autoimport')\n                     import_assists.append(p)\n         return proposals\n",
        "code_toks_joined": "def _calculate_proposals ( self ) : <NEWLINE> <INDENT> self . interface . _check_project ( ) <NEWLINE> resource = self . interface . resource <NEWLINE> maxfixes = self . env . get ( <STRING> ) <NEWLINE> proposals = codeassist . code_assist ( <NEWLINE> <INDENT> self . interface . project , self . source , self . offset , <NEWLINE> resource , maxfixes = maxfixes ) <NEWLINE> <DEDENT> proposals = codeassist . sorted_proposals ( proposals ) <NEWLINE> if self . autoimport is not None : <NEWLINE> <INDENT> if self . starting . strip ( ) and <STRING> not in self . expression : <NEWLINE> <INDENT> import_assists = self . autoimport . import_assist ( self . starting ) <NEWLINE> for assist in import_assists : <NEWLINE> <INDENT> p = codeassist . CompletionProposal ( <STRING> . join ( assist ) , <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> import_assists . append ( p ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return proposals <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'codeassist_maxfixes'",
                "'.'",
                "' : '",
                "'autoimport'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "81a5a7862b3540b4bc2f41f499220d25": {
        "code_string": "assert all(t1 < t2 for t1, t2 in zip(time_points, time_points[1:])), time_points\n     epoch = time_points[0]\n",
        "code_toks_joined": "assert all ( t1 < t2 for t1 , t2 in zip ( time_points , time_points [ 1 : ] ) ) , time_points <NEWLINE> <INDENT> epoch = time_points [ 0 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a1d54cb61caf489da6179546515e00d4": {
        "code_string": "@pytest.fixture\n def app(session):\n     return HttpAPI(session.bind, [model])\n",
        "code_toks_joined": "@ pytest . fixture <NEWLINE> <INDENT> def app ( session ) : <NEWLINE> <INDENT> return HttpAPI ( session . bind , [ model ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f6d431e61e124c509e06bd03358ffbeb": {
        "code_string": "def _set_job(cls, job_hash, status, session):\n         key = cls._build_jobs_key()\n         session.redis_bind.hset(key, job_hash, json.dumps(status))\n         if session.redis_bind.ttl(key) > 0:\n             session.redis_bind.expire(key, 7*24*60*60)\n",
        "code_toks_joined": "def _set_job ( cls , job_hash , status , session ) : <NEWLINE> <INDENT> key = cls . _build_jobs_key ( ) <NEWLINE> session . redis_bind . hset ( key , job_hash , json . dumps ( status ) ) <NEWLINE> if session . redis_bind . ttl ( key ) > 0 : <NEWLINE> <INDENT> session . redis_bind . expire ( key , 7 * 24 * 60 * 60 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "80f10970a2854eb7aa47447c4730cc92": {
        "code_string": "def __eq__(self, other):\n         if not isinstance(other, Individual):\n             return NotImplemented\n         try:\n             return self.score < other.score\n         except ValueError:\n             return NotImplemented\n",
        "code_toks_joined": "def __eq__ ( self , other ) : <NEWLINE> <INDENT> if not isinstance ( other , Individual ) : <NEWLINE> <INDENT> return NotImplemented <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> return self . score < other . score <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> return NotImplemented <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cfcf305a208345cdb7560746e70d1155": {
        "code_string": "chronRates.loc[pdpind, 'EnergyCharge'] = pdpchrg\n",
        "code_toks_joined": "chronRates . loc [ pdpind , <STRING> ] = pdpchrg <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'EnergyCharge'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "abce2764c1374c74a29c5a818037869d": {
        "code_string": "for slot_data in packet.get_slots():\n             StreamIO.write_short(stream, slot_data.get_id())\n             if slot_data.is_empty():\n                 StreamIO.write_byte(stream, slot_data.get_count())\n                 StreamIO.write_short(stream, slot_data.get_damage())\n                 NBTSerializer.write(stream, slot_data.get_tag())\n",
        "code_toks_joined": "for slot_data in packet . get_slots ( ) : <NEWLINE> <INDENT> StreamIO . write_short ( stream , slot_data . get_id ( ) ) <NEWLINE> if slot_data . is_empty ( ) : <NEWLINE> <INDENT> StreamIO . write_byte ( stream , slot_data . get_count ( ) ) <NEWLINE> StreamIO . write_short ( stream , slot_data . get_damage ( ) ) <NEWLINE> NBTSerializer . write ( stream , slot_data . get_tag ( ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b6df0b9d1e404287934e4e5c50609658": {
        "code_string": "@preprocess.command()\n @click.option('-i', '--input_pkls', default=['./preprocess_outputs/methyl_array.pkl'], multiple=True, help='Input pickles for beta and phenotype data.', type=click.Path(exists=False), show_default=True)\n @click.option('-d', '--optional_input_pkl_dir', default='', help='Auto grab input pkls.', type=click.Path(exists=False), show_default=True)\n @click.option('-o', '--output_pkl', default='./combined_outputs/methyl_array.pkl', help='Output database for beta and phenotype data.', type=click.Path(exists=False), show_default=True)\n @click.option('-e', '--exclude', default=[], multiple=True, help='If -d selected, these diseases will be excluded from study.', show_default=True)\n def combine_methylation_arrays(input_pkls, optional_input_pkl_dir, output_pkl, exclude):\n     \"\"\"If split MethylationArrays by subtype for either preprocessing or imputation, can use to recombine data for downstream step.\"\"\"\n     os.makedirs(output_pkl[:output_pkl.rfind('/')],exist_ok=True)\n     if optional_input_pkl_dir:\n         input_pkls=glob.glob(os.path.join(optional_input_pkl_dir,'*','methyl_array.pkl'))\n         if exclude:\n             input_pkls=(np.array(input_pkls)[~np.isin(np.vectorize(lambda x: x.split('/')[-2])(input_pkls),np.array(exclude))]).tolist()\n     if len(input_pkls) > 0:\n         base_methyl_array=MethylationArray(*extract_pheno_beta_df_from_pickle_dict(pickle.load(open(input_pkls[0],'rb')), ''))\n         methyl_arrays_generator = (MethylationArray(*extract_pheno_beta_df_from_pickle_dict(pickle.load(open(input_pkl,'rb')), '')) for input_pkl in input_pkls[1:])\n         list_methyl_arrays = MethylationArrays([base_methyl_array])\n         combined_methyl_array = list_methyl_arrays.combine(methyl_arrays_generator)\n     else:\n         combined_methyl_array=MethylationArray(*extract_pheno_beta_df_from_pickle_dict(pickle.load(open(input_pkls[0],'rb')), ''))\n     combined_methyl_array.write_pickle(output_pkl)\n",
        "code_toks_joined": "@ preprocess . command ( ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , default = [ <STRING> ] , multiple = True , help = <STRING> , type = click . Path ( exists = False ) , show_default = True ) <NEWLINE> @ click . option ( <STRING> , <STRING> , default = <STRING> , help = <STRING> , type = click . Path ( exists = False ) , show_default = True ) <NEWLINE> @ click . option ( <STRING> , <STRING> , default = <STRING> , help = <STRING> , type = click . Path ( exists = False ) , show_default = True ) <NEWLINE> @ click . option ( <STRING> , <STRING> , default = [ ] , multiple = True , help = <STRING> , show_default = True ) <NEWLINE> def combine_methylation_arrays ( input_pkls , optional_input_pkl_dir , output_pkl , exclude ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> os . makedirs ( output_pkl [ : output_pkl . rfind ( <STRING> ) ] , exist_ok = True ) <NEWLINE> if optional_input_pkl_dir : <NEWLINE> <INDENT> input_pkls = glob . glob ( os . path . join ( optional_input_pkl_dir , <STRING> , <STRING> ) ) <NEWLINE> if exclude : <NEWLINE> <INDENT> input_pkls = ( np . array ( input_pkls ) [ ~ np . isin ( np . vectorize ( lambda x : x . split ( <STRING> ) [ - 2 ] ) ( input_pkls ) , np . array ( exclude ) ) ] ) . tolist ( ) <NEWLINE> <DEDENT> <DEDENT> if len ( input_pkls ) > 0 : <NEWLINE> <INDENT> base_methyl_array = MethylationArray ( * extract_pheno_beta_df_from_pickle_dict ( pickle . load ( open ( input_pkls [ 0 ] , <STRING> ) ) , <STRING> ) ) <NEWLINE> methyl_arrays_generator = ( MethylationArray ( * extract_pheno_beta_df_from_pickle_dict ( pickle . load ( open ( input_pkl , <STRING> ) ) , <STRING> ) ) for input_pkl in input_pkls [ 1 : ] ) <NEWLINE> list_methyl_arrays = MethylationArrays ( [ base_methyl_array ] ) <NEWLINE> combined_methyl_array = list_methyl_arrays . combine ( methyl_arrays_generator ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> combined_methyl_array = MethylationArray ( * extract_pheno_beta_df_from_pickle_dict ( pickle . load ( open ( input_pkls [ 0 ] , <STRING> ) ) , <STRING> ) ) <NEWLINE> <DEDENT> combined_methyl_array . write_pickle ( output_pkl ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'-i'",
                "'--input_pkls'",
                "'./preprocess_outputs/methyl_array.pkl'",
                "'Input pickles for beta and phenotype data.'",
                "'-d'",
                "'--optional_input_pkl_dir'",
                "''",
                "'Auto grab input pkls.'",
                "'-o'",
                "'--output_pkl'",
                "'./combined_outputs/methyl_array.pkl'",
                "'Output database for beta and phenotype data.'",
                "'-e'",
                "'--exclude'",
                "'If -d selected, these diseases will be excluded from study.'",
                "\"\"\"If split MethylationArrays by subtype for either preprocessing or imputation, can use to recombine data for downstream step.\"\"\"",
                "'/'",
                "'*'",
                "'methyl_array.pkl'",
                "'/'",
                "'rb'",
                "''",
                "'rb'",
                "''",
                "'rb'",
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1262a1c42cee4641b8bf2aa5606415e3": {
        "code_string": "@then('categories.number_format is {value}')\n def then_categories_number_format_is_value(context, value):\n     expected_value = value\n     number_format = context.categories.number_format\n     assert number_format == expected_value, 'got %s' % expected_value\n",
        "code_toks_joined": "@ then ( <STRING> ) <NEWLINE> <INDENT> def then_categories_number_format_is_value ( context , value ) : <NEWLINE> <INDENT> expected_value = value <NEWLINE> number_format = context . categories . number_format <NEWLINE> assert number_format == expected_value , <STRING> % expected_value <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'categories.number_format is {value}'",
                "'got %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ea2ab849f9764ecd8ec4ec51ace18b0a": {
        "code_string": "def validate_array(schema, data):\n     if schema.get('type') != 'array' or not schema.get('items'):\n         return\n     col_map = {'csv':   ',',\n                'ssv':   ' ',\n                'tsv':   '\\t',\n                'pipes': '|',\n                'multi': '&'}\n     col_fmt = schema.get('collectionFormat', 'csv')\n     delimiter = col_map.get(col_fmt)\n     if not delimiter:\n         logger.error(\"Unrecognized collectionFormat, cannot validate: %s\", col_fmt)\n         return\n     if col_fmt == 'multi':\n         logger.debug(\"collectionFormat 'multi' is not validated by Connexion\")\n         return\n     subschema = schema.get('items')\n     items = data.split(delimiter)\n     for subval in items:\n         converted_value, error = validate_type(subschema, subval, schema['in'], schema['name'])\n         if error:\n             return error\n         # Run each sub-item through the list of validators.\n         for func in VALIDATORS:\n             error = func(subschema, subval)\n             if error:\n                 return error\n",
        "code_toks_joined": "def validate_array ( schema , data ) : <NEWLINE> <INDENT> if schema . get ( <STRING> ) != <STRING> or not schema . get ( <STRING> ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> col_map = { <STRING> : <STRING> , <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> } <NEWLINE> <DEDENT> col_fmt = schema . get ( <STRING> , <STRING> ) <NEWLINE> delimiter = col_map . get ( col_fmt ) <NEWLINE> if not delimiter : <NEWLINE> <INDENT> logger . error ( <STRING> , col_fmt ) <NEWLINE> return <NEWLINE> <DEDENT> if col_fmt == <STRING> : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT> subschema = schema . get ( <STRING> ) <NEWLINE> items = data . split ( delimiter ) <NEWLINE> for subval in items : <NEWLINE> <INDENT> converted_value , error = validate_type ( subschema , subval , schema [ <STRING> ] , schema [ <STRING> ] ) <NEWLINE> if error : <NEWLINE> <INDENT> return error <NEWLINE> <COMMENT> <NL> <DEDENT> for func in VALIDATORS : <NEWLINE> <INDENT> error = func ( subschema , subval ) <NEWLINE> if error : <NEWLINE> <INDENT> return error <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'type'",
                "'array'",
                "'items'",
                "'csv'",
                "','",
                "'ssv'",
                "' '",
                "'tsv'",
                "'\\t'",
                "'pipes'",
                "'|'",
                "'multi'",
                "'&'",
                "'collectionFormat'",
                "'csv'",
                "\"Unrecognized collectionFormat, cannot validate: %s\"",
                "'multi'",
                "\"collectionFormat 'multi' is not validated by Connexion\"",
                "'items'",
                "'in'",
                "'name'"
            ],
            "<COMMENT>": [
                "# Run each sub-item through the list of validators."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1092dc283ad541df8e2524f751c6409b": {
        "code_string": "if not destination:\n             print('Unabled to find destination \"{}\" - aborting'.format(search_for))\n             return instructions, user_projects\n         for project in projects:\n             instructions.append((project, destination))\n     if user:\n         names = None\n         if user['projects'] != '--all--':\n             names = content['projects']\n         user_projects = glc.user_projects(conn_src, names=names, statistics=False)\n     return instructions, user_projects\n",
        "code_toks_joined": "if not destination : <NEWLINE> <INDENT> print ( <STRING> . format ( search_for ) ) <NEWLINE> return instructions , user_projects <NEWLINE> for project in projects : <NEWLINE> instructions . append ( ( project , destination ) ) <NEWLINE> if user : <NEWLINE> names = None <NEWLINE> if user [ <STRING> ] != <STRING> : <NEWLINE> names = content [ <STRING> ] <NEWLINE> user_projects = glc . user_projects ( conn_src , names = names , statistics = False ) <NEWLINE> return instructions , user_projects <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Unabled to find destination \"{}\" - aborting'",
                "'projects'",
                "'--all--'",
                "'projects'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b28d5d10e3e47febe177768dc49bd85": {
        "code_string": "gl_src = glc.connect(src_server.url, src_server.auth_token, ssl_verify=src_server.ssl_verify)\n     gl_dst = glc.connect(dst_server.url, dst_server.auth_token, ssl_verify=src_server.ssl_verify)\n",
        "code_toks_joined": "gl_src = glc . connect ( src_server . url , src_server . auth_token , ssl_verify = src_server . ssl_verify ) <NEWLINE> <INDENT> gl_dst = glc . connect ( dst_server . url , dst_server . auth_token , ssl_verify = src_server . ssl_verify ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f90d1fd00922406480f127179285b026": {
        "code_string": "def test_hiding_internal_options():\n     with MockIO('--help') as mockio:\n         CliBuilder(hide_internal=True).run()\n         assert '--bash-install' in mockio.output()\n         assert '--bash-autocomplete' not in mockio.output()\n     with MockIO('--help') as mockio:\n         CliBuilder(hide_internal=False).run()\n         assert '--bash-install' in mockio.output()\n         assert '--bash-autocomplete' in mockio.output()\n",
        "code_toks_joined": "def test_hiding_internal_options ( ) : <NEWLINE> <INDENT> with MockIO ( <STRING> ) as mockio : <NEWLINE> <INDENT> CliBuilder ( hide_internal = True ) . run ( ) <NEWLINE> assert <STRING> in mockio . output ( ) <NEWLINE> assert <STRING> not in mockio . output ( ) <NEWLINE> <DEDENT> with MockIO ( <STRING> ) as mockio : <NEWLINE> <INDENT> CliBuilder ( hide_internal = False ) . run ( ) <NEWLINE> assert <STRING> in mockio . output ( ) <NEWLINE> assert <STRING> in mockio . output ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'--help'",
                "'--bash-install'",
                "'--bash-autocomplete'",
                "'--help'",
                "'--bash-install'",
                "'--bash-autocomplete'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8b84340b1f12495abf330a732c40883b": {
        "code_string": "def add_node(\n         self, id_, key=None, text='', shape=None,\n         height=10.0, width=10.0, x=0.0, y=0.0,\n         fill_color=\"#ffffff\", transparent=False,\n         border_color=\"#000000\", border_type='line', border_width=1.0,\n         geometry=None, label=None, fill=None, border=None,\n         **label_kwargs\n     ):\n         if geometry is None:\n             geometry = Geometry(height, width, x, y)\n         if fill is None:\n             fill = Fill(fill_color, transparent)\n         if border is None:\n             border = Style(border_color, border_type, border_width)\n         if label is None:\n             label = Label(**label_kwargs)\n         self.nodes[id_] = Node(id_, key, text, shape, label, geometry, fill, border)\n",
        "code_toks_joined": "def add_node ( <NEWLINE> <INDENT> self , id_ , key = None , text = <STRING> , shape = None , <NEWLINE> height = 10.0 , width = 10.0 , x = 0.0 , y = 0.0 , <NEWLINE> fill_color = <STRING> , transparent = False , <NEWLINE> border_color = <STRING> , border_type = <STRING> , border_width = 1.0 , <NEWLINE> geometry = None , label = None , fill = None , border = None , <NEWLINE> ** label_kwargs <NEWLINE> ) : <NEWLINE> if geometry is None : <NEWLINE> <INDENT> geometry = Geometry ( height , width , x , y ) <NEWLINE> <DEDENT> if fill is None : <NEWLINE> <INDENT> fill = Fill ( fill_color , transparent ) <NEWLINE> <DEDENT> if border is None : <NEWLINE> <INDENT> border = Style ( border_color , border_type , border_width ) <NEWLINE> <DEDENT> if label is None : <NEWLINE> <INDENT> label = Label ( ** label_kwargs ) <NEWLINE> <DEDENT> self . nodes [ id_ ] = Node ( id_ , key , text , shape , label , geometry , fill , border ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''",
                "\"#ffffff\"",
                "\"#000000\"",
                "'line'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9b8ef5eb58624a30866590b3cb3fc20e": {
        "code_string": "rawcof = (sigcpd) ** 2 / ((pds1) * (pds1))\n",
        "code_toks_joined": "rawcof = ( sigcpd ) ** 2 / ( ( pds1 ) * ( pds1 ) ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fdd4d1b871e54d18b4ef0c6b8194bd46": {
        "code_string": "if input:\n                 plugin, text = self.brain.query(input)\n                 if plugin and text:\n                     try:\n                         plugin.handle(input, self.mic)\n                     except Exception:\n                         self._logger.error('Failed to execute module',\n                                            exc_info=True)\n                         self.mic.say(\"I'm sorry. I had some trouble with \" +\n                                      \"that operation. Please try again later.\")\n                     else:\n                         self._logger.debug(\"Handling of phrase '%s' by \" +\n                                            \"module '%s' completed\", text,\n                                            plugin.info.name)\n             else:\n                 self.mic.say(\"Pardon?\")\n",
        "code_toks_joined": "if input : <NEWLINE> <INDENT> plugin , text = self . brain . query ( input ) <NEWLINE> if plugin and text : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> plugin . handle ( input , self . mic ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> self . _logger . error ( <STRING> , <NEWLINE> <INDENT> exc_info = True ) <NEWLINE> <DEDENT> self . mic . say ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . _logger . debug ( <STRING> + <NEWLINE> <INDENT> <STRING> , text , <NEWLINE> plugin . info . name ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . mic . say ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Failed to execute module'",
                "\"I'm sorry. I had some trouble with \"",
                "\"that operation. Please try again later.\"",
                "\"Handling of phrase '%s' by \"",
                "\"module '%s' completed\"",
                "\"Pardon?\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2f364d52ffcd4b9ebebcb0aa2188501c": {
        "code_string": "def url(self, url=None, username=None, password=None, sha1=None, ignoreUrlErrors=None):\n         \"\"\"Image URL and URL download properties\"\"\"\n         if url is None:\n             self._url = url  # this does not change anything else, better to use constructor \n         if username is not None:\n             self._urluser = username  # basic authentication\n         if password is not None:\n             self._urlpassword = password  # basic authentication\n         if sha1 is not None:\n             self._urlsha1 = sha1  # file integrity\n         if ignoreUrlErrors is not None:\n             self._ignoreErrors = ignoreUrlErrors\n         if url is None and username is None and password is None and sha1 is None and ignoreUrlErrors is None:\n             return self._url\n         else:\n             return self\n",
        "code_toks_joined": "def url ( self , url = None , username = None , password = None , sha1 = None , ignoreUrlErrors = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if url is None : <NEWLINE> <INDENT> self . _url = url <COMMENT> <NEWLINE> <DEDENT> if username is not None : <NEWLINE> <INDENT> self . _urluser = username <COMMENT> <NEWLINE> <DEDENT> if password is not None : <NEWLINE> <INDENT> self . _urlpassword = password <COMMENT> <NEWLINE> <DEDENT> if sha1 is not None : <NEWLINE> <INDENT> self . _urlsha1 = sha1 <COMMENT> <NEWLINE> <DEDENT> if ignoreUrlErrors is not None : <NEWLINE> <INDENT> self . _ignoreErrors = ignoreUrlErrors <NEWLINE> <DEDENT> if url is None and username is None and password is None and sha1 is None and ignoreUrlErrors is None : <NEWLINE> <INDENT> return self . _url <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Image URL and URL download properties\"\"\""
            ],
            "<COMMENT>": [
                "# this does not change anything else, better to use constructor ",
                "# basic authentication",
                "# basic authentication",
                "# file integrity"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "359f68e95e064b2dbee2aeeb4a17959e": {
        "code_string": "if pyautoguiAvailable:\n     def clickGraphic(imagepath,delay=10,confidence=1):\n         elemToClick = None\n         numTries = 1\n         print(\"    (0): locating {} ...\".format(imagepath))\n         while (elemToClick is None) and (numTries < delay):\n             try:\n                 elemToClick = pyautogui.locateOnScreen(imagepath,confidence)\n             except Exception as exp:\n                 if isinstance(exp, pyautogui.pyscreeze.ImageNotFoundException):\n                     print(\"    ({}): locating {} ...\".format(numTries,imagepath))\n                 else:\n                     print(exp)\n                     break\n             finally:\n                 numTries += 1\n                 time.sleep(1)\n         if elemToClick is None:\n             print(\"    (x): could not locate image {}\".format(elemToClick))\n             return False\n         time.sleep(1)\n         pyautogui.click(pyautogui.center(elemToClick))\n         print(\"    (\u2713): clicked {}\".format(elemToClick))\n         time.sleep(1)\n         return True\n else:\n     def clickGraphic(imagepath,delay=10,confidence=1):\n         print(\"ERROR: no module 'pyautogui' - 'clickGraphic()' is not available\")\n         print(\"       maybe you missed to also install the 'Xlib' library on which pyautogui depends on\")\n         return False\n",
        "code_toks_joined": "if pyautoguiAvailable : <NEWLINE> <INDENT> def clickGraphic ( imagepath , delay = 10 , confidence = 1 ) : <NEWLINE> <INDENT> elemToClick = None <NEWLINE> numTries = 1 <NEWLINE> print ( <STRING> . format ( imagepath ) ) <NEWLINE> while ( elemToClick is None ) and ( numTries < delay ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> elemToClick = pyautogui . locateOnScreen ( imagepath , confidence ) <NEWLINE> <DEDENT> except Exception as exp : <NEWLINE> <INDENT> if isinstance ( exp , pyautogui . pyscreeze . ImageNotFoundException ) : <NEWLINE> <INDENT> print ( <STRING> . format ( numTries , imagepath ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( exp ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> numTries += 1 <NEWLINE> time . sleep ( 1 ) <NEWLINE> <DEDENT> <DEDENT> if elemToClick is None : <NEWLINE> <INDENT> print ( <STRING> . format ( elemToClick ) ) <NEWLINE> return False <NEWLINE> <DEDENT> time . sleep ( 1 ) <NEWLINE> pyautogui . click ( pyautogui . center ( elemToClick ) ) <NEWLINE> print ( <STRING> . format ( elemToClick ) ) <NEWLINE> time . sleep ( 1 ) <NEWLINE> return True <NEWLINE> else : <NEWLINE> <DEDENT> def clickGraphic ( imagepath , delay = 10 , confidence = 1 ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> return False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"    (0): locating {} ...\"",
                "\"    ({}): locating {} ...\"",
                "\"    (x): could not locate image {}\"",
                "\"    (\u2713): clicked {}\"",
                "\"ERROR: no module 'pyautogui' - 'clickGraphic()' is not available\"",
                "\"       maybe you missed to also install the 'Xlib' library on which pyautogui depends on\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9b4d981196524bc4aec5c151eb34f23f": {
        "code_string": "class UserPlanQuerySet(models.QuerySet):\n     def expires_in(self, days=7):\n         threshold = now() - timedelta(days=days)\n         return self.filter(expiration=threshold.date())\n",
        "code_toks_joined": "class UserPlanQuerySet ( models . QuerySet ) : <NEWLINE> <INDENT> def expires_in ( self , days = 7 ) : <NEWLINE> <INDENT> threshold = now ( ) - timedelta ( days = days ) <NEWLINE> return self . filter ( expiration = threshold . date ( ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "73e4bf5135804c43944de6ede55ad714": {
        "code_string": "fregions = prefix + \".regions.bed\"\n     with open(fregions, \"w\") as fh:\n         list(peaks.peaks(prefix + \".fdr.bed\", -1, threshold, seed,\n             step, fh, operator.le))\n     n_regions = sum(1 for _ in open(fregions))\n     print >>sys.stderr, \"wrote: %s (%i regions)\" % (fregions, n_regions)\n",
        "code_toks_joined": "fregions = prefix + <STRING> <NEWLINE> <INDENT> with open ( fregions , <STRING> ) as fh : <NEWLINE> <INDENT> list ( peaks . peaks ( prefix + <STRING> , - 1 , threshold , seed , <NEWLINE> <INDENT> step , fh , operator . le ) ) <NEWLINE> <DEDENT> <DEDENT> n_regions = sum ( 1 for _ in open ( fregions ) ) <NEWLINE> print >> sys . stderr , <STRING> % ( fregions , n_regions ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\".regions.bed\"",
                "\"w\"",
                "\".fdr.bed\"",
                "\"wrote: %s (%i regions)\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "66e1b6095eb642a7a91db4cb58181ee7": {
        "code_string": "def bm25_idf(N, df):\n     assert(N > df)\n     return log( (N - df + 0.5) / (df + 0.5) )\n",
        "code_toks_joined": "def bm25_idf ( N , df ) : <NEWLINE> <INDENT> assert ( N > df ) <NEWLINE> return log ( ( N - df + 0.5 ) / ( df + 0.5 ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "27f40d0ee247447ca5537a3e668f7c96": {
        "code_string": "with tf.variable_scope(\"bert\", scope):\n       with tf.variable_scope(\"embeddings\"):\n         # Perform embedding lookup on the word ids.\n         (self.embedding_output, self.embedding_table) = embedding_lookup(\n             input_ids=input_ids,\n             vocab_size=config.vocab_size,\n             embedding_size=config.hidden_size,\n             initializer_range=config.initializer_range,\n             word_embedding_name=\"word_embeddings\",\n             use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "code_toks_joined": "with tf . variable_scope ( <STRING> , scope ) : <NEWLINE> <INDENT> with tf . variable_scope ( <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> ( self . embedding_output , self . embedding_table ) = embedding_lookup ( <NEWLINE> <INDENT> input_ids = input_ids , <NEWLINE> vocab_size = config . vocab_size , <NEWLINE> embedding_size = config . hidden_size , <NEWLINE> initializer_range = config . initializer_range , <NEWLINE> word_embedding_name = <STRING> , <NEWLINE> use_one_hot_embeddings = use_one_hot_embeddings ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"bert\"",
                "\"embeddings\"",
                "\"word_embeddings\""
            ],
            "<COMMENT>": [
                "# Perform embedding lookup on the word ids."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6112a848ec01455aa2546b0bbcc4889a": {
        "code_string": "args = parser.parse_args()\n     param_str = '\\n'.join(['%20s = %s' % (k, v) for k, v in sorted(vars(args).items())])\n     print('usage:\\n{0}\\nparameters: \\n{1}'.format(' '.join([x for x in sys.argv]), param_str))\n     return parser\n",
        "code_toks_joined": "args = parser . parse_args ( ) <NEWLINE> <INDENT> param_str = <STRING> . join ( [ <STRING> % ( k , v ) for k , v in sorted ( vars ( args ) . items ( ) ) ] ) <NEWLINE> print ( <STRING> . format ( <STRING> . join ( [ x for x in sys . argv ] ) , param_str ) ) <NEWLINE> return parser <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'\\n'",
                "'%20s = %s'",
                "'usage:\\n{0}\\nparameters: \\n{1}'",
                "' '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "df5c795093414dfc8cc6b8f2a571913e": {
        "code_string": "def input_fn_builder(self, worker):\n         def gen():\n             while not True:\n                 if self.result:\n                     num_result = len(self.result)\n                     worker.send_multipart([ident, b'', pickle.dumps(self.result)])\n                     self.result.clear()\n                     time_used = time.clock() - start\n                     logger.info('encoded %d strs from %s in %.2fs @ %d/s' %\n                                 (num_result, ident, time_used, int(num_result / time_used)))\n                 ident, empty, msg = worker.recv_multipart()\n                 start = time.clock()\n                 msg = pickle.loads(msg)\n                 if self.is_valid_input(msg):\n                     tmp_f = list(convert_lst_to_features(msg, self.max_seq_len, self.tokenizer))\n                     yield {\n                         'input_ids': [f.input_ids for f in tmp_f],\n                         'input_mask': [f.input_mask for f in tmp_f],\n                         'input_type_ids': [f.input_type_ids for f in tmp_f]\n                     }\n                 else:\n                     logger.warning('worker %d: received unsupported type! sending back None' % self.id)\n                     worker.send_multipart([ident, b'', pickle.dumps(None)])\n",
        "code_toks_joined": "def input_fn_builder ( self , worker ) : <NEWLINE> <INDENT> def gen ( ) : <NEWLINE> <INDENT> while not True : <NEWLINE> <INDENT> if self . result : <NEWLINE> <INDENT> num_result = len ( self . result ) <NEWLINE> worker . send_multipart ( [ ident , <STRING> , pickle . dumps ( self . result ) ] ) <NEWLINE> self . result . clear ( ) <NEWLINE> time_used = time . clock ( ) - start <NEWLINE> logger . info ( <STRING> % <NEWLINE> <INDENT> ( num_result , ident , time_used , int ( num_result / time_used ) ) ) <NEWLINE> <DEDENT> <DEDENT> ident , empty , msg = worker . recv_multipart ( ) <NEWLINE> start = time . clock ( ) <NEWLINE> msg = pickle . loads ( msg ) <NEWLINE> if self . is_valid_input ( msg ) : <NEWLINE> <INDENT> tmp_f = list ( convert_lst_to_features ( msg , self . max_seq_len , self . tokenizer ) ) <NEWLINE> yield { <NEWLINE> <INDENT> <STRING> : [ f . input_ids for f in tmp_f ] , <NEWLINE> <STRING> : [ f . input_mask for f in tmp_f ] , <NEWLINE> <STRING> : [ f . input_type_ids for f in tmp_f ] <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . warning ( <STRING> % self . id ) <NEWLINE> worker . send_multipart ( [ ident , <STRING> , pickle . dumps ( None ) ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "b''",
                "'encoded %d strs from %s in %.2fs @ %d/s'",
                "'input_ids'",
                "'input_mask'",
                "'input_type_ids'",
                "'worker %d: received unsupported type! sending back None'",
                "b''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2e7497e2a57f4e61a2e64a2332f81383": {
        "code_string": "f = os.path.join(os.path.dirname(__file__), \"en-model.slp\")\n lexicon.model = Model.load(lexicon, f)\n",
        "code_toks_joined": "f = os . path . join ( os . path . dirname ( __file__ ) , <STRING> ) <NEWLINE> <INDENT> lexicon . model = Model . load ( lexicon , f ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"en-model.slp\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "296e1669008046e3b726d34f67e3c411": {
        "code_string": "# Calculate current level\n     inputs_coeffs = tuple(modwt_level_nd(x, level, axes) for x in inputs)\n     bin_args = tuple(modwt_level_nd(x, level, axes, approx_only=True) for x in inputs)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> inputs_coeffs = tuple ( modwt_level_nd ( x , level , axes ) for x in inputs ) <NEWLINE> bin_args = tuple ( modwt_level_nd ( x , level , axes , approx_only = True ) for x in inputs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Calculate current level"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5c43299c82ab4592bb2b4244905c6309": {
        "code_string": "# For each minor version of Python 3.x supported by this application,\n     # formally classify this version as such.\n     for python_version_minor in range(\n         python_version_min_parts[1], python_version_minor_max):\n         classifiers.append(\n             'Programming Language :: Python :: {}.{}'.format(\n                 PYTHON_VERSION_MAJOR, python_version_minor,))\n     # print('classifiers: {}'.format(_CLASSIFIERS))\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> for python_version_minor in range ( <NEWLINE> <INDENT> python_version_min_parts [ 1 ] , python_version_minor_max ) : <NEWLINE> classifiers . append ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> PYTHON_VERSION_MAJOR , python_version_minor , ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# For each minor version of Python 3.x supported by this application,",
                "# formally classify this version as such.",
                "# print('classifiers: {}'.format(_CLASSIFIERS))"
            ],
            "<STRING>": [
                "'Programming Language :: Python :: {}.{}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9d1c73d2a9fb40f7a9a3b0b132f0d120": {
        "code_string": "return original_node.with_changes(\n             operator=cst.Is(\n                 whitespace_after=original_node.operator.whitespace_after,\n                 whitespace_before=original_node.operator.whitespace_before,\n             )\n         )\n",
        "code_toks_joined": "return original_node . with_changes ( <NEWLINE> <INDENT> operator = cst . Is ( <NEWLINE> <INDENT> whitespace_after = original_node . operator . whitespace_after , <NEWLINE> whitespace_before = original_node . operator . whitespace_before , <NEWLINE> <DEDENT> ) <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "50419a5eaaa54a14836afb485640af94": {
        "code_string": "return original_node.with_changes(value=changed_tuple)\n",
        "code_toks_joined": "return original_node . with_changes ( value = changed_tuple ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4298c2c62ea7465d80fb9fce7f90f431": {
        "code_string": "def get_payment_status(self, d=None):\n         if d is None:\n             d = date.today()\n         explanation = ',\\n'.join(\n             discount.explanation.strip()\n             for discount in self.all_discounts\n             if discount.accounted.date() <= d and discount.explanation.strip()\n         )\n         return PaymentStatus(\n             price=self.price if self.approved and self.approved.date() < d else 0,\n             discount=self.get_discounted(d),\n             explanation=explanation,\n             paid=self.get_paid(d),\n             current_date=d,\n             due_from=self.subject.event.due_from,\n             due_date=self.subject.event.due_date,\n         )\n",
        "code_toks_joined": "def get_payment_status ( self , d = None ) : <NEWLINE> <INDENT> if d is None : <NEWLINE> <INDENT> d = date . today ( ) <NEWLINE> <DEDENT> explanation = <STRING> . join ( <NEWLINE> <INDENT> discount . explanation . strip ( ) <NEWLINE> for discount in self . all_discounts <NEWLINE> if discount . accounted . date ( ) <= d and discount . explanation . strip ( ) <NEWLINE> <DEDENT> ) <NEWLINE> return PaymentStatus ( <NEWLINE> <INDENT> price = self . price if self . approved and self . approved . date ( ) < d else 0 , <NEWLINE> discount = self . get_discounted ( d ) , <NEWLINE> explanation = explanation , <NEWLINE> paid = self . get_paid ( d ) , <NEWLINE> current_date = d , <NEWLINE> due_from = self . subject . event . due_from , <NEWLINE> due_date = self . subject . event . due_date , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "',\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9e06ebf998b440018710d3019cf0e85e": {
        "code_string": "# Convert from ParameterType to str\n             parameter_type_str = (\n                 \"average\"\n                 if parameter_type == ParameterType.AVERAGE_TIMESERIES\n                 else \"point\"\n             )\n             res._meta.loc[grp.index] = res._meta.loc[grp.index].assign(\n                 parameter_type=parameter_type_str\n             )\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> parameter_type_str = ( <NEWLINE> <INDENT> <STRING> <NEWLINE> if parameter_type == ParameterType . AVERAGE_TIMESERIES <NEWLINE> else <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> res . _meta . loc [ grp . index ] = res . _meta . loc [ grp . index ] . assign ( <NEWLINE> <INDENT> parameter_type = parameter_type_str <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Convert from ParameterType to str"
            ],
            "<STRING>": [
                "\"average\"",
                "\"point\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b776336442ab47b4baba0ac694e13f0b": {
        "code_string": "def _parse_text(self, term, pos):\n         end = pos + len(term)\n         part = self.source[pos : end]\n         yield ParseResult(part, end) if part == term else ParseFailure\n",
        "code_toks_joined": "def _parse_text ( self , term , pos ) : <NEWLINE> <INDENT> end = pos + len ( term ) <NEWLINE> part = self . source [ pos : end ] <NEWLINE> yield ParseResult ( part , end ) if part == term else ParseFailure <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "017e16d02c34438aaaba9aa0500b0d07": {
        "code_string": "# Find all paths of length <= path_lim (fast multiplication implementation)\n def indirect_paths(g, path_lim, aggregation, criterion):\n     if path_lim == 1:  # path length is 1\n         return g\n     else:  # find all paths\n         if path_lim % 2 == 0:\n             return indirect_paths(compute_path(g, g, aggregation, criterion), path_lim // 2, type, criterion)\n         else:\n             return compute_path(g, indirect_paths(g, path_lim - 1, aggregation, criterion), type, criterion)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def indirect_paths ( g , path_lim , aggregation , criterion ) : <NEWLINE> <INDENT> if path_lim == 1 : <COMMENT> <NEWLINE> <INDENT> return g <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> if path_lim % 2 == 0 : <NEWLINE> <INDENT> return indirect_paths ( compute_path ( g , g , aggregation , criterion ) , path_lim // 2 , type , criterion ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return compute_path ( g , indirect_paths ( g , path_lim - 1 , aggregation , criterion ) , type , criterion ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Find all paths of length <= path_lim (fast multiplication implementation)",
                "# path length is 1",
                "# find all paths"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "81124640f7cf41e3845f5c4aa95d866a": {
        "code_string": "#labels=change_to_continuous(q_pred)\n         y_pred=np.asarray(np.argmax(q,axis=1),dtype=int)\n         labels=y_pred.astype('U')\n         labels=pd.Categorical(values=labels,categories=natsorted(np.unique(y_pred).astype('U')))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> y_pred = np . asarray ( np . argmax ( q , axis = 1 ) , dtype = int ) <NEWLINE> labels = y_pred . astype ( <STRING> ) <NEWLINE> labels = pd . Categorical ( values = labels , categories = natsorted ( np . unique ( y_pred ) . astype ( <STRING> ) ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#labels=change_to_continuous(q_pred)"
            ],
            "<STRING>": [
                "'U'",
                "'U'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "264a8a505c024ea9a215aad41297065a": {
        "code_string": "target_ds = None\n         for r in Clone.__call__(\n                 source=clone_src,\n                 path=path,\n                 dataset=dataset,\n                 description=description,\n                 reckless=ephemeral,\n                 alt_sources=alt_sources,\n                 result_filter=None,\n                 result_renderer='disabled',\n                 on_failure='stop'):\n             if r.get('status', None) == 'ok' \\\n                     and r.get('action', None) == 'install':\n                 target_ds = Dataset(r['path'])\n             yield r\n",
        "code_toks_joined": "target_ds = None <NEWLINE> <INDENT> for r in Clone . __call__ ( <NEWLINE> <INDENT> source = clone_src , <NEWLINE> path = path , <NEWLINE> dataset = dataset , <NEWLINE> description = description , <NEWLINE> reckless = ephemeral , <NEWLINE> alt_sources = alt_sources , <NEWLINE> result_filter = None , <NEWLINE> result_renderer = <STRING> , <NEWLINE> on_failure = <STRING> ) : <NEWLINE> if r . get ( <STRING> , None ) == <STRING> and r . get ( <STRING> , None ) == <STRING> : <NEWLINE> target_ds = Dataset ( r [ <STRING> ] ) <NEWLINE> yield r <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'disabled'",
                "'stop'",
                "'status'",
                "'ok'",
                "'action'",
                "'install'",
                "'path'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "63ee595a85eb4e7aaa840352ddb0257b": {
        "code_string": "if refseq != None:\n         seqlen = len(refseq)\n     else:\n         seqlen = reads.end.max()\n     f = None\n     reads = reads[reads.reads>cutoff]\n     if by is not None:\n         reads = reads.sort_values(by, ascending=False)\n",
        "code_toks_joined": "if refseq != None : <NEWLINE> <INDENT> seqlen = len ( refseq ) <NEWLINE> else : <NEWLINE> seqlen = reads . end . max ( ) <NEWLINE> f = None <NEWLINE> reads = reads [ reads . reads > cutoff ] <NEWLINE> if by is not None : <NEWLINE> reads = reads . sort_values ( by , ascending = False ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "61bd0cd2ab584a41bd1ba3cb69862939": {
        "code_string": "#novel prediction\n         if ref_genome != '':\n             print ('predicting novel mirnas..')\n             allreads = utils.combine_aligned_reads(temp_path, files, ref_genome)\n             new,cl = novel.find_mirnas(allreads, cow_fasta)\n             new.to_csv(os.path.join(out,'novel.csv'), index=False)\n             novel.create_report(new, cl, species, filename=os.path.join(out, 'novel.html'))\n         else:\n             print ('no reference genome for novel mirna prediction')\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if ref_genome != <STRING> : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> allreads = utils . combine_aligned_reads ( temp_path , files , ref_genome ) <NEWLINE> new , cl = novel . find_mirnas ( allreads , cow_fasta ) <NEWLINE> new . to_csv ( os . path . join ( out , <STRING> ) , index = False ) <NEWLINE> novel . create_report ( new , cl , species , filename = os . path . join ( out , <STRING> ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#novel prediction"
            ],
            "<STRING>": [
                "''",
                "'predicting novel mirnas..'",
                "'novel.csv'",
                "'novel.html'",
                "'no reference genome for novel mirna prediction'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c68a496fc3894e279f7e1a5913475070": {
        "code_string": "headers = self.headers(encoded_data)\n         if headers:\n             rq_kwargs[\"headers\"] = headers\n         body = encoded_data\n         if body:\n             rq_kwargs[\"data\"] = data\n",
        "code_toks_joined": "headers = self . headers ( encoded_data ) <NEWLINE> <INDENT> if headers : <NEWLINE> <INDENT> rq_kwargs [ <STRING> ] = headers <NEWLINE> <DEDENT> body = encoded_data <NEWLINE> if body : <NEWLINE> <INDENT> rq_kwargs [ <STRING> ] = data <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"headers\"",
                "\"data\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aed0b57387b8468e8dab7d614a5e1a93": {
        "code_string": "def _fom(self, data1: Data, data2: Data) -> float:\n         clusters1 = set(data1.df[\"cluster\"].unique())\n         clusters2 = set(data2.df[\"cluster\"].unique())\n         if not clusters1 == clusters2:\n             return np.nan\n         clusters = clusters1\n         cluster2bpoint = {}\n         for cluster in clusters:\n             bpoints1 = data1.df[\n                 (data1.df[\"cluster\"] == cluster) & data1.df[\"bpoint\"]\n             ]\n             bpoints2 = data2.df[\n                 (data1.df[\"cluster\"] == cluster) & data2.df[\"bpoint\"]\n             ]\n             msg = \"Found {} bpoints instead of 1 for dataset {}.\"\n             if len(bpoints1) != 1:\n                 raise ValueError(msg.format(len(bpoints1), 1))\n             if len(bpoints2) != 1:\n                 raise ValueError(msg.format(len(bpoints2), 2))\n             bpoint1 = bpoints1.iloc[0][data1.par_cols]\n             bpoint2 = bpoints2.iloc[0][data2.par_cols]\n             cluster2bpoint[cluster] = (bpoint1, bpoint2)\n         return self._fom2(cluster2bpoint)\n",
        "code_toks_joined": "def _fom ( self , data1 : Data , data2 : Data ) -> float : <NEWLINE> <INDENT> clusters1 = set ( data1 . df [ <STRING> ] . unique ( ) ) <NEWLINE> clusters2 = set ( data2 . df [ <STRING> ] . unique ( ) ) <NEWLINE> if not clusters1 == clusters2 : <NEWLINE> <INDENT> return np . nan <NEWLINE> <DEDENT> clusters = clusters1 <NEWLINE> cluster2bpoint = { } <NEWLINE> for cluster in clusters : <NEWLINE> <INDENT> bpoints1 = data1 . df [ <NEWLINE> <INDENT> ( data1 . df [ <STRING> ] == cluster ) & data1 . df [ <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> bpoints2 = data2 . df [ <NEWLINE> <INDENT> ( data1 . df [ <STRING> ] == cluster ) & data2 . df [ <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> msg = <STRING> <NEWLINE> if len ( bpoints1 ) != 1 : <NEWLINE> <INDENT> raise ValueError ( msg . format ( len ( bpoints1 ) , 1 ) ) <NEWLINE> <DEDENT> if len ( bpoints2 ) != 1 : <NEWLINE> <INDENT> raise ValueError ( msg . format ( len ( bpoints2 ) , 2 ) ) <NEWLINE> <DEDENT> bpoint1 = bpoints1 . iloc [ 0 ] [ data1 . par_cols ] <NEWLINE> bpoint2 = bpoints2 . iloc [ 0 ] [ data2 . par_cols ] <NEWLINE> cluster2bpoint [ cluster ] = ( bpoint1 , bpoint2 ) <NEWLINE> <DEDENT> return self . _fom2 ( cluster2bpoint ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"cluster\"",
                "\"cluster\"",
                "\"cluster\"",
                "\"bpoint\"",
                "\"cluster\"",
                "\"bpoint\"",
                "\"Found {} bpoints instead of 1 for dataset {}.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "18097529592e43fa93fbbdf1efefbf7d": {
        "code_string": "hf_kw = dict(color=light_color)\n         hf_kw.update(hist_kwargs)\n",
        "code_toks_joined": "hf_kw = dict ( color = light_color ) <NEWLINE> <INDENT> hf_kw . update ( hist_kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9be758c1141c4519820d7afdec80b4de": {
        "code_string": "def check_dependencies(self):\n         \"\"\" Update rules after they have been read from the deployment file\n         a required process that is not in the starting sequence is forced to optional\n         If addresses are not defined, all addresses are applicable \"\"\"\n         # required MUST have start_sequence, so force to optional if no start_sequence\n         if self.required and self.start_sequence <= 0:\n             self.logger.warn('required forced to False because no start_sequence defined')\n             self.required = False\n         # if no addresses, consider all addresses\n         if not self.addresses:\n             self.addresses = ['*']\n             self.logger.warn('no address defined so all Supervisors addresses are applicable')\n",
        "code_toks_joined": "def check_dependencies ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if self . required and self . start_sequence <= 0 : <NEWLINE> <INDENT> self . logger . warn ( <STRING> ) <NEWLINE> self . required = False <NEWLINE> <COMMENT> <NL> <DEDENT> if not self . addresses : <NEWLINE> <INDENT> self . addresses = [ <STRING> ] <NEWLINE> self . logger . warn ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Update rules after they have been read from the deployment file\n         a required process that is not in the starting sequence is forced to optional\n         If addresses are not defined, all addresses are applicable \"\"\"",
                "'required forced to False because no start_sequence defined'",
                "'*'",
                "'no address defined so all Supervisors addresses are applicable'"
            ],
            "<COMMENT>": [
                "# required MUST have start_sequence, so force to optional if no start_sequence",
                "# if no addresses, consider all addresses"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bda1fc7efe4c419489ebc627087b3da8": {
        "code_string": "try:\n         result = []\n         for container in input:\n             client = LXCContainer(container)\n             result.append(client.create())\n         return response.reply(result, message='Container {} created successfully.'.format(input.get('name')))\n     except ValueError as ex:\n         return response.reply(message=ex.__str__(), status=403)\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for container in input : <NEWLINE> <INDENT> client = LXCContainer ( container ) <NEWLINE> result . append ( client . create ( ) ) <NEWLINE> <DEDENT> return response . reply ( result , message = <STRING> . format ( input . get ( <STRING> ) ) ) <NEWLINE> except ValueError as ex : <NEWLINE> return response . reply ( message = ex . __str__ ( ) , status = 403 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Container {} created successfully.'",
                "'name'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "331320bfc75b4f9e890131adcdc220df": {
        "code_string": "class Par2File(object):\n     def __init__(self, obj_or_path):\n         \"\"\"A convenient object that reads and makes sense of Par2 blocks.\"\"\"\n         self.path = None\n         if isinstance(obj_or_path, basestring):\n             with open(obj_or_path) as f:\n                 self.contents = f.read()\n                 self.path = obj_or_path\n         else:\n             self.contents = obj_or_path.read()\n             if getattr(obj_or_path, 'name', None):\n                 self.path = f.name\n         self.packets = self.read_packets()\n",
        "code_toks_joined": "class Par2File ( object ) : <NEWLINE> <INDENT> def __init__ ( self , obj_or_path ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . path = None <NEWLINE> if isinstance ( obj_or_path , basestring ) : <NEWLINE> <INDENT> with open ( obj_or_path ) as f : <NEWLINE> <INDENT> self . contents = f . read ( ) <NEWLINE> self . path = obj_or_path <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . contents = obj_or_path . read ( ) <NEWLINE> if getattr ( obj_or_path , <STRING> , None ) : <NEWLINE> <INDENT> self . path = f . name <NEWLINE> <DEDENT> <DEDENT> self . packets = self . read_packets ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"A convenient object that reads and makes sense of Par2 blocks.\"\"\"",
                "'name'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "250fe3c2e280410ea7bbd426faf583ef": {
        "code_string": "return render(request, 'cked/elfinder.html', {\n         'options': json_encode(options),\n     })\n",
        "code_toks_joined": "return render ( request , <STRING> , { <NEWLINE> <INDENT> <STRING> : json_encode ( options ) , <NEWLINE> } ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'cked/elfinder.html'",
                "'options'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c2351dc81d3c410184834531136111c5": {
        "code_string": "def check(**kwargs):\n     result = SimpleNamespace(ok=False, time=0, size=None, err=None)\n     try:\n         t = os.path.getmtime(kwargs['path'])\n         size = os.path.getsize(kwargs['path'])\n     except:\n         return result\n     result.time = t\n     result.size = size\n     if 'min-size' in kwargs:\n         result.ok = size > kwargs.get('min-size')\n         if not result.ok:\n             result.err = 'Too small'\n     else:\n         result.ok = True\n     return result\n",
        "code_toks_joined": "def check ( ** kwargs ) : <NEWLINE> <INDENT> result = SimpleNamespace ( ok = False , time = 0 , size = None , err = None ) <NEWLINE> try : <NEWLINE> <INDENT> t = os . path . getmtime ( kwargs [ <STRING> ] ) <NEWLINE> size = os . path . getsize ( kwargs [ <STRING> ] ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> return result <NEWLINE> <DEDENT> result . time = t <NEWLINE> result . size = size <NEWLINE> if <STRING> in kwargs : <NEWLINE> <INDENT> result . ok = size > kwargs . get ( <STRING> ) <NEWLINE> if not result . ok : <NEWLINE> <INDENT> result . err = <STRING> <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> result . ok = True <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'path'",
                "'path'",
                "'min-size'",
                "'min-size'",
                "'Too small'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a4183166d28a4944a20602be54f7197f": {
        "code_string": "row = 1\n     for (msgid, message) in messages:\n         column = 0\n         sheet.write(row, 0, msgid)\n         column += 1\n         if 'reference' in options.comments:\n             o = []\n             for (entry, lineno) in msg.occurrences:\n                 if lineno:\n                     o.append(u'%s:%s' % (entry, lineno))\n                 else:\n                     o.append(entry)\n             sheet.write(row, column, u', '.join(o))\n             column += 1\n         if 'extracted' in options.comments:\n             sheet.write(row, column, msg.comment)\n             column += 1\n         if 'translator' in options.comments:\n             sheet.write(row, column, msg.tcomment)\n             column += 1\n         for (i, cat) in enumerate(catalogs):\n             cat = cat[1]\n             msg = cat.find(msgid)\n             if msgid is not None:\n                 if 'fuzzy' in msg.flags:\n                     sheet.write(row, column, msg.msgstr, italic_style)\n                 else:\n                     sheet.write(row, column, msg.msgstr)\n             column += 1\n         row += 1\n",
        "code_toks_joined": "row = 1 <NEWLINE> <INDENT> for ( msgid , message ) in messages : <NEWLINE> <INDENT> column = 0 <NEWLINE> sheet . write ( row , 0 , msgid ) <NEWLINE> column += 1 <NEWLINE> if <STRING> in options . comments : <NEWLINE> <INDENT> o = [ ] <NEWLINE> for ( entry , lineno ) in msg . occurrences : <NEWLINE> <INDENT> if lineno : <NEWLINE> <INDENT> o . append ( <STRING> % ( entry , lineno ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> o . append ( entry ) <NEWLINE> <DEDENT> <DEDENT> sheet . write ( row , column , <STRING> . join ( o ) ) <NEWLINE> column += 1 <NEWLINE> <DEDENT> if <STRING> in options . comments : <NEWLINE> <INDENT> sheet . write ( row , column , msg . comment ) <NEWLINE> column += 1 <NEWLINE> <DEDENT> if <STRING> in options . comments : <NEWLINE> <INDENT> sheet . write ( row , column , msg . tcomment ) <NEWLINE> column += 1 <NEWLINE> <DEDENT> for ( i , cat ) in enumerate ( catalogs ) : <NEWLINE> <INDENT> cat = cat [ 1 ] <NEWLINE> msg = cat . find ( msgid ) <NEWLINE> if msgid is not None : <NEWLINE> <INDENT> if <STRING> in msg . flags : <NEWLINE> <INDENT> sheet . write ( row , column , msg . msgstr , italic_style ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sheet . write ( row , column , msg . msgstr ) <NEWLINE> <DEDENT> <DEDENT> column += 1 <NEWLINE> <DEDENT> row += 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'reference'",
                "u'%s:%s'",
                "u', '",
                "'extracted'",
                "'translator'",
                "'fuzzy'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "77b49b20513146b799c0830b6c69dd19": {
        "code_string": "while lo <= hi:\n             mid = lo + (hi - lo) / 2\n             if less(x, a[mid]):\n                 hi = mid - 1\n             elif less(a[mid], x):\n                 lo = mid + 1\n             else:\n                 return mid\n",
        "code_toks_joined": "while lo <= hi : <NEWLINE> <INDENT> mid = lo + ( hi - lo ) / 2 <NEWLINE> if less ( x , a [ mid ] ) : <NEWLINE> <INDENT> hi = mid - 1 <NEWLINE> <DEDENT> elif less ( a [ mid ] , x ) : <NEWLINE> <INDENT> lo = mid + 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return mid <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2e0b523dbbe64786b63e7152ddc8d167": {
        "code_string": "with self.assertRaises(ImmutablePropertyException):\n       entity.name = 'anything'\n",
        "code_toks_joined": "with self . assertRaises ( ImmutablePropertyException ) : <NEWLINE> <INDENT> entity . name = <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'anything'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b8e3421980fa4e18852dfc4d8d14c0ea": {
        "code_string": "for k in targets:\n         item = targets[k]\n         data = item['data']\n         if not allgenes:\n             if not 'hvg' in item:\n                 raise Exception('Run adobo.dr.find_hvg() first.')\n             hvg = item['hvg']['genes']\n             data = data[data.index.isin(hvg)]\n         elif verbose:\n             print('Using all genes')\n         if scale:\n             d_scaled = sklearn_scale(\n                             data.transpose(),  # cells as rows and genes as columns\n                             axis=0,            # over genes, i.e. features (columns)\n                             with_mean=True,    # subtracting the column means\n                             with_std=True)     # scale the data to unit variance\n             d_scaled = pd.DataFrame(d_scaled.transpose(), index=data.index)\n         if verbose:\n             v = (method, k, data.shape[0], data.shape[1])\n             print('Running PCA (method=%s) on the %s normalization (dimensions \\\n %sx%s)' % v)\n         if method == 'irlb':\n             comp, contr = irlb(data, ncomp, seed)\n         elif method == 'svd':\n             comp, contr = svd(data, ncomp)\n         else:\n             raise Exception('Unkown PCA method spefified. Valid choices are: irlb and svd')\n         obj.index = data.columns\n         obj.norm_data[k]['dr']['pca'] = {'comp' : comp,\n                                          'contr' : contr,\n                                          'method' : method}\n         obj.set_assay(sys._getframe().f_code.co_name, method)\n",
        "code_toks_joined": "for k in targets : <NEWLINE> <INDENT> item = targets [ k ] <NEWLINE> data = item [ <STRING> ] <NEWLINE> if not allgenes : <NEWLINE> <INDENT> if not <STRING> in item : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> hvg = item [ <STRING> ] [ <STRING> ] <NEWLINE> data = data [ data . index . isin ( hvg ) ] <NEWLINE> <DEDENT> elif verbose : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> if scale : <NEWLINE> <INDENT> d_scaled = sklearn_scale ( <NEWLINE> <INDENT> data . transpose ( ) , <COMMENT> <NEWLINE> axis = 0 , <COMMENT> <NEWLINE> with_mean = True , <COMMENT> <NEWLINE> with_std = True ) <COMMENT> <NEWLINE> <DEDENT> d_scaled = pd . DataFrame ( d_scaled . transpose ( ) , index = data . index ) <NEWLINE> <DEDENT> if verbose : <NEWLINE> <INDENT> v = ( method , k , data . shape [ 0 ] , data . shape [ 1 ] ) <NEWLINE> print ( <STRING> % v ) <NEWLINE> <DEDENT> if method == <STRING> : <NEWLINE> <INDENT> comp , contr = irlb ( data , ncomp , seed ) <NEWLINE> <DEDENT> elif method == <STRING> : <NEWLINE> <INDENT> comp , contr = svd ( data , ncomp ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> obj . index = data . columns <NEWLINE> obj . norm_data [ k ] [ <STRING> ] [ <STRING> ] = { <STRING> : comp , <NEWLINE> <INDENT> <STRING> : contr , <NEWLINE> <STRING> : method } <NEWLINE> <DEDENT> obj . set_assay ( sys . _getframe ( ) . f_code . co_name , method ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'data'",
                "'hvg'",
                "'Run adobo.dr.find_hvg() first.'",
                "'hvg'",
                "'genes'",
                "'Using all genes'",
                "'Running PCA (method=%s) on the %s normalization (dimensions \\\n %sx%s)'",
                "'irlb'",
                "'svd'",
                "'Unkown PCA method spefified. Valid choices are: irlb and svd'",
                "'dr'",
                "'pca'",
                "'comp'",
                "'contr'",
                "'method'"
            ],
            "<COMMENT>": [
                "# cells as rows and genes as columns",
                "# over genes, i.e. features (columns)",
                "# subtracting the column means",
                "# scale the data to unit variance"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ca468e5aef7483bb8cb0b331ead4b10": {
        "code_string": "def verify_unit_interval(value):\n     \"\"\"Throw an exception if the value is not on the unit interval [0,1].\n     \"\"\"\n     if not (value >= 0 or value <= 1):\n         raise ValueError(\"expected value on the interval [0, 1].\")\n     return value\n",
        "code_toks_joined": "def verify_unit_interval ( value ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not ( value >= 0 or value <= 1 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> return value <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Throw an exception if the value is not on the unit interval [0,1].\n     \"\"\"",
                "\"expected value on the interval [0, 1].\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3cdbb568ce6348869c98fdbf76cc85ec": {
        "code_string": "def transform(self, df, n_feats=10):\n         assert df.shape[1] <= n_feats\n         step = self.find_set_of_size(n_feats)\n         return df[self.features_at_step(step)]\n",
        "code_toks_joined": "def transform ( self , df , n_feats = 10 ) : <NEWLINE> <INDENT> assert df . shape [ 1 ] <= n_feats <NEWLINE> step = self . find_set_of_size ( n_feats ) <NEWLINE> return df [ self . features_at_step ( step ) ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7818ae5be7824a1cb4042a809ec7b61f": {
        "code_string": "@classmethod\n     def hue_gump(cls, bitmap, hue, partial_hue):\n         copy = bitmap.copy()\n         if hue:\n             hue = (hue & 0x3FFF) - 1\n             return Hues.HUES[hue].apply_to(bitmap, only_grey_pixels=partial_hue)\n         return copy\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def hue_gump ( cls , bitmap , hue , partial_hue ) : <NEWLINE> <INDENT> copy = bitmap . copy ( ) <NEWLINE> if hue : <NEWLINE> <INDENT> hue = ( hue & 0x3FFF ) - 1 <NEWLINE> return Hues . HUES [ hue ] . apply_to ( bitmap , only_grey_pixels = partial_hue ) <NEWLINE> <DEDENT> return copy <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "490e6a3ec86841668cd8998d8742118f": {
        "code_string": "if (board_height <= 0):\n             raise ValueError(\"the board height cannot be non-positive!\")\n         else:\n             self.board_height = board_width\n",
        "code_toks_joined": "if ( board_height <= 0 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> else : <NEWLINE> self . board_height = board_width <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"the board height cannot be non-positive!\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f5f38671897b496ba2862aad18b7e52e": {
        "code_string": "if (board_height <= 0):\n             raise ValueError(\"the board height cannot be non-positive!\")\n         else:\n             self.board_height = board_width\n",
        "code_toks_joined": "if ( board_height <= 0 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> else : <NEWLINE> self . board_height = board_width <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"the board height cannot be non-positive!\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ada199cf5174a9ca307961dedfc543f": {
        "code_string": "args = {\n             'openid.mode': 'associate',\n             'openid.assoc_type':'HMAC-SHA1',\n             'openid.session_type':'DH-SHA1',\n             'openid.dh_modulus': to_b64(long2a(p)),\n             'openid.dh_gen': to_b64(long2a(g)),\n             'openid.dh_consumer_public': to_b64(long2a(pow(p, priv_key, p))),\n             }\n",
        "code_toks_joined": "args = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : to_b64 ( long2a ( p ) ) , <NEWLINE> <STRING> : to_b64 ( long2a ( g ) ) , <NEWLINE> <STRING> : to_b64 ( long2a ( pow ( p , priv_key , p ) ) ) , <NEWLINE> } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'openid.mode'",
                "'associate'",
                "'openid.assoc_type'",
                "'HMAC-SHA1'",
                "'openid.session_type'",
                "'DH-SHA1'",
                "'openid.dh_modulus'",
                "'openid.dh_gen'",
                "'openid.dh_consumer_public'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4c3f8ceebf454ab894e1799d7476594f": {
        "code_string": "def make_app(redis_connection_obj, port, host_url, host_name, datadir):\n     app.register_blueprint(rpcblueprint, url_prefix=\"/rpc\")\n     app.port = port\n     if gevent:\n         redis.connection.socket = gevent.socket\n     rpcblueprint.r = redis.StrictRedis(host=redis_connection_obj['host'],\n                                        port=redis_connection_obj['port'],\n                                        db=redis_connection_obj['db'])\n     rpcblueprint.task_queue = TaskQueue(rpcblueprint.r)\n     server_manager = Servers(rpcblueprint.r)\n     settings.setup_server(rpcblueprint.r, datadir, host_url, host_name,\n                           Catalog(rpcblueprint.r, datadir, host_url),\n                           server_manager\n     )\n     rpcblueprint.heartbeat_thread = HeartbeatThread()\n     return app\n",
        "code_toks_joined": "def make_app ( redis_connection_obj , port , host_url , host_name , datadir ) : <NEWLINE> <INDENT> app . register_blueprint ( rpcblueprint , url_prefix = <STRING> ) <NEWLINE> app . port = port <NEWLINE> if gevent : <NEWLINE> <INDENT> redis . connection . socket = gevent . socket <NEWLINE> <DEDENT> rpcblueprint . r = redis . StrictRedis ( host = redis_connection_obj [ <STRING> ] , <NEWLINE> <INDENT> port = redis_connection_obj [ <STRING> ] , <NEWLINE> db = redis_connection_obj [ <STRING> ] ) <NEWLINE> <DEDENT> rpcblueprint . task_queue = TaskQueue ( rpcblueprint . r ) <NEWLINE> server_manager = Servers ( rpcblueprint . r ) <NEWLINE> settings . setup_server ( rpcblueprint . r , datadir , host_url , host_name , <NEWLINE> <INDENT> Catalog ( rpcblueprint . r , datadir , host_url ) , <NEWLINE> server_manager <NEWLINE> <DEDENT> ) <NEWLINE> rpcblueprint . heartbeat_thread = HeartbeatThread ( ) <NEWLINE> return app <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"/rpc\"",
                "'host'",
                "'port'",
                "'db'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9705ee73f9cc405b89d452c9f92cf4f5": {
        "code_string": "def run(redis_connection, node_url, node_name, queue, datadir):\n     if node_name is None:\n         node_name = node_url\n     redis_connection_obj = parse_redis_connection(redis_connection)\n     r = redis.StrictRedis(host=redis_connection_obj['host'],\n                           port=redis_connection_obj['port'],\n                           db=redis_connection_obj['db'])\n     server_manager = Servers(r)\n     settings.setup_server(r, datadir, node_url, node_name,\n                           Catalog(r, datadir, node_url),\n                           server_manager\n     )\n     if queue is None:\n         queue = ['default']\n     with Connection(r):\n         queues = []\n         node_queue = KitchenSinkRedisQueue(node_url)\n         queues.append(node_queue)\n         for q in queue:\n             if '|' in q:\n                 raise Exception(\"queue names cannot contain colons\")\n             queues.append(KitchenSinkRedisQueue(q))\n             queues.append(KitchenSinkRedisQueue(\"%s|%s\" % (q, node_name)))\n             w = KitchenSinkWorker(queues, default_result_ttl=86400)\n     w.work(burst=False)\n",
        "code_toks_joined": "def run ( redis_connection , node_url , node_name , queue , datadir ) : <NEWLINE> <INDENT> if node_name is None : <NEWLINE> <INDENT> node_name = node_url <NEWLINE> <DEDENT> redis_connection_obj = parse_redis_connection ( redis_connection ) <NEWLINE> r = redis . StrictRedis ( host = redis_connection_obj [ <STRING> ] , <NEWLINE> <INDENT> port = redis_connection_obj [ <STRING> ] , <NEWLINE> db = redis_connection_obj [ <STRING> ] ) <NEWLINE> <DEDENT> server_manager = Servers ( r ) <NEWLINE> settings . setup_server ( r , datadir , node_url , node_name , <NEWLINE> <INDENT> Catalog ( r , datadir , node_url ) , <NEWLINE> server_manager <NEWLINE> <DEDENT> ) <NEWLINE> if queue is None : <NEWLINE> <INDENT> queue = [ <STRING> ] <NEWLINE> <DEDENT> with Connection ( r ) : <NEWLINE> <INDENT> queues = [ ] <NEWLINE> node_queue = KitchenSinkRedisQueue ( node_url ) <NEWLINE> queues . append ( node_queue ) <NEWLINE> for q in queue : <NEWLINE> <INDENT> if <STRING> in q : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> queues . append ( KitchenSinkRedisQueue ( q ) ) <NEWLINE> queues . append ( KitchenSinkRedisQueue ( <STRING> % ( q , node_name ) ) ) <NEWLINE> w = KitchenSinkWorker ( queues , default_result_ttl = 86400 ) <NEWLINE> <DEDENT> <DEDENT> w . work ( burst = False ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'host'",
                "'port'",
                "'db'",
                "'default'",
                "'|'",
                "\"queue names cannot contain colons\"",
                "\"%s|%s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "884b8f2055ec465db848fd55a8851d93": {
        "code_string": "if x_type == 'indices':\n             # Keep indices, set start_time\n             if start_time is None:\n                 start_time = 0\n         else:\n             # Get indices removing start_time\n             if start_time is None:\n                 start_time = x_values[0]\n             else:\n                 assert start_time >= x_values[0], \"More than one sample at or before start_time\"\n             # WARN: limitation to 10 decimals due to workaround to prevent wrong cast flooring\n             # (e.g. np.floor(0.29 * 100) == 28)\n             x_values = _np.round((x_values - start_time) * sampling_freq, 10).astype(int)\n",
        "code_toks_joined": "if x_type == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> if start_time is None : <NEWLINE> <INDENT> start_time = 0 <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <DEDENT> if start_time is None : <NEWLINE> <INDENT> start_time = x_values [ 0 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> assert start_time >= x_values [ 0 ] , <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> x_values = _np . round ( ( x_values - start_time ) * sampling_freq , 10 ) . astype ( int ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'indices'",
                "\"More than one sample at or before start_time\""
            ],
            "<COMMENT>": [
                "# Keep indices, set start_time",
                "# Get indices removing start_time",
                "# WARN: limitation to 10 decimals due to workaround to prevent wrong cast flooring",
                "# (e.g. np.floor(0.29 * 100) == 28)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "10020863643c41709cfaef34a8d95eda": {
        "code_string": "@classmethod\n     def from_dict(cls, message_dict: typing.Dict[str, typing.Any]):\n         \"\"\"Construct message from dictionary.\"\"\"\n         message_dict = message_dict.only_fields(message_dict)\n         intent_dict = message_dict.pop(\"intent\", {})\n         slot_dicts = message_dict.pop(\"slots\", [])\n         message = NluIntent(  # type: ignore\n             **message_dict, intent=Intent(**intent_dict)\n         )\n         message.slots = [Slot.from_dict(s) for s in slot_dicts]\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def from_dict ( cls , message_dict : typing . Dict [ str , typing . Any ] ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> message_dict = message_dict . only_fields ( message_dict ) <NEWLINE> intent_dict = message_dict . pop ( <STRING> , { } ) <NEWLINE> slot_dicts = message_dict . pop ( <STRING> , [ ] ) <NEWLINE> message = NluIntent ( <COMMENT> <NEWLINE> <INDENT> ** message_dict , intent = Intent ( ** intent_dict ) <NEWLINE> <DEDENT> ) <NEWLINE> message . slots = [ Slot . from_dict ( s ) for s in slot_dicts ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Construct message from dictionary.\"\"\"",
                "\"intent\"",
                "\"slots\""
            ],
            "<COMMENT>": [
                "# type: ignore"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ec7261b0f63d4187bd87f63d2b9d1588": {
        "code_string": "cdl = cdl.resample(freq).agg({'open': 'first',\n                                       'high': 'max',\n                                       'low': 'min',\n                                       'close': 'last',\n                                       'volume': 'sum'})\n         # don't use dt.replace. use localize\n         # (https://stackoverflow.com/a/1592837/2739124)\n         cdl = cdl.loc[\n               pytz.timezone(NY).localize(dtbegin):\n               pytz.timezone(NY).localize(dtbegin)\n               ].dropna(subset=['high'])\n         records = cdl.reset_index().to_dict('records')\n         for r in records:\n             r['time'] = r['timestamp']\n             q.put(r)\n         q.put({})  # end of transmission\n",
        "code_toks_joined": "cdl = cdl . resample ( freq ) . agg ( { <STRING> : <STRING> , <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> } ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> cdl = cdl . loc [ <NEWLINE> pytz . timezone ( NY ) . localize ( dtbegin ) : <NEWLINE> pytz . timezone ( NY ) . localize ( dtbegin ) <NEWLINE> ] . dropna ( subset = [ <STRING> ] ) <NEWLINE> records = cdl . reset_index ( ) . to_dict ( <STRING> ) <NEWLINE> for r in records : <NEWLINE> r [ <STRING> ] = r [ <STRING> ] <NEWLINE> q . put ( r ) <NEWLINE> q . put ( { } ) <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'open'",
                "'first'",
                "'high'",
                "'max'",
                "'low'",
                "'min'",
                "'close'",
                "'last'",
                "'volume'",
                "'sum'",
                "'high'",
                "'records'",
                "'time'",
                "'timestamp'"
            ],
            "<COMMENT>": [
                "# don't use dt.replace. use localize",
                "# (https://stackoverflow.com/a/1592837/2739124)",
                "# end of transmission"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9e5606fb95264cb6b70a21cbd51e17d6": {
        "code_string": "# Add cookies to headers\n         if reqCookies and not u\"Cookie\" in headers:\n             header = u\"; \".join([u\"{}={}\".format(key, value) for key, value in reqCookies.items()])\n             reqHeaders[u\"Cookie\"] = header\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if reqCookies and not <STRING> in headers : <NEWLINE> <INDENT> header = <STRING> . join ( [ <STRING> . format ( key , value ) for key , value in reqCookies . items ( ) ] ) <NEWLINE> reqHeaders [ <STRING> ] = header <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Add cookies to headers"
            ],
            "<STRING>": [
                "u\"Cookie\"",
                "u\"; \"",
                "u\"{}={}\"",
                "u\"Cookie\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "35bcefa317f644d6a46a48466df8b8b9": {
        "code_string": "logger.debug(\"Requesting resource: %s\", url)\n         if req_params:\n             logger.debug(\"Request params: %s\", req_params)\n         if req_headers:\n             logger.debug(\"Request headers: %s\", req_headers)\n         if req_cookies:\n             logger.debug(\"Request cookies: %s\", req_cookies)\n         if json:\n             logger.debug(\"Request json: %s\", req_cookies)\n         if data:\n             logger.debug(\"Request data: %s\", data)\n",
        "code_toks_joined": "logger . debug ( <STRING> , url ) <NEWLINE> <INDENT> if req_params : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_params ) <NEWLINE> <DEDENT> if req_headers : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_headers ) <NEWLINE> <DEDENT> if req_cookies : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_cookies ) <NEWLINE> <DEDENT> if json : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_cookies ) <NEWLINE> <DEDENT> if data : <NEWLINE> <INDENT> logger . debug ( <STRING> , data ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Requesting resource: %s\"",
                "\"Request params: %s\"",
                "\"Request headers: %s\"",
                "\"Request cookies: %s\"",
                "\"Request json: %s\"",
                "\"Request data: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d5e6295ba4ff4fe5a170d77524f88695": {
        "code_string": "def print_position(lines, line_no):\n     lines = [l.strip(\" \\n\").split(\"\\t\") for l in lines]\n     word = lines[0][0]\n     position = [word]\n     for i, line in enumerate(lines):\n         assert line[0] == word, (\n             f\"Expected first attribute {word} but got {line[0]} in vertical \"\n             f\"#{i+1} at line #{line_no+1}. Are you sure the verticals \"\n             \"represent the same corpus?\"\n         )\n         position.extend(line[1:])\n     print(\"\\t\".join(line))\n",
        "code_toks_joined": "def print_position ( lines , line_no ) : <NEWLINE> <INDENT> lines = [ l . strip ( <STRING> ) . split ( <STRING> ) for l in lines ] <NEWLINE> word = lines [ 0 ] [ 0 ] <NEWLINE> position = [ word ] <NEWLINE> for i , line in enumerate ( lines ) : <NEWLINE> <INDENT> assert line [ 0 ] == word , ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> position . extend ( line [ 1 : ] ) <NEWLINE> <DEDENT> print ( <STRING> . join ( line ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\" \\n\"",
                "\"\\t\"",
                "f\"Expected first attribute {word} but got {line[0]} in vertical \"",
                "f\"#{i+1} at line #{line_no+1}. Are you sure the verticals \"",
                "\"represent the same corpus?\"",
                "\"\\t\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9610075a6d084bbf81f7da9e60c1c609": {
        "code_string": "def zero_prefix_int(num):\n     \"\"\"\n     zero_prefix_number(int) -> str\\n\n     Puts a zero in fron of 1 digit numbers.\\n\n     otherwise just returns the int\n     \"\"\"\n     strnum = str(num)\n     if len(strnum) == 1:\n         return '0'+strnum\n     return num\n",
        "code_toks_joined": "def zero_prefix_int ( num ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> strnum = str ( num ) <NEWLINE> if len ( strnum ) == 1 : <NEWLINE> <INDENT> return <STRING> + strnum <NEWLINE> <DEDENT> return num <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     zero_prefix_number(int) -> str\\n\n     Puts a zero in fron of 1 digit numbers.\\n\n     otherwise just returns the int\n     \"\"\"",
                "'0'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9e615b5e335f4af3a76decf51f5d8286": {
        "code_string": "def chunks(l, n):\n     big_list = []\n     n = max(1, n)\n     step = int(len(l) / n)\n     for i in range(0, len(l), step):\n         big_list.append(l[i:i+n])\n     return big_list\n",
        "code_toks_joined": "def chunks ( l , n ) : <NEWLINE> <INDENT> big_list = [ ] <NEWLINE> n = max ( 1 , n ) <NEWLINE> step = int ( len ( l ) / n ) <NEWLINE> for i in range ( 0 , len ( l ) , step ) : <NEWLINE> <INDENT> big_list . append ( l [ i : i + n ] ) <NEWLINE> <DEDENT> return big_list <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2339bbe0caec49eda04b1dd01d0abbc4": {
        "code_string": "# Select the shortest lncRNA transcript\n             if gene_name not in lnc_seq:\n                 lnc_seq[gene_name] = str(record.seq)\n             else:\n                 if len(lnc_seq[gene_name]) > len(str(record.seq)):\n                     lnc_seq[gene_name] = str(record.seq)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if gene_name not in lnc_seq : <NEWLINE> <INDENT> lnc_seq [ gene_name ] = str ( record . seq ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if len ( lnc_seq [ gene_name ] ) > len ( str ( record . seq ) ) : <NEWLINE> <INDENT> lnc_seq [ gene_name ] = str ( record . seq ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Select the shortest lncRNA transcript"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6d72f9f3487646e38e7e1b27e3f56535": {
        "code_string": "go_terms_parents = annotation.map(\n             lambda x: list({term for term in x if term not in leaf_terms}) \\\n                 if isinstance(x, list) else None)\n         return go_terms_parents\n",
        "code_toks_joined": "go_terms_parents = annotation . map ( <NEWLINE> <INDENT> lambda x : list ( { term for term in x if term not in leaf_terms } ) if isinstance ( x , list ) else None ) <NEWLINE> return go_terms_parents <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9fd59a17204145a6adb0372169f680c5": {
        "code_string": "go_terms_parents = annotation.map(\n             lambda x: list({term for term in x if term not in leaf_terms}) \\\n                 if isinstance(x, list) else None)\n         return go_terms_parents\n",
        "code_toks_joined": "go_terms_parents = annotation . map ( <NEWLINE> <INDENT> lambda x : list ( { term for term in x if term not in leaf_terms } ) if isinstance ( x , list ) else None ) <NEWLINE> return go_terms_parents <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3b88347096dc4bf3a3e22740057ea250": {
        "code_string": "# XXX: fix this mechanism\n     def _set_cache_for(self, name, value):\n         self.get._cache_set(value, name)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def _set_cache_for ( self , name , value ) : <NEWLINE> <INDENT> self . get . _cache_set ( value , name ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# XXX: fix this mechanism"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "afb494965541493f8694b9c444d15ab7": {
        "code_string": "if not dataapi.data.exists(type_name):\n             dataapi.data.set(type_name, name, value)\n",
        "code_toks_joined": "if not dataapi . data . exists ( type_name ) : <NEWLINE> <INDENT> dataapi . data . set ( type_name , name , value ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "12320e5d22fe42b5b7be2bb572f696d0": {
        "code_string": "return x_red[max_index]\n",
        "code_toks_joined": "return x_red [ max_index ] <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c4dfec0b8e414c718726c43085637725": {
        "code_string": "# assert that the solver is called with the right arguments\n         self.assertEqual(solver.update_after_step.call_count, 1)\n         pd.testing.assert_series_equal(solver_voltages, solver.update_after_step.call_args[0][0])\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> self . assertEqual ( solver . update_after_step . call_count , 1 ) <NEWLINE> pd . testing . assert_series_equal ( solver_voltages , solver . update_after_step . call_args [ 0 ] [ 0 ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# assert that the solver is called with the right arguments"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "09ef1ce8bb0f49559a66f666a83866ac": {
        "code_string": "def update(self):\n         super(VLibrasVideoViewlet, self).update()\n         self.youtube_url = get_video_url(self.context)\n         self.is_ready = self.youtube_url is not None\n         self.enabled = self.is_ready and not api.user.is_anonymous()\n         if self.is_ready:\n             self.klass = 'ready'\n",
        "code_toks_joined": "def update ( self ) : <NEWLINE> <INDENT> super ( VLibrasVideoViewlet , self ) . update ( ) <NEWLINE> self . youtube_url = get_video_url ( self . context ) <NEWLINE> self . is_ready = self . youtube_url is not None <NEWLINE> self . enabled = self . is_ready and not api . user . is_anonymous ( ) <NEWLINE> if self . is_ready : <NEWLINE> <INDENT> self . klass = <STRING> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ready'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "716e2889389f4ffbb394624356eb651e": {
        "code_string": "def sanitise(s, debug_name):\n         if not isinstance(s, basestring):\n             raise TypeError('{}: Expected str, got {}'.format(\n                 debug_name, type(s)))\n         elif not regex.match(s):\n             raise ValueError(\n                 '{}: {!r} did not match regex {!r}'.format(\n                     debug_name, s, regex_str))\n         else:\n             if isinstance(s, unicode):\n                 s = s.encode('utf-8')\n             s = escape(s)\n             if max_len is None:\n                 if len(s) <= min_len:\n                     raise ValueError(\n                         '{}: String longer than {} characters'.format(\n                             debug_name, min_len))\n             else:\n                 if not min_len <= len(s) <= max_len:\n                     raise ValueError(\n                         '{}: String length out of range {}-{}'.format(\n                             debug_name, min_len, max_len))\n             return s\n     return sanitise\n",
        "code_toks_joined": "def sanitise ( s , debug_name ) : <NEWLINE> <INDENT> if not isinstance ( s , basestring ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> . format ( <NEWLINE> <INDENT> debug_name , type ( s ) ) ) <NEWLINE> <DEDENT> <DEDENT> elif not regex . match ( s ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> debug_name , s , regex_str ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if isinstance ( s , unicode ) : <NEWLINE> <INDENT> s = s . encode ( <STRING> ) <NEWLINE> <DEDENT> s = escape ( s ) <NEWLINE> if max_len is None : <NEWLINE> <INDENT> if len ( s ) <= min_len : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> debug_name , min_len ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if not min_len <= len ( s ) <= max_len : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> debug_name , min_len , max_len ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return s <NEWLINE> return sanitise <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'{}: Expected str, got {}'",
                "'{}: {!r} did not match regex {!r}'",
                "'utf-8'",
                "'{}: String longer than {} characters'",
                "'{}: String length out of range {}-{}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2ceaa346585c43e3b6748f896e0bdc5f": {
        "code_string": "self._refresh_resources(False)\n         return self._store.get_proxy(filter_opts, self._blacklist)\n",
        "code_toks_joined": "self . _refresh_resources ( False ) <NEWLINE> <INDENT> return self . _store . get_proxy ( filter_opts , self . _blacklist ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a1df614537a84a71bb6d54e0675e1fb9": {
        "code_string": "if content_type_to_sync:\n         log.info('Syncing %s.' % content_type_to_sync)\n         try:\n             content_type = content_registry[content_type_to_sync]\n             sync_content_type(content_type)\n         except KeyError:\n             log.error('%s is not in the content registry.' % content_type_to_sync)\n     else:\n         for content_type in content_registry.keys():\n             sync_content_type(content_type)\n",
        "code_toks_joined": "if content_type_to_sync : <NEWLINE> <INDENT> log . info ( <STRING> % content_type_to_sync ) <NEWLINE> try : <NEWLINE> <INDENT> content_type = content_registry [ content_type_to_sync ] <NEWLINE> sync_content_type ( content_type ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> log . error ( <STRING> % content_type_to_sync ) <NEWLINE> else : <NEWLINE> <DEDENT> for content_type in content_registry . keys ( ) : <NEWLINE> <INDENT> sync_content_type ( content_type ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Syncing %s.'",
                "'%s is not in the content registry.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3681d0a764b540b3aeecced9eee015e5": {
        "code_string": "def channel_sort(self,queryset,channel=channel,profile=None,basekey=None,flavor=None,limit=0):\n         \"\"\"\n         Sorts the passed queryset by rlevel, with the most relevant results first.\n         \"\"\"\n         channel_results = self._channel(queryset,channel=channel,profile=profile,basekey=basekey,flavor=flavor,limit=limit,include_endorsements=True)\n         remainder_results = queryset.exclude(pk__in=[item.pk for item in channel_results])\n         final_results = channel_results + [ContentItemWrapper(item,0) for item in remainder_results]\n         final_results.sort(cmp=lambda x,y: cmp(x.rlevel,y.rlevel))\n         return final_results\n",
        "code_toks_joined": "def channel_sort ( self , queryset , channel = channel , profile = None , basekey = None , flavor = None , limit = 0 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> channel_results = self . _channel ( queryset , channel = channel , profile = profile , basekey = basekey , flavor = flavor , limit = limit , include_endorsements = True ) <NEWLINE> remainder_results = queryset . exclude ( pk__in = [ item . pk for item in channel_results ] ) <NEWLINE> final_results = channel_results + [ ContentItemWrapper ( item , 0 ) for item in remainder_results ] <NEWLINE> final_results . sort ( cmp = lambda x , y : cmp ( x . rlevel , y . rlevel ) ) <NEWLINE> return final_results <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Sorts the passed queryset by rlevel, with the most relevant results first.\n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "34ae0a04012644a085cf0da8f117f4a6": {
        "code_string": "scope = get_scope_at(project, fixed_source, lineno, filename, ast_nodes)\n         if not ctx:\n             names = get_scope_names(scope, lineno)\n         else:\n             obj = infer(ctx, scope, position)\n             names = [obj.get_names()]\n     elif ctx_type == 'import':\n         names = (project.get_possible_imports(ctx, filename),)\n     elif ctx_type == 'from':\n         names = (project.get_possible_imports(ctx, filename),)\n     elif ctx_type == 'from-import':\n         names = (\n             project.get_module(ctx, filename).get_names(),\n             project.get_possible_imports(ctx, filename))\n     elif ctx_type == 'none':\n         return []\n",
        "code_toks_joined": "scope = get_scope_at ( project , fixed_source , lineno , filename , ast_nodes ) <NEWLINE> <INDENT> if not ctx : <NEWLINE> <INDENT> names = get_scope_names ( scope , lineno ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> obj = infer ( ctx , scope , position ) <NEWLINE> names = [ obj . get_names ( ) ] <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> <DEDENT> names = ( project . get_possible_imports ( ctx , filename ) , ) <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> names = ( project . get_possible_imports ( ctx , filename ) , ) <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> names = ( <NEWLINE> <INDENT> project . get_module ( ctx , filename ) . get_names ( ) , <NEWLINE> project . get_possible_imports ( ctx , filename ) ) <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> <DEDENT> return [ ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'import'",
                "'from'",
                "'from-import'",
                "'none'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "439f9166110f4f6f8637574cbd9c29c8": {
        "code_string": "try:\n                     func = scope.eval(func, False)\n                 except:\n                     continue\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> func = scope . eval ( func , False ) <NEWLINE> except : <NEWLINE> continue <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ad4e77012e3487d8949311af09011a1": {
        "code_string": "while length > 0:\n       if maxpoints > length: maxpoints = length\n       data = self._h5[startpos: startpos+maxpoints]\n       if isinstance(self.clock, UniformClock):\n         yield DataSegment(self.clock[startpos], UniformTimeSeries(data, self.clock.rate))\n       else:\n         yield DataSegment(0, TimeSeries(self.clock[startpos: startpos+maxpoints], data))\n       startpos += len(data)\n       length -= len(data)\n",
        "code_toks_joined": "while length > 0 : <NEWLINE> <INDENT> if maxpoints > length : maxpoints = length <NEWLINE> data = self . _h5 [ startpos : startpos + maxpoints ] <NEWLINE> if isinstance ( self . clock , UniformClock ) : <NEWLINE> <INDENT> yield DataSegment ( self . clock [ startpos ] , UniformTimeSeries ( data , self . clock . rate ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> yield DataSegment ( 0 , TimeSeries ( self . clock [ startpos : startpos + maxpoints ] , data ) ) <NEWLINE> <DEDENT> startpos += len ( data ) <NEWLINE> length -= len ( data ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dc0da3d51b6b47d8abfcaf31f4bf93a0": {
        "code_string": "try:\n             if filename.endswith (\".pickle\") or ispickle == True:\n                 dict = eval (filename)\n                 try:\n                     pickle.dump (dict, filename)\n                 except pickle.PickleError as perr:\n                     print (\"Pickling error occurred: \" + str (perr))\n                     return\n             else:\n                 with open (filename, \"w\") as fileToWrite:\n                     print (filedata, file=fileToWrite)\n         except IOError as ioerr:\n             print (\"IOError occurred: \" + str (ioerr))\n             return\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> if filename . endswith ( <STRING> ) or ispickle == True : <NEWLINE> <INDENT> dict = eval ( filename ) <NEWLINE> try : <NEWLINE> <INDENT> pickle . dump ( dict , filename ) <NEWLINE> <DEDENT> except pickle . PickleError as perr : <NEWLINE> <INDENT> print ( <STRING> + str ( perr ) ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> with open ( filename , <STRING> ) as fileToWrite : <NEWLINE> <INDENT> print ( filedata , file = fileToWrite ) <NEWLINE> except IOError as ioerr : <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> + str ( ioerr ) ) <NEWLINE> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\".pickle\"",
                "\"Pickling error occurred: \"",
                "\"w\"",
                "\"IOError occurred: \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fa7d42b31dd84f6faa0ef9c65d765a55": {
        "code_string": "def __init__(self, doctype, src, dst, options=None, settings=None):\n         self.doctype = doctype\n         self.src = src\n         self.dst = dst\n         self.options = options or {}\n         self.settings = options or {}\n         self.logfile = pathjoin(self.dst, 'melba.log')\n         self.status = None\n         self.exc_info = None\n         self._build_func = self._get_build_func()\n         self._log = None\n",
        "code_toks_joined": "def __init__ ( self , doctype , src , dst , options = None , settings = None ) : <NEWLINE> <INDENT> self . doctype = doctype <NEWLINE> self . src = src <NEWLINE> self . dst = dst <NEWLINE> self . options = options or { } <NEWLINE> self . settings = options or { } <NEWLINE> self . logfile = pathjoin ( self . dst , <STRING> ) <NEWLINE> self . status = None <NEWLINE> self . exc_info = None <NEWLINE> self . _build_func = self . _get_build_func ( ) <NEWLINE> self . _log = None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'melba.log'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "60b6beafd0d74c32873441cd182efc5e": {
        "code_string": "if 'fitz' in dir():\n         pdf_file = fitz.open(file)\n         while i < len(pdf_file):\n             text += pdf_file[i].getText('text')\n             i += 1\n     else:\n         pdf_file = open(file, 'rb')\n         pdf_reader = PyPDF2.PdfFileReader(file)\n         while i < pdf_reader.numPages:\n             payload = pdf_reader.getPage(i).extractText().replace('\\n', '')\n             text += payload.encode('ascii', 'ignore').decode('unicode_escape')\n             i += 1\n     return text\n",
        "code_toks_joined": "if <STRING> in dir ( ) : <NEWLINE> <INDENT> pdf_file = fitz . open ( file ) <NEWLINE> while i < len ( pdf_file ) : <NEWLINE> <INDENT> text += pdf_file [ i ] . getText ( <STRING> ) <NEWLINE> i += 1 <NEWLINE> else : <NEWLINE> <DEDENT> pdf_file = open ( file , <STRING> ) <NEWLINE> pdf_reader = PyPDF2 . PdfFileReader ( file ) <NEWLINE> while i < pdf_reader . numPages : <NEWLINE> <INDENT> payload = pdf_reader . getPage ( i ) . extractText ( ) . replace ( <STRING> , <STRING> ) <NEWLINE> text += payload . encode ( <STRING> , <STRING> ) . decode ( <STRING> ) <NEWLINE> i += 1 <NEWLINE> return text <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'fitz'",
                "'text'",
                "'rb'",
                "'\\n'",
                "''",
                "'ascii'",
                "'ignore'",
                "'unicode_escape'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f0a7cd770a104fbe84914f3046550904": {
        "code_string": "try:\n             scandir_it = scandir(top)\n         except OSError as error:\n             onerror(error)\n             return\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> scandir_it = scandir ( top ) <NEWLINE> except OSError as error : <NEWLINE> onerror ( error ) <NEWLINE> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d6e584b3833146f79a08c7e3371f488e": {
        "code_string": "def put_unconfirmed(self, tx):\n         assert tx.height is None, 'Not unconfirmed tx {}'.format(tx)\n         if tx.type not in (C.TX_POW_REWARD, C.TX_POS_REWARD):\n             return  # It is Reword tx\n         elif tx.hash in self.unconfirmed:\n             logging.debug('Already unconfirmed tx. {}'.format(tx))\n             return\n         self.unconfirmed[tx.hash] = tx\n         if tx.hash in self.chained_tx:\n             logging.debug('Already chained tx. {}'.format(tx))\n             return\n         user_account.affect_new_tx(tx)\n         # WebSocket api\u306b\u901a\u77e5\n         if P.NEW_CHAIN_INFO_QUE:\n             P.NEW_CHAIN_INFO_QUE.put_nowait(('tx', tx.getinfo()))\n",
        "code_toks_joined": "def put_unconfirmed ( self , tx ) : <NEWLINE> <INDENT> assert tx . height is None , <STRING> . format ( tx ) <NEWLINE> if tx . type not in ( C . TX_POW_REWARD , C . TX_POS_REWARD ) : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> elif tx . hash in self . unconfirmed : <NEWLINE> <INDENT> logging . debug ( <STRING> . format ( tx ) ) <NEWLINE> return <NEWLINE> <DEDENT> self . unconfirmed [ tx . hash ] = tx <NEWLINE> if tx . hash in self . chained_tx : <NEWLINE> <INDENT> logging . debug ( <STRING> . format ( tx ) ) <NEWLINE> return <NEWLINE> <DEDENT> user_account . affect_new_tx ( tx ) <NEWLINE> <COMMENT> <NL> if P . NEW_CHAIN_INFO_QUE : <NEWLINE> <INDENT> P . NEW_CHAIN_INFO_QUE . put_nowait ( ( <STRING> , tx . getinfo ( ) ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Not unconfirmed tx {}'",
                "'Already unconfirmed tx. {}'",
                "'Already chained tx. {}'",
                "'tx'"
            ],
            "<COMMENT>": [
                "# It is Reword tx",
                "# WebSocket api\u306b\u901a\u77e5"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e04bbdb4a0ce449d9051d5baf8ac7d38": {
        "code_string": "def contract_signature_check(extra_tx: TX, v: Validator, include_block: Block):\n     signed_cks = get_signed_cks(extra_tx)\n     accept_cks = signed_cks & set(v.validators)\n     reject_cks = signed_cks - set(v.validators)\n     if len(reject_cks) > 0:\n         raise BlockChainError('Unrelated signature include, reject={}'.format(reject_cks))\n     elif include_block:\n         # check satisfy require?\n         if len(accept_cks) != v.require:\n             raise BlockChainError('Not satisfied require signature. [signed={}, accepted={}, require={}]'\n                                   .format(signed_cks, accept_cks, v.require))\n     else:\n         # check can marge?\n         original_tx = tx_builder.get_tx(txhash=extra_tx.hash)\n         if original_tx is None:\n             # not accept before\n             if 0 < v.require and len(accept_cks) == 0:\n                 raise BlockChainError('No acceptable signature. signed={}'.format(signed_cks))\n             if len(accept_cks) > v.require:\n                 raise BlockChainError('Too many signatures, accept={} req={}'.format(accept_cks, v.require))\n         else:\n             # need to marge signature\n             if original_tx.height is not None:\n                 raise BlockChainError('Already included tx. height={}'.format(original_tx.height))\n             if v.require == 0:\n                 raise BlockChainError('Don\\t need to marge signature.')\n             original_cks = get_signed_cks(original_tx)\n             accept_new_cks = (signed_cks - original_cks) & set(v.validators)\n             if len(accept_new_cks) == 0:\n                 raise BlockChainError('No new acceptable cks. ({} - {}) & {}'\n                                       .format(signed_cks, original_cks, set(v.validators)))\n             if len(accept_new_cks) + len(original_cks) > v.require:\n                 raise BlockChainError('Too many signatures, new={} original={} req={}'\n                                       .format(accept_new_cks, original_cks, v.require))\n",
        "code_toks_joined": "def contract_signature_check ( extra_tx : TX , v : Validator , include_block : Block ) : <NEWLINE> <INDENT> signed_cks = get_signed_cks ( extra_tx ) <NEWLINE> accept_cks = signed_cks & set ( v . validators ) <NEWLINE> reject_cks = signed_cks - set ( v . validators ) <NEWLINE> if len ( reject_cks ) > 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( reject_cks ) ) <NEWLINE> <DEDENT> elif include_block : <NEWLINE> <COMMENT> <NL> <INDENT> if len ( accept_cks ) != v . require : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> <NEWLINE> <INDENT> . format ( signed_cks , accept_cks , v . require ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> original_tx = tx_builder . get_tx ( txhash = extra_tx . hash ) <NEWLINE> if original_tx is None : <NEWLINE> <COMMENT> <NL> <INDENT> if 0 < v . require and len ( accept_cks ) == 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( signed_cks ) ) <NEWLINE> <DEDENT> if len ( accept_cks ) > v . require : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( accept_cks , v . require ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> if original_tx . height is not None : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( original_tx . height ) ) <NEWLINE> <DEDENT> if v . require == 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> ) <NEWLINE> <DEDENT> original_cks = get_signed_cks ( original_tx ) <NEWLINE> accept_new_cks = ( signed_cks - original_cks ) & set ( v . validators ) <NEWLINE> if len ( accept_new_cks ) == 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> <NEWLINE> <INDENT> . format ( signed_cks , original_cks , set ( v . validators ) ) ) <NEWLINE> <DEDENT> <DEDENT> if len ( accept_new_cks ) + len ( original_cks ) > v . require : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> <NEWLINE> <INDENT> . format ( accept_new_cks , original_cks , v . require ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Unrelated signature include, reject={}'",
                "'Not satisfied require signature. [signed={}, accepted={}, require={}]'",
                "'No acceptable signature. signed={}'",
                "'Too many signatures, accept={} req={}'",
                "'Already included tx. height={}'",
                "'Don\\t need to marge signature.'",
                "'No new acceptable cks. ({} - {}) & {}'",
                "'Too many signatures, new={} original={} req={}'"
            ],
            "<COMMENT>": [
                "# check satisfy require?",
                "# check can marge?",
                "# not accept before",
                "# need to marge signature"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4966222a404f4d48b43008b7040664c5": {
        "code_string": "def validator_fill_iter(v: Validator, best_block=None, best_chain=None):\n     # database\n     v_iter = builder.db.read_validator_iter(c_address=v.c_address, start_idx=v.db_index)\n     for index, address, flag, txhash, sig_diff in v_iter:\n         yield index, flag, address, sig_diff, txhash\n     # memory\n     if best_chain:\n         _best_chain = None\n     elif best_block and best_block == builder.best_block:\n         _best_chain = builder.best_chain\n     else:\n         dummy, _best_chain = builder.get_best_chain(best_block=best_block)\n     for block in reversed(best_chain or _best_chain):\n         for tx in block.txs:\n             if tx.type != C.TX_VALIDATOR_EDIT:\n                 continue\n             c_address, address, flag, sig_diff = decode(tx.message)\n             if c_address != v.c_address:\n                 continue\n             index = tx.height * 0xffffffff + block.txs.index(tx)\n             yield index, flag, address, sig_diff, tx.hash\n     # unconfirmed\n     if best_block is None:\n         for tx in sorted(tx_builder.unconfirmed.values(), key=lambda x: x.create_time):\n             if tx.type != C.TX_VALIDATOR_EDIT:\n                 continue\n             c_address, address, flag, sig_diff = decode(tx.message)\n             if c_address != v.c_address:\n                 continue\n             if len(tx.signature) < v.require:\n                 continue\n             yield None, flag, address, sig_diff, tx.hash\n",
        "code_toks_joined": "def validator_fill_iter ( v : Validator , best_block = None , best_chain = None ) : <NEWLINE> <COMMENT> <NL> <INDENT> v_iter = builder . db . read_validator_iter ( c_address = v . c_address , start_idx = v . db_index ) <NEWLINE> for index , address , flag , txhash , sig_diff in v_iter : <NEWLINE> <INDENT> yield index , flag , address , sig_diff , txhash <NEWLINE> <COMMENT> <NL> <DEDENT> if best_chain : <NEWLINE> <INDENT> _best_chain = None <NEWLINE> <DEDENT> elif best_block and best_block == builder . best_block : <NEWLINE> <INDENT> _best_chain = builder . best_chain <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> dummy , _best_chain = builder . get_best_chain ( best_block = best_block ) <NEWLINE> <DEDENT> for block in reversed ( best_chain or _best_chain ) : <NEWLINE> <INDENT> for tx in block . txs : <NEWLINE> <INDENT> if tx . type != C . TX_VALIDATOR_EDIT : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> c_address , address , flag , sig_diff = decode ( tx . message ) <NEWLINE> if c_address != v . c_address : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> index = tx . height * 0xffffffff + block . txs . index ( tx ) <NEWLINE> yield index , flag , address , sig_diff , tx . hash <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if best_block is None : <NEWLINE> <INDENT> for tx in sorted ( tx_builder . unconfirmed . values ( ) , key = lambda x : x . create_time ) : <NEWLINE> <INDENT> if tx . type != C . TX_VALIDATOR_EDIT : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> c_address , address , flag , sig_diff = decode ( tx . message ) <NEWLINE> if c_address != v . c_address : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if len ( tx . signature ) < v . require : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> yield None , flag , address , sig_diff , tx . hash <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# database",
                "# memory",
                "# unconfirmed"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5b1a1b123686422abe1292050a647486": {
        "code_string": "async def new_address(request):\n     with closing(create_db(V.DB_ACCOUNT_PATH)) as db:\n         cur = db.cursor()\n         user_name = request.query.get('account', C.account2name[C.ANT_UNKNOWN])\n         user_id = read_name2user(user_name, cur)\n         address = create_new_user_keypair(user_name, cur)\n         db.commit()\n         if user_id == C.ANT_CONTRACT:\n             address = convert_address(address, V.BLOCK_CONTRACT_PREFIX)\n     return web_base.json_res({'account': user_name, 'user_id': user_id, 'address': address})\n",
        "code_toks_joined": "async def new_address ( request ) : <NEWLINE> <INDENT> with closing ( create_db ( V . DB_ACCOUNT_PATH ) ) as db : <NEWLINE> <INDENT> cur = db . cursor ( ) <NEWLINE> user_name = request . query . get ( <STRING> , C . account2name [ C . ANT_UNKNOWN ] ) <NEWLINE> user_id = read_name2user ( user_name , cur ) <NEWLINE> address = create_new_user_keypair ( user_name , cur ) <NEWLINE> db . commit ( ) <NEWLINE> if user_id == C . ANT_CONTRACT : <NEWLINE> <INDENT> address = convert_address ( address , V . BLOCK_CONTRACT_PREFIX ) <NEWLINE> <DEDENT> <DEDENT> return web_base . json_res ( { <STRING> : user_name , <STRING> : user_id , <STRING> : address } ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'account'",
                "'account'",
                "'user_id'",
                "'address'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fabe5db0942f4ff1878fc16d20cb0ace": {
        "code_string": "def check_already_started():\n     assert V.DB_HOME_DIR is not None\n     # check already started\n     pid_path = os.path.join(V.DB_HOME_DIR, 'pid.lock')\n     if os.path.exists(pid_path):\n         with open(pid_path, mode='r') as fp:\n             pid = int(fp.read())\n         if psutil.pid_exists(pid):\n             raise RuntimeError('Already running blockchain-py pid={}'.format(pid))\n     new_pid = os.getpid()\n     with open(pid_path, mode='w') as fp:\n         fp.write(str(new_pid))\n     log.info(\"create new process lock file pid={}\".format(pid))\n",
        "code_toks_joined": "def check_already_started ( ) : <NEWLINE> <INDENT> assert V . DB_HOME_DIR is not None <NEWLINE> <COMMENT> <NL> pid_path = os . path . join ( V . DB_HOME_DIR , <STRING> ) <NEWLINE> if os . path . exists ( pid_path ) : <NEWLINE> <INDENT> with open ( pid_path , mode = <STRING> ) as fp : <NEWLINE> <INDENT> pid = int ( fp . read ( ) ) <NEWLINE> <DEDENT> if psutil . pid_exists ( pid ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( pid ) ) <NEWLINE> <DEDENT> <DEDENT> new_pid = os . getpid ( ) <NEWLINE> with open ( pid_path , mode = <STRING> ) as fp : <NEWLINE> <INDENT> fp . write ( str ( new_pid ) ) <NEWLINE> <DEDENT> log . info ( <STRING> . format ( pid ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# check already started"
            ],
            "<STRING>": [
                "'pid.lock'",
                "'r'",
                "'Already running blockchain-py pid={}'",
                "'w'",
                "\"create new process lock file pid={}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b87324e50a7746a38e056b747e73bb9a": {
        "code_string": "if not keystroke_history and keystroke_history[-1] == 'g':\n",
        "code_toks_joined": "if not keystroke_history and keystroke_history [ - 1 ] == <STRING> : <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'g'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9e4a674ea4fd41baa05a9dce633bcf3f": {
        "code_string": "@contextmanager\n def timer(name=''):\n     start = time()\n     try:\n         yield\n     finally:\n         duration = time() - start\n         if name:\n             logging.warning('done in %.4g seconds', duration)\n         else:\n             logging.warning('%s - done in %.4g seconds', name, duration)\n",
        "code_toks_joined": "@ contextmanager <NEWLINE> <INDENT> def timer ( name = <STRING> ) : <NEWLINE> <INDENT> start = time ( ) <NEWLINE> try : <NEWLINE> <INDENT> yield <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> duration = time ( ) - start <NEWLINE> if name : <NEWLINE> <INDENT> logging . warning ( <STRING> , duration ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logging . warning ( <STRING> , name , duration ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''",
                "'done in %.4g seconds'",
                "'%s - done in %.4g seconds'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "249b11268a854568bfb8090ede52c8e7": {
        "code_string": "def _discover_run(self, packages, missing=True):\n         #pydev friendly printing\n         def formatPathPrint(path, line=None):\n             if not line:\n                 line = 1\n             path = os.path.realpath(path)\n             return '  File \"%s\", line %d\\n' % (path, line)\n         total, failed = [], []\n         s = SmokeTestDiscover()\n         for pkg_pth in packages:\n             pkg = importlib.import_module(self._path_to_modstr(pkg_pth))\n             #run and count\n             t,f = s.discover_run(pkg)\n             total += t\n             failed += f\n             #Print missing\n             if missing:\n                 for m in s.get_missing(pkg):\n                     pth = m.__file__\n                     if pth.endswith('.pyc'):\n                         pth = f[:-1]\n                     s.log('Missing test in module %s' % m)\n                     s.log(formatPathPrint(pth))\n         #return results\n         return total, failed\n",
        "code_toks_joined": "def _discover_run ( self , packages , missing = True ) : <NEWLINE> <COMMENT> <NL> <INDENT> def formatPathPrint ( path , line = None ) : <NEWLINE> <INDENT> if not line : <NEWLINE> <INDENT> line = 1 <NEWLINE> <DEDENT> path = os . path . realpath ( path ) <NEWLINE> return <STRING> % ( path , line ) <NEWLINE> <DEDENT> total , failed = [ ] , [ ] <NEWLINE> s = SmokeTestDiscover ( ) <NEWLINE> for pkg_pth in packages : <NEWLINE> <INDENT> pkg = importlib . import_module ( self . _path_to_modstr ( pkg_pth ) ) <NEWLINE> <COMMENT> <NL> t , f = s . discover_run ( pkg ) <NEWLINE> total += t <NEWLINE> failed += f <NEWLINE> <COMMENT> <NL> if missing : <NEWLINE> <INDENT> for m in s . get_missing ( pkg ) : <NEWLINE> <INDENT> pth = m . __file__ <NEWLINE> if pth . endswith ( <STRING> ) : <NEWLINE> <INDENT> pth = f [ : - 1 ] <NEWLINE> <DEDENT> s . log ( <STRING> % m ) <NEWLINE> s . log ( formatPathPrint ( pth ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> return total , failed <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#pydev friendly printing",
                "#run and count",
                "#Print missing",
                "#return results"
            ],
            "<STRING>": [
                "'  File \"%s\", line %d\\n'",
                "'.pyc'",
                "'Missing test in module %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9e995f360214445c821b3022e24d3696": {
        "code_string": "#write in the steps requested into the pipeline\n             for step in steps:\n                 #if Complete Genomics data, i.e., did split gap then pipeline requires different scripts with shorter reads due to splitting into multiple reads at the gap\n                 if 'split_gap' in steps and step == 'remove_numts':\n                     job_name = 'remove_numts_no_split_gap.sh'\n                 #step only is name of the step, not the name of the script\n                 else:\n                     job_name = step + \".sh\"\n                 pipeline.write(self.get_template(slurm, prev_step, job_name, step))\n                 if \"gatk\" in step or step not in self.softwares:\n                     prev_step = self.task_names[step]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for step in steps : <NEWLINE> <COMMENT> <NL> <INDENT> if <STRING> in steps and step == <STRING> : <NEWLINE> <INDENT> job_name = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> else : <NEWLINE> <INDENT> job_name = step + <STRING> <NEWLINE> <DEDENT> pipeline . write ( self . get_template ( slurm , prev_step , job_name , step ) ) <NEWLINE> if <STRING> in step or step not in self . softwares : <NEWLINE> <INDENT> prev_step = self . task_names [ step ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#write in the steps requested into the pipeline",
                "#if Complete Genomics data, i.e., did split gap then pipeline requires different scripts with shorter reads due to splitting into multiple reads at the gap",
                "#step only is name of the step, not the name of the script"
            ],
            "<STRING>": [
                "'split_gap'",
                "'remove_numts'",
                "'remove_numts_no_split_gap.sh'",
                "\".sh\"",
                "\"gatk\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "88db08f7db4b429280ca6f1b0b8bd157": {
        "code_string": "# ax_groupdelay = fig2.add_subplot(313)\n     # ax_groupdelay = plotwidget.axes5\n     ax_groupdelay = ax_power.twinx()\n     # set_as_3rd_yaxis(ax_groupdelay)\n     ax_groupdelay.plot(omega/2/np.pi,\n                        -np.gradient(np.unwrap(\n                            np.angle(transferfunc)))/np.gradient(omega)*1000,\n                        'r--')\n     ax_power.set_title('Speaker Performance')\n     ax_power.set_ylabel('SPL (dB 1W1m)', color='b')\n     # ax_phase.set_ylabel('Phase (degrees)')\n     ax_groupdelay.set_ylabel('Group Delay (ms)', color='r')\n     ax_groupdelay.set_xlabel('Frequency (Hz)')\n     ax_power.set_xscale('log')\n     ax_power.set_xlim([20, 20000])\n     ax_power.xaxis.set_major_formatter(\n         matplotlib.ticker.FormatStrFormatter(\"%d\"))\n     # ax_phase.set_xscale('log')\n     # ax_phase.xaxis.set_major_formatter(\n     #     matplotlib.ticker.FormatStrFormatter(\"%d\"))\n     ax_groupdelay.set_xscale('log')\n     ax_groupdelay.xaxis.set_major_formatter(\n         matplotlib.ticker.FormatStrFormatter(\"%d\"))\n     ax_power.grid(True, which=\"both\", color=\"0.65\", ls='-')\n     # ax_phase.grid(True, which=\"both\", color=\"0.65\", ls='-')\n     ax_groupdelay.grid(True, which=\"both\", color=\"0.65\", ls='-')\n     for tlabel in ax_power.get_yticklabels():\n         tlabel.set_color('b')\n     for tlabel in ax_groupdelay.get_yticklabels():\n         tlabel.set_color('r')\n     align_y_axis(ax_power, ax_groupdelay, 1, .1)\n     # fig2.show()\n     plotwidget.draw()\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> ax_groupdelay = ax_power . twinx ( ) <NEWLINE> <COMMENT> <NL> ax_groupdelay . plot ( omega / 2 / np . pi , <NEWLINE> <INDENT> - np . gradient ( np . unwrap ( <NEWLINE> <INDENT> np . angle ( transferfunc ) ) ) / np . gradient ( omega ) * 1000 , <NEWLINE> <DEDENT> <STRING> ) <NEWLINE> <DEDENT> ax_power . set_title ( <STRING> ) <NEWLINE> ax_power . set_ylabel ( <STRING> , color = <STRING> ) <NEWLINE> <COMMENT> <NL> ax_groupdelay . set_ylabel ( <STRING> , color = <STRING> ) <NEWLINE> ax_groupdelay . set_xlabel ( <STRING> ) <NEWLINE> ax_power . set_xscale ( <STRING> ) <NEWLINE> ax_power . set_xlim ( [ 20 , 20000 ] ) <NEWLINE> ax_power . xaxis . set_major_formatter ( <NEWLINE> <INDENT> matplotlib . ticker . FormatStrFormatter ( <STRING> ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> ax_groupdelay . set_xscale ( <STRING> ) <NEWLINE> ax_groupdelay . xaxis . set_major_formatter ( <NEWLINE> <INDENT> matplotlib . ticker . FormatStrFormatter ( <STRING> ) ) <NEWLINE> <DEDENT> ax_power . grid ( True , which = <STRING> , color = <STRING> , ls = <STRING> ) <NEWLINE> <COMMENT> <NL> ax_groupdelay . grid ( True , which = <STRING> , color = <STRING> , ls = <STRING> ) <NEWLINE> for tlabel in ax_power . get_yticklabels ( ) : <NEWLINE> <INDENT> tlabel . set_color ( <STRING> ) <NEWLINE> <DEDENT> for tlabel in ax_groupdelay . get_yticklabels ( ) : <NEWLINE> <INDENT> tlabel . set_color ( <STRING> ) <NEWLINE> <DEDENT> align_y_axis ( ax_power , ax_groupdelay , 1 , .1 ) <NEWLINE> <COMMENT> <NL> plotwidget . draw ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ax_groupdelay = fig2.add_subplot(313)",
                "# ax_groupdelay = plotwidget.axes5",
                "# set_as_3rd_yaxis(ax_groupdelay)",
                "# ax_phase.set_ylabel('Phase (degrees)')",
                "# ax_phase.set_xscale('log')",
                "# ax_phase.xaxis.set_major_formatter(",
                "#     matplotlib.ticker.FormatStrFormatter(\"%d\"))",
                "# ax_phase.grid(True, which=\"both\", color=\"0.65\", ls='-')",
                "# fig2.show()"
            ],
            "<STRING>": [
                "'r--'",
                "'Speaker Performance'",
                "'SPL (dB 1W1m)'",
                "'b'",
                "'Group Delay (ms)'",
                "'r'",
                "'Frequency (Hz)'",
                "'log'",
                "\"%d\"",
                "'log'",
                "\"%d\"",
                "\"both\"",
                "\"0.65\"",
                "'-'",
                "\"both\"",
                "\"0.65\"",
                "'-'",
                "'b'",
                "'r'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c18acee939db42f2abba611ecda1a8ab": {
        "code_string": "def load_dataset(self):\n         if(self._source is None):\n             self._data_set = self._source.retrieve_dataset()\n             self._is_load_information = True\n",
        "code_toks_joined": "def load_dataset ( self ) : <NEWLINE> <INDENT> if ( self . _source is None ) : <NEWLINE> <INDENT> self . _data_set = self . _source . retrieve_dataset ( ) <NEWLINE> self . _is_load_information = True <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3ad2582fd942420aa354c5bcf9a5f918": {
        "code_string": "dgt = asyncdgt.auto_connect(port_globs, loop)\n",
        "code_toks_joined": "dgt = asyncdgt . auto_connect ( port_globs , loop ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9a88839214274120aa809a41233cb809": {
        "code_string": "fields = data.get(\"fields\", list())\n         for (field) in fields:\n             field = Field.from_dict(field)\n             fragment.add_field(fields)\n",
        "code_toks_joined": "fields = data . get ( <STRING> , list ( ) ) <NEWLINE> <INDENT> for ( field ) in fields : <NEWLINE> <INDENT> field = Field . from_dict ( field ) <NEWLINE> fragment . add_field ( fields ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"fields\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d45485b5e8234970b01b1c4e66b2c67c": {
        "code_string": "return topic\n",
        "code_toks_joined": "return topic <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aa6c4d92e6ae4285a7456f1f8739cc8b": {
        "code_string": "if is_url(input_value):\n         detected_type = 'url'\n         new_actions.append(set_input_type(detected_type))\n         new_actions.append(add_task(FETCH_HTTP_NODE, url=input_value))\n         new_actions.append(set_validation_subject(input_value))\n     elif input_is_json(input_value):\n         id_url = find_id_in_jsonld(input_value, options.get('jsonld_options', jsonld_use_cache))\n         if is_url(id_url):\n             detected_type = 'url'\n             new_actions.append(store_input(id_url))\n         else:\n             detected_type = 'json'\n         new_actions.append(set_input_type(detected_type))\n         if detected_type == 'url':\n             new_actions.append(add_task(FETCH_HTTP_NODE, url=id_url))\n             new_actions.append(set_validation_subject(input_value))\n     elif input_is_jws(input_value):\n         detected_type = 'jws'\n         new_actions.append(set_input_type(detected_type))\n         new_actions.append(add_task(PROCESS_JWS_INPUT, data=input_value))\n     else:\n         raise NotImplementedError(\"only URL, JSON, or JWS input implemented so far\")\n",
        "code_toks_joined": "if is_url ( input_value ) : <NEWLINE> <INDENT> detected_type = <STRING> <NEWLINE> new_actions . append ( set_input_type ( detected_type ) ) <NEWLINE> new_actions . append ( add_task ( FETCH_HTTP_NODE , url = input_value ) ) <NEWLINE> new_actions . append ( set_validation_subject ( input_value ) ) <NEWLINE> elif input_is_json ( input_value ) : <NEWLINE> id_url = find_id_in_jsonld ( input_value , options . get ( <STRING> , jsonld_use_cache ) ) <NEWLINE> if is_url ( id_url ) : <NEWLINE> <INDENT> detected_type = <STRING> <NEWLINE> new_actions . append ( store_input ( id_url ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> detected_type = <STRING> <NEWLINE> <DEDENT> new_actions . append ( set_input_type ( detected_type ) ) <NEWLINE> if detected_type == <STRING> : <NEWLINE> <INDENT> new_actions . append ( add_task ( FETCH_HTTP_NODE , url = id_url ) ) <NEWLINE> new_actions . append ( set_validation_subject ( input_value ) ) <NEWLINE> elif input_is_jws ( input_value ) : <NEWLINE> <DEDENT> detected_type = <STRING> <NEWLINE> new_actions . append ( set_input_type ( detected_type ) ) <NEWLINE> new_actions . append ( add_task ( PROCESS_JWS_INPUT , data = input_value ) ) <NEWLINE> else : <NEWLINE> raise NotImplementedError ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'url'",
                "'jsonld_options'",
                "'url'",
                "'json'",
                "'url'",
                "'jws'",
                "\"only URL, JSON, or JWS input implemented so far\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3519294f49b84e19af184be0289deb16": {
        "code_string": "def collision_free(object1, object2, car_scale_factor, pedestrian_scale_factor):\n     #the if else statements determines whether the object is pedestrian or not so it can unpack its coordinates and angle orientation, and determines if it should get the vertices of a car or pedestrian\n     #it returns True if no collision has happened, False otherwise \n     object1_vertices, x, y, radius = get_bounding_box(object1, car_scale_factor, pedestrian_scale_factor)\n     object2_vertices, x2, y2, radius2 = get_bounding_box(object1, car_scale_factor, pedestrian_scale_factor)\n",
        "code_toks_joined": "def collision_free ( object1 , object2 , car_scale_factor , pedestrian_scale_factor ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> object1_vertices , x , y , radius = get_bounding_box ( object1 , car_scale_factor , pedestrian_scale_factor ) <NEWLINE> object2_vertices , x2 , y2 , radius2 = get_bounding_box ( object1 , car_scale_factor , pedestrian_scale_factor ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#the if else statements determines whether the object is pedestrian or not so it can unpack its coordinates and angle orientation, and determines if it should get the vertices of a car or pedestrian",
                "#it returns True if no collision has happened, False otherwise "
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5856c904bd0443a78a2b01624f642700": {
        "code_string": "self.set_title(page, utils._remove_ext(obj))\n",
        "code_toks_joined": "self . set_title ( page , utils . _remove_ext ( obj ) ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "971fef59fe41443da4dcec2e28e6ed02": {
        "code_string": "poss = []\n     height, pos = left_peak_height_pos(mmr_size)\n     poss.append(pos)\n     while height > 0:\n         height, pos = get_right_peak(height, pos, mmr_size)\n         if height > 0:\n             poss.append(pos)\n     return poss\n",
        "code_toks_joined": "poss = [ ] <NEWLINE> <INDENT> height , pos = left_peak_height_pos ( mmr_size ) <NEWLINE> poss . append ( pos ) <NEWLINE> while height > 0 : <NEWLINE> <INDENT> height , pos = get_right_peak ( height , pos , mmr_size ) <NEWLINE> if height > 0 : <NEWLINE> <INDENT> poss . append ( pos ) <NEWLINE> <DEDENT> <DEDENT> return poss <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9adae18041a74f86a8e318ebe4621598": {
        "code_string": "def random_interval():\n     now = datetime.now()\n     start = now - timedelta(days=random.randint(0, 365*3))\n     end = start - timedelta(days=random.randint(1, 10))\n     return start.isoformat(), end.isoformat()\n",
        "code_toks_joined": "def random_interval ( ) : <NEWLINE> <INDENT> now = datetime . now ( ) <NEWLINE> start = now - timedelta ( days = random . randint ( 0 , 365 * 3 ) ) <NEWLINE> end = start - timedelta ( days = random . randint ( 1 , 10 ) ) <NEWLINE> return start . isoformat ( ) , end . isoformat ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8651298c7f13433ba8c6ce72b7e1f119": {
        "code_string": "logs.append(logs)\n",
        "code_toks_joined": "logs . append ( logs ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c7fa038cc4ec4e21ac8428655ceea1f9": {
        "code_string": "data_files[location][parameter] = filename\n                 else:\n                     data_files[location][parameter] = None\n",
        "code_toks_joined": "data_files [ location ] [ parameter ] = filename <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> data_files [ location ] [ parameter ] = None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "32e60f248e984c55a973e0648c63a65c": {
        "code_string": "# update feature geometry metadata\n         with rasterio.open(dst) as f:\n             geometry = util.bbox2poly(f.bounds.left, f.bounds.right, f.bounds.bottom, f.bounds.top, as_shapely=True)\n         update_metadata(feature, quest_metadata={'geometry': geometry.to_wkt()})\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> with rasterio . open ( dst ) as f : <NEWLINE> <INDENT> geometry = util . bbox2poly ( f . bounds . left , f . bounds . right , f . bounds . bottom , f . bounds . top , as_shapely = True ) <NEWLINE> <DEDENT> update_metadata ( feature , quest_metadata = { <STRING> : geometry . to_wkt ( ) } ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# update feature geometry metadata"
            ],
            "<STRING>": [
                "'geometry'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "74c714535d134c5c9fa440e0e7982605": {
        "code_string": "snap = options.get('snap_outlets')\n             if snap is not None:\n                 if snap.lower() == 'maximum_flow_accumulation':\n                     proj_points = snap_points_max_flow_acc(flow_accumulation, proj_points, options.get('search_box_pixels'))\n                 if snap.lower() == 'jenson':\n                     stream_threshold_pct = options.get('stream_threshold_pct')\n                     stream_threshold_abs = options.get('stream_threshold_abs')\n                     outlet_points = snap_points_jenson(flow_accumulation, proj_points, \n                         stream_threshold_pct=stream_threshold_pct, stream_threshold_abs=stream_threshold_abs)\n                 if p.is_latlong():\n                     snapped_points = [src.xy(*point) for point in proj_points]\n                 else:\n                     snapped_points = [src.xy(*p(*point, inverse=True)) for point in proj_points]\n",
        "code_toks_joined": "snap = options . get ( <STRING> ) <NEWLINE> <INDENT> if snap is not None : <NEWLINE> <INDENT> if snap . lower ( ) == <STRING> : <NEWLINE> <INDENT> proj_points = snap_points_max_flow_acc ( flow_accumulation , proj_points , options . get ( <STRING> ) ) <NEWLINE> <DEDENT> if snap . lower ( ) == <STRING> : <NEWLINE> <INDENT> stream_threshold_pct = options . get ( <STRING> ) <NEWLINE> stream_threshold_abs = options . get ( <STRING> ) <NEWLINE> outlet_points = snap_points_jenson ( flow_accumulation , proj_points , <NEWLINE> <INDENT> stream_threshold_pct = stream_threshold_pct , stream_threshold_abs = stream_threshold_abs ) <NEWLINE> <DEDENT> <DEDENT> if p . is_latlong ( ) : <NEWLINE> <INDENT> snapped_points = [ src . xy ( * point ) for point in proj_points ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> snapped_points = [ src . xy ( * p ( * point , inverse = True ) ) for point in proj_points ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'snap_outlets'",
                "'maximum_flow_accumulation'",
                "'search_box_pixels'",
                "'jenson'",
                "'stream_threshold_pct'",
                "'stream_threshold_abs'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6c7c82880d6244279ff6f738a0ae9327": {
        "code_string": "collection_path = os.path.join(project_path, feature_metadata['collection'])\n",
        "code_toks_joined": "collection_path = os . path . join ( project_path , feature_metadata [ <STRING> ] ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'collection'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9833ba1e3a5f423884d811de04a7be3b": {
        "code_string": "def readReg(self, addr):\n     return self.spi.xfer([addr | 0x7F, 0])\n",
        "code_toks_joined": "def readReg ( self , addr ) : <NEWLINE> <INDENT> return self . spi . xfer ( [ addr | 0x7F , 0 ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "48f366d175894257afc64b7152f792cc": {
        "code_string": "model_store = resource.ModelStore(complete_conf)\n     model_store.dump_trained_model(model_conf, model, metrics)\n",
        "code_toks_joined": "model_store = resource . ModelStore ( complete_conf ) <NEWLINE> <INDENT> model_store . dump_trained_model ( model_conf , model , metrics ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "777c5ae735714d87bc1ed09dbafd4eaf": {
        "code_string": "def is_valid(data, explain=False):\n         if not explain:\n             return predicate(data)\n         elems = list(data)\n         valid, explanation = predicate(elems, explain=True)\n         return (True, explanation) if valid else (False, {\n             elems[i]: value for i, value in explanation.items()\n         })\n     return is_if(is_set, predicate, else_valid=False)\n",
        "code_toks_joined": "def is_valid ( data , explain = False ) : <NEWLINE> <INDENT> if not explain : <NEWLINE> <INDENT> return predicate ( data ) <NEWLINE> <DEDENT> elems = list ( data ) <NEWLINE> valid , explanation = predicate ( elems , explain = True ) <NEWLINE> return ( True , explanation ) if valid else ( False , { <NEWLINE> <INDENT> elems [ i ] : value for i , value in explanation . items ( ) <NEWLINE> <DEDENT> } ) <NEWLINE> return is_if ( is_set , predicate , else_valid = False ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cb044e8618e644adbfb092b12865ed48": {
        "code_string": "pool = Pool(50)\n         i = 0\n         with ProgressBar(max_value=len(self.dataframe)) as p:\n             for _ in pool.imap_unordered(save, self.iter()):\n                 i += 1\n                 if i % 10 == 0:\n                     pool.update(i)\n",
        "code_toks_joined": "pool = Pool ( 50 ) <NEWLINE> <INDENT> i = 0 <NEWLINE> with ProgressBar ( max_value = len ( self . dataframe ) ) as p : <NEWLINE> <INDENT> for _ in pool . imap_unordered ( save , self . iter ( ) ) : <NEWLINE> <INDENT> i += 1 <NEWLINE> if i % 10 == 0 : <NEWLINE> <INDENT> pool . update ( i ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d624707b4fb0476ea8ffdc9a03ed1939": {
        "code_string": "if self.parent is not None:\n             for k in self.parent.keys():\n                 if k in _keys_set:\n                     _keys_set.add(k)\n                     yield k\n",
        "code_toks_joined": "if self . parent is not None : <NEWLINE> <INDENT> for k in self . parent . keys ( ) : <NEWLINE> <INDENT> if k in _keys_set : <NEWLINE> <INDENT> _keys_set . add ( k ) <NEWLINE> yield k <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "868a4e1b2ef6462c852de271af83b17a": {
        "code_string": "if not (path.exists() and path.suffix):\n                 for ext in Node._strip_exts:\n                     new_path = path.with_suffix(ext)\n                     if new_path.exists():\n                         return str(new_path)\n             elif path.is_dir():\n                 new_path = pathlib.Path(path, Node._index_file)\n                 if new_path.exists():\n                     return str(new_path)\n",
        "code_toks_joined": "if not ( path . exists ( ) and path . suffix ) : <NEWLINE> <INDENT> for ext in Node . _strip_exts : <NEWLINE> <INDENT> new_path = path . with_suffix ( ext ) <NEWLINE> if new_path . exists ( ) : <NEWLINE> <INDENT> return str ( new_path ) <NEWLINE> elif path . is_dir ( ) : <NEWLINE> <DEDENT> <DEDENT> new_path = pathlib . Path ( path , Node . _index_file ) <NEWLINE> if new_path . exists ( ) : <NEWLINE> <INDENT> return str ( new_path ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7236b32da9b54c14bdb96f91597f0ba9": {
        "code_string": "#now add to amount recruited\n         recruit+=t_count\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> recruit += t_count <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#now add to amount recruited"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3e8ffbea80754fd9829342b902e6b5cb": {
        "code_string": "fe = fg.add_entry()\n             fe.author(name=page_id, email=\"%s.facebook.no-reply@fb2feed.org\" % page_id)\n             fe.id(post_url)\n             fe.link(href=post_url, rel=\"alternate\")\n             fe.published(timestamp)\n             fe.updated(timestamp)\n             fe.title(' '.join(txt.split(' ')[:15]))\n             fe.content(entry_content, type=\"html\")\n",
        "code_toks_joined": "fe = fg . add_entry ( ) <NEWLINE> <INDENT> fe . author ( name = page_id , email = <STRING> % page_id ) <NEWLINE> fe . id ( post_url ) <NEWLINE> fe . link ( href = post_url , rel = <STRING> ) <NEWLINE> fe . published ( timestamp ) <NEWLINE> fe . updated ( timestamp ) <NEWLINE> fe . title ( <STRING> . join ( txt . split ( <STRING> ) [ : 15 ] ) ) <NEWLINE> fe . content ( entry_content , type = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"%s.facebook.no-reply@fb2feed.org\"",
                "\"alternate\"",
                "' '",
                "' '",
                "\"html\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "14fc91b00ef74ee5ab0a62043191b7e1": {
        "code_string": "# If a new extra is being supplied, update the metadata\n         if reason and node.metadata and reason.extras and set(reason.extras) & node.extras:\n             metadata_to_apply = node.metadata\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if reason and node . metadata and reason . extras and set ( reason . extras ) & node . extras : <NEWLINE> <INDENT> metadata_to_apply = node . metadata <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If a new extra is being supplied, update the metadata"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9a450da98d66426082b9d7bf2121dc74": {
        "code_string": "def _validate_oid(self):\n         # Verify the oid is a UUID type variable\n         if isinstance(getattr(self, 'oid'), uuid.UUID):\n             return \"oid not of type uuid.UUID but type {}, value {}\".format(type(getattr(self, 'oid')), getattr(self, 'oid'))\n         return None\n",
        "code_toks_joined": "def _validate_oid ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> if isinstance ( getattr ( self , <STRING> ) , uuid . UUID ) : <NEWLINE> <INDENT> return <STRING> . format ( type ( getattr ( self , <STRING> ) ) , getattr ( self , <STRING> ) ) <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Verify the oid is a UUID type variable"
            ],
            "<STRING>": [
                "'oid'",
                "\"oid not of type uuid.UUID but type {}, value {}\"",
                "'oid'",
                "'oid'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b585577e92034476a0c8c0eaecc55ead": {
        "code_string": "try:\n             value = __next__(self)\n         finally:\n             self._started_iteration = True\n             bound_method = __next__.__get__(self.__class__, self)\n             self.__next__ = bound_method  # <- Replace __next__ method!\n         return value\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> value = __next__ ( self ) <NEWLINE> finally : <NEWLINE> self . _started_iteration = True <NEWLINE> bound_method = __next__ . __get__ ( self . __class__ , self ) <NEWLINE> self . __next__ = bound_method <COMMENT> <NEWLINE> return value <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# <- Replace __next__ method!"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d6d61327735c4113b93c4b9eb0ffa163": {
        "code_string": "if not self.dry_run:\n             if os.path.isfile(self.imagefile):\n                 raise RuntimeError(\"Image {} does not exist\".format(self.imagefile))\n",
        "code_toks_joined": "if not self . dry_run : <NEWLINE> <INDENT> if os . path . isfile ( self . imagefile ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( self . imagefile ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Image {} does not exist\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "445ad69a70f34312b8669671ecdc49f0": {
        "code_string": "if  epoch_start_time < epoch_end_time:\n         raise Exception(INVALID_DATES)\n",
        "code_toks_joined": "if epoch_start_time < epoch_end_time : <NEWLINE> <INDENT> raise Exception ( INVALID_DATES ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1876136ae86e478ead95ec2d4c5a7402": {
        "code_string": "def act_cz(self, control, target):\n         \"\"\" Act a CU somewhere \"\"\"\n         control = 1 << control\n         target = 1 << control\n         for i in xrange(self.d):\n             if (i & control) and (i & target):\n                 self.state[i, 0] *= -1\n",
        "code_toks_joined": "def act_cz ( self , control , target ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> control = 1 << control <NEWLINE> target = 1 << control <NEWLINE> for i in xrange ( self . d ) : <NEWLINE> <INDENT> if ( i & control ) and ( i & target ) : <NEWLINE> <INDENT> self . state [ i , 0 ] *= - 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Act a CU somewhere \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "726b0b1711b14550a4d4df4bf4b82c74": {
        "code_string": "ci = self.get_connection_info(a, b)\n         if ci[\"non1\"] and not clifford.is_diagonal(self.node[a][\"vop\"]):\n             debug(\"cphase: left one needs treatment again -> putting it to Id\")\n             self.remove_vop(b, a)\n",
        "code_toks_joined": "ci = self . get_connection_info ( a , b ) <NEWLINE> <INDENT> if ci [ <STRING> ] and not clifford . is_diagonal ( self . node [ a ] [ <STRING> ] ) : <NEWLINE> <INDENT> debug ( <STRING> ) <NEWLINE> self . remove_vop ( b , a ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"non1\"",
                "\"vop\"",
                "\"cphase: left one needs treatment again -> putting it to Id\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "32ae6825fd13478c91fd962a3af5ba8a": {
        "code_string": "if mode == 'parse':\n             chunk_info = data['chunk_info']\n             self.id_        = chunk_info['id']\n             self.head       = chunk_info['head']\n             self.dep        = chunk_info['dep']\n             self.chunk_head = chunk_info['chunk_head']\n             self.chunk_func = chunk_info['chunk_func']\n             self.links      = [\n                 Reshape(mode='links', data=link)\n                 for link in chunk_info['links']\n             ]\n             self.predicate  = chunk_info['predicate'] if 'predicate' in data else []\n",
        "code_toks_joined": "if mode == <STRING> : <NEWLINE> <INDENT> chunk_info = data [ <STRING> ] <NEWLINE> self . id_ = chunk_info [ <STRING> ] <NEWLINE> self . head = chunk_info [ <STRING> ] <NEWLINE> self . dep = chunk_info [ <STRING> ] <NEWLINE> self . chunk_head = chunk_info [ <STRING> ] <NEWLINE> self . chunk_func = chunk_info [ <STRING> ] <NEWLINE> self . links = [ <NEWLINE> <INDENT> Reshape ( mode = <STRING> , data = link ) <NEWLINE> for link in chunk_info [ <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> self . predicate = chunk_info [ <STRING> ] if <STRING> in data else [ ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'parse'",
                "'chunk_info'",
                "'id'",
                "'head'",
                "'dep'",
                "'chunk_head'",
                "'chunk_func'",
                "'links'",
                "'links'",
                "'predicate'",
                "'predicate'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "24d731c9684044aa86d3868196179b5f": {
        "code_string": "return build_dataframe(raw_data, aggregateby, groupby)\n",
        "code_toks_joined": "return build_dataframe ( raw_data , aggregateby , groupby ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aeccdcad351c4e21b779fa18f8b27d50": {
        "code_string": "def formfield(self, **kwargs):\n         defaults = {\n             'max_length': self.max_length,\n             'min_length': self.min_length,\n         }\n         defaults.update(kwargs)\n         return super(RandomStringFieldBase, self).formfield(**kwargs)\n",
        "code_toks_joined": "def formfield ( self , ** kwargs ) : <NEWLINE> <INDENT> defaults = { <NEWLINE> <INDENT> <STRING> : self . max_length , <NEWLINE> <STRING> : self . min_length , <NEWLINE> <DEDENT> } <NEWLINE> defaults . update ( kwargs ) <NEWLINE> return super ( RandomStringFieldBase , self ) . formfield ( ** kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'max_length'",
                "'min_length'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bbaf7b4f7b84422892d5a7ceb1a56c5e": {
        "code_string": "for x in etree.iter():\n             if x.tag == 'method':\n                 self.name = x.attrib['name']\n             if x.tag == 'arg':\n                 if x.attrib['direction'] == 'out':\n                     self.return_signature = x.attrib['type']\n                 elif x.attrib['direction'] == 'in':\n                     self.arg_signatures.append(x.attrib['type'])\n",
        "code_toks_joined": "for x in etree . iter ( ) : <NEWLINE> <INDENT> if x . tag == <STRING> : <NEWLINE> <INDENT> self . name = x . attrib [ <STRING> ] <NEWLINE> <DEDENT> if x . tag == <STRING> : <NEWLINE> <INDENT> if x . attrib [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> self . return_signature = x . attrib [ <STRING> ] <NEWLINE> <DEDENT> elif x . attrib [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> self . arg_signatures . append ( x . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'method'",
                "'name'",
                "'arg'",
                "'direction'",
                "'out'",
                "'type'",
                "'direction'",
                "'in'",
                "'type'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "277d6ffbe0314df8b7bd89ec6fd6a963": {
        "code_string": "# Automatically determine device and data_format\n   (device, data_format) = ('/gpu:0', 'channels_first')\n   if flags_obj.no_gpu or tf.test.is_gpu_available():\n     (device, data_format) = ('/cpu:0', 'channels_last')\n   # If data_format is defined in FLAGS, overwrite automatically set value.\n   if flags_obj.data_format is not None:\n     data_format = flags_obj.data_format\n   print('Using device %s, and data format %s.' % (device, data_format))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> if flags_obj . no_gpu or tf . test . is_gpu_available ( ) : <NEWLINE> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if flags_obj . data_format is not None : <NEWLINE> <INDENT> data_format = flags_obj . data_format <NEWLINE> <DEDENT> print ( <STRING> % ( device , data_format ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Automatically determine device and data_format",
                "# If data_format is defined in FLAGS, overwrite automatically set value."
            ],
            "<STRING>": [
                "'/gpu:0'",
                "'channels_first'",
                "'/cpu:0'",
                "'channels_last'",
                "'Using device %s, and data format %s.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2f41f08ebc03447da1c6e03f06a300d5": {
        "code_string": "policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions,\n                                                                  logits=logits)\n     policy_loss *= tf.stop_gradient(advantage)\n     policy_loss = 0.01 * entropy\n     total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n     return total_loss\n",
        "code_toks_joined": "policy_loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = memory . actions , <NEWLINE> <INDENT> logits = logits ) <NEWLINE> policy_loss *= tf . stop_gradient ( advantage ) <NEWLINE> policy_loss = 0.01 * entropy <NEWLINE> total_loss = tf . reduce_mean ( ( 0.5 * value_loss + policy_loss ) ) <NEWLINE> return total_loss <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3b4f080ad9594b4583f101c772ae47bc": {
        "code_string": "for f in w.required_fields:\n         f_name = f.replace(\":\",\"_\")\n         setattr(SingleForm,f_name,StringField(f_name))\n",
        "code_toks_joined": "for f in w . required_fields : <NEWLINE> <INDENT> f_name = f . replace ( <STRING> , <STRING> ) <NEWLINE> setattr ( SingleForm , f_name , StringField ( f_name ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\":\"",
                "\"_\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "93b0e06310f84b82850fc58527f994cb": {
        "code_string": "def login(request):\n     \"Displays the login form and handles the login action.\"\n     manipulator = AuthenticationForm(request)\n     redirect_to = request.REQUEST.get(REDIRECT_FIELD_NAME, '')\n     if request.POST:\n         errors = manipulator.get_validation_errors(request.POST)\n         if not errors:\n             # Light security check -- make sure redirect_to isn't garbage.\n             if not redirect_to or '://' in redirect_to or ' ' in redirect_to:\n                 redirect_to = '/accounts/profile/'\n             request.session[users.SESSION_KEY] = manipulator.get_user_id()\n             return HttpResponseRedirect(redirect_to)\n     else:\n         errors = {}\n     response = HttpResponse()\n     response.session.set_test_cookie()\n     t = template_loader.get_template('registration/login')\n     c = Context(request, {\n         'form': formfields.FormWrapper(manipulator, request.POST, errors),\n         REDIRECT_FIELD_NAME: redirect_to,\n         'site_name': sites.get_current().name,\n     })\n     response.write(t.render(c))\n     return response\n",
        "code_toks_joined": "def login ( request ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> manipulator = AuthenticationForm ( request ) <NEWLINE> redirect_to = request . REQUEST . get ( REDIRECT_FIELD_NAME , <STRING> ) <NEWLINE> if request . POST : <NEWLINE> <INDENT> errors = manipulator . get_validation_errors ( request . POST ) <NEWLINE> if not errors : <NEWLINE> <COMMENT> <NL> <INDENT> if not redirect_to or <STRING> in redirect_to or <STRING> in redirect_to : <NEWLINE> <INDENT> redirect_to = <STRING> <NEWLINE> <DEDENT> request . session [ users . SESSION_KEY ] = manipulator . get_user_id ( ) <NEWLINE> return HttpResponseRedirect ( redirect_to ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> errors = { } <NEWLINE> <DEDENT> response = HttpResponse ( ) <NEWLINE> response . session . set_test_cookie ( ) <NEWLINE> t = template_loader . get_template ( <STRING> ) <NEWLINE> c = Context ( request , { <NEWLINE> <INDENT> <STRING> : formfields . FormWrapper ( manipulator , request . POST , errors ) , <NEWLINE> REDIRECT_FIELD_NAME : redirect_to , <NEWLINE> <STRING> : sites . get_current ( ) . name , <NEWLINE> <DEDENT> } ) <NEWLINE> response . write ( t . render ( c ) ) <NEWLINE> return response <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Displays the login form and handles the login action.\"",
                "''",
                "'://'",
                "' '",
                "'/accounts/profile/'",
                "'registration/login'",
                "'form'",
                "'site_name'"
            ],
            "<COMMENT>": [
                "# Light security check -- make sure redirect_to isn't garbage."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6b95af52d15640aca2b3f197fc580f00": {
        "code_string": "def test_cache_page_old_style(self):\n         \"\"\"\n         Test that we can call cache_page the old way\n         \"\"\"\n         def my_view(request):\n             return \"response\"\n         my_view_cached = cache_page(123, my_view)\n         self.assertEqual(my_view_cached(HttpRequest()), \"response\")\n",
        "code_toks_joined": "def test_cache_page_old_style ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def my_view ( request ) : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> my_view_cached = cache_page ( 123 , my_view ) <NEWLINE> self . assertEqual ( my_view_cached ( HttpRequest ( ) ) , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Test that we can call cache_page the old way\n         \"\"\"",
                "\"response\"",
                "\"response\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2af347ca43c64ee598f54763672b7a77": {
        "code_string": "def visit_binary_expr(self, expr):\n         left = expr.left.accept(self)\n         right = expr.right.accept(self)\n         if left is expr.left and right is expr.right:\n             return expr\n         return Make.binary_op(left, right, expr.op)\n",
        "code_toks_joined": "def visit_binary_expr ( self , expr ) : <NEWLINE> <INDENT> left = expr . left . accept ( self ) <NEWLINE> right = expr . right . accept ( self ) <NEWLINE> if left is expr . left and right is expr . right : <NEWLINE> <INDENT> return expr <NEWLINE> <DEDENT> return Make . binary_op ( left , right , expr . op ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b4a51bb8de14428592db126212b207d6": {
        "code_string": "def request(self, verb, method, **kwargs):\n         verb = verb.upper()\n         request_kwargs = {}\n         if method == 'GET':\n             request_kwargs['params'] = kwargs\n         else:\n             request_kwargs['data'] = kwargs\n         url = self.config['base_url'] + method\n         logger.debug('%s %s' % (verb, url))\n         r = self.requester.request(verb, url, **request_kwargs)\n         if r.status_code != 200:\n             raise APIError(r.status_code)\n         return r.json()\n",
        "code_toks_joined": "def request ( self , verb , method , ** kwargs ) : <NEWLINE> <INDENT> verb = verb . upper ( ) <NEWLINE> request_kwargs = { } <NEWLINE> if method == <STRING> : <NEWLINE> <INDENT> request_kwargs [ <STRING> ] = kwargs <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> request_kwargs [ <STRING> ] = kwargs <NEWLINE> <DEDENT> url = self . config [ <STRING> ] + method <NEWLINE> logger . debug ( <STRING> % ( verb , url ) ) <NEWLINE> r = self . requester . request ( verb , url , ** request_kwargs ) <NEWLINE> if r . status_code != 200 : <NEWLINE> <INDENT> raise APIError ( r . status_code ) <NEWLINE> <DEDENT> return r . json ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'GET'",
                "'params'",
                "'data'",
                "'base_url'",
                "'%s %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f05e321531814f97bd2ac4ad8ec00ac1": {
        "code_string": "def _step(self):\n         \"\"\"Step the iterator once, saving the new key and data. Returns True if\n         the iterator is still within the bounds of the collection prefix,\n         otherwise False.\"\"\"\n         keys_raw, self.data = next(self.it, ('', ''))\n         keys = keylib.KeyList.from_raw(self.prefix, keys_raw)\n         self.keys = keys\n         return keys is not None\n",
        "code_toks_joined": "def _step ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> keys_raw , self . data = next ( self . it , ( <STRING> , <STRING> ) ) <NEWLINE> keys = keylib . KeyList . from_raw ( self . prefix , keys_raw ) <NEWLINE> self . keys = keys <NEWLINE> return keys is not None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Step the iterator once, saving the new key and data. Returns True if\n         the iterator is still within the bounds of the collection prefix,\n         otherwise False.\"\"\"",
                "''",
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dc9f4ba39dec4a899b9aa2b7205b2cb9": {
        "code_string": "def _encode(self, s):\n         return keylib.packs(self.prefix, s)\n",
        "code_toks_joined": "def _encode ( self , s ) : <NEWLINE> <INDENT> return keylib . packs ( self . prefix , s ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8eaed12dde794e2e8005b65d34262e85": {
        "code_string": "writer = self.null\n         else:\n           if not key in self.map:\n             writer = Subject()\n             self.map[key] = value\n             fireNewMapEntry = True\n       except Exception as e:\n         self.onError(e)\n         return\n",
        "code_toks_joined": "writer = self . null <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> if not key in self . map : <NEWLINE> <INDENT> writer = Subject ( ) <NEWLINE> self . map [ key ] = value <NEWLINE> fireNewMapEntry = True <NEWLINE> except Exception as e : <NEWLINE> <DEDENT> <DEDENT> self . onError ( e ) <NEWLINE> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "62f56c9cadfc45ea96c8c3823fe2b4d9": {
        "code_string": "validator = ProjectValidator()\n     data = validator.deserialize(validator)\n",
        "code_toks_joined": "validator = ProjectValidator ( ) <NEWLINE> <INDENT> data = validator . deserialize ( validator ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "621fc033cd8b4ea58fba270cb47120f0": {
        "code_string": "@blueprint.route('/api/1/projects/<slug>/schemata/<name>', methods=['POST', 'PUT'])\n def update(slug, name):\n     project = object_or_404(Project.by_slug(slug))\n     authz.require(authz.project_manage(project))\n     schema = object_or_404(Schema.by_name(project, name))\n     data = request_data({'project': project})\n     project = schemata.save(data, schema=schema)\n     db.session.commit()\n     return jsonify(schemata.to_rest(schema))\n",
        "code_toks_joined": "@ blueprint . route ( <STRING> , methods = [ <STRING> , <STRING> ] ) <NEWLINE> <INDENT> def update ( slug , name ) : <NEWLINE> <INDENT> project = object_or_404 ( Project . by_slug ( slug ) ) <NEWLINE> authz . require ( authz . project_manage ( project ) ) <NEWLINE> schema = object_or_404 ( Schema . by_name ( project , name ) ) <NEWLINE> data = request_data ( { <STRING> : project } ) <NEWLINE> project = schemata . save ( data , schema = schema ) <NEWLINE> db . session . commit ( ) <NEWLINE> return jsonify ( schemata . to_rest ( schema ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/api/1/projects/<slug>/schemata/<name>'",
                "'POST'",
                "'PUT'",
                "'project'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9cc44f853cf14e1bb4c96d4a53906054": {
        "code_string": "if self._forces is None:\n             # Parallel calculation\n             if self.parallel != 0:\n                 with Pool(processes=self.parallel) as pool:\n                     image_number = len(self.images)\n                     par_images = pool.map(self.par_calc, range(image_number))\n                     \"\"\"\n                     for pi in par_images:\n                         print(pi.calculator.name)\n                     \"\"\"\n                     self.images = par_images\n             # Serial calculation\n             else:\n                 [image.calc_energy_and_forces() for image in self.images]\n",
        "code_toks_joined": "if self . _forces is None : <NEWLINE> <COMMENT> <NL> <INDENT> if self . parallel != 0 : <NEWLINE> <INDENT> with Pool ( processes = self . parallel ) as pool : <NEWLINE> <INDENT> image_number = len ( self . images ) <NEWLINE> par_images = pool . map ( self . par_calc , range ( image_number ) ) <NEWLINE> <STRING> <NEWLINE> self . images = par_images <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> [ image . calc_energy_and_forces ( ) for image in self . images ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Parallel calculation",
                "# Serial calculation"
            ],
            "<STRING>": [
                "\"\"\"\n                     for pi in par_images:\n                         print(pi.calculator.name)\n                     \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "86f8e8d7204143a89b9bffed640836d7": {
        "code_string": "def track_root(self, atoms, coords):\n         \"\"\"Store the information of the current iteration and if possible\n         calculate the overlap with the previous iteration.\"\"\"\n         self.store_wfo_data(atoms, coords)\n         # In the first iteration we have nothing to compare to\n         old_root = self.root\n         if self.calc_counter >= 1:\n             last_two_coords = self.wfow.last_two_coords\n             self.root = self.wfow.track(old_root=self.root)\n             if self.root != old_root:\n                 self.log(\"Found a root flip from {old_root} to {self.root}!\")\n",
        "code_toks_joined": "def track_root ( self , atoms , coords ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . store_wfo_data ( atoms , coords ) <NEWLINE> <COMMENT> <NL> old_root = self . root <NEWLINE> if self . calc_counter >= 1 : <NEWLINE> <INDENT> last_two_coords = self . wfow . last_two_coords <NEWLINE> self . root = self . wfow . track ( old_root = self . root ) <NEWLINE> if self . root != old_root : <NEWLINE> <INDENT> self . log ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Store the information of the current iteration and if possible\n         calculate the overlap with the previous iteration.\"\"\"",
                "\"Found a root flip from {old_root} to {self.root}!\""
            ],
            "<COMMENT>": [
                "# In the first iteration we have nothing to compare to"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bea060688c0041d0a230408a74faa3f6": {
        "code_string": "def get_geoms(xyz_fns, idpp=False, between=0, dump=False, multiple_geoms=False):\n     \"\"\"Returns a list of Geometry objects.\"\"\"\n     # Handle a single .xyz file\n     if isinstance(xyz_fns, str) and xyz_fns.endswith(\".xyz\"):\n         geoms = [geom_from_xyz_file(xyz_fns), ]\n     # Handle a single .trj file\n     elif len(xyz_fns) == 1 and xyz_fns[0].endswith(\".trj\"):\n         geoms = geoms_from_trj(xyz_fns[0])\n     # How is this different from above?\n     elif isinstance(xyz_fns, str) and xyz_fns.endswith(\".trj\"):\n         geoms = geoms_from_trj(xyz_fns)\n     elif multiple_geoms:\n         geoms = geoms_from_trj(xyz_fns[0])\n     # Handle multiple .xyz files\n     else:\n         geoms = [geom_from_xyz_file(fn) for fn in xyz_fns]\n",
        "code_toks_joined": "def get_geoms ( xyz_fns , idpp = False , between = 0 , dump = False , multiple_geoms = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if isinstance ( xyz_fns , str ) and xyz_fns . endswith ( <STRING> ) : <NEWLINE> <INDENT> geoms = [ geom_from_xyz_file ( xyz_fns ) , ] <NEWLINE> <COMMENT> <NL> <DEDENT> elif len ( xyz_fns ) == 1 and xyz_fns [ 0 ] . endswith ( <STRING> ) : <NEWLINE> <INDENT> geoms = geoms_from_trj ( xyz_fns [ 0 ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> elif isinstance ( xyz_fns , str ) and xyz_fns . endswith ( <STRING> ) : <NEWLINE> <INDENT> geoms = geoms_from_trj ( xyz_fns ) <NEWLINE> <DEDENT> elif multiple_geoms : <NEWLINE> <INDENT> geoms = geoms_from_trj ( xyz_fns [ 0 ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> else : <NEWLINE> <INDENT> geoms = [ geom_from_xyz_file ( fn ) for fn in xyz_fns ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Returns a list of Geometry objects.\"\"\"",
                "\".xyz\"",
                "\".trj\"",
                "\".trj\""
            ],
            "<COMMENT>": [
                "# Handle a single .xyz file",
                "# Handle a single .trj file",
                "# How is this different from above?",
                "# Handle multiple .xyz files"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b1119ef8ce184e988d3c3d3bb05bfb07": {
        "code_string": "ax2.plot(max_forces, **ax_kwargs)\n     ax2.set_title(\"rms(forces)\")\n     ax2.set_xlabel(\"Step\")\n     ax2.set_ylabel(\"$E_h$ Bohr\u207b\u00b9 (rad)\u207b\u00b9\")\n",
        "code_toks_joined": "ax2 . plot ( max_forces , ** ax_kwargs ) <NEWLINE> <INDENT> ax2 . set_title ( <STRING> ) <NEWLINE> ax2 . set_xlabel ( <STRING> ) <NEWLINE> ax2 . set_ylabel ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"rms(forces)\"",
                "\"Step\"",
                "\"$E_h$ Bohr\u207b\u00b9 (rad)\u207b\u00b9\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8f2f554da4504d3782e18b4badee276e": {
        "code_string": "inp = make_input(**inp_kwargs)\n     inp_fn = \"packmol.inp\"\n     with open(inp_fn, \"w\") as handle:\n         handle.write(inp_fn)\n     print(f\"Wrote packmol input to '{inp_fn}'\")\n",
        "code_toks_joined": "inp = make_input ( ** inp_kwargs ) <NEWLINE> <INDENT> inp_fn = <STRING> <NEWLINE> with open ( inp_fn , <STRING> ) as handle : <NEWLINE> <INDENT> handle . write ( inp_fn ) <NEWLINE> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"packmol.inp\"",
                "\"w\"",
                "f\"Wrote packmol input to '{inp_fn}'\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "daacc842d72f4dcd872673feebfaf136": {
        "code_string": "# cs has alpha\n         if cs_alpha:\n             base_img = img.copy()\n             base_img.paste(img, mask=cs_alpha)\n             img = base_img\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if cs_alpha : <NEWLINE> <INDENT> base_img = img . copy ( ) <NEWLINE> base_img . paste ( img , mask = cs_alpha ) <NEWLINE> img = base_img <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# cs has alpha"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "256b5a70c67a4151b557ce8b42c1c296": {
        "code_string": "src_lists = []\n         for s in arguments:\n             key, value = s.split(\"=\")\n             gl = re.match(r'glob\\((.+)\\)', s)\n             if ',' in value:\n                 possible_values=value.split(',')\n             elif ':' in value:\n                 possible_values=range(*[int(v) for v in value.split(':')])\n             elif gl:\n                 possible_values=list(glob.glob(gl[1], recursive=True))\n             else:\n                 possible_values=[value]\n             src_lists.append([\"{}={}\".format(key, val) for val in possible_values])\n",
        "code_toks_joined": "src_lists = [ ] <NEWLINE> <INDENT> for s in arguments : <NEWLINE> <INDENT> key , value = s . split ( <STRING> ) <NEWLINE> gl = re . match ( <STRING> , s ) <NEWLINE> if <STRING> in value : <NEWLINE> <INDENT> possible_values = value . split ( <STRING> ) <NEWLINE> <DEDENT> elif <STRING> in value : <NEWLINE> <INDENT> possible_values = range ( * [ int ( v ) for v in value . split ( <STRING> ) ] ) <NEWLINE> <DEDENT> elif gl : <NEWLINE> <INDENT> possible_values = list ( glob . glob ( gl [ 1 ] , recursive = True ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> possible_values = [ value ] <NEWLINE> <DEDENT> src_lists . append ( [ <STRING> . format ( key , val ) for val in possible_values ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"=\"",
                "r'glob\\((.+)\\)'",
                "','",
                "','",
                "':'",
                "':'",
                "\"{}={}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1c443ed5b88c46a09125337c942267c6": {
        "code_string": "data = np.append(dop, po4, axis=0)\n         util.io.save_npy(npy_file, data, make_read_only=True, create_path_if_not_exists=True)\n",
        "code_toks_joined": "data = np . append ( dop , po4 , axis = 0 ) <NEWLINE> <INDENT> util . io . save_npy ( npy_file , data , make_read_only = True , create_path_if_not_exists = True ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6df06ab29425484192459199a8fc1a57": {
        "code_string": "assert po4_var.units == measurements.po4.wod.constants.PO4_UNIT\n                 po4 = z_var.data\n",
        "code_toks_joined": "assert po4_var . units == measurements . po4 . wod . constants . PO4_UNIT <NEWLINE> <INDENT> po4 = z_var . data <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "69d0d2e9347944d19402c8a4d4ad468c": {
        "code_string": "# plot\n         cm = matplotlib.pyplot.cm.winter_r\n         util.plot.data(data, file, land_value=0, power_limit=10, colormap=cm)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> cm = matplotlib . pyplot . cm . winter_r <NEWLINE> util . plot . data ( data , file , land_value = 0 , power_limit = 10 , colormap = cm ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# plot"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "125f14b13dc1495fa594398b86203f51": {
        "code_string": "def _plot_map(data, lsm, file, layer=None, v_min=None, v_max=None, use_log_scale=False, colorbar_kwargs={'fraction':0.021, 'pad':0.05, 'aspect':20, 'orientation':'vertical'}):\n     data = lsm.insert_index_values_in_map(data, no_data_value=np.inf)\n     if layer is not None:\n         data = data[:, :, :, layer]\n         data = data.reshape(data.shape + (1,))\n     file = _prepare_filename(file, lsm)\n     util.plot.data(data, file, no_data_value=np.inf, v_min=v_min, v_max=v_max, use_log_scale=use_log_scale, contours=False, colorbar=True, power_limit=0, colorbar_kwargs=colorbar_kwargs)\n",
        "code_toks_joined": "def _plot_map ( data , lsm , file , layer = None , v_min = None , v_max = None , use_log_scale = False , colorbar_kwargs = { <STRING> : 0.021 , <STRING> : 0.05 , <STRING> : 20 , <STRING> : <STRING> } ) : <NEWLINE> <INDENT> data = lsm . insert_index_values_in_map ( data , no_data_value = np . inf ) <NEWLINE> if layer is not None : <NEWLINE> <INDENT> data = data [ : , : , : , layer ] <NEWLINE> data = data . reshape ( data . shape + ( 1 , ) ) <NEWLINE> <DEDENT> file = _prepare_filename ( file , lsm ) <NEWLINE> util . plot . data ( data , file , no_data_value = np . inf , v_min = v_min , v_max = v_max , use_log_scale = use_log_scale , contours = False , colorbar = True , power_limit = 0 , colorbar_kwargs = colorbar_kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'fraction'",
                "'pad'",
                "'aspect'",
                "'orientation'",
                "'vertical'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "91431c4195094e58938acf8b301f0b25": {
        "code_string": "def push_to_list(self, queue, instance, trim=500, redis_conn=None, bump=True, site=None):\n         backend = RedisBackend(conn=redis_conn)\n         key = self.get_key(queue, site=site)\n         current_list = backend.get_ids(queue)\n         known_length = len(current_list) + 1\n         if bump:\n             if instance.pk in current_list:\n                 backend.remove(key, instance.pk)\n                 known_length -= 1\n         backend.add(key, instance.pk)\n         if trim and known_length > trim:\n             backend.trim(key, trim)\n",
        "code_toks_joined": "def push_to_list ( self , queue , instance , trim = 500 , redis_conn = None , bump = True , site = None ) : <NEWLINE> <INDENT> backend = RedisBackend ( conn = redis_conn ) <NEWLINE> key = self . get_key ( queue , site = site ) <NEWLINE> current_list = backend . get_ids ( queue ) <NEWLINE> known_length = len ( current_list ) + 1 <NEWLINE> if bump : <NEWLINE> <INDENT> if instance . pk in current_list : <NEWLINE> <INDENT> backend . remove ( key , instance . pk ) <NEWLINE> known_length -= 1 <NEWLINE> <DEDENT> <DEDENT> backend . add ( key , instance . pk ) <NEWLINE> if trim and known_length > trim : <NEWLINE> <INDENT> backend . trim ( key , trim ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "434d0b1597c94f2eab1bbc18e99dfa5c": {
        "code_string": "relations[name] = getattr(self, method_name, None)\n         return relations\n",
        "code_toks_joined": "relations [ name ] = getattr ( self , method_name , None ) <NEWLINE> <INDENT> return relations <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "10f11895231f4f588e18d20d3199d593": {
        "code_string": "def _cmp_value_to_nodes(base_op, first, second):\n     node_values = set([number(node) for node in second])\n     first = number(first)\n     verbose_print('Comparing {0} nodes in node set to value \"{1}\"'.format(len(node_values), second))\n",
        "code_toks_joined": "def _cmp_value_to_nodes ( base_op , first , second ) : <NEWLINE> <INDENT> node_values = set ( [ number ( node ) for node in second ] ) <NEWLINE> first = number ( first ) <NEWLINE> verbose_print ( <STRING> . format ( len ( node_values ) , second ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Comparing {0} nodes in node set to value \"{1}\"'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9e0f43274d4244d4a617d75c639666d1": {
        "code_string": "# Add uniques to payload\n     unique_ids_query = prepare_unique_query(session, section)\n     for unique_id in unique_ids_query:\n         db_contour_unique = session.query(Contour).get(contour_A_id)\n         unique_reconstruct_contour = section.contours[db_contour_unique.index]\n         unique_dict = prepare_contour_dict_for_frontend(\n             unique_reconstruct_contour,\n             unique_id,\n             section,\n             keep=True\n         )\n         section_matches['unique'].append(unique_dict)\n     return section_matches\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> unique_ids_query = prepare_unique_query ( session , section ) <NEWLINE> for unique_id in unique_ids_query : <NEWLINE> <INDENT> db_contour_unique = session . query ( Contour ) . get ( contour_A_id ) <NEWLINE> unique_reconstruct_contour = section . contours [ db_contour_unique . index ] <NEWLINE> unique_dict = prepare_contour_dict_for_frontend ( <NEWLINE> <INDENT> unique_reconstruct_contour , <NEWLINE> unique_id , <NEWLINE> section , <NEWLINE> keep = True <NEWLINE> <DEDENT> ) <NEWLINE> section_matches [ <STRING> ] . append ( unique_dict ) <NEWLINE> <DEDENT> return section_matches <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Add uniques to payload"
            ],
            "<STRING>": [
                "'unique'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "23589208700c4650815eed1e690d0ec6": {
        "code_string": "def onverbalization(e):\n         for t in e:\n             text = pr2.knowledge[\"%s verbalisesTo *\" % t][0]\n             logger.warning(\"New verbalization from Dialogs: <%s>\" % text)\n             pr2.say(t)\n",
        "code_toks_joined": "def onverbalization ( e ) : <NEWLINE> <INDENT> for t in e : <NEWLINE> <INDENT> text = pr2 . knowledge [ <STRING> % t ] [ 0 ] <NEWLINE> logger . warning ( <STRING> % text ) <NEWLINE> pr2 . say ( t ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"%s verbalisesTo *\"",
                "\"New verbalization from Dialogs: <%s>\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2cad8586067c4bc1afee2c9b164d6a64": {
        "code_string": "def add_metric(tensor, name=None):\n   return tf.add_to_collection(\n       METRICS,\n       tensor if name is None else util.rename(name, tensor))\n",
        "code_toks_joined": "def add_metric ( tensor , name = None ) : <NEWLINE> <INDENT> return tf . add_to_collection ( <NEWLINE> <INDENT> METRICS , <NEWLINE> tensor if name is None else util . rename ( name , tensor ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7009288ace854c7aaeb59bf2ff4941ac": {
        "code_string": "return (predictions,\n             loss + l2_regularization_loss(regularization_scale),\n             train.minimize(loss),\n             _evaluate(predictions, true_label))\n",
        "code_toks_joined": "return ( predictions , <NEWLINE> <INDENT> loss + l2_regularization_loss ( regularization_scale ) , <NEWLINE> train . minimize ( loss ) , <NEWLINE> _evaluate ( predictions , true_label ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "02ff0ebac9034fc19066f0bdffc03efa": {
        "code_string": "def atstart(self, received):\n         file = \"{}{}\".format(__file__, \".remote.log\")\n         if hasattr(self, 'mlogger'):\n             raise RuntimeError(\"Self missing mlogger method.\")\n         self.mlogger.debug(\"Opening file: {}.\".format(file))\n         self.file = open(file, 'w')\n",
        "code_toks_joined": "def atstart ( self , received ) : <NEWLINE> <INDENT> file = <STRING> . format ( __file__ , <STRING> ) <NEWLINE> if hasattr ( self , <STRING> ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> self . mlogger . debug ( <STRING> . format ( file ) ) <NEWLINE> self . file = open ( file , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{}{}\"",
                "\".remote.log\"",
                "'mlogger'",
                "\"Self missing mlogger method.\"",
                "\"Opening file: {}.\"",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f89cca5aa83a42db9668b78dd1d8c423": {
        "code_string": "if len(fields) > 0:\n                 raise Exception(\"Unable to determine PBSpro or Torque\")\n",
        "code_toks_joined": "if len ( fields ) > 0 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Unable to determine PBSpro or Torque\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c97de8c30fd142128dc393b239900168": {
        "code_string": "def dependencies_iterator(xs):\n     if hasattr(xs, \"items\"):\n         for k, v in xs.items():\n             yield k, v\n     else:\n         for e in xs:\n             yield k, None\n",
        "code_toks_joined": "def dependencies_iterator ( xs ) : <NEWLINE> <INDENT> if hasattr ( xs , <STRING> ) : <NEWLINE> <INDENT> for k , v in xs . items ( ) : <NEWLINE> <INDENT> yield k , v <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> for e in xs : <NEWLINE> <INDENT> yield k , None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"items\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "61ef88625b4149868f0b444e72a92e86": {
        "code_string": "generating = Generating(config)\n     for c in params[\"total\"][\"pro\"]:\n         if \"skip\" not in params[c]:\n             sys.stderr.write(\"create package: {}? (y or n)\\n\".format(c))\n             sys.stderr.flush()\n             skip = \"y\" == sys.stdin.readline().strip().lower()\n         else:\n             skip = params[c][\"skip\"]\n         if skip:\n             sys.stderr.write(\"skipped: {}\\n\".format(c))\n         else:\n             generating.generate(params[c], args.dst)\n",
        "code_toks_joined": "generating = Generating ( config ) <NEWLINE> <INDENT> for c in params [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> if <STRING> not in params [ c ] : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> . format ( c ) ) <NEWLINE> sys . stderr . flush ( ) <NEWLINE> skip = <STRING> == sys . stdin . readline ( ) . strip ( ) . lower ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> skip = params [ c ] [ <STRING> ] <NEWLINE> <DEDENT> if skip : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> . format ( c ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> generating . generate ( params [ c ] , args . dst ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"total\"",
                "\"pro\"",
                "\"skip\"",
                "\"create package: {}? (y or n)\\n\"",
                "\"y\"",
                "\"skip\"",
                "\"skipped: {}\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "df5eeb2f621140078207234ea4118b5d": {
        "code_string": "# setup.py\n     setup_path = os.path.join(path, \"setup.py\")\n     if not os.path.exists(setup_path):\n         raise RuntimeError(\"script \\\"update_version\\\" cannot find the root path of Grid2op. Please provide a valid \\\"--path\\\" argument.\")\n     with open(setup_path, \"r\") as f:\n         new_setup = f.read()\n     try:\n         old_version = re.search(\"version='[0-9]+\\.[0-9]+\\.[0-9]+'\", new_setup).group(0)\n     except Exception as e:\n         raise RuntimeError(\"Impossible to find the old version number. Stopping here\")\n     old_version = re.sub(\"version=\", \"\", old_version)\n     old_version = re.sub(\"'\", \"\", old_version)\n     old_version = re.sub('\"', \"\", old_version)\n     if version <= old_version:\n         raise RuntimeError(\"You provided the \\\"new\\\" version \\\"{}\\\" which is older (or equal) to the current version \"\n                            \"found: \\\"{}\\\".\".format(version, old_version))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> setup_path = os . path . join ( path , <STRING> ) <NEWLINE> if not os . path . exists ( setup_path ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> with open ( setup_path , <STRING> ) as f : <NEWLINE> <INDENT> new_setup = f . read ( ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> old_version = re . search ( <STRING> , new_setup ) . group ( 0 ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> old_version = re . sub ( <STRING> , <STRING> , old_version ) <NEWLINE> old_version = re . sub ( <STRING> , <STRING> , old_version ) <NEWLINE> old_version = re . sub ( <STRING> , <STRING> , old_version ) <NEWLINE> if version <= old_version : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( version , old_version ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# setup.py"
            ],
            "<STRING>": [
                "\"setup.py\"",
                "\"script \\\"update_version\\\" cannot find the root path of Grid2op. Please provide a valid \\\"--path\\\" argument.\"",
                "\"r\"",
                "\"version='[0-9]+\\.[0-9]+\\.[0-9]+'\"",
                "\"Impossible to find the old version number. Stopping here\"",
                "\"version=\"",
                "\"\"",
                "\"'\"",
                "\"\"",
                "'\"'",
                "\"\"",
                "\"You provided the \\\"new\\\" version \\\"{}\\\" which is older (or equal) to the current version \"",
                "\"found: \\\"{}\\\".\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "870df0d97b0643e19177fa79e21c0431": {
        "code_string": "return_key_values = _get_versions()\n     return_key_values[\"authors\"] = pieces[\"authors\"]\n     return return_key_values\n",
        "code_toks_joined": "return_key_values = _get_versions ( ) <NEWLINE> <INDENT> return_key_values [ <STRING> ] = pieces [ <STRING> ] <NEWLINE> return return_key_values <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"authors\"",
                "\"authors\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6438277ffed7493eae3f955a98f58afb": {
        "code_string": "# Set values according to inputs\n     for key, value in dart.prior_kwargs.items():\n         if key == 'kick_sigma': kick_sigma = value\n         if key == 'M1_alpha': M1_alpha = value\n         if key == 'M1_min': M1_min = value\n         if key == 'M1_max': M1_min = value\n         if key == 'M2_min': M2_min = value\n         if key == 'a_min': a_min = value\n         if key == 'a_max': a_max = value\n         if key == 't_min': t_min = value\n         if key == 't_max': t_max = value\n         if key == 'mass_function': mass_function = value\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for key , value in dart . prior_kwargs . items ( ) : <NEWLINE> <INDENT> if key == <STRING> : kick_sigma = value <NEWLINE> if key == <STRING> : M1_alpha = value <NEWLINE> if key == <STRING> : M1_min = value <NEWLINE> if key == <STRING> : M1_min = value <NEWLINE> if key == <STRING> : M2_min = value <NEWLINE> if key == <STRING> : a_min = value <NEWLINE> if key == <STRING> : a_max = value <NEWLINE> if key == <STRING> : t_min = value <NEWLINE> if key == <STRING> : t_max = value <NEWLINE> if key == <STRING> : mass_function = value <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Set values according to inputs"
            ],
            "<STRING>": [
                "'kick_sigma'",
                "'M1_alpha'",
                "'M1_min'",
                "'M1_max'",
                "'M2_min'",
                "'a_min'",
                "'a_max'",
                "'t_min'",
                "'t_max'",
                "'mass_function'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "14159ab10c41448c89e92a22a4299284": {
        "code_string": "if ntemps != 1 or ntemps is not None:\n             if ln_likelihood_function is None:\n                 print(\"You must include a likelihood function when using the parallel tempering MCMC method.\")\n                 sys.exit(-1)\n",
        "code_toks_joined": "if ntemps != 1 or ntemps is not None : <NEWLINE> <INDENT> if ln_likelihood_function is None : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sys . exit ( - 1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"You must include a likelihood function when using the parallel tempering MCMC method.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9a7f6a6af79f47f2993e3e1152f96fb2": {
        "code_string": "shutil.copyfile(os.path.join(builddir, SO_NAME),\n                         os.path.join(build_lib, \"themis\", SO_NAME))\n         shutil.copyfile(os.path.join(builddir, HEADER_NAME),\n                         os.path.join(build_lib, \"themis\", os.path.basename(HEADER_NAME)))\n     print(\"finished compiling frc hal\")\n",
        "code_toks_joined": "shutil . copyfile ( os . path . join ( builddir , SO_NAME ) , <NEWLINE> <INDENT> os . path . join ( build_lib , <STRING> , SO_NAME ) ) <NEWLINE> shutil . copyfile ( os . path . join ( builddir , HEADER_NAME ) , <NEWLINE> os . path . join ( build_lib , <STRING> , os . path . basename ( HEADER_NAME ) ) ) <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"themis\"",
                "\"themis\"",
                "\"finished compiling frc hal\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "560b8b1a0cf54236b2994c4e2c686362": {
        "code_string": "def __city_and_province(self):\n         if self.city.isNotEmpty() and self.province.isNotEmpty():\n             if not self.city.isBlong(self.province.name):\n                 if self.city.precision > self.province.precision:\n                     self.province.name = self.city.belong\n                 else:\n                     self.city.reset()\n         elif self.city.isNotEmpty() and self.province.isEmpty():\n             self.province.name = self.city.belong\n",
        "code_toks_joined": "def __city_and_province ( self ) : <NEWLINE> <INDENT> if self . city . isNotEmpty ( ) and self . province . isNotEmpty ( ) : <NEWLINE> <INDENT> if not self . city . isBlong ( self . province . name ) : <NEWLINE> <INDENT> if self . city . precision > self . province . precision : <NEWLINE> <INDENT> self . province . name = self . city . belong <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . city . reset ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif self . city . isNotEmpty ( ) and self . province . isEmpty ( ) : <NEWLINE> <INDENT> self . province . name = self . city . belong <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "25e68ccb427640d7a65d3abfbddf5982": {
        "code_string": "if status_code in range(200, 299):\n             return text\n",
        "code_toks_joined": "if status_code in range ( 200 , 299 ) : <NEWLINE> <INDENT> return text <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b9007bfd3802405bb267ffe84c1e5180": {
        "code_string": "if init_progress:\n             print('optimization took {:.2f} seconds'.format(t2 - t1))\n",
        "code_toks_joined": "if init_progress : <NEWLINE> <INDENT> print ( <STRING> . format ( t2 - t1 ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'optimization took {:.2f} seconds'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bb8735d64c194b188ee4312c95a35771": {
        "code_string": "x0 = self._initialize_params_triangulation(\n             p3ds_med, constraints, constraints_weak)\n",
        "code_toks_joined": "x0 = self . _initialize_params_triangulation ( <NEWLINE> <INDENT> p3ds_med , constraints , constraints_weak ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "75b19be663ad424a85e9e312fca0e3be": {
        "code_string": "def _get_single_mod(self, modno, mod_slices):\n         try:\n             mod_data = self._data[modno]\n         except KeyError:\n             if modno > self._nmodules:\n                 raise IndexError(modno)\n             mod_data = np.full(self._mod_shape, self._fillvalue, self.dtype)\n",
        "code_toks_joined": "def _get_single_mod ( self , modno , mod_slices ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> mod_data = self . _data [ modno ] <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> if modno > self . _nmodules : <NEWLINE> <INDENT> raise IndexError ( modno ) <NEWLINE> <DEDENT> mod_data = np . full ( self . _mod_shape , self . _fillvalue , self . dtype ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6faaa976409a4c0b96b17bc3c6bf28c1": {
        "code_string": "def __init__(self, host, port, verify, cert_path, pkey_path, pkey_passphrase=''):\n         self.host = host\n         self.port = port\n         try:\n             self.pkey = crypto.load_privatekey(crypto.FILETYPE_PEM, open(pkey_path, 'rb').read(), pkey_passphrase)\n         except IOError:\n             raise eStreamerKeyError(\"Unable to locate key file {}\".format(pkey_path))\n         except crypto.Error:\n             raise eStreamerKeyError(\"Invalid key file or bad passphrase {}\".format(cert_path))\n         try:\n             self.cert = crypto.load_certificate(crypto.FILETYPE_PEM, open(cert_path, 'rb').read())\n         except IOError:\n             raise eStreamerCertError(\"Unable to locate cert file {}\".format(cert_path))\n         except crypto.Error:\n             raise eStreamerCertError(\"Invalid certificate {}\".format(cert_path))\n         self.verify = verify\n         self.ctx = None\n         self.sock = None\n         self._bytes = None\n",
        "code_toks_joined": "def __init__ ( self , host , port , verify , cert_path , pkey_path , pkey_passphrase = <STRING> ) : <NEWLINE> <INDENT> self . host = host <NEWLINE> self . port = port <NEWLINE> try : <NEWLINE> <INDENT> self . pkey = crypto . load_privatekey ( crypto . FILETYPE_PEM , open ( pkey_path , <STRING> ) . read ( ) , pkey_passphrase ) <NEWLINE> <DEDENT> except IOError : <NEWLINE> <INDENT> raise eStreamerKeyError ( <STRING> . format ( pkey_path ) ) <NEWLINE> <DEDENT> except crypto . Error : <NEWLINE> <INDENT> raise eStreamerKeyError ( <STRING> . format ( cert_path ) ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> self . cert = crypto . load_certificate ( crypto . FILETYPE_PEM , open ( cert_path , <STRING> ) . read ( ) ) <NEWLINE> <DEDENT> except IOError : <NEWLINE> <INDENT> raise eStreamerCertError ( <STRING> . format ( cert_path ) ) <NEWLINE> <DEDENT> except crypto . Error : <NEWLINE> <INDENT> raise eStreamerCertError ( <STRING> . format ( cert_path ) ) <NEWLINE> <DEDENT> self . verify = verify <NEWLINE> self . ctx = None <NEWLINE> self . sock = None <NEWLINE> self . _bytes = None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''",
                "'rb'",
                "\"Unable to locate key file {}\"",
                "\"Invalid key file or bad passphrase {}\"",
                "'rb'",
                "\"Unable to locate cert file {}\"",
                "\"Invalid certificate {}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "038b1c91cb524ddc9ecd46e08d305362": {
        "code_string": "if dataDimensions:\n \t\t\tdataDimensions = list(map(lambda x : \"seq_%s_5\" % (x), dataDimensions))\n",
        "code_toks_joined": "if dataDimensions : <NEWLINE> <INDENT> dataDimensions = list ( map ( lambda x : <STRING> % ( x ) , dataDimensions ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"seq_%s_5\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ebbe5e6d4da443da8897c374dcbe5307": {
        "code_string": "if not self.model.onModelLoad(loadedState[\"model_state\"]):\n \t\t\tloaded = loadedState[\"model_state\"]\n \t\t\tcurrent = self.model.onModelSave()\n \t\t\tStr = \"Could not correclty load the model state loaded: %s vs. current: %s.\\nDiffs:\\n\" % (loaded, current)\n \t\t\tfor key in set(list(loaded.keys()) + list(current.keys())):\n \t\t\t\tif not key in current:\n \t\t\t\t\tStr += \"\\t- %s in loaded, not in current\" % (key)\n \t\t\t\t\tcontinue\n \t\t\t\tif not key in loaded:\n \t\t\t\t\tStr += \"\\t- %s in current, not in loaded\"% (key)\n \t\t\t\t\tcontinue\n \t\t\t\tif current[key] != loaded[key]:\n \t\t\t\t\tStr += \"\\t- current[%s]=%s. loaded[%s]=%s\" % (key, current[key], key, current[key])\n \t\t\traise Exception(Str)\n",
        "code_toks_joined": "if not self . model . onModelLoad ( loadedState [ <STRING> ] ) : <NEWLINE> <INDENT> loaded = loadedState [ <STRING> ] <NEWLINE> current = self . model . onModelSave ( ) <NEWLINE> Str = <STRING> % ( loaded , current ) <NEWLINE> for key in set ( list ( loaded . keys ( ) ) + list ( current . keys ( ) ) ) : <NEWLINE> <INDENT> if not key in current : <NEWLINE> <INDENT> Str += <STRING> % ( key ) <NEWLINE> continue <NEWLINE> <DEDENT> if not key in loaded : <NEWLINE> <INDENT> Str += <STRING> % ( key ) <NEWLINE> continue <NEWLINE> <DEDENT> if current [ key ] != loaded [ key ] : <NEWLINE> <INDENT> Str += <STRING> % ( key , current [ key ] , key , current [ key ] ) <NEWLINE> <DEDENT> <DEDENT> raise Exception ( Str ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"model_state\"",
                "\"model_state\"",
                "\"Could not correclty load the model state loaded: %s vs. current: %s.\\nDiffs:\\n\"",
                "\"\\t- %s in loaded, not in current\"",
                "\"\\t- %s in current, not in loaded\"",
                "\"\\t- current[%s]=%s. loaded[%s]=%s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9f9adcee2ea64d709f3a760d1399df43": {
        "code_string": "def __debug(self, i, facet0, facet1):\n         \"Fancy printing of progress\"\n         if facet0 == facet1 == None:\n             debug(\"Computing tensor representation for term %d...\" % i)\n         elif facet0 == None:\n             debug(\"Computing tensor representation for term %d, facet %d...\" % (i, facet0))\n         else:\n             debug(\"Computing tensor representation for term %d, facets (%d, %d)...\" % (i, facet0, facet1))\n",
        "code_toks_joined": "def __debug ( self , i , facet0 , facet1 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if facet0 == facet1 == None : <NEWLINE> <INDENT> debug ( <STRING> % i ) <NEWLINE> <DEDENT> elif facet0 == None : <NEWLINE> <INDENT> debug ( <STRING> % ( i , facet0 ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> debug ( <STRING> % ( i , facet0 , facet1 ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Fancy printing of progress\"",
                "\"Computing tensor representation for term %d...\"",
                "\"Computing tensor representation for term %d, facet %d...\"",
                "\"Computing tensor representation for term %d, facets (%d, %d)...\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6c508bf4bff54ad2b8da85f4fb786ffa": {
        "code_string": "# Skip small values\n                 if abs(a0) > epsilon: continue\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if abs ( a0 ) > epsilon : continue <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Skip small values"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dd956100449b4b7b8f00ef2a82a67fed": {
        "code_string": "# Get common cell\n     common_cell = form_data[0].cell\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> common_cell = form_data [ 0 ] . cell <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Get common cell"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b8ffec5e7b7f48d0868c8d6e5709f614": {
        "code_string": "# Get code snippets for Jacobian, Inverse of Jacobian and mapping of\n     # coordinates from physical element to the FIAT reference element.\n     # FIXME: KBO: Change this when supporting R^2 in R^3 elements.\n     code += [format[\"jacobian and inverse\"](geometric_dimension, topological_dimension)]\n     code += [\"\", format[\"fiat coordinate map\"](element_cell_domain, topological_dimension)]\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> code += [ format [ <STRING> ] ( geometric_dimension , topological_dimension ) ] <NEWLINE> code += [ <STRING> , format [ <STRING> ] ( element_cell_domain , topological_dimension ) ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Get code snippets for Jacobian, Inverse of Jacobian and mapping of",
                "# coordinates from physical element to the FIAT reference element.",
                "# FIXME: KBO: Change this when supporting R^2 in R^3 elements."
            ],
            "<STRING>": [
                "\"jacobian and inverse\"",
                "\"\"",
                "\"fiat coordinate map\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6a65ab86684c4aa7b8449169c7372da4": {
        "code_string": "def generate_xi_from_x_snippets(cell, restriction):\n     \"Generate combined definition and declaration of xi = K (x - v).\"\n     gd = cell.geometric_dimension()\n     td = cell.topological_dimension()\n     name_A = \"K%s\" % restriction\n     name_x = \"x%s\" % restriction\n     name_y = \"vertex_coordinates%s\" % restriction\n     name_z = \"xi%s\" % restriction\n     return generate_z_Axmy_snippets(name_z, name_A, name_x, name_y, gd, td)\n",
        "code_toks_joined": "def generate_xi_from_x_snippets ( cell , restriction ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> gd = cell . geometric_dimension ( ) <NEWLINE> td = cell . topological_dimension ( ) <NEWLINE> name_A = <STRING> % restriction <NEWLINE> name_x = <STRING> % restriction <NEWLINE> name_y = <STRING> % restriction <NEWLINE> name_z = <STRING> % restriction <NEWLINE> return generate_z_Axmy_snippets ( name_z , name_A , name_x , name_y , gd , td ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Generate combined definition and declaration of xi = K (x - v).\"",
                "\"K%s\"",
                "\"x%s\"",
                "\"vertex_coordinates%s\"",
                "\"xi%s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d441cf03dd3429b81fd923e6ff46625": {
        "code_string": "# Create inner product of basis and inverse of Jacobian.\n             inner = [f_mul([inv_jacobian_column[j], basis_col[j]]) for j in range(tdim)]\n             value = f_group(f_add(inner))\n             name = f_component(f_derivatives+_p, f_matrix_index(i, f_r, f_num_derivs(_t)))\n             lines += [f_assign(name, value)]\n     elif mapping == \"pullback as metric\":\n         lines += [\"\", f_comment(\"Using metric pullback to map values back to the physical element\")]\n         lines += [f_const_double(f_tmp(i),\n                                 f_component(f_derivatives,\n                                             f_matrix_index(i, f_r, f_num_derivs(_t))))\n                   for i in range(num_components)]\n         basis_col = [f_tmp(j) for j in range(num_components)]\n         for p in range(num_components):\n             # unflatten the indices\n             i = p // tdim\n             l = p % tdim\n             # g_il = K_ji G_jk K_kl\n             value = f_group(f_inner(\n                 [f_inner([f_transform(\"JINV\", j, i, tdim, gdim, None)\n                           for j in range(tdim)],\n                          [basis_col[j * tdim + k] for j in range(tdim)])\n                  for k in range(tdim)],\n                 [f_transform(\"JINV\", k, l, tdim, gdim, None)\n                  for k in range(tdim)]))\n             name = f_component(f_derivatives+_p, f_matrix_index(i, f_r, f_num_derivs(_t)))\n             lines += [f_assign(name, value)]\n     else:\n         error(\"Unknown mapping: %s\" % mapping)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> inner = [ f_mul ( [ inv_jacobian_column [ j ] , basis_col [ j ] ] ) for j in range ( tdim ) ] <NEWLINE> value = f_group ( f_add ( inner ) ) <NEWLINE> name = f_component ( f_derivatives + _p , f_matrix_index ( i , f_r , f_num_derivs ( _t ) ) ) <NEWLINE> lines += [ f_assign ( name , value ) ] <NEWLINE> elif mapping == <STRING> : <NEWLINE> lines += [ <STRING> , f_comment ( <STRING> ) ] <NEWLINE> lines += [ f_const_double ( f_tmp ( i ) , <NEWLINE> <INDENT> f_component ( f_derivatives , <NEWLINE> <INDENT> f_matrix_index ( i , f_r , f_num_derivs ( _t ) ) ) ) <NEWLINE> for i in range ( num_components ) ] <NEWLINE> basis_col = [ f_tmp ( j ) for j in range ( num_components ) ] <NEWLINE> for p in range ( num_components ) : <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> i = p // tdim <NEWLINE> l = p % tdim <NEWLINE> <COMMENT> <NL> value = f_group ( f_inner ( <NEWLINE> <INDENT> [ f_inner ( [ f_transform ( <STRING> , j , i , tdim , gdim , None ) <NEWLINE> <INDENT> for j in range ( tdim ) ] , <NEWLINE> [ basis_col [ j * tdim + k ] for j in range ( tdim ) ] ) <NEWLINE> for k in range ( tdim ) ] , <NEWLINE> <DEDENT> [ f_transform ( <STRING> , k , l , tdim , gdim , None ) <NEWLINE> <INDENT> for k in range ( tdim ) ] ) ) <NEWLINE> <DEDENT> <DEDENT> name = f_component ( f_derivatives + _p , f_matrix_index ( i , f_r , f_num_derivs ( _t ) ) ) <NEWLINE> lines += [ f_assign ( name , value ) ] <NEWLINE> else : <NEWLINE> error ( <STRING> % mapping ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Create inner product of basis and inverse of Jacobian.",
                "# unflatten the indices",
                "# g_il = K_ji G_jk K_kl"
            ],
            "<STRING>": [
                "\"pullback as metric\"",
                "\"\"",
                "\"Using metric pullback to map values back to the physical element\"",
                "\"JINV\"",
                "\"JINV\"",
                "\"Unknown mapping: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fd45008532074778881d7f1cdf652464": {
        "code_string": "# Quadrature weight was removed in representation, add it back now\n         if num_points is None:\n             weight = L.LiteralFloat(1.0)\n         elif self.ir[\"integral_type\"] in custom_integral_types:\n             weights = self.backend.symbols.custom_weights_table()\n             weight = weights[iq]\n         else:\n             weight = self.backend.symbols.weights_table(num_points)\n             weight = weights[iq]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if num_points is None : <NEWLINE> <INDENT> weight = L . LiteralFloat ( 1.0 ) <NEWLINE> <DEDENT> elif self . ir [ <STRING> ] in custom_integral_types : <NEWLINE> <INDENT> weights = self . backend . symbols . custom_weights_table ( ) <NEWLINE> weight = weights [ iq ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> weight = self . backend . symbols . weights_table ( num_points ) <NEWLINE> weight = weights [ iq ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Quadrature weight was removed in representation, add it back now"
            ],
            "<STRING>": [
                "\"integral_type\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "79fb821774384c87ba4fe742532dd398": {
        "code_string": "s = sreg.create_schedule(sched_type, c, p)\n",
        "code_toks_joined": "s = sreg . create_schedule ( sched_type , c , p ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a36192c5bc0a41338e543e34ca9bc8d9": {
        "code_string": "self.train_loader = DataLoader(train_set, batch_size, sampler=train_sampler, num_workers=4)\n         self.val_loader = DataLoader(train_set, batch_size, sampler=valid_sampler, num_workers=4)\n         self.test_loader = DataLoader(test_set, batch_size, num_workers=4)\n",
        "code_toks_joined": "self . train_loader = DataLoader ( train_set , batch_size , sampler = train_sampler , num_workers = 4 ) <NEWLINE> <INDENT> self . val_loader = DataLoader ( train_set , batch_size , sampler = valid_sampler , num_workers = 4 ) <NEWLINE> self . test_loader = DataLoader ( test_set , batch_size , num_workers = 4 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eaa44842f941455a9df662764046cfc1": {
        "code_string": "def private_encrypt(self, value, padding=RSA_PKCS1_PADDING):\n         buf = create_string_buffer(value, len(value))\n         size = RSA_size(self.key)\n         output = create_string_buffer(size)\n         ret = RSA_private_encrypt(len(buf), buf, output, self.key, padding)\n         if ret == 0:\n             raise SSLError('Unable to encrypt data')\n         return output.raw[:ret]\n",
        "code_toks_joined": "def private_encrypt ( self , value , padding = RSA_PKCS1_PADDING ) : <NEWLINE> <INDENT> buf = create_string_buffer ( value , len ( value ) ) <NEWLINE> size = RSA_size ( self . key ) <NEWLINE> output = create_string_buffer ( size ) <NEWLINE> ret = RSA_private_encrypt ( len ( buf ) , buf , output , self . key , padding ) <NEWLINE> if ret == 0 : <NEWLINE> <INDENT> raise SSLError ( <STRING> ) <NEWLINE> <DEDENT> return output . raw [ : ret ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Unable to encrypt data'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1e2799e54c9b4a3cbb1c47e6af89cf7f": {
        "code_string": "variables = getattr(schema.schema, 'postman_environment_variables', None)\n                         if variables:\n                             for environment_key, response_key in variables.items():\n                                 environment_key = camelcase(environment_key)\n                                 response_key = camelcase(response_key)\n                                 tests.append(\n                                     'if (answer.%s){ postman.setEnvironmentVariable(\"%s\", answer.%s); }'\n                                     % (environment_key, environment_key, response_key))\n",
        "code_toks_joined": "variables = getattr ( schema . schema , <STRING> , None ) <NEWLINE> <INDENT> if variables : <NEWLINE> <INDENT> for environment_key , response_key in variables . items ( ) : <NEWLINE> <INDENT> environment_key = camelcase ( environment_key ) <NEWLINE> response_key = camelcase ( response_key ) <NEWLINE> tests . append ( <NEWLINE> <INDENT> <STRING> <NEWLINE> % ( environment_key , environment_key , response_key ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'postman_environment_variables'",
                "'if (answer.%s){ postman.setEnvironmentVariable(\"%s\", answer.%s); }'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6e7bb6b0793b4efab594381722804199": {
        "code_string": "def not_inactives_filter(column):\n     return and_(\n         or_(column.start_date <= func.now(), column.start_date.is_(None)),\n         or_(column.end_date >= func.now(), column.end_date.is_(None)))\n",
        "code_toks_joined": "def not_inactives_filter ( column ) : <NEWLINE> <INDENT> return and_ ( <NEWLINE> <INDENT> or_ ( column . start_date <= func . now ( ) , column . start_date . is_ ( None ) ) , <NEWLINE> or_ ( column . end_date >= func . now ( ) , column . end_date . is_ ( None ) ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d91ecc52755e4c979ba74a3e52b7c4f6": {
        "code_string": "# Make depth cutoff mask from depth cutoffs, if available.\n         cut_deep = np.inf\n         if hasattr(sitegrp['chronology'], 'cut_deep'):\n             cut_deep = float(sitegrp['chronology'].cut_deep)\n         cut_shallow = -np.inf\n         if hasattr(sitegrp['chronology'], 'cut_shallow'):\n             cut_deep = float(sitegrp['chronology'].cut_shallow)\n         depth = sitegrp['data'].variables['depth'][:]\n         cutoff_msk = (depth >= cut_shallow) & (depth <= cut_deep)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> cut_deep = np . inf <NEWLINE> if hasattr ( sitegrp [ <STRING> ] , <STRING> ) : <NEWLINE> <INDENT> cut_deep = float ( sitegrp [ <STRING> ] . cut_deep ) <NEWLINE> <DEDENT> cut_shallow = - np . inf <NEWLINE> if hasattr ( sitegrp [ <STRING> ] , <STRING> ) : <NEWLINE> <INDENT> cut_deep = float ( sitegrp [ <STRING> ] . cut_shallow ) <NEWLINE> <DEDENT> depth = sitegrp [ <STRING> ] . variables [ <STRING> ] [ : ] <NEWLINE> cutoff_msk = ( depth >= cut_shallow ) & ( depth <= cut_deep ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Make depth cutoff mask from depth cutoffs, if available."
            ],
            "<STRING>": [
                "'chronology'",
                "'cut_deep'",
                "'chronology'",
                "'chronology'",
                "'cut_shallow'",
                "'chronology'",
                "'data'",
                "'depth'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8d725cd9b9ee496a9859a8987e58dcc9": {
        "code_string": "assert flank in ['rise', 'decay']\n     pos = sig <= 0 if flank == 'rise' else sig >= 0\n",
        "code_toks_joined": "assert flank in [ <STRING> , <STRING> ] <NEWLINE> <INDENT> pos = sig <= 0 if flank == <STRING> else sig >= 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'rise'",
                "'decay'",
                "'rise'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "745fd300ae5e4034b6a5a78f133e3479": {
        "code_string": "def render(self, name, value, attrs=None):\n         #if self.is_localized:\n         #    self.widget.is_localized = self.is_localized\n         # value is a list of values, each corresponding to a widget\n         # in self.widgets.\n         if not isinstance(value, list):\n             value = self.decompress(value)\n         output = []\n         final_attrs = self.build_attrs(attrs)\n         id_ = final_attrs.get('id', None)\n         i = 0\n         for i, widget_value in enumerate(value):\n             if id_:\n                 final_attrs = dict(final_attrs, id='%s_%s' % (id_, i))\n             output.append(self.subfield.widget.render(name + '_%s' % i, widget_value, final_attrs))\n         output.append(self.subfield.widget.render(name + '_%s' % (i+1), None, final_attrs))\n         output.append(HiddenInput().render(name + '_count', str(i+1), {}))\n         return mark_safe(self.format_output(output))\n",
        "code_toks_joined": "def render ( self , name , value , attrs = None ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if not isinstance ( value , list ) : <NEWLINE> <INDENT> value = self . decompress ( value ) <NEWLINE> <DEDENT> output = [ ] <NEWLINE> final_attrs = self . build_attrs ( attrs ) <NEWLINE> id_ = final_attrs . get ( <STRING> , None ) <NEWLINE> i = 0 <NEWLINE> for i , widget_value in enumerate ( value ) : <NEWLINE> <INDENT> if id_ : <NEWLINE> <INDENT> final_attrs = dict ( final_attrs , id = <STRING> % ( id_ , i ) ) <NEWLINE> <DEDENT> output . append ( self . subfield . widget . render ( name + <STRING> % i , widget_value , final_attrs ) ) <NEWLINE> <DEDENT> output . append ( self . subfield . widget . render ( name + <STRING> % ( i + 1 ) , None , final_attrs ) ) <NEWLINE> output . append ( HiddenInput ( ) . render ( name + <STRING> , str ( i + 1 ) , { } ) ) <NEWLINE> return mark_safe ( self . format_output ( output ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#if self.is_localized:",
                "#    self.widget.is_localized = self.is_localized",
                "# value is a list of values, each corresponding to a widget",
                "# in self.widgets."
            ],
            "<STRING>": [
                "'id'",
                "'%s_%s'",
                "'_%s'",
                "'_%s'",
                "'_count'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "137d369940cc4ffc8912bd59c96fd313": {
        "code_string": "def private_keys_for_decryption(self, identifier: ID) -> Optional[list]:\n         keys = super().private_keys_for_decryption(identifier=identifier)\n         if keys is None:\n             # DIMP v1.0:\n             #     decrypt key not found, use the same with sign key?\n             key = self.private_key_for_signature(identifier)\n             if key is not None:\n                 keys = [keys]\n         return keys\n",
        "code_toks_joined": "def private_keys_for_decryption ( self , identifier : ID ) -> Optional [ list ] : <NEWLINE> <INDENT> keys = super ( ) . private_keys_for_decryption ( identifier = identifier ) <NEWLINE> if keys is None : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> key = self . private_key_for_signature ( identifier ) <NEWLINE> if key is not None : <NEWLINE> <INDENT> keys = [ keys ] <NEWLINE> <DEDENT> <DEDENT> return keys <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# DIMP v1.0:",
                "#     decrypt key not found, use the same with sign key?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "93a3f77e0c934c868ac32fd09a26afe8": {
        "code_string": "if pat_error != _PATError.PAT_SUCCESS:\n         return\n     elif pat_error == _PATError.PAT_INTERRUPTED_ERROR:\n         os.kill(os.getpid(), signal.SIGINT)\n     elif pat_error == _PATError.PAT_TERMINATED_ERROR:\n         os.kill(os.getpid(), signal.SIGTERM)\n         return\n     else:\n         raise PATException('Could not play {}.\\n{}'.format(audio_path, _error_to_str(pat_error)))\n",
        "code_toks_joined": "if pat_error != _PATError . PAT_SUCCESS : <NEWLINE> <INDENT> return <NEWLINE> elif pat_error == _PATError . PAT_INTERRUPTED_ERROR : <NEWLINE> os . kill ( os . getpid ( ) , signal . SIGINT ) <NEWLINE> elif pat_error == _PATError . PAT_TERMINATED_ERROR : <NEWLINE> os . kill ( os . getpid ( ) , signal . SIGTERM ) <NEWLINE> return <NEWLINE> else : <NEWLINE> raise PATException ( <STRING> . format ( audio_path , _error_to_str ( pat_error ) ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Could not play {}.\\n{}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e140c869d2924767b2e6fde491418050": {
        "code_string": "# Iterate over edges to create a DataFrame of actions\n     for i in time_dataframe.edges_iter(data=True):\n         if \"node\" in i[2]:\n             node = i[2][\"node\"]\n         else:\n             node = \"None\"\n         if \"msg\" in i[2]:\n             msg = i[2][\"msg\"]\n         else:\n             msg = \"None\"\n         # Create a new row\n         new_row = [\n             i[0],\n             i[1],\n             node,\n             msg,\n             i[2][\"type\"],\n             i[2][\"endopen\"],\n             i[2][\"start\"],\n             1\n             ]\n         # Add the new row to the DataFrame of actions\n         time_dataframe.loc[len(time_dataframe)] = new_row\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for i in time_dataframe . edges_iter ( data = True ) : <NEWLINE> <INDENT> if <STRING> in i [ 2 ] : <NEWLINE> <INDENT> node = i [ 2 ] [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> node = <STRING> <NEWLINE> <DEDENT> if <STRING> in i [ 2 ] : <NEWLINE> <INDENT> msg = i [ 2 ] [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> msg = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> new_row = [ <NEWLINE> <INDENT> i [ 0 ] , <NEWLINE> i [ 1 ] , <NEWLINE> node , <NEWLINE> msg , <NEWLINE> i [ 2 ] [ <STRING> ] , <NEWLINE> i [ 2 ] [ <STRING> ] , <NEWLINE> i [ 2 ] [ <STRING> ] , <NEWLINE> 1 <NEWLINE> ] <NEWLINE> <COMMENT> <NL> <DEDENT> time_dataframe . loc [ len ( time_dataframe ) ] = new_row <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Iterate over edges to create a DataFrame of actions",
                "# Create a new row",
                "# Add the new row to the DataFrame of actions"
            ],
            "<STRING>": [
                "\"node\"",
                "\"node\"",
                "\"None\"",
                "\"msg\"",
                "\"msg\"",
                "\"None\"",
                "\"type\"",
                "\"endopen\"",
                "\"start\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "baa1d3f90f1f426f97008ce1fb22dddf": {
        "code_string": "def _search_other(self, increment):\n         original_start = self.option.start\n         self.option.start += increment\n         papers = self._search()\n         if papers:\n             self.option.start = original_start\n         return papers\n",
        "code_toks_joined": "def _search_other ( self , increment ) : <NEWLINE> <INDENT> original_start = self . option . start <NEWLINE> self . option . start += increment <NEWLINE> papers = self . _search ( ) <NEWLINE> if papers : <NEWLINE> <INDENT> self . option . start = original_start <NEWLINE> <DEDENT> return papers <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "65aeaec504ff4b2080e4c4a2c059e93c": {
        "code_string": "def nic_type(self):\n         #nic speed\n         #nic type\n         nic = []\n         out = {'result':'Fail','optional':False,'out':nic,'text':'System Nic Info'}\n         try:\n             proc = subprocess.Popen(\"ls -I br* -I lo -I vir* /sys/class/net/\", stdout=subprocess.PIPE, shell=True)\n             (output,err) = proc.communicate()\n             output = str(output).strip()\n             output = output.split('\\n')\n             for o in output:\n                 did = subprocess.Popen(\"ls -al /sys/class/net/%s/device\"%o, stdout=subprocess.PIPE, shell=True)\n                 (didout,err) = did.communicate()\n                 didout = str(didout).strip()[-7:]\n                 #use lspci and grep to get nicbrand\n                 brand = subprocess.Popen(\"lspci | grep %s\"%didout, stdout=subprocess.PIPE, shell=True)\n                 (brandout,err) = brand.communicate()\n                 nic_brand = str(brandout).strip()[29:]\n                 if(nic_brand == ''):\n                     nic_brand = 'Unknown'\n                 try:\n                     speed = open(\"/sys/class/net/%s/speed\"%o,'r')\n                     nic_speed = 0\n                     if(int(speed.read().strip()) > 0):\n                         nic_speed = int(speed.read().strip())\n                         if(nic_speed == 1000):\n                             nic.append({'nic_name':o,'nic_speed':nic_speed,'nic_brand':nic_brand,'text':'NIC minimum config'})\n                         elif(nic_speed == 10000):\n                             nic.append({'nic_name':o,'nic_speed':nic_speed,'nic_brand':nic_brand,'text':'NIC recommended config'})\n                 except Exception as e:\n                     nic.append({'nic_name':o,'nic_speed':'Unknown','nic_brand':nic_brand,'text':'NIC Unknown'})\n             out = {'result':'Pass','optional':False,'out':nic,'text':'System Nic Info'}\n         except Exception as e:\n             out = {'result':e,'optional':False,'out':nic,'text':'System Nic Info'}\n",
        "code_toks_joined": "def nic_type ( self ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> nic = [ ] <NEWLINE> out = { <STRING> : <STRING> , <STRING> : False , <STRING> : nic , <STRING> : <STRING> } <NEWLINE> try : <NEWLINE> <INDENT> proc = subprocess . Popen ( <STRING> , stdout = subprocess . PIPE , shell = True ) <NEWLINE> ( output , err ) = proc . communicate ( ) <NEWLINE> output = str ( output ) . strip ( ) <NEWLINE> output = output . split ( <STRING> ) <NEWLINE> for o in output : <NEWLINE> <INDENT> did = subprocess . Popen ( <STRING> % o , stdout = subprocess . PIPE , shell = True ) <NEWLINE> ( didout , err ) = did . communicate ( ) <NEWLINE> didout = str ( didout ) . strip ( ) [ - 7 : ] <NEWLINE> <COMMENT> <NL> brand = subprocess . Popen ( <STRING> % didout , stdout = subprocess . PIPE , shell = True ) <NEWLINE> ( brandout , err ) = brand . communicate ( ) <NEWLINE> nic_brand = str ( brandout ) . strip ( ) [ 29 : ] <NEWLINE> if ( nic_brand == <STRING> ) : <NEWLINE> <INDENT> nic_brand = <STRING> <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> speed = open ( <STRING> % o , <STRING> ) <NEWLINE> nic_speed = 0 <NEWLINE> if ( int ( speed . read ( ) . strip ( ) ) > 0 ) : <NEWLINE> <INDENT> nic_speed = int ( speed . read ( ) . strip ( ) ) <NEWLINE> if ( nic_speed == 1000 ) : <NEWLINE> <INDENT> nic . append ( { <STRING> : o , <STRING> : nic_speed , <STRING> : nic_brand , <STRING> : <STRING> } ) <NEWLINE> <DEDENT> elif ( nic_speed == 10000 ) : <NEWLINE> <INDENT> nic . append ( { <STRING> : o , <STRING> : nic_speed , <STRING> : nic_brand , <STRING> : <STRING> } ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> nic . append ( { <STRING> : o , <STRING> : <STRING> , <STRING> : nic_brand , <STRING> : <STRING> } ) <NEWLINE> <DEDENT> <DEDENT> out = { <STRING> : <STRING> , <STRING> : False , <STRING> : nic , <STRING> : <STRING> } <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> out = { <STRING> : e , <STRING> : False , <STRING> : nic , <STRING> : <STRING> } <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#nic speed",
                "#nic type",
                "#use lspci and grep to get nicbrand"
            ],
            "<STRING>": [
                "'result'",
                "'Fail'",
                "'optional'",
                "'out'",
                "'text'",
                "'System Nic Info'",
                "\"ls -I br* -I lo -I vir* /sys/class/net/\"",
                "'\\n'",
                "\"ls -al /sys/class/net/%s/device\"",
                "\"lspci | grep %s\"",
                "''",
                "'Unknown'",
                "\"/sys/class/net/%s/speed\"",
                "'r'",
                "'nic_name'",
                "'nic_speed'",
                "'nic_brand'",
                "'text'",
                "'NIC minimum config'",
                "'nic_name'",
                "'nic_speed'",
                "'nic_brand'",
                "'text'",
                "'NIC recommended config'",
                "'nic_name'",
                "'nic_speed'",
                "'Unknown'",
                "'nic_brand'",
                "'text'",
                "'NIC Unknown'",
                "'result'",
                "'Pass'",
                "'optional'",
                "'out'",
                "'text'",
                "'System Nic Info'",
                "'result'",
                "'optional'",
                "'out'",
                "'text'",
                "'System Nic Info'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "29f6c252d82145e6b7c276d5dcc612c2": {
        "code_string": "def write(pattern, f, settings=None):\n     if settings is not None:\n         settings = {}\n",
        "code_toks_joined": "def write ( pattern , f , settings = None ) : <NEWLINE> <INDENT> if settings is not None : <NEWLINE> <INDENT> settings = { } <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6c6f73d4ef414e5e9c9d568684313d07": {
        "code_string": "@classmethod\n             def _create_instance_from_data(cls, data):\n                 subcls = cls._unmodified_cls._search_subclass(data['_cls'])\n                 return subcls(subcls.connection_alias, **data)\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def _create_instance_from_data ( cls , data ) : <NEWLINE> <INDENT> subcls = cls . _unmodified_cls . _search_subclass ( data [ <STRING> ] ) <NEWLINE> return subcls ( subcls . connection_alias , ** data ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'_cls'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "df66031e498e4199a6bf6d796a7ce6c5": {
        "code_string": "@classmethod\n     def from_bytes(cls, in_data):\n         version = int.from_bytes(in_data[0:2], 'big')\n         source_wport = int.from_bytes(in_data[2:4], 'big')\n         destination_wport = int.from_bytes(in_data[4:6], 'big')\n         length = int.from_bytes(in_data[6:8], 'big')\n         body = in_data[8:]\n         body_length = len(body)\n         if not length == body_length:\n             raise ValueError((\n                 f'Length of data in UDP message ({body_length}) does not match '\n                 f'the length parameter in the UDP Wrapper Header ({length})'))\n         return cls(source_wport, destination_wport, in_data, version)\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def from_bytes ( cls , in_data ) : <NEWLINE> <INDENT> version = int . from_bytes ( in_data [ 0 : 2 ] , <STRING> ) <NEWLINE> source_wport = int . from_bytes ( in_data [ 2 : 4 ] , <STRING> ) <NEWLINE> destination_wport = int . from_bytes ( in_data [ 4 : 6 ] , <STRING> ) <NEWLINE> length = int . from_bytes ( in_data [ 6 : 8 ] , <STRING> ) <NEWLINE> body = in_data [ 8 : ] <NEWLINE> body_length = len ( body ) <NEWLINE> if not length == body_length : <NEWLINE> <INDENT> raise ValueError ( ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> return cls ( source_wport , destination_wport , in_data , version ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'big'",
                "'big'",
                "'big'",
                "'big'",
                "f'Length of data in UDP message ({body_length}) does not match '",
                "f'the length parameter in the UDP Wrapper Header ({length})'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1562324139d3482cbe2303f7f6232f2b": {
        "code_string": "name = self.default_module_name\n             resource_name = ''\n             # split path at slash to separate every parameter\n             split = re.split(\"/\", operation.path)\n             for s in split:\n                 # find the parameter, where a variable was defined to exlude it in resource_name\n                 pattern = re.compile(r\"\\{[a-zA-Z-_]+\\}\")\n                 if not s and pattern.search(s) is None:\n                     resource_name += s.title()\n",
        "code_toks_joined": "name = self . default_module_name <NEWLINE> <INDENT> resource_name = <STRING> <NEWLINE> <COMMENT> <NL> split = re . split ( <STRING> , operation . path ) <NEWLINE> for s in split : <NEWLINE> <COMMENT> <NL> <INDENT> pattern = re . compile ( <STRING> ) <NEWLINE> if not s and pattern . search ( s ) is None : <NEWLINE> <INDENT> resource_name += s . title ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''",
                "\"/\"",
                "r\"\\{[a-zA-Z-_]+\\}\""
            ],
            "<COMMENT>": [
                "# split path at slash to separate every parameter",
                "# find the parameter, where a variable was defined to exlude it in resource_name"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0bb22e1f72b04948937efbaf4a204fea": {
        "code_string": "def is_subpath(base, path, sep=os.path.sep):\n     \"\"\"\n     Check if the given path is a proper subpath of a base path.\n     \"\"\"\n     if path.startswith(path):\n         trailing = base[len(base):]\n         return trailing == '' or trailing[0] == sep\n     return False\n",
        "code_toks_joined": "def is_subpath ( base , path , sep = os . path . sep ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if path . startswith ( path ) : <NEWLINE> <INDENT> trailing = base [ len ( base ) : ] <NEWLINE> return trailing == <STRING> or trailing [ 0 ] == sep <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Check if the given path is a proper subpath of a base path.\n     \"\"\"",
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "da5ba7c5b8d24519869581d6839637dc": {
        "code_string": "xlim1 = ax1.get_xlim()\n     ylim1 = ax2.get_ylim()\n",
        "code_toks_joined": "xlim1 = ax1 . get_xlim ( ) <NEWLINE> <INDENT> ylim1 = ax2 . get_ylim ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "82f09a5db56146ad806365068fbbba4b": {
        "code_string": "def __init__(self, multihash_length: int, digest_length: int) -> None:\n         template = \"length from data ({}) and metadata ({}) don't match\"\n         super().__init__(template.format(multihash_length, digest_length))\n",
        "code_toks_joined": "def __init__ ( self , multihash_length : int , digest_length : int ) -> None : <NEWLINE> <INDENT> template = <STRING> <NEWLINE> super ( ) . __init__ ( template . format ( multihash_length , digest_length ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"length from data ({}) and metadata ({}) don't match\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0a7bfc2dd4734d0b82ed1946e4b37262": {
        "code_string": "def read_version():\n     \"\"\"\n     Read the contents of relative file.\n     \"\"\"\n     file_path = os.path.join(\n         os.path.dirname(__file__), PACKAGE_NAME, 'version.py'\n     )\n     regex = re.compile('__version__ = [\\'\\\"]([^\\'\\\"]*)[\\'\\\"]')\n     with codecs.open(file_path, encoding='utf-8') as fobj:\n         for line in fobj:\n             mobj = regex.match(line)\n             if mobj:\n                 return regex.group(1)\n     raise Exception('Failed to read version')\n",
        "code_toks_joined": "def read_version ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> file_path = os . path . join ( <NEWLINE> <INDENT> os . path . dirname ( __file__ ) , PACKAGE_NAME , <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> regex = re . compile ( <STRING> ) <NEWLINE> with codecs . open ( file_path , encoding = <STRING> ) as fobj : <NEWLINE> <INDENT> for line in fobj : <NEWLINE> <INDENT> mobj = regex . match ( line ) <NEWLINE> if mobj : <NEWLINE> <INDENT> return regex . group ( 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Read the contents of relative file.\n     \"\"\"",
                "'version.py'",
                "'__version__ = [\\'\\\"]([^\\'\\\"]*)[\\'\\\"]'",
                "'utf-8'",
                "'Failed to read version'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d79c3af8f28440bbbe23bd5a4316efd7": {
        "code_string": "if (\n             # Ignore Exception Objects.\n             type(value) == type\n             # Ignore Special Methods.\n             or is_dunder(attr)\n             # Ignore Functions.\n             or inspect.isfunction(attr)\n             # Ignore Django Model Attributes.\n             or attr in (\"objects\", \"id\", \"_meta\")\n             # Ignore Fields.\n             or attr in fields\n             # Ignore if is instance of Field.\n             or isinstance(value, Field)\n             # Ignore Properties.\n             or isinstance(value, property)\n             # Ignore Descriptors.\n             or isinstance(value, RELATED_DESCRIPTORS)\n         ):\n             return False\n         else:\n             return True\n",
        "code_toks_joined": "if ( <NEWLINE> <COMMENT> <NL> <INDENT> type ( value ) == type <NEWLINE> <COMMENT> <NL> or is_dunder ( attr ) <NEWLINE> <COMMENT> <NL> or inspect . isfunction ( attr ) <NEWLINE> <COMMENT> <NL> or attr in ( <STRING> , <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> or attr in fields <NEWLINE> <COMMENT> <NL> or isinstance ( value , Field ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , property ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , RELATED_DESCRIPTORS ) <NEWLINE> ) : <NEWLINE> return False <NEWLINE> else : <NEWLINE> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Ignore Exception Objects.",
                "# Ignore Special Methods.",
                "# Ignore Functions.",
                "# Ignore Django Model Attributes.",
                "# Ignore Fields.",
                "# Ignore if is instance of Field.",
                "# Ignore Properties.",
                "# Ignore Descriptors."
            ],
            "<STRING>": [
                "\"objects\"",
                "\"id\"",
                "\"_meta\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "efb45148e15d47638fe380ea095621c1": {
        "code_string": "return results\n",
        "code_toks_joined": "return results <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d87d8e8a03da42c3a9ba09082507e153": {
        "code_string": "suites = collect_suites(cwd)\n",
        "code_toks_joined": "suites = collect_suites ( cwd ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "667a3c957a974a7182fced19c927e094": {
        "code_string": "suites = collect_suites(cwd)\n",
        "code_toks_joined": "suites = collect_suites ( cwd ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a7bf332bd9c74076b250050ede3f76ac": {
        "code_string": "def _get(name, default=None, compat=None):\n     compat = compat or []\n     if default is None:\n         default = _DEFAULTS.get(name)\n     compat = [name] + compat\n     for i, alias in enumerate(compat):\n         try:\n             value = getattr(settings, name)\n             i > 0 and warnings.warn(DeprecationWarning(_DEPRECATION_FMT % (\n                                                         alias, name)))\n             return value\n         except AttributeError:\n             pass\n     return default\n",
        "code_toks_joined": "def _get ( name , default = None , compat = None ) : <NEWLINE> <INDENT> compat = compat or [ ] <NEWLINE> if default is None : <NEWLINE> <INDENT> default = _DEFAULTS . get ( name ) <NEWLINE> <DEDENT> compat = [ name ] + compat <NEWLINE> for i , alias in enumerate ( compat ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> value = getattr ( settings , name ) <NEWLINE> i > 0 and warnings . warn ( DeprecationWarning ( _DEPRECATION_FMT % ( <NEWLINE> <INDENT> alias , name ) ) ) <NEWLINE> <DEDENT> return value <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> return default <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5d631354643b4e979f384667cebb39c7": {
        "code_string": "def route(self, options, task, args=(), kwargs={}):\n         # Expand \"queue\" keys in options.\n         options = self.expand_destination(options)\n         if self.routes:\n             route = self.lookup_route(task, args, kwargs)\n             if route:\n                 # Also expand \"queue\" keys in route.\n                 return merge(options, self.expand_destination(route))\n         return options\n",
        "code_toks_joined": "def route ( self , options , task , args = ( ) , kwargs = { } ) : <NEWLINE> <COMMENT> <NL> <INDENT> options = self . expand_destination ( options ) <NEWLINE> if self . routes : <NEWLINE> <INDENT> route = self . lookup_route ( task , args , kwargs ) <NEWLINE> if route : <NEWLINE> <COMMENT> <NL> <INDENT> return merge ( options , self . expand_destination ( route ) ) <NEWLINE> <DEDENT> <DEDENT> return options <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Expand \"queue\" keys in options.",
                "# Also expand \"queue\" keys in route."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a4f3a594a5d746e1837b5a4523176017": {
        "code_string": "if self.state != RUN or self.started != len(parent.steps):\n             # Not fully started, can safely exit.\n             self.state = TERMINATE\n             self.shutdown_complete.set()\n             return\n         self.close(parent)\n         self.state = CLOSE\n         self.restart(parent, what, 'terminate' if terminate else 'stop')\n",
        "code_toks_joined": "if self . state != RUN or self . started != len ( parent . steps ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . state = TERMINATE <NEWLINE> self . shutdown_complete . set ( ) <NEWLINE> return <NEWLINE> self . close ( parent ) <NEWLINE> self . state = CLOSE <NEWLINE> self . restart ( parent , what , <STRING> if terminate else <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Not fully started, can safely exit."
            ],
            "<STRING>": [
                "'terminate'",
                "'stop'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e1d289bce565488daba22b64705b5434": {
        "code_string": "def test_stop(self):\n         c = Mock()\n         tasks = Tasks(c)\n         self.assertIsNone(c.task_consumer)\n         self.assertIsNone(c.qos)\n         self.assertEqual(tasks.initial_prefetch_count, 2)\n",
        "code_toks_joined": "def test_stop ( self ) : <NEWLINE> <INDENT> c = Mock ( ) <NEWLINE> tasks = Tasks ( c ) <NEWLINE> self . assertIsNone ( c . task_consumer ) <NEWLINE> self . assertIsNone ( c . qos ) <NEWLINE> self . assertEqual ( tasks . initial_prefetch_count , 2 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f4c5e9730b2e44c58244e9d8f35559fd": {
        "code_string": "self.host = uhost or config.get('host', self.host)\n         self.port = int(uport or config.get('port', self.port))\n         self.bucket_name = ubucket or config.get('bucket', self.bucket_name)\n         self.protocol = uprot or config.get('protocol', self.protocol)\n",
        "code_toks_joined": "self . host = uhost or config . get ( <STRING> , self . host ) <NEWLINE> <INDENT> self . port = int ( uport or config . get ( <STRING> , self . port ) ) <NEWLINE> self . bucket_name = ubucket or config . get ( <STRING> , self . bucket_name ) <NEWLINE> self . protocol = uprot or config . get ( <STRING> , self . protocol ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'host'",
                "'port'",
                "'bucket'",
                "'protocol'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c43609fcfe884a28923d108615c9e087": {
        "code_string": "def config_from_envvar(self, variable_name, silent=False, force=False):\n         module_name = os.environ.get(variable_name)\n         if not module_name:\n             if silent:\n                 return False\n             raise ImproperlyConfigured(ERR_ENVVAR_NOT_SET.format(module_name))\n         return self.config_from_object(module_name, silent=silent, force=force)\n",
        "code_toks_joined": "def config_from_envvar ( self , variable_name , silent = False , force = False ) : <NEWLINE> <INDENT> module_name = os . environ . get ( variable_name ) <NEWLINE> if not module_name : <NEWLINE> <INDENT> if silent : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> raise ImproperlyConfigured ( ERR_ENVVAR_NOT_SET . format ( module_name ) ) <NEWLINE> <DEDENT> return self . config_from_object ( module_name , silent = silent , force = force ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6df85007cb6c4c6e89b47db9854522ef": {
        "code_string": "attrs = {\n         attr_name: (prepare_attr(attr) if prepare_attr else attr)\n         for attr_name, attr in items(attrs)\n     }\n     module = sys.modules[fqdn] = type(modname, (base, ), cls_attrs)(fqdn)\n     module.__dict__.update(attrs)\n     return module\n",
        "code_toks_joined": "attrs = { <NEWLINE> <INDENT> attr_name : ( prepare_attr ( attr ) if prepare_attr else attr ) <NEWLINE> for attr_name , attr in items ( attrs ) <NEWLINE> } <NEWLINE> module = sys . modules [ fqdn ] = type ( modname , ( base , ) , cls_attrs ) ( fqdn ) <NEWLINE> module . __dict__ . update ( attrs ) <NEWLINE> return module <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5c047d2f83114f45a61eb30e3fb89e14": {
        "code_string": "def on_timeout(self, soft, timeout):\n         \"\"\"Handler called if the task times out.\"\"\"\n         task_ready(self)\n         if soft:\n             warn('Soft time limit (%ss) exceeded for %s[%s]',\n                  soft, self.name, self.id)\n             exc = SoftTimeLimitExceeded(soft)\n         else:\n             error('Hard time limit (%ss) exceeded for %s[%s]',\n                   timeout, self.name, self.id)\n             exc = TimeLimitExceeded(timeout)\n",
        "code_toks_joined": "def on_timeout ( self , soft , timeout ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> task_ready ( self ) <NEWLINE> if soft : <NEWLINE> <INDENT> warn ( <STRING> , <NEWLINE> <INDENT> soft , self . name , self . id ) <NEWLINE> <DEDENT> exc = SoftTimeLimitExceeded ( soft ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> error ( <STRING> , <NEWLINE> <INDENT> timeout , self . name , self . id ) <NEWLINE> <DEDENT> exc = TimeLimitExceeded ( timeout ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Handler called if the task times out.\"\"\"",
                "'Soft time limit (%ss) exceeded for %s[%s]'",
                "'Hard time limit (%ss) exceeded for %s[%s]'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d02911f64ef0443798d9e94cca2ad9ae": {
        "code_string": "if knockins is not None:\n         for cs in carbon_sources:\n             add_metabolite_exchange(model, cs)\n",
        "code_toks_joined": "if knockins is not None : <NEWLINE> <INDENT> for cs in carbon_sources : <NEWLINE> <INDENT> add_metabolite_exchange ( model , cs ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "844bcd22873f4f22ad0abdd9a9315f60": {
        "code_string": "bw.wire.TristateBuffer(input_1, switch, output)\n",
        "code_toks_joined": "bw . wire . TristateBuffer ( input_1 , switch , output ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "89fe368e6c964b669491367faee3bc66": {
        "code_string": "self.carry_in = carry_in\n         self.a = a\n         self.b = a\n         self.carry_out = carry_out\n         self.sum = sum\n",
        "code_toks_joined": "self . carry_in = carry_in <NEWLINE> <INDENT> self . a = a <NEWLINE> self . b = a <NEWLINE> self . carry_out = carry_out <NEWLINE> self . sum = sum <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b95814ec2bf640ca86fb8669b0d598c9": {
        "code_string": "if self.oauth_error != None and self.oauth_token != None or self.state != None:\n",
        "code_toks_joined": "if self . oauth_error != None and self . oauth_token != None or self . state != None : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "53989b5ed786403c9b67a6b8e1134efe": {
        "code_string": "with tempfile.TemporaryDirectory() as temp_dir:\n         file = pathlib.Path(temp_dir) / 'horizons.h5'\n         sxs.horizons.xor_multishuffle_bzip2.save(file, horizons_spec)\n         with pytest.raises(ValueError):\n             horizons_error = sxs.horizons.spec_horizons_h5.load(file)\n         horizons_xmb = sxs.horizons.xor_multishuffle_bzip2.load(file)\n",
        "code_toks_joined": "with tempfile . TemporaryDirectory ( ) as temp_dir : <NEWLINE> <INDENT> file = pathlib . Path ( temp_dir ) / <STRING> <NEWLINE> sxs . horizons . xor_multishuffle_bzip2 . save ( file , horizons_spec ) <NEWLINE> with pytest . raises ( ValueError ) : <NEWLINE> <INDENT> horizons_error = sxs . horizons . spec_horizons_h5 . load ( file ) <NEWLINE> <DEDENT> horizons_xmb = sxs . horizons . xor_multishuffle_bzip2 . load ( file ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'horizons.h5'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2c59a97de6234d9db4b143af285c1acd": {
        "code_string": "return wellbarofixed, drift_info\n",
        "code_toks_joined": "return wellbarofixed , drift_info <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b51a2055f56f40c2bbda1d63c7c9d6eb": {
        "code_string": "def get_modules(THIRD_PARTY, INTERNAL, PROJ_PATH,\n                 SO_SUFFIX, source_for_module_with_pyinit):\n     \"\"\" Get all modules this package needs compiled \"\"\"\n     PYSQLITE2 = INTERNAL + '/pysqlite2'\n     APSW = INTERNAL + '/apsw'\n     PYSQLITE = THIRD_PARTY + '/_pysqlite'\n     APSW_TP = THIRD_PARTY + '/_apsw'\n     SQLITE3 = THIRD_PARTY + '/sqlite3'\n     ICU_UNIX = SQLITE3 + '/icu_unix'\n     ICU_WIN32 = SQLITE3 + '/icu_win32'\n     includes = [os.path.relpath(SQLITE3, PROJ_PATH)]\n     libraries = [os.path.relpath(SQLITE3, PROJ_PATH)]\n     link_args = [\"-flto\"]\n     if sys.platform == 'win32':\n         libraries.append(ICU_WIN32)\n         includes.append(ICU_WIN32)\n         link_args.append('-L' + ICU_WIN32)\n     else:\n         libraries.append(ICU_UNIX)\n         includes.append(ICU_UNIX)\n         link_args.append('-L' + ICU_WIN32)\n",
        "code_toks_joined": "def get_modules ( THIRD_PARTY , INTERNAL , PROJ_PATH , <NEWLINE> <INDENT> SO_SUFFIX , source_for_module_with_pyinit ) : <NEWLINE> <STRING> <NEWLINE> PYSQLITE2 = INTERNAL + <STRING> <NEWLINE> APSW = INTERNAL + <STRING> <NEWLINE> PYSQLITE = THIRD_PARTY + <STRING> <NEWLINE> APSW_TP = THIRD_PARTY + <STRING> <NEWLINE> SQLITE3 = THIRD_PARTY + <STRING> <NEWLINE> ICU_UNIX = SQLITE3 + <STRING> <NEWLINE> ICU_WIN32 = SQLITE3 + <STRING> <NEWLINE> includes = [ os . path . relpath ( SQLITE3 , PROJ_PATH ) ] <NEWLINE> libraries = [ os . path . relpath ( SQLITE3 , PROJ_PATH ) ] <NEWLINE> link_args = [ <STRING> ] <NEWLINE> if sys . platform == <STRING> : <NEWLINE> libraries . append ( ICU_WIN32 ) <NEWLINE> includes . append ( ICU_WIN32 ) <NEWLINE> link_args . append ( <STRING> + ICU_WIN32 ) <NEWLINE> else : <NEWLINE> libraries . append ( ICU_UNIX ) <NEWLINE> includes . append ( ICU_UNIX ) <NEWLINE> link_args . append ( <STRING> + ICU_WIN32 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Get all modules this package needs compiled \"\"\"",
                "'/pysqlite2'",
                "'/apsw'",
                "'/_pysqlite'",
                "'/_apsw'",
                "'/sqlite3'",
                "'/icu_unix'",
                "'/icu_win32'",
                "\"-flto\"",
                "'win32'",
                "'-L'",
                "'-L'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d39c4b9f9b6464b8b00ae78934f4c62": {
        "code_string": "if state != 0:\n             if grepy(outfile, 'JOB DONE.'):\n                 pass\n             else:\n                 raise RuntimeError(\n                     'Execution returned a non-zero state: '\n                     '{}'.format(' '.join(command)))\n",
        "code_toks_joined": "if state != 0 : <NEWLINE> <INDENT> if grepy ( outfile , <STRING> ) : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> . format ( <STRING> . join ( command ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'JOB DONE.'",
                "'Execution returned a non-zero state: '",
                "'{}'",
                "' '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dc38efde34384e629cb237fc889d98b7": {
        "code_string": "# Open the submission window\n         submission_info = SUBMISSION_FILE.format(name=sub, content='')\n         curses.endwin()\n         submission_text = open_editor(submission_info)\n         curses.doupdate()\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> submission_info = SUBMISSION_FILE . format ( name = sub , content = <STRING> ) <NEWLINE> curses . endwin ( ) <NEWLINE> submission_text = open_editor ( submission_info ) <NEWLINE> curses . doupdate ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Open the submission window"
            ],
            "<STRING>": [
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b7dedebe3c704c39973fd97c5d4f65bf": {
        "code_string": "# Prune empty lines at the bottom of the textbox.\n         for item in stack[::-1]:\n             if item:\n                 stack.pop()\n             else:\n                 break\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for item in stack [ : : - 1 ] : <NEWLINE> <INDENT> if item : <NEWLINE> <INDENT> stack . pop ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Prune empty lines at the bottom of the textbox."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0f1f584bd1ec4908b3868e6701767e0c": {
        "code_string": "source = ColumnDataSource(data=dict(\n         x=params.values,\n         y=np.arange(1, len(params.index) + 1),\n         factor_names=params.index.values,\n         bar_colours=bar_colours,\n         bar_signs=bar_signs,\n         full_names=full_names,\n         original_magnitude_with_sign=beta_str,\n         alias_strings=alias_strings,\n     ))\n     TOOLTIPS = [\n         (\"Short name\", \"@factor_names\"),\n         (\"Full name\", \"@full_names\"),\n         (\"Magnitude and sign\", \"@original_magnitude_with_sign\"),\n     ]\n     if len(alias_strings) == 0:\n         TOOLTIPS.append((\"Aliasing\", \"@alias_strings\"),)\n",
        "code_toks_joined": "source = ColumnDataSource ( data = dict ( <NEWLINE> <INDENT> x = params . values , <NEWLINE> y = np . arange ( 1 , len ( params . index ) + 1 ) , <NEWLINE> factor_names = params . index . values , <NEWLINE> bar_colours = bar_colours , <NEWLINE> bar_signs = bar_signs , <NEWLINE> full_names = full_names , <NEWLINE> original_magnitude_with_sign = beta_str , <NEWLINE> alias_strings = alias_strings , <NEWLINE> ) ) <NEWLINE> TOOLTIPS = [ <NEWLINE> ( <STRING> , <STRING> ) , <NEWLINE> ( <STRING> , <STRING> ) , <NEWLINE> ( <STRING> , <STRING> ) , <NEWLINE> ] <NEWLINE> if len ( alias_strings ) == 0 : <NEWLINE> TOOLTIPS . append ( ( <STRING> , <STRING> ) , ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Short name\"",
                "\"@factor_names\"",
                "\"Full name\"",
                "\"@full_names\"",
                "\"Magnitude and sign\"",
                "\"@original_magnitude_with_sign\"",
                "\"Aliasing\"",
                "\"@alias_strings\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0df65b8bb3c64e7e9b3ea4eb309138eb": {
        "code_string": "next_character_position = found + len(old_approach) + 1\n         if next_character_position > len(line):\n             return found\n",
        "code_toks_joined": "next_character_position = found + len ( old_approach ) + 1 <NEWLINE> <INDENT> if next_character_position > len ( line ) : <NEWLINE> <INDENT> return found <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c647ccc018594396b64e32f4520532f0": {
        "code_string": "def __deepcopy__(self, memo):\n         snippet = PMXSnippet(self.hash, self.namespace)\n         memo[\"snippet\"] = deepcopy(self.snippet, memo)\n         snippet.bundle = self.bundle\n         return snippet\n",
        "code_toks_joined": "def __deepcopy__ ( self , memo ) : <NEWLINE> <INDENT> snippet = PMXSnippet ( self . hash , self . namespace ) <NEWLINE> memo [ <STRING> ] = deepcopy ( self . snippet , memo ) <NEWLINE> snippet . bundle = self . bundle <NEWLINE> return snippet <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"snippet\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a2cac1d2bf2d4bd7844488f7be989d3d": {
        "code_string": "def setFilterNamespace(self, namespace):\n         if namespace:\n             self.namespacesFilter = [ \"prymatex\", \"user\" ]\n         else:\n             self.namespacesFilter = namespace.split()\n         self.setFilterRegExp(\"\")\n",
        "code_toks_joined": "def setFilterNamespace ( self , namespace ) : <NEWLINE> <INDENT> if namespace : <NEWLINE> <INDENT> self . namespacesFilter = [ <STRING> , <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . namespacesFilter = namespace . split ( ) <NEWLINE> <DEDENT> self . setFilterRegExp ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"prymatex\"",
                "\"user\"",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ac5af38272914f6ba622c2f0ef12840c": {
        "code_string": "@classmethod\n     def flyweightScopeFactory(cls, scopeStack):\n         scopeHash = hash(scopeStack)\n         if scopeHash not in cls.SCOPES:\n             scopeName = \" \".join(scopeStack)\n             cls.SCOPES[scopeHash] = CodeEditorScope(\n                 name = scopeName,\n                 path = scopeStack,\n                 settings = cls.application.supportManager.getPreferenceSettings(scopeName),\n                 group = PMXSyntax.findGroup(scopeStack[::-1])\n             )\n         return scopeHash\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def flyweightScopeFactory ( cls , scopeStack ) : <NEWLINE> <INDENT> scopeHash = hash ( scopeStack ) <NEWLINE> if scopeHash not in cls . SCOPES : <NEWLINE> <INDENT> scopeName = <STRING> . join ( scopeStack ) <NEWLINE> cls . SCOPES [ scopeHash ] = CodeEditorScope ( <NEWLINE> <INDENT> name = scopeName , <NEWLINE> path = scopeStack , <NEWLINE> settings = cls . application . supportManager . getPreferenceSettings ( scopeName ) , <NEWLINE> group = PMXSyntax . findGroup ( scopeStack [ : : - 1 ] ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return scopeHash <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\" \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f9d6bcce1a224777b2cdc4b3f71e63b2": {
        "code_string": "def tokenAtPosition(self, pos):\n         for token in self.__tokens[::-1]:\n             if token.start <= pos <= token.end:\n                 return token\n",
        "code_toks_joined": "def tokenAtPosition ( self , pos ) : <NEWLINE> <INDENT> for token in self . __tokens [ : : - 1 ] : <NEWLINE> <INDENT> if token . start <= pos <= token . end : <NEWLINE> <INDENT> return token <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9a2fe1de066743b1b12df8fadef46592": {
        "code_string": "def blockRevision(self, block):\n         return _revision(block.text() + \"\\n\", self.scope_name, block.previous().userState())\n",
        "code_toks_joined": "def blockRevision ( self , block ) : <NEWLINE> <INDENT> return _revision ( block . text ( ) + <STRING> , self . scope_name , block . previous ( ) . userState ( ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "968aac9fe7f94e0eb94d15e4acece365": {
        "code_string": "trajectory_table, _ = import_trajectory_table(trajectories_filename)\n \tgenotype_table = calculate_genotypes.workflow(trajectories_filename, options = goptions)\n \t# return trajectory_table, genotype_table\n",
        "code_toks_joined": "trajectory_table , _ = import_trajectory_table ( trajectories_filename ) <NEWLINE> <INDENT> genotype_table = calculate_genotypes . workflow ( trajectories_filename , options = goptions ) <NEWLINE> <COMMENT> <NL> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# return trajectory_table, genotype_table"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a8c5002e8089460381eb2209eec20571": {
        "code_string": "def waitForChromeToClose(self):\n         \"\"\"Terminate chrome.\"\"\"\n         if self.proc.returncode is not None and not self.chromeClosed:\n             self.chromeClosed = True\n             if psutil.pid_exists(self.proc.pid):\n                 self.proc.terminate()\n                 self.proc.kill()\n",
        "code_toks_joined": "def waitForChromeToClose ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . proc . returncode is not None and not self . chromeClosed : <NEWLINE> <INDENT> self . chromeClosed = True <NEWLINE> if psutil . pid_exists ( self . proc . pid ) : <NEWLINE> <INDENT> self . proc . terminate ( ) <NEWLINE> self . proc . kill ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Terminate chrome.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "61ee21d94e4d422b818257a4de946cf0": {
        "code_string": "ppath = ctx.obj[\"ppath\"]\n     if norm_id is not None:\n         normfolder = ppath.basedir + \"{}_{}\".format(norm_id, folder_suffix)\n         norm = get_normalisation(fdname=normfolder, name=norm_name)\n     else:\n         import numpy as np\n         norm = np.array(1)\n",
        "code_toks_joined": "ppath = ctx . obj [ <STRING> ] <NEWLINE> <INDENT> if norm_id is not None : <NEWLINE> <INDENT> normfolder = ppath . basedir + <STRING> . format ( norm_id , folder_suffix ) <NEWLINE> norm = get_normalisation ( fdname = normfolder , name = norm_name ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> import numpy as np <NEWLINE> norm = np . array ( 1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"ppath\"",
                "\"{}_{}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8913bb159f1e4a06ab19c2df6fb01250": {
        "code_string": "doc.update(d)\n     yaml.dump(d,\n               stream=open(path, 'w'),\n               default_flow_style=False,\n               indent=2,\n               width=72)\n",
        "code_toks_joined": "doc . update ( d ) <NEWLINE> <INDENT> yaml . dump ( d , <NEWLINE> <INDENT> stream = open ( path , <STRING> ) , <NEWLINE> default_flow_style = False , <NEWLINE> indent = 2 , <NEWLINE> width = 72 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b05edc4f048e49799d9b569638373161": {
        "code_string": "while xs or ys:\n         if xs and not ys:\n             yield xs.pop(), None\n         elif ys and not xs:\n             yield None, ys.pop()\n         elif key(xs[-1]) == key(ys[-1]):\n             yield xs.pop(), ys.pop()\n         elif key(xs[-1]) > key(ys[-1]):\n             yield xs.pop(), None\n         else:\n             yield None, ys.pop()\n",
        "code_toks_joined": "while xs or ys : <NEWLINE> <INDENT> if xs and not ys : <NEWLINE> <INDENT> yield xs . pop ( ) , None <NEWLINE> <DEDENT> elif ys and not xs : <NEWLINE> <INDENT> yield None , ys . pop ( ) <NEWLINE> <DEDENT> elif key ( xs [ - 1 ] ) == key ( ys [ - 1 ] ) : <NEWLINE> <INDENT> yield xs . pop ( ) , ys . pop ( ) <NEWLINE> <DEDENT> elif key ( xs [ - 1 ] ) > key ( ys [ - 1 ] ) : <NEWLINE> <INDENT> yield xs . pop ( ) , None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> yield None , ys . pop ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1855dbe99adb4a168943ce8f34fd9cf1": {
        "code_string": "curate.remove_old_genomes(genbank_mirror, assembly_summary, local_genomes, logger)\n     sync.sync_latest_genomes(genbank_mirror, assembly_summary, new_genomes, logger)\n     curate.unzip_genbank_mirror(genbank_mirror)\n     rename.rename(genbank_mirror, assembly_summary)\n",
        "code_toks_joined": "curate . remove_old_genomes ( genbank_mirror , assembly_summary , local_genomes , logger ) <NEWLINE> <INDENT> sync . sync_latest_genomes ( genbank_mirror , assembly_summary , new_genomes , logger ) <NEWLINE> curate . unzip_genbank_mirror ( genbank_mirror ) <NEWLINE> rename . rename ( genbank_mirror , assembly_summary ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b459d1f87211463d81e8d5c220252831": {
        "code_string": "sigmahat = VonMises_std(self.theta, self.phi)\n         self.suggested_bandwidth = 1.06*sigmahat*len(weights)**-0.2\n",
        "code_toks_joined": "sigmahat = VonMises_std ( self . theta , self . phi ) <NEWLINE> <INDENT> self . suggested_bandwidth = 1.06 * sigmahat * len ( weights ) ** - 0.2 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c6eb162cffa8492c92cdb7c096e1ef76": {
        "code_string": "class ModelServer(object):\n     def __init__(self, config, source_path, resource_path, model_conf, pipe):\n         self._pipe = pipe\n         self._resources = ModelServer.acquire_resources(config, model_conf, resource_path)\n         self._model_class = ModelServer.import_model(model_conf['path'], source_path)\n         self._model = self._model_class(self._resources, config=config)\n",
        "code_toks_joined": "class ModelServer ( object ) : <NEWLINE> <INDENT> def __init__ ( self , config , source_path , resource_path , model_conf , pipe ) : <NEWLINE> <INDENT> self . _pipe = pipe <NEWLINE> self . _resources = ModelServer . acquire_resources ( config , model_conf , resource_path ) <NEWLINE> self . _model_class = ModelServer . import_model ( model_conf [ <STRING> ] , source_path ) <NEWLINE> self . _model = self . _model_class ( self . _resources , config = config ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'path'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a0e50bb95b974c318baf67fdbf644ad3": {
        "code_string": "if not self.app.mount.mountUp:\n             return False\n         if self.ui.checkRefracNone.isChecked():\n             return False\n         if self.ui.checkRefracNoTrack.isChecked():\n             if self.app.mount.obsSite.status != 0:\n                 return False\n         temp, press = self.app.environment.getFilteredRefracParams()\n         if temp is None or press is None:\n             return False\n         suc = self.app.mount.obsSite.setRefractionParam(temperature=temp,\n                                                         pressure=press)\n         if not suc:\n             self.app.message.emit('Cannot perform refraction update', 2)\n             return False\n         return True\n",
        "code_toks_joined": "if not self . app . mount . mountUp : <NEWLINE> <INDENT> return False <NEWLINE> if self . ui . checkRefracNone . isChecked ( ) : <NEWLINE> return False <NEWLINE> if self . ui . checkRefracNoTrack . isChecked ( ) : <NEWLINE> if self . app . mount . obsSite . status != 0 : <NEWLINE> <INDENT> return False <NEWLINE> temp , press = self . app . environment . getFilteredRefracParams ( ) <NEWLINE> if temp is None or press is None : <NEWLINE> <DEDENT> return False <NEWLINE> suc = self . app . mount . obsSite . setRefractionParam ( temperature = temp , <NEWLINE> <INDENT> pressure = press ) <NEWLINE> if not suc : <NEWLINE> <DEDENT> self . app . message . emit ( <STRING> , 2 ) <NEWLINE> return False <NEWLINE> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Cannot perform refraction update'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8076d2acbdbd43788e12165efb36d271": {
        "code_string": "self.data[element] = value\n",
        "code_toks_joined": "self . data [ element ] = value <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "95e4f80c90e44dcc8d3898d52b6bb3fc": {
        "code_string": "def isNoneOrEmpty(obj):\n     '''\n     \u5224\u65ad\u5217\u8868\u6216\u8005\u5b57\u7b26\u4e32\u662f\u5426\u4e3a\u7a7a\n     '''\n     if obj is None:\n         return False\n     if isinstance(obj, list):\n         return len(obj) <= 0\n     if isinstance(obj, str):\n         return len(obj.replace(' ', '')) <= 0\n     return True",
        "code_toks_joined": "def isNoneOrEmpty ( obj ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if obj is None : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> if isinstance ( obj , list ) : <NEWLINE> <INDENT> return len ( obj ) <= 0 <NEWLINE> <DEDENT> if isinstance ( obj , str ) : <NEWLINE> <INDENT> return len ( obj . replace ( <STRING> , <STRING> ) ) <= 0 <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'''\n     \u5224\u65ad\u5217\u8868\u6216\u8005\u5b57\u7b26\u4e32\u662f\u5426\u4e3a\u7a7a\n     '''",
                "' '",
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d142986b67eb48f1ac24ae67f153f801": {
        "code_string": "@cli.command()\n @click.pass_context\n def upgrade(ctx):\n     local_packages = ctx.obj.get('packages', {}).items()\n     remote_packages = map(get_package, map(operator.itemgetter(0), local_packages))\n     remote_package_versions = map(lambda x: x and x['Version'], remote_packages)\n     for (n, lv), rv in zip(local_packages, remote_package_versions):\n         if rv is None:\n             print('failed to find {} on server'.format(n))\n             continue\n         elif lv != rv:\n             print('found new version {} (old: {}) for {}, do you want to upgrade? [Y/n] '.format(lv, rv, n), end='')\n             sys.stdout.flush()\n             answer = sys.stdin.readline().strip()\n             if answer in ('', ' ', 'Y', 'y'):\n                 print('upgrading {} from {} to {}'.format(n, lv, rv))\n                 install(ctx, n)\n             else:\n                 print('skipping upgrading {} from {} to {}'.format(n, lv, rv))\n         else:\n             print('{} is up to date'.format(n))\n     print('done')\n",
        "code_toks_joined": "@ cli . command ( ) <NEWLINE> <INDENT> @ click . pass_context <NEWLINE> def upgrade ( ctx ) : <NEWLINE> <INDENT> local_packages = ctx . obj . get ( <STRING> , { } ) . items ( ) <NEWLINE> remote_packages = map ( get_package , map ( operator . itemgetter ( 0 ) , local_packages ) ) <NEWLINE> remote_package_versions = map ( lambda x : x and x [ <STRING> ] , remote_packages ) <NEWLINE> for ( n , lv ) , rv in zip ( local_packages , remote_package_versions ) : <NEWLINE> <INDENT> if rv is None : <NEWLINE> <INDENT> print ( <STRING> . format ( n ) ) <NEWLINE> continue <NEWLINE> <DEDENT> elif lv != rv : <NEWLINE> <INDENT> print ( <STRING> . format ( lv , rv , n ) , end = <STRING> ) <NEWLINE> sys . stdout . flush ( ) <NEWLINE> answer = sys . stdin . readline ( ) . strip ( ) <NEWLINE> if answer in ( <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> print ( <STRING> . format ( n , lv , rv ) ) <NEWLINE> install ( ctx , n ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> . format ( n , lv , rv ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> . format ( n ) ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'packages'",
                "'Version'",
                "'failed to find {} on server'",
                "'found new version {} (old: {}) for {}, do you want to upgrade? [Y/n] '",
                "''",
                "''",
                "' '",
                "'Y'",
                "'y'",
                "'upgrading {} from {} to {}'",
                "'skipping upgrading {} from {} to {}'",
                "'{} is up to date'",
                "'done'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2de13d72240f49718f466f4727f6d827": {
        "code_string": "@B.dispatch(LowRank, LowRank)\n def matmul(a, b, tr_a=False, tr_b=False):\n     _assert_composable(a, b, tr_a=tr_a, tr_b=tr_b)\n     a = _tr(a, tr_a)\n     b = _tr(b, tr_b)\n     middle = B.matmul(a.right, b.left, tr_a=True)\n     rows, cols = B.shape(middle)\n     if rows < cols:\n         return LowRank(B.matmul(a.left, middle), b.right)\n     else:\n         return LowRank(a.left, B.matmul(b.right, middle, tr_b=True))\n",
        "code_toks_joined": "@ B . dispatch ( LowRank , LowRank ) <NEWLINE> <INDENT> def matmul ( a , b , tr_a = False , tr_b = False ) : <NEWLINE> <INDENT> _assert_composable ( a , b , tr_a = tr_a , tr_b = tr_b ) <NEWLINE> a = _tr ( a , tr_a ) <NEWLINE> b = _tr ( b , tr_b ) <NEWLINE> middle = B . matmul ( a . right , b . left , tr_a = True ) <NEWLINE> rows , cols = B . shape ( middle ) <NEWLINE> if rows < cols : <NEWLINE> <INDENT> return LowRank ( B . matmul ( a . left , middle ) , b . right ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return LowRank ( a . left , B . matmul ( b . right , middle , tr_b = True ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "73f7a4f25efd4507b9d69cdeec093d61": {
        "code_string": "@classmethod\n     def download(\n         cls,\n         id_package_list: Optional[List[int]] = None,\n         guide_list: Optional[List[str]] = None,\n         ivoy_guide_list: Optional[List[str]] = None,\n     ):\n         if not any([id_package_list, guide_list, ivoy_guide_list]):\n             raise ValueError(\"Any kind of id's are needed for waybills.\")\n         json_data = cls._download_json(\n             id_package_list, guide_list, ivoy_guide_list\n         )\n         resp = cls._client.post(cls._endpoint, json=json_data)\n         return cls(\n             id_package_list=id_package_list,\n             guide_list=ivoy_guide_list,\n             ivoy_guide_list=ivoy_guide_list,\n             byte_content=resp.content,\n         )\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def download ( <NEWLINE> <INDENT> cls , <NEWLINE> id_package_list : Optional [ List [ int ] ] = None , <NEWLINE> guide_list : Optional [ List [ str ] ] = None , <NEWLINE> ivoy_guide_list : Optional [ List [ str ] ] = None , <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> if not any ( [ id_package_list , guide_list , ivoy_guide_list ] ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> json_data = cls . _download_json ( <NEWLINE> <INDENT> id_package_list , guide_list , ivoy_guide_list <NEWLINE> <DEDENT> ) <NEWLINE> resp = cls . _client . post ( cls . _endpoint , json = json_data ) <NEWLINE> return cls ( <NEWLINE> <INDENT> id_package_list = id_package_list , <NEWLINE> guide_list = ivoy_guide_list , <NEWLINE> ivoy_guide_list = ivoy_guide_list , <NEWLINE> byte_content = resp . content , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Any kind of id's are needed for waybills.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9d22aa4ec72b4934bae9c4b43e0f366e": {
        "code_string": "sccoord = np.atleast_2d(sccoord)\n     if sccoord.shape[0] == 3:\n         T = np.zeros((6, 6))\n         T[:3, :3] = sccoord\n         T[3:, 3:] = sccoord\n         return sccoord\n",
        "code_toks_joined": "sccoord = np . atleast_2d ( sccoord ) <NEWLINE> <INDENT> if sccoord . shape [ 0 ] == 3 : <NEWLINE> <INDENT> T = np . zeros ( ( 6 , 6 ) ) <NEWLINE> T [ : 3 , : 3 ] = sccoord <NEWLINE> T [ 3 : , 3 : ] = sccoord <NEWLINE> return sccoord <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8a52378ddfac46e3b6131c5ee91e96c0": {
        "code_string": "if datasets[self.dataset_name].get(self.split_name) is None:\n             logger.error('can\\'t find split={} for dataset={}'.format(self.dataset_name, self.split_name))\n             self.dataset_name = None\n             return\n",
        "code_toks_joined": "if datasets [ self . dataset_name ] . get ( self . split_name ) is None : <NEWLINE> <INDENT> logger . error ( <STRING> . format ( self . dataset_name , self . split_name ) ) <NEWLINE> self . dataset_name = None <NEWLINE> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'can\\'t find split={} for dataset={}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f940c251abc94352b09afbf3ff8950c8": {
        "code_string": "def orientation_with(self, point: Point) -> int:\n         return Angle(self.start, self.end, point).orientation\n",
        "code_toks_joined": "def orientation_with ( self , point : Point ) -> int : <NEWLINE> <INDENT> return Angle ( self . start , self . end , point ) . orientation <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "53c19a4745a54c79abaca5ce7d7f3822": {
        "code_string": "@given(strategies.points, strategies.points, strategies.points)\n def test_basic(vertex: Point,\n                first_ray_point: Point,\n                second_ray_point: Point) -> None:\n     angle = Angle(vertex, first_ray_point, second_ray_point)\n",
        "code_toks_joined": "@ given ( strategies . points , strategies . points , strategies . points ) <NEWLINE> <INDENT> def test_basic ( vertex : Point , <NEWLINE> <INDENT> first_ray_point : Point , <NEWLINE> second_ray_point : Point ) -> None : <NEWLINE> angle = Angle ( vertex , first_ray_point , second_ray_point ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5c1d61a5131f4a88a2f62eb1119a6df4": {
        "code_string": "cnx = dbmgr.get(db_file)\n     try:\n         with open(sql_file, 'r') as sf:\n             log.debug(chlogger, {\n                 \"name\"      : __name__,\n                 \"src\"       : resource_name,\n                 \"method\"    : \"insert_file\",\n                 \"db_file\"   : db_file,\n                 \"file_idx\"  : idx,\n                 \"sql_file\"  : sql_file,\n                 \"depth\"     : depth,\n                 \"dbmgr\"     : str(dbmgr),\n                 \"message\"   : \"started\",\n                 })\n             cnx.executescript(sf.read())\n             log.debug(chlogger, {\n                 \"name\"      : __name__,\n                 \"src\"       : resource_name,\n                 \"method\"    : \"insert_file\",\n                 \"db_file\"   : db_file,\n                 \"file_idx\"  : idx,\n                 \"sql_file\"  : sql_file,\n                 \"depth\"     : depth,\n                 \"dbmgr\"     : str(dbmgr),\n                 \"message\"   : \"completed\",\n                 })\n         return sql_file\n     except Exception as e:\n         log.error(chlogger, {\n             \"name\"      : __name__,\n             \"src\"       : resource_name,\n             \"method\"    : \"insert_file\",\n             \"file_idx\"  : idx,\n             \"db_file\"   : db_file,\n             \"sql_file\"  : sql_file,\n             \"depth\"     : depth,\n             \"dbmgr\"     : str(dbmgr),\n             \"ERROR\"     : \"insert sql_file failed\",\n             \"exception\": str(e),\n             })\n         insert_file(logger, resource_name, dbmgr, sql_dir, db_dir, sql_file, idx, depth+1, max_depth)\n",
        "code_toks_joined": "cnx = dbmgr . get ( db_file ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> with open ( sql_file , <STRING> ) as sf : <NEWLINE> <INDENT> log . debug ( chlogger , { <NEWLINE> <INDENT> <STRING> : __name__ , <NEWLINE> <STRING> : resource_name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : db_file , <NEWLINE> <STRING> : idx , <NEWLINE> <STRING> : sql_file , <NEWLINE> <STRING> : depth , <NEWLINE> <STRING> : str ( dbmgr ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> } ) <NEWLINE> <DEDENT> cnx . executescript ( sf . read ( ) ) <NEWLINE> log . debug ( chlogger , { <NEWLINE> <INDENT> <STRING> : __name__ , <NEWLINE> <STRING> : resource_name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : db_file , <NEWLINE> <STRING> : idx , <NEWLINE> <STRING> : sql_file , <NEWLINE> <STRING> : depth , <NEWLINE> <STRING> : str ( dbmgr ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> } ) <NEWLINE> <DEDENT> <DEDENT> return sql_file <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> log . error ( chlogger , { <NEWLINE> <INDENT> <STRING> : __name__ , <NEWLINE> <STRING> : resource_name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : idx , <NEWLINE> <STRING> : db_file , <NEWLINE> <STRING> : sql_file , <NEWLINE> <STRING> : depth , <NEWLINE> <STRING> : str ( dbmgr ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : str ( e ) , <NEWLINE> } ) <NEWLINE> <DEDENT> insert_file ( logger , resource_name , dbmgr , sql_dir , db_dir , sql_file , idx , depth + 1 , max_depth ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'r'",
                "\"name\"",
                "\"src\"",
                "\"method\"",
                "\"insert_file\"",
                "\"db_file\"",
                "\"file_idx\"",
                "\"sql_file\"",
                "\"depth\"",
                "\"dbmgr\"",
                "\"message\"",
                "\"started\"",
                "\"name\"",
                "\"src\"",
                "\"method\"",
                "\"insert_file\"",
                "\"db_file\"",
                "\"file_idx\"",
                "\"sql_file\"",
                "\"depth\"",
                "\"dbmgr\"",
                "\"message\"",
                "\"completed\"",
                "\"name\"",
                "\"src\"",
                "\"method\"",
                "\"insert_file\"",
                "\"file_idx\"",
                "\"db_file\"",
                "\"sql_file\"",
                "\"depth\"",
                "\"dbmgr\"",
                "\"ERROR\"",
                "\"insert sql_file failed\"",
                "\"exception\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "150fc4c3e5464ebe8b67054c3b3136c4": {
        "code_string": "@given(\n     binary(),\n     binary(),\n     binary(),\n     binary(),\n     binary(),\n )\n def test_build_regular_packet(iv,\n                               iv_hash,\n                               payload_hash,\n                               handshake_key,\n                               payload):\n     data = join_encode_data([iv,\n                              iv_hash,\n                              payload_hash,\n                              handshake_key,\n                              payload])\n     if (len(iv) == packets.IV_LEN and\n             len(iv_hash) == packets.HASH_LEN and\n             len(payload_hash) == packets.HASH_LEN and\n             not len(handshake_key) and\n             len(payload_hash)):\n         assert isinstance(packets.build_regular_packet(data),\n                           packets.RegularPacket)\n     else:\n         with pytest.raises(errors.MalformedPacketError):\n             packets.build_regular_packet(data)\n",
        "code_toks_joined": "@ given ( <NEWLINE> <INDENT> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> ) <NEWLINE> def test_build_regular_packet ( iv , <NEWLINE> <INDENT> iv_hash , <NEWLINE> payload_hash , <NEWLINE> handshake_key , <NEWLINE> payload ) : <NEWLINE> <DEDENT> data = join_encode_data ( [ iv , <NEWLINE> <INDENT> iv_hash , <NEWLINE> payload_hash , <NEWLINE> handshake_key , <NEWLINE> payload ] ) <NEWLINE> <DEDENT> if ( len ( iv ) == packets . IV_LEN and <NEWLINE> <INDENT> len ( iv_hash ) == packets . HASH_LEN and <NEWLINE> len ( payload_hash ) == packets . HASH_LEN and <NEWLINE> not len ( handshake_key ) and <NEWLINE> len ( payload_hash ) ) : <NEWLINE> assert isinstance ( packets . build_regular_packet ( data ) , <NEWLINE> <INDENT> packets . RegularPacket ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> with pytest . raises ( errors . MalformedPacketError ) : <NEWLINE> <INDENT> packets . build_regular_packet ( data ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "71bbec43a42747dd9f82981a7bf69ae9": {
        "code_string": "else:\n         pat = re.compile(text)\n         if text.search(state.student_result):\n             state.do_test(msg.format(text))\n",
        "code_toks_joined": "else : <NEWLINE> <INDENT> pat = re . compile ( text ) <NEWLINE> if text . search ( state . student_result ) : <NEWLINE> <INDENT> state . do_test ( msg . format ( text ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "474b3e1c82c44e759d72d2fef518d9b7": {
        "code_string": "def set_states(self, time):\n         if time > self.memory_dump.x[-1] - self.T:\n             fit = self.memory[0]\n         else:\n             fit = self.memory_dump\n",
        "code_toks_joined": "def set_states ( self , time ) : <NEWLINE> <INDENT> if time > self . memory_dump . x [ - 1 ] - self . T : <NEWLINE> <INDENT> fit = self . memory [ 0 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> fit = self . memory_dump <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f6063d392b44ffe87f1f6d94a0db26b": {
        "code_string": "if convergence:\n             vc = t_list.apply(pd.Series.value_counts)\n             vc = vc[target_id_type].dropna().sort_index()\n             count = vc.ix[target_id]#t_list[t_list[target_id_type]==target_id]\n         else:\n             vc = t_list.apply(pd.Series.value_counts)\n             vc = vc[source_id_type].dropna().sort_index()\n             count = vc.ix[source_id]#count = s_list[s_list[source_id_type]==source_id]\n",
        "code_toks_joined": "if convergence : <NEWLINE> <INDENT> vc = t_list . apply ( pd . Series . value_counts ) <NEWLINE> vc = vc [ target_id_type ] . dropna ( ) . sort_index ( ) <NEWLINE> count = vc . ix [ target_id ] <COMMENT> <NEWLINE> else : <NEWLINE> vc = t_list . apply ( pd . Series . value_counts ) <NEWLINE> vc = vc [ source_id_type ] . dropna ( ) . sort_index ( ) <NEWLINE> count = vc . ix [ source_id ] <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#t_list[t_list[target_id_type]==target_id]",
                "#count = s_list[s_list[source_id_type]==source_id]"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d28eb30b8a5140f9b1711f6f74d12d1c": {
        "code_string": "segpassivewidget = SegregationPassiveWidget(fir_widget,ctg.root_sec.cell(), other_cells,section_selected,ctg.mechanism_dict,gleak_var=gleak,eleak_var=eleak)\n     ctg.add_widget(window_index,column_index,widget)\n",
        "code_toks_joined": "segpassivewidget = SegregationPassiveWidget ( fir_widget , ctg . root_sec . cell ( ) , other_cells , section_selected , ctg . mechanism_dict , gleak_var = gleak , eleak_var = eleak ) <NEWLINE> <INDENT> ctg . add_widget ( window_index , column_index , widget ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c2a8ed835d3f4fb3bc10d5529932b125": {
        "code_string": "def as_dict(self):\n         d = super(GCECredentials, self).as_dict()\n         gce_creds = json.loads(self.credentials)\n         # Overwrite with super values in case gce_creds also has an id property\n         gce_creds.update(d)\n         return d\n",
        "code_toks_joined": "def as_dict ( self ) : <NEWLINE> <INDENT> d = super ( GCECredentials , self ) . as_dict ( ) <NEWLINE> gce_creds = json . loads ( self . credentials ) <NEWLINE> <COMMENT> <NL> gce_creds . update ( d ) <NEWLINE> return d <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Overwrite with super values in case gce_creds also has an id property"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "219bf9da96344785b129d879d4614a0b": {
        "code_string": "line = \" \".join(line_list)\n         try:\n             if self.settings[\"discord\"][\"plaintext_ping\"]:\n                 exp = re.compile(message.guild.me.display_name, re.IGNORECASE)\n                 line = exp.sub(line, \"#nick\")\n         except KeyError:\n             pass\n",
        "code_toks_joined": "line = <STRING> . join ( line_list ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if self . settings [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> exp = re . compile ( message . guild . me . display_name , re . IGNORECASE ) <NEWLINE> line = exp . sub ( line , <STRING> ) <NEWLINE> <DEDENT> <DEDENT> except KeyError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\" \"",
                "\"discord\"",
                "\"plaintext_ping\"",
                "\"#nick\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2864d0aa0e5b40ff92d68448b3b76776": {
        "code_string": "performance=weights,\n",
        "code_toks_joined": "performance = weights , <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "897c843b8c1048fb95bf4b7ef079533b": {
        "code_string": "def connect(self, host=None, port=None):\n         if port is not None:\n             self._host = host\n",
        "code_toks_joined": "def connect ( self , host = None , port = None ) : <NEWLINE> <INDENT> if port is not None : <NEWLINE> <INDENT> self . _host = host <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7a16573acb3440b089e9b6386f4ec59d": {
        "code_string": "if cond_val is True:\n                     comp_insts = []\n                     last_comp = None\n                     for comp_time in cond_comp.comp_times:\n                         comp_time_inst = comp_time.eval(context, last_comp)\n                         comp_insts.append(comp_time_inst)\n                         last_comp = comp_time_inst\n                     setattr(self, f'ph_{phase}', comp_insts)\n                     break\n",
        "code_toks_joined": "if cond_val is True : <NEWLINE> <INDENT> comp_insts = [ ] <NEWLINE> last_comp = None <NEWLINE> for comp_time in cond_comp . comp_times : <NEWLINE> <INDENT> comp_time_inst = comp_time . eval ( context , last_comp ) <NEWLINE> comp_insts . append ( comp_time_inst ) <NEWLINE> last_comp = comp_time_inst <NEWLINE> <DEDENT> setattr ( self , <STRING> , comp_insts ) <NEWLINE> break <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f'ph_{phase}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ec3d6a53b8414529bd9e15bc05e47573": {
        "code_string": "# Create cond template which will be used to instantiate concrete\n             # expanded table rows.\n             for idx, var_exp in enumerate(cond_spec):\n                 if type(var_exp) is LoopExpression:\n                     loops.append(var_exp.exp.resolve())\n                     loops_idx.append(idx)\n                     cond_template.append(None)\n                 else:\n                     # If not a loop expression then cycle if list-like\n                     # expression (e.g. List or Range) or repeat otherwise\n                     var_exp_resolved = var_exp.resolve()\n                     if isinstance(var_exp_resolved, Sequence):\n                         if has_loops or len(var_exp_resolved) < max_len:\n                             cond_template.append(cycle(var_exp_resolved))\n                         else:\n                             cond_template.append(iter(var_exp_resolved))\n                     else:\n                         if has_sequences:\n                             cond_template.append(repeat(var_exp))\n                         else:\n                             cond_template.append(iter([var_exp]))\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> for idx , var_exp in enumerate ( cond_spec ) : <NEWLINE> <INDENT> if type ( var_exp ) is LoopExpression : <NEWLINE> <INDENT> loops . append ( var_exp . exp . resolve ( ) ) <NEWLINE> loops_idx . append ( idx ) <NEWLINE> cond_template . append ( None ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> var_exp_resolved = var_exp . resolve ( ) <NEWLINE> if isinstance ( var_exp_resolved , Sequence ) : <NEWLINE> <INDENT> if has_loops or len ( var_exp_resolved ) < max_len : <NEWLINE> <INDENT> cond_template . append ( cycle ( var_exp_resolved ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cond_template . append ( iter ( var_exp_resolved ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if has_sequences : <NEWLINE> <INDENT> cond_template . append ( repeat ( var_exp ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cond_template . append ( iter ( [ var_exp ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Create cond template which will be used to instantiate concrete",
                "# expanded table rows.",
                "# If not a loop expression then cycle if list-like",
                "# expression (e.g. List or Range) or repeat otherwise"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7f7065da131641a8a5d4a460bae0917d": {
        "code_string": "#\n             # Select the good type of input\n             #\n             if DEEP_ENTRY_INPUT in metric_args:\n                 if DEEP_ENTRY_LABEL in metric_args:\n                     if DEEP_ENTRY_ADDITIONAL_DATA in metric_args:\n                         temp_metric_result = metric_method(inputs, outputs, labels, additional_data)\n                     else:\n                         temp_metric_result = metric_method(outputs, labels)\n                 else:\n                     if DEEP_ENTRY_ADDITIONAL_DATA in metric_args:\n                         temp_metric_result = metric_method(inputs, outputs, additional_data)\n                     else:\n                         temp_metric_result = metric_method(inputs, outputs)\n             else:\n                 if DEEP_ENTRY_LABEL in metric_args:\n                     if DEEP_ENTRY_ADDITIONAL_DATA in metric_args:\n                         temp_metric_result = metric_method(outputs, labels, additional_data)\n                     else:\n                         temp_metric_result = metric_method(outputs, labels)\n                 else:\n                     if DEEP_ENTRY_ADDITIONAL_DATA in metric_args:\n                         temp_metric_result = metric_method(outputs, additional_data)\n                     else:\n                         temp_metric_result = metric_method(outputs)\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if DEEP_ENTRY_INPUT in metric_args : <NEWLINE> <INDENT> if DEEP_ENTRY_LABEL in metric_args : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , outputs , labels , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , labels ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , outputs , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , outputs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if DEEP_ENTRY_LABEL in metric_args : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , labels , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , labels ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#",
                "# Select the good type of input",
                "#"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "da740f2aadef4abf8ae95ddfea317e46": {
        "code_string": "def __contains__(self, point):\n         return self.low <= point <= self.high\n",
        "code_toks_joined": "def __contains__ ( self , point ) : <NEWLINE> <INDENT> return self . low <= point <= self . high <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fa54e865599a40b082d2613dea00fa0c": {
        "code_string": "if genes is None and isinstance(X, (pd.SparseDataFrame,\n                                             sparse.spmatrix)) and \\\n                 np.prod(X.shape) > 5000 * 20000:\n             warnings.warn(\"Returning imputed values for all genes on a ({} x \"\n                           \"{}) matrix will require approximately {}GB of \"\n                           \"memory. Suppress this warning with \"\n                           \"`genes='all_genes'`\".format(\n                               X.shape[0], X.shape[1],\n                               np.prod(X.shape) * 8 / (1024**3)),\n                           UserWarning)\n         if genes == \"all_genes\":\n             genes = None\n         elif genes is not None:\n             genes = np.array([genes]).flatten()\n             if not issubclass(genes.dtype.type, numbers.Integral):\n                 # gene names\n                 if not isinstance(X, pd.DataFrame):\n                     raise ValueError(\n                         \"Non-integer gene names only valid with pd.DataFrame \"\n                         \"input. X is a {}, genes = {}\".format(type(X).__name__,\n                                                               genes))\n                 if not np.all(np.isin(genes, X.columns)):\n                     warnings.warn(\"genes {} missing from input data\".format(\n                         genes[~np.isin(genes, X.columns)]))\n                 genes = np.argwhere(np.isin(genes, X.columns)).reshape(-1)\n",
        "code_toks_joined": "if genes is None and isinstance ( X , ( pd . SparseDataFrame , <NEWLINE> <INDENT> sparse . spmatrix ) ) and np . prod ( X . shape ) > 5000 * 20000 : <NEWLINE> warnings . warn ( <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> . format ( <NEWLINE> X . shape [ 0 ] , X . shape [ 1 ] , <NEWLINE> np . prod ( X . shape ) * 8 / ( 1024 ** 3 ) ) , <NEWLINE> UserWarning ) <NEWLINE> if genes == <STRING> : <NEWLINE> genes = None <NEWLINE> elif genes is not None : <NEWLINE> genes = np . array ( [ genes ] ) . flatten ( ) <NEWLINE> if not issubclass ( genes . dtype . type , numbers . Integral ) : <NEWLINE> <COMMENT> <NL> if not isinstance ( X , pd . DataFrame ) : <NEWLINE> raise ValueError ( <NEWLINE> <STRING> <NEWLINE> <STRING> . format ( type ( X ) . __name__ , <NEWLINE> <INDENT> genes ) ) <NEWLINE> if not np . all ( np . isin ( genes , X . columns ) ) : <NEWLINE> warnings . warn ( <STRING> . format ( <NEWLINE> genes [ ~ np . isin ( genes , X . columns ) ] ) ) <NEWLINE> genes = np . argwhere ( np . isin ( genes , X . columns ) ) . reshape ( - 1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Returning imputed values for all genes on a ({} x \"",
                "\"{}) matrix will require approximately {}GB of \"",
                "\"memory. Suppress this warning with \"",
                "\"`genes='all_genes'`\"",
                "\"all_genes\"",
                "\"Non-integer gene names only valid with pd.DataFrame \"",
                "\"input. X is a {}, genes = {}\"",
                "\"genes {} missing from input data\""
            ],
            "<COMMENT>": [
                "# gene names"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "553aef3ad8eb460a907015a3a9f60751": {
        "code_string": "if not getattr(django_settings, name):\n                 self.assertNotContains(\n                     response, '%s' % getattr(django_settings, name)\n                 )\n",
        "code_toks_joined": "if not getattr ( django_settings , name ) : <NEWLINE> <INDENT> self . assertNotContains ( <NEWLINE> <INDENT> response , <STRING> % getattr ( django_settings , name ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "862d4cd0783148d4b450a5b5090831de": {
        "code_string": "for j, low in enumerate(all_splits[:-1]):\n             high = all_splits[j+1]\n             pff, tsj, weights, pmap, pixels_sub = do_lc(tpf,\n                         ts,(low,high),sub,order,maxiter=101,split_times=None,w_init=w_init,random_init=random_init,\n                 thresh=thresh,minflux=minflux,consensus=consensus,analytic=analytic,sigclip=sigclip,verbose=verbose)\n             tss.append(tsj)\n             if low is None:\n                 cad1.append(ts['cadence'][0])\n             else:\n                 cad1.append(ts['cadence'][low])\n             if high is None:\n                 cad1.append(ts['cadence'][-1])\n             else:\n                 cad2.append(ts['cadence'][high])\n             sat.append(pmap[\"sat_pixels\"])\n             weightmap.append(pmap[\"weightmap\"])\n         wmap = {\n         \"initial_cadence\": cad1,\n         \"final_cadence\": cad2,\n         \"sat_pixels\": sat,\n         \"weightmap\": weightmap\n         }\n         ts = stitch(tss)\n",
        "code_toks_joined": "for j , low in enumerate ( all_splits [ : - 1 ] ) : <NEWLINE> <INDENT> high = all_splits [ j + 1 ] <NEWLINE> pff , tsj , weights , pmap , pixels_sub = do_lc ( tpf , <NEWLINE> <INDENT> ts , ( low , high ) , sub , order , maxiter = 101 , split_times = None , w_init = w_init , random_init = random_init , <NEWLINE> thresh = thresh , minflux = minflux , consensus = consensus , analytic = analytic , sigclip = sigclip , verbose = verbose ) <NEWLINE> <DEDENT> tss . append ( tsj ) <NEWLINE> if low is None : <NEWLINE> <INDENT> cad1 . append ( ts [ <STRING> ] [ 0 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cad1 . append ( ts [ <STRING> ] [ low ] ) <NEWLINE> <DEDENT> if high is None : <NEWLINE> <INDENT> cad1 . append ( ts [ <STRING> ] [ - 1 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cad2 . append ( ts [ <STRING> ] [ high ] ) <NEWLINE> <DEDENT> sat . append ( pmap [ <STRING> ] ) <NEWLINE> weightmap . append ( pmap [ <STRING> ] ) <NEWLINE> wmap = { <NEWLINE> <STRING> : cad1 , <NEWLINE> <STRING> : cad2 , <NEWLINE> <STRING> : sat , <NEWLINE> <STRING> : weightmap <NEWLINE> } <NEWLINE> ts = stitch ( tss ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'cadence'",
                "'cadence'",
                "'cadence'",
                "'cadence'",
                "\"sat_pixels\"",
                "\"weightmap\"",
                "\"initial_cadence\"",
                "\"final_cadence\"",
                "\"sat_pixels\"",
                "\"weightmap\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1c521b0754e94ba1895ce1a248925630": {
        "code_string": "if self.deltax > 0:\n",
        "code_toks_joined": "if self . deltax > 0 : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "17d4c77429e04f2d963a8e595ff6c5db": {
        "code_string": "# Loop through predictions and find matching ground truth boxes\n     match_count = 0\n     pred_match = -1 * np.ones([pred_boxes.shape[0]])\n     gt_match = -1 * np.ones([gt_boxes.shape[0]])\n     for i in range(len(pred_boxes)):\n         # Find best matching ground truth box\n         # 1. Sort matches by score\n         sorted_ixs = np.argsort(overlaps[i])[::-1]\n         # 2. Remove low scores\n         low_score_idx = np.where(overlaps[i, sorted_ixs] < score_threshold)[0]\n         if low_score_idx.size > 0:\n             sorted_ixs = sorted_ixs[:low_score_idx[0]]\n         # 3. Find the match\n         for j in sorted_ixs:\n             # If ground truth box is already matched, go to next one\n             if gt_match[j] > 0:\n                 continue\n             # If we reach IoU smaller than the threshold, end the loop\n             iou = overlaps[i, j]\n             if iou < iou_threshold:\n                 break\n             # Do we have a match?\n             if pred_class_ids[i] == gt_class_ids[j]:\n                 match_count += 1\n                 gt_match[j] = i\n                 pred_match[i] = j\n                 break\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> match_count = 0 <NEWLINE> pred_match = - 1 * np . ones ( [ pred_boxes . shape [ 0 ] ] ) <NEWLINE> gt_match = - 1 * np . ones ( [ gt_boxes . shape [ 0 ] ] ) <NEWLINE> for i in range ( len ( pred_boxes ) ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> sorted_ixs = np . argsort ( overlaps [ i ] ) [ : : - 1 ] <NEWLINE> <COMMENT> <NL> low_score_idx = np . where ( overlaps [ i , sorted_ixs ] < score_threshold ) [ 0 ] <NEWLINE> if low_score_idx . size > 0 : <NEWLINE> <INDENT> sorted_ixs = sorted_ixs [ : low_score_idx [ 0 ] ] <NEWLINE> <COMMENT> <NL> <DEDENT> for j in sorted_ixs : <NEWLINE> <COMMENT> <NL> <INDENT> if gt_match [ j ] > 0 : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> iou = overlaps [ i , j ] <NEWLINE> if iou < iou_threshold : <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> if pred_class_ids [ i ] == gt_class_ids [ j ] : <NEWLINE> <INDENT> match_count += 1 <NEWLINE> gt_match [ j ] = i <NEWLINE> pred_match [ i ] = j <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Loop through predictions and find matching ground truth boxes",
                "# Find best matching ground truth box",
                "# 1. Sort matches by score",
                "# 2. Remove low scores",
                "# 3. Find the match",
                "# If ground truth box is already matched, go to next one",
                "# If we reach IoU smaller than the threshold, end the loop",
                "# Do we have a match?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ff1cc0c1567c48a7bb226b7a04300309": {
        "code_string": "def read_xattrs_from_disk(self,myfile):\n \t\tid_table = _Xattr_table()\n \t\tif self.sBlk.xattr_id_table_start == SQUASHFS_INVALID_BLK:\n \t\t\treturn SQUASHFS_INVALID_BLK\n \t\tmyfile.seek(self.offset + self.sBlk.xattr_id_table_start)\n \t\tid_table.read(myfile)\n \t\tids = id_table.xattr_ids\n \t\txattr_table_start = id_table.xattr_table_start\n \t\tindex_bytes = SQUASHFS_XATTR_BLOCK_BYTES(ids)\n \t\tindexes = SQUASHFS_XATTR_BLOCKS(ids)\n \t\tindex = []\n \t\tfor r in range(0,ids):\n \t\t\tindex.append( self.makeInteger(myfile,SQUASHFS_XATTR_BLOCK_BYTES(1)) )\n \t\tbytes = SQUASHFS_XATTR_BYTES(ids)\n \t\txattr_ids = {}\n \t\tfor i in range(0,indexes):\n \t\t\tblock,next,byte_count = self.read_block(myfile,index[i])\n \t\t\tcur_idx = (i * SQUASHFS_METADATA_SIZE)/16\n \t\t\tofs = 0\n \t\t\twhile ofs<len(block):\n \t\t\t\txattr_id = _Xattr_id()\n \t\t\t\txattr_id.fill(block,ofs)\n \t\t\t\txattr_ids[cur_idx]=xattr_id\n \t\t\t\tcur_idx+=1\n \t\t\t\tofs+=16\n \t\tstart = xattr_table_start\n \t\tend = index[0]\n \t\txattr_values = {}\n \t\ti = 0\n \t\twhile start<end:\n \t\t\tself.hash_table[start]= (i * SQUASHFS_METADATA_SIZE)\n \t\t\tblock,start,byte_count = self.read_block(myfile,start)\n \t\t\tfor i in range(len(block),SQUASHFS_METADATA_SIZE):\n \t\t\t\tblock+=b'\\x00'\n \t\t\tself.xattrs += block\t\n \t\t\ti+=1\n \t\treturn ids\n",
        "code_toks_joined": "def read_xattrs_from_disk ( self , myfile ) : <NEWLINE> <INDENT> id_table = _Xattr_table ( ) <NEWLINE> if self . sBlk . xattr_id_table_start == SQUASHFS_INVALID_BLK : <NEWLINE> <INDENT> return SQUASHFS_INVALID_BLK <NEWLINE> <DEDENT> myfile . seek ( self . offset + self . sBlk . xattr_id_table_start ) <NEWLINE> id_table . read ( myfile ) <NEWLINE> ids = id_table . xattr_ids <NEWLINE> xattr_table_start = id_table . xattr_table_start <NEWLINE> index_bytes = SQUASHFS_XATTR_BLOCK_BYTES ( ids ) <NEWLINE> indexes = SQUASHFS_XATTR_BLOCKS ( ids ) <NEWLINE> index = [ ] <NEWLINE> for r in range ( 0 , ids ) : <NEWLINE> <INDENT> index . append ( self . makeInteger ( myfile , SQUASHFS_XATTR_BLOCK_BYTES ( 1 ) ) ) <NEWLINE> <DEDENT> bytes = SQUASHFS_XATTR_BYTES ( ids ) <NEWLINE> xattr_ids = { } <NEWLINE> for i in range ( 0 , indexes ) : <NEWLINE> <INDENT> block , next , byte_count = self . read_block ( myfile , index [ i ] ) <NEWLINE> cur_idx = ( i * SQUASHFS_METADATA_SIZE ) / 16 <NEWLINE> ofs = 0 <NEWLINE> while ofs < len ( block ) : <NEWLINE> <INDENT> xattr_id = _Xattr_id ( ) <NEWLINE> xattr_id . fill ( block , ofs ) <NEWLINE> xattr_ids [ cur_idx ] = xattr_id <NEWLINE> cur_idx += 1 <NEWLINE> ofs += 16 <NEWLINE> <DEDENT> <DEDENT> start = xattr_table_start <NEWLINE> end = index [ 0 ] <NEWLINE> xattr_values = { } <NEWLINE> i = 0 <NEWLINE> while start < end : <NEWLINE> <INDENT> self . hash_table [ start ] = ( i * SQUASHFS_METADATA_SIZE ) <NEWLINE> block , start , byte_count = self . read_block ( myfile , start ) <NEWLINE> for i in range ( len ( block ) , SQUASHFS_METADATA_SIZE ) : <NEWLINE> <INDENT> block += <STRING> <NEWLINE> <DEDENT> self . xattrs += block <NEWLINE> i += 1 <NEWLINE> <DEDENT> return ids <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "b'\\x00'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1cac6307e0a848f0af6ec13e20953c3f": {
        "code_string": "def process_batch(key, batch_info, iv):\n     \"\"\"Loops through items in a batch and processes them.\"\"\"\n     batch_info = pickle.loads(decrypt(key, batch_info, iv.decode('hex')))\n     if valid_batch(batch_info):\n         items = serializers.deserialize('json', batch_info['items'])\n         success = True\n         for item in items:\n             item.save()\n             if isinstance(Version, item.object):\n                 version = item.object\n                 if version.type == VERSION_DELETE:\n                     if version.object:\n                         version.object.delete()\n                 else:\n                     item.object.revert()\n     return success\n",
        "code_toks_joined": "def process_batch ( key , batch_info , iv ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> batch_info = pickle . loads ( decrypt ( key , batch_info , iv . decode ( <STRING> ) ) ) <NEWLINE> if valid_batch ( batch_info ) : <NEWLINE> <INDENT> items = serializers . deserialize ( <STRING> , batch_info [ <STRING> ] ) <NEWLINE> success = True <NEWLINE> for item in items : <NEWLINE> <INDENT> item . save ( ) <NEWLINE> if isinstance ( Version , item . object ) : <NEWLINE> <INDENT> version = item . object <NEWLINE> if version . type == VERSION_DELETE : <NEWLINE> <INDENT> if version . object : <NEWLINE> <INDENT> version . object . delete ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> item . object . revert ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return success <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Loops through items in a batch and processes them.\"\"\"",
                "'hex'",
                "'json'",
                "'items'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eca13f9867334aaaba8b017d2729d019": {
        "code_string": "def _clean_class(self, user_class, string_only=None):\n         if type(user_class) != str:\n             return \"\"\n         if \":\" in user_class:\n             if self._vocab and (user_class in self._vocab):\n                 user_class = self._vocab[user_class]\n             else:\n                 user_class = \"scm:\" + user_class\n         if string_only:\n             return user_class\n         else:\n             return self._expand_variable(user_class)\n",
        "code_toks_joined": "def _clean_class ( self , user_class , string_only = None ) : <NEWLINE> <INDENT> if type ( user_class ) != str : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> if <STRING> in user_class : <NEWLINE> <INDENT> if self . _vocab and ( user_class in self . _vocab ) : <NEWLINE> <INDENT> user_class = self . _vocab [ user_class ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> user_class = <STRING> + user_class <NEWLINE> <DEDENT> <DEDENT> if string_only : <NEWLINE> <INDENT> return user_class <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . _expand_variable ( user_class ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"",
                "\":\"",
                "\"scm:\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dde122b21be84d49b42f7240e08a56e2": {
        "code_string": "def full_build_is_required():\n     full_build = _load_state(_FULL_BUILD)\n     if not full_build:\n         return True\n",
        "code_toks_joined": "def full_build_is_required ( ) : <NEWLINE> <INDENT> full_build = _load_state ( _FULL_BUILD ) <NEWLINE> if not full_build : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "37b9a2244ea74e05ab56206395fb458e": {
        "code_string": "for item in org_tree:\n         if isinstance(item, tuple):\n             value = values[item]\n             if value < 0:\n                 raise ValueError('Negative value {} for {}'.format(value, item))\n",
        "code_toks_joined": "for item in org_tree : <NEWLINE> <INDENT> if isinstance ( item , tuple ) : <NEWLINE> <INDENT> value = values [ item ] <NEWLINE> if value < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( value , item ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Negative value {} for {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "333edac1921d40c3a7441f2d26cae533": {
        "code_string": "def available_places_validator(self, context, request, value):\n         registration = context.getParentNode().getParentNode()\n         period = registration.get(request.form.get('period'))\n         if int(value) < period.available_places:\n             return False\n         return _('Not enough places left in the selected period')\n",
        "code_toks_joined": "def available_places_validator ( self , context , request , value ) : <NEWLINE> <INDENT> registration = context . getParentNode ( ) . getParentNode ( ) <NEWLINE> period = registration . get ( request . form . get ( <STRING> ) ) <NEWLINE> if int ( value ) < period . available_places : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return _ ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'period'",
                "'Not enough places left in the selected period'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5667d145e98649588323d74a6ba286c2": {
        "code_string": "def test_invalid_password(self):\n         connection = Connection(\n             credentials=Credentials(username='root', password='rooted'),\n             base_url='http://localhost:9876'\n         )\n         self.assertRaises(connection.login, LoginError)\n",
        "code_toks_joined": "def test_invalid_password ( self ) : <NEWLINE> <INDENT> connection = Connection ( <NEWLINE> <INDENT> credentials = Credentials ( username = <STRING> , password = <STRING> ) , <NEWLINE> base_url = <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> self . assertRaises ( connection . login , LoginError ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'root'",
                "'rooted'",
                "'http://localhost:9876'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5da2e4b774be480f95c7f81286898809": {
        "code_string": "def terminal(self, state):\n         \"\"\"Return a batched tensor indicating which states are terminal.\"\"\"\n         return (state[..., 0] < -self.params.x_threshold) & (\n             state[..., 0] > self.params.x_threshold\n         )\n",
        "code_toks_joined": "def terminal ( self , state ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return ( state [ ... , 0 ] < - self . params . x_threshold ) & ( <NEWLINE> <INDENT> state [ ... , 0 ] > self . params . x_threshold <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return a batched tensor indicating which states are terminal.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fc0411eae32143b4a9361a07e697f861": {
        "code_string": "if args.destroy:\n             docker_image_remove(ctx.image_id)\n",
        "code_toks_joined": "if args . destroy : <NEWLINE> <INDENT> docker_image_remove ( ctx . image_id ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e501fb7a921d413c84ebb1e96e71b99c": {
        "code_string": "self.logger = get_logger()\n         if mail_dir is None or isinstance(mail_dir, str):\n             msg = \"Invalid mail_dir variable: {}\".format(mail_dir)\n             self.logger.error(msg)\n             raise SystemExit(msg)\n         elif not os.path.isdir(mail_dir):\n             try:\n                 os.mkdir(mail_dir)\n             except IOError as io_error:\n                 self.logger.error(str(io_error))\n                 raise\n         self.mail_dir = mail_dir\n         self.print_messages = print_messages is True\n         self.logger.info(\"Starting nullsmtp started on {:s}:{:d}\".format(*localaddr))\n         self.logger.info(\"Mail Directory: {:s}\".format(self.mail_dir))\n",
        "code_toks_joined": "self . logger = get_logger ( ) <NEWLINE> <INDENT> if mail_dir is None or isinstance ( mail_dir , str ) : <NEWLINE> <INDENT> msg = <STRING> . format ( mail_dir ) <NEWLINE> self . logger . error ( msg ) <NEWLINE> raise SystemExit ( msg ) <NEWLINE> <DEDENT> elif not os . path . isdir ( mail_dir ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> os . mkdir ( mail_dir ) <NEWLINE> <DEDENT> except IOError as io_error : <NEWLINE> <INDENT> self . logger . error ( str ( io_error ) ) <NEWLINE> raise <NEWLINE> <DEDENT> <DEDENT> self . mail_dir = mail_dir <NEWLINE> self . print_messages = print_messages is True <NEWLINE> self . logger . info ( <STRING> . format ( * localaddr ) ) <NEWLINE> self . logger . info ( <STRING> . format ( self . mail_dir ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Invalid mail_dir variable: {}\"",
                "\"Starting nullsmtp started on {:s}:{:d}\"",
                "\"Mail Directory: {:s}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3bee789307ac4b028bad11e8b95a5af2": {
        "code_string": "def wrapper():\n         while True:\n             try:\n                 return fn()\n             except AssertionError:\n                 if time.time() < timeout:\n                     raise\n     return wrapper\n",
        "code_toks_joined": "def wrapper ( ) : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> return fn ( ) <NEWLINE> <DEDENT> except AssertionError : <NEWLINE> <INDENT> if time . time ( ) < timeout : <NEWLINE> <INDENT> raise <NEWLINE> return wrapper <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "39ffd63412464183a3b008f3a9e19cdd": {
        "code_string": "if status_code != 502:\n             print('HTTP Request failed - status %(status_code)s' % locals())\n             print(url)\n             self.csvDicts = []\n             return self.csvDicts\n",
        "code_toks_joined": "if status_code != 502 : <NEWLINE> <INDENT> print ( <STRING> % locals ( ) ) <NEWLINE> print ( url ) <NEWLINE> self . csvDicts = [ ] <NEWLINE> return self . csvDicts <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'HTTP Request failed - status %(status_code)s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ce1609be04c5451e88685161cfb51952": {
        "code_string": "def getLinks(self, outwardOnly=False):\n         if getattr(self, 'links', None) is None:\n             return self.youtrack.getLinks(self.id, outwardOnly)\n         else:\n             return [l for l in self.links if l.source != self.id or not outwardOnly]\n",
        "code_toks_joined": "def getLinks ( self , outwardOnly = False ) : <NEWLINE> <INDENT> if getattr ( self , <STRING> , None ) is None : <NEWLINE> <INDENT> return self . youtrack . getLinks ( self . id , outwardOnly ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return [ l for l in self . links if l . source != self . id or not outwardOnly ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'links'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8f35d6f67478404ab826723a2c77734c": {
        "code_string": "def _import_issues(self, project_id):\n         limit = 100\n         all_issues = self._get_issues(project_id)\n         while True:\n             issues = list(itertools.islice(all_issues, None, limit))\n             if not len(issues):\n                 break\n             self._target.importIssues(project_id, project_id + u' assignees',\n                 [self._to_yt_issue(issue, project_id) for issue in issues])\n             for issue in issues:\n                 issue_id = self._get_issue_id(issue)\n                 issue_attachments = self._get_attachments(issue_id)\n                 yt_issue_id = u'%s-%s' % (project_id, issue_id)\n                 self._import_attachments(yt_issue_id, issue_attachments)\n",
        "code_toks_joined": "def _import_issues ( self , project_id ) : <NEWLINE> <INDENT> limit = 100 <NEWLINE> all_issues = self . _get_issues ( project_id ) <NEWLINE> while True : <NEWLINE> <INDENT> issues = list ( itertools . islice ( all_issues , None , limit ) ) <NEWLINE> if not len ( issues ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> self . _target . importIssues ( project_id , project_id + <STRING> , <NEWLINE> <INDENT> [ self . _to_yt_issue ( issue , project_id ) for issue in issues ] ) <NEWLINE> <DEDENT> for issue in issues : <NEWLINE> <INDENT> issue_id = self . _get_issue_id ( issue ) <NEWLINE> issue_attachments = self . _get_attachments ( issue_id ) <NEWLINE> yt_issue_id = <STRING> % ( project_id , issue_id ) <NEWLINE> self . _import_attachments ( yt_issue_id , issue_attachments ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "u' assignees'",
                "u'%s-%s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3f5c9f5e20f2420d9996bd2272d04973": {
        "code_string": "with open('out.json', 'w') as fp:\n             json.dump(obj=files, fp=fp, indent=4, sort_keys=True)\n             request.outputs['outputs'].file = fp.name\n         return response\n",
        "code_toks_joined": "with open ( <STRING> , <STRING> ) as fp : <NEWLINE> <INDENT> json . dump ( obj = files , fp = fp , indent = 4 , sort_keys = True ) <NEWLINE> request . outputs [ <STRING> ] . file = fp . name <NEWLINE> return response <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'out.json'",
                "'w'",
                "'outputs'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ebd231b382e143909515bd09e24aceff": {
        "code_string": "cleaned_img1d = geometry_converter.image_2d_to_1d(reference_img, fits_metadata_dict['cam_id'])\n                         hillas_params_2_cleaned_img = get_hillas_parameters(geom1d, cleaned_img1d, HILLAS_IMPLEMENTATION)    # GEOM\n",
        "code_toks_joined": "cleaned_img1d = geometry_converter . image_2d_to_1d ( reference_img , fits_metadata_dict [ <STRING> ] ) <NEWLINE> <INDENT> hillas_params_2_cleaned_img = get_hillas_parameters ( geom1d , cleaned_img1d , HILLAS_IMPLEMENTATION ) <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'cam_id'"
            ],
            "<COMMENT>": [
                "# GEOM"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e0dec6835502442d92740b939e490921": {
        "code_string": "p = Page(b)\n     q = Question(b, 'Where do you shop?', qtype='free', var='shop')\n",
        "code_toks_joined": "p = Page ( b ) <NEWLINE> <INDENT> q = Question ( b , <STRING> , qtype = <STRING> , var = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Where do you shop?'",
                "'free'",
                "'shop'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5530983f191e44409b1a823c15cf0a16": {
        "code_string": "def truncate_recent(max_records):\n     \"\"\"\n     Count the number of records in the recent visitors table, and cut it down\n     to ``max_records``. This doesn't lock in between doing the count and the\n     delete, and solves the resulting race condition by erring on the side of\n     deleting too few rows (leaving too many records behind). So, this function\n     should not be counted on to make the table a particular size, just to keep\n     it relatively around that size.\n     \"\"\"\n     t = recent_visitors_table\n     q = select([t.c.last_timestamp]).\\\n             order_by(t.c.last_timestamp.desc()).\\\n             limit(1).\\\n             offset(max_records)\n     delete_before = q.scalar()\n     if delete_before:\n         q = t.delete().where(t.c.last_timestamp >= delete_before)\n         meta.Session.execute(q)\n",
        "code_toks_joined": "def truncate_recent ( max_records ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> t = recent_visitors_table <NEWLINE> q = select ( [ t . c . last_timestamp ] ) . order_by ( t . c . last_timestamp . desc ( ) ) . limit ( 1 ) . offset ( max_records ) <NEWLINE> delete_before = q . scalar ( ) <NEWLINE> if delete_before : <NEWLINE> <INDENT> q = t . delete ( ) . where ( t . c . last_timestamp >= delete_before ) <NEWLINE> meta . Session . execute ( q ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Count the number of records in the recent visitors table, and cut it down\n     to ``max_records``. This doesn't lock in between doing the count and the\n     delete, and solves the resulting race condition by erring on the side of\n     deleting too few rows (leaving too many records behind). So, this function\n     should not be counted on to make the table a particular size, just to keep\n     it relatively around that size.\n     \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "74eee96015c44e95b2584c5f557146b4": {
        "code_string": "@classmethod\n     def get_parse_trade_url(cls, trade_url):\n         regex = re.compile(r'^https?://steamcommunity\\.com/tradeoffer/new/\\?partner=(\\d+)&token=([a-zA-Z0-9_-]+)$')\n         match = regex.match(trade_url)\n         if match:\n             return None\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def get_parse_trade_url ( cls , trade_url ) : <NEWLINE> <INDENT> regex = re . compile ( <STRING> ) <NEWLINE> match = regex . match ( trade_url ) <NEWLINE> if match : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "r'^https?://steamcommunity\\.com/tradeoffer/new/\\?partner=(\\d+)&token=([a-zA-Z0-9_-]+)$'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fc5a47316f0c4be0aa52c1e6332ee487": {
        "code_string": "def is_display_small():\n     \"\"\"Return True if screen is smaller than 1300x850.\"\"\"\n     size = wx.GetDisplaySize()\n     if size is not None:\n         w, h = size\n         return w < 1300 and h < 850\n     return False\n",
        "code_toks_joined": "def is_display_small ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> size = wx . GetDisplaySize ( ) <NEWLINE> if size is not None : <NEWLINE> <INDENT> w , h = size <NEWLINE> return w < 1300 and h < 850 <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return True if screen is smaller than 1300x850.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "06d45b28fd2941288c0ee122aafe978f": {
        "code_string": "def parse_escape_markers(self):\n         \"\"\"Parses self.string for any CSI escape sequences or other\n         non-printable characters.\"\"\"\n         sequence_start = None\n         self.escape_markers = []\n         for index, char in enumerate(self.string):\n             # Mark the stop of an escape sequence\n             if sequence_start is None and char in string.letters:\n                 self.escape_markers.append(EscapeMarker(sequence_start, index))\n                 sequence_start = None  # Reset start sequence \n",
        "code_toks_joined": "def parse_escape_markers ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sequence_start = None <NEWLINE> self . escape_markers = [ ] <NEWLINE> for index , char in enumerate ( self . string ) : <NEWLINE> <COMMENT> <NL> <INDENT> if sequence_start is None and char in string . letters : <NEWLINE> <INDENT> self . escape_markers . append ( EscapeMarker ( sequence_start , index ) ) <NEWLINE> sequence_start = None <COMMENT> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Parses self.string for any CSI escape sequences or other\n         non-printable characters.\"\"\""
            ],
            "<COMMENT>": [
                "# Mark the stop of an escape sequence",
                "# Reset start sequence "
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1e2ff1e9a37f4369bb053ff3ecff4299": {
        "code_string": "if isinstance(stmt, str):\n         return stmt, tuple(pos_args) if pos_args is None else ()\n     else:\n         compiled = stmt.compile(dialect=_d)\n         params = compiled.construct_params(named_args)\n         return compiled.string, tuple(params[p] for p in compiled.positiontup)\n",
        "code_toks_joined": "if isinstance ( stmt , str ) : <NEWLINE> <INDENT> return stmt , tuple ( pos_args ) if pos_args is None else ( ) <NEWLINE> else : <NEWLINE> compiled = stmt . compile ( dialect = _d ) <NEWLINE> params = compiled . construct_params ( named_args ) <NEWLINE> return compiled . string , tuple ( params [ p ] for p in compiled . positiontup ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "68f9529ab2ba42ed9046868ab9118bcb": {
        "code_string": "def update(self, obj, **kwargs):\n         for h in self._pheap:\n             if h[1] == obj:\n                 self._pheap.remove(obj)\n                 break\n         heapq.heappush(self._pheap, (kwargs['cost'], obj))\n",
        "code_toks_joined": "def update ( self , obj , ** kwargs ) : <NEWLINE> <INDENT> for h in self . _pheap : <NEWLINE> <INDENT> if h [ 1 ] == obj : <NEWLINE> <INDENT> self . _pheap . remove ( obj ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> heapq . heappush ( self . _pheap , ( kwargs [ <STRING> ] , obj ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'cost'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9019c7685cf1458c8a66f97634d34896": {
        "code_string": "### Convert field as list of models\n         elif isinstance(field_name, list) and len(field_value) > 0:\n             if isinstance(field_value[0], Model):\n                 model_dict[serialized_name] = [model_converter(vi)\n                                                for vi in field_value]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> elif isinstance ( field_name , list ) and len ( field_value ) > 0 : <NEWLINE> <INDENT> if isinstance ( field_value [ 0 ] , Model ) : <NEWLINE> <INDENT> model_dict [ serialized_name ] = [ model_converter ( vi ) <NEWLINE> <INDENT> for vi in field_value ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "### Convert field as list of models"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5df657bc961149bd99857386638a483f": {
        "code_string": "def request(self, method):\n         m_type = method.m_type\n         auth_ = method.auth\n         url = self.__get_url(method)\n         if m_type == 'GET':\n             assert method.body is None, 'For GET method body must be empty'\n             if self.proxies is None:\n                 r = requests.get(url=url, params=method.params, headers=method.headers, auth=auth_)\n             else:\n                 r = requests.get(url=url, params=method.params, headers=method.headers, proxies=self.proxies, auth=auth_)\n         elif m_type == 'FILE':\n             assert method.files is None, 'For FILE attribute file must not be empty'\n             if self.proxies is not None:\n                 r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_,\n                                   files=method.files)\n             else:\n                 r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers,\n                                   proxies=self.proxies, auth=auth_, files=method.files)\n         elif m_type == 'POST':\n             if self.proxies is None:\n                 r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n             else:\n                 r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers,\n                                   proxies=self.proxies, auth=auth_)\n         elif m_type == 'DELETE':\n             if self.proxies is None:\n                 r = requests.delete(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n             else:\n                 r = requests.delete(url=url, params=method.params, data=method.body, headers=method.headers,\n                                     proxies=self.proxies, auth=auth_)\n         elif m_type == 'PATCH':\n             if self.proxies is None:\n                 r = requests.patch(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n             else:\n                 r = requests.patch(url=url, params=method.params, data=method.body, headers=method.headers,\n                                     proxies=self.proxies, auth=auth_)\n         elif m_type == 'PUT':\n             if self.proxies is None:\n                 r = requests.put(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n             else:\n                 r = requests.put(url=url, params=method.params, data=method.body, headers=method.headers,\n                                     proxies=self.proxies, auth=auth_)\n         else:\n             raise Exception(\"\\nnot implemented method request: %s\" % method.m_type)\n         try:\n             if r is None or len(r.content) == 0:\n                 return method.response_process({})\n             return method.response_process(r.json(), r.status_code)\n         except Exception as e:\n             logging.info(f'not a json response: {e}')\n             return method.response_process({}, r.status_code)\n",
        "code_toks_joined": "def request ( self , method ) : <NEWLINE> <INDENT> m_type = method . m_type <NEWLINE> auth_ = method . auth <NEWLINE> url = self . __get_url ( method ) <NEWLINE> if m_type == <STRING> : <NEWLINE> <INDENT> assert method . body is None , <STRING> <NEWLINE> if self . proxies is None : <NEWLINE> <INDENT> r = requests . get ( url = url , params = method . params , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . get ( url = url , params = method . params , headers = method . headers , proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> assert method . files is None , <STRING> <NEWLINE> if self . proxies is not None : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ , <NEWLINE> <INDENT> files = method . files ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ , files = method . files ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . delete ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . delete ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . patch ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . patch ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . put ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . put ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> % method . m_type ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> if r is None or len ( r . content ) == 0 : <NEWLINE> <INDENT> return method . response_process ( { } ) <NEWLINE> <DEDENT> return method . response_process ( r . json ( ) , r . status_code ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> logging . info ( <STRING> ) <NEWLINE> return method . response_process ( { } , r . status_code ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'GET'",
                "'For GET method body must be empty'",
                "'FILE'",
                "'For FILE attribute file must not be empty'",
                "'POST'",
                "'DELETE'",
                "'PATCH'",
                "'PUT'",
                "\"\\nnot implemented method request: %s\"",
                "f'not a json response: {e}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0f3d1d3833c24024beb98614a62034b2": {
        "code_string": "@classmethod\n     def _parse_ymd(cls, period):\n         # can even parse strings like '-1B-2Y-4Q+5M' but also '0B', '-1Y2M3D' as well.\n         def _parse(p, letter):\n             if p.find(letter) > 0:\n                 s, p = p.split(letter, 1)\n                 s = s[1:] if s.startswith('+') else s\n                 sgn, s = (-1, s[1:]) if s.startswith('-') else (1, s)\n                 if not s.isdigit():\n                     raise ValueError(\"Unable to parse %s in %s as %s\" % (s, p, cls.__name__))\n                 return sgn * int(s), p\n             return 0, p\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def _parse_ymd ( cls , period ) : <NEWLINE> <COMMENT> <NL> <INDENT> def _parse ( p , letter ) : <NEWLINE> <INDENT> if p . find ( letter ) > 0 : <NEWLINE> <INDENT> s , p = p . split ( letter , 1 ) <NEWLINE> s = s [ 1 : ] if s . startswith ( <STRING> ) else s <NEWLINE> sgn , s = ( - 1 , s [ 1 : ] ) if s . startswith ( <STRING> ) else ( 1 , s ) <NEWLINE> if not s . isdigit ( ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> % ( s , p , cls . __name__ ) ) <NEWLINE> <DEDENT> return sgn * int ( s ) , p <NEWLINE> <DEDENT> return 0 , p <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# can even parse strings like '-1B-2Y-4Q+5M' but also '0B', '-1Y2M3D' as well."
            ],
            "<STRING>": [
                "'+'",
                "'-'",
                "\"Unable to parse %s in %s as %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2f55e7630d0c43838478029bcd26faf0": {
        "code_string": "def is_not_regex_match(self, pattern: str) -> StringValidator:\n         \"\"\"\n         Checks whether the given value does not match the supplied `pattern`. An exception is thrown otherwise.\n         \"\"\"\n         if not RegexHelper.is_match(pattern, self.value):\n             raise ArgumentPatternError(\n                 f'The argument `{self.argument_name}` should match the pattern `{pattern}`',\n                 self.value,\n                 self.argument_name,\n                 pattern\n             )\n",
        "code_toks_joined": "def is_not_regex_match ( self , pattern : str ) -> StringValidator : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not RegexHelper . is_match ( pattern , self . value ) : <NEWLINE> <INDENT> raise ArgumentPatternError ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> self . value , <NEWLINE> self . argument_name , <NEWLINE> pattern <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Checks whether the given value does not match the supplied `pattern`. An exception is thrown otherwise.\n         \"\"\"",
                "f'The argument `{self.argument_name}` should match the pattern `{pattern}`'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "15c7ec5c5ccb4c2696fcf4cf1a5e9f50": {
        "code_string": "def authenticate(get_realm, realm_param=\"realm\"):\n     def partial(func):\n         @wraps(func)\n         def f(self, request, **kw):\n             realm = kw[realm_param]\n             del kw[realm_param]\n             realm = get_realm(self, realm)\n             username = None\n             password = None\n             if request.authorization:\n                 password = b64decode(request.authorization[1])\n                 username, password = password.decode(\"utf-8\").split(\":\", 1)\n             if username is None or realm.authenticate(username, password):\n                 raise HTTPUnauthorized(headers=[\n                     (\"WWW-Authenticate\",\n                         \"Basic realm=\\\"{}\\\"\".format(realm.description)),\n                 ])\n             return func(self, request, **kw)\n         return f\n     return partial\n",
        "code_toks_joined": "def authenticate ( get_realm , realm_param = <STRING> ) : <NEWLINE> <INDENT> def partial ( func ) : <NEWLINE> <INDENT> @ wraps ( func ) <NEWLINE> def f ( self , request , ** kw ) : <NEWLINE> <INDENT> realm = kw [ realm_param ] <NEWLINE> del kw [ realm_param ] <NEWLINE> realm = get_realm ( self , realm ) <NEWLINE> username = None <NEWLINE> password = None <NEWLINE> if request . authorization : <NEWLINE> <INDENT> password = b64decode ( request . authorization [ 1 ] ) <NEWLINE> username , password = password . decode ( <STRING> ) . split ( <STRING> , 1 ) <NEWLINE> <DEDENT> if username is None or realm . authenticate ( username , password ) : <NEWLINE> <INDENT> raise HTTPUnauthorized ( headers = [ <NEWLINE> <INDENT> ( <STRING> , <NEWLINE> <INDENT> <STRING> . format ( realm . description ) ) , <NEWLINE> <DEDENT> <DEDENT> ] ) <NEWLINE> <DEDENT> return func ( self , request , ** kw ) <NEWLINE> <DEDENT> return f <NEWLINE> <DEDENT> return partial <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"realm\"",
                "\"utf-8\"",
                "\":\"",
                "\"WWW-Authenticate\"",
                "\"Basic realm=\\\"{}\\\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a2614a491ff74b7da4ffbba53eb86d96": {
        "code_string": "l = line.split()\n     self.channel = \"\"\n     self.verb = \"\"\n     ind = 0\n     privmsg_index = 0\n     for e in l:\n       ind+=1\n       if e == \"PRIVMSG\":\n         privmsg_index = ind\n       if e.startswith(\"#\"):\n         self.channel = e\n         break\n     for v in l:\n       if v in [\"JOIN\",\"PART\",\"QUIT\",\"NICK\",\"KICK\",\"PRIVMSG\",\"TOPIC\", \"NOTICE\", \"PING\", \"PONG\", \"MODE\"]:\n         self.verb = v\n         break\n     # channel is unset if it does not begin with #\n     if self.verb == \"PRIVMSG\" and len(self.channel):\n       self.is_pm = True\n     for s in self.subscribers:\n       try:\n         s.handle(self)\n       except AttributeError:\n         pass\n",
        "code_toks_joined": "l = line . split ( ) <NEWLINE> <INDENT> self . channel = <STRING> <NEWLINE> self . verb = <STRING> <NEWLINE> ind = 0 <NEWLINE> privmsg_index = 0 <NEWLINE> for e in l : <NEWLINE> <INDENT> ind += 1 <NEWLINE> if e == <STRING> : <NEWLINE> <INDENT> privmsg_index = ind <NEWLINE> <DEDENT> if e . startswith ( <STRING> ) : <NEWLINE> <INDENT> self . channel = e <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> for v in l : <NEWLINE> <INDENT> if v in [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] : <NEWLINE> <INDENT> self . verb = v <NEWLINE> break <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if self . verb == <STRING> and len ( self . channel ) : <NEWLINE> <INDENT> self . is_pm = True <NEWLINE> <DEDENT> for s in self . subscribers : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> s . handle ( self ) <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"",
                "\"\"",
                "\"PRIVMSG\"",
                "\"#\"",
                "\"JOIN\"",
                "\"PART\"",
                "\"QUIT\"",
                "\"NICK\"",
                "\"KICK\"",
                "\"PRIVMSG\"",
                "\"TOPIC\"",
                "\"NOTICE\"",
                "\"PING\"",
                "\"PONG\"",
                "\"MODE\"",
                "\"PRIVMSG\""
            ],
            "<COMMENT>": [
                "# channel is unset if it does not begin with #"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "26fcce857f3c4ec194ab09624d96f702": {
        "code_string": "def cross(self, other):     # Corss product\n         return vector(self.y*other.z - self.z*other.y, self.z*other.x - self.x*other.z, self.x*other.y - self.y-other.x)\n",
        "code_toks_joined": "def cross ( self , other ) : <COMMENT> <NEWLINE> <INDENT> return vector ( self . y * other . z - self . z * other . y , self . z * other . x - self . x * other . z , self . x * other . y - self . y - other . x ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Corss product"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "880af4206fa24ad49e065e6fb915f9a2": {
        "code_string": "def load_salts(f):\n   if hasattr(f, 'read'):\n     return json.loads(rot13(f.read()))\n   else:\n     with open(f) as true_f:\n       return load_salts(true_f)\n def dump_salts(f, salts):\n   payload = rot13(json.dumps(salts, sort_keys=True, indent='  '))\n   if hasattr(f, 'write'):\n     f.write(payload)\n   else:\n     with open(f, 'w') as true_f:\n       f.write(payload)\n",
        "code_toks_joined": "def load_salts ( f ) : <NEWLINE> <INDENT> if hasattr ( f , <STRING> ) : <NEWLINE> <INDENT> return json . loads ( rot13 ( f . read ( ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( f ) as true_f : <NEWLINE> <INDENT> return load_salts ( true_f ) <NEWLINE> def dump_salts ( f , salts ) : <NEWLINE> <DEDENT> <DEDENT> payload = rot13 ( json . dumps ( salts , sort_keys = True , indent = <STRING> ) ) <NEWLINE> if hasattr ( f , <STRING> ) : <NEWLINE> <INDENT> f . write ( payload ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( f , <STRING> ) as true_f : <NEWLINE> <INDENT> f . write ( payload ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'read'",
                "'  '",
                "'write'",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e7b105932d3340fca66bd0c56027095f": {
        "code_string": "def _createReservationErrback(self, error, function_name, uuid):\n         LOGGER.error(\"Unable to create reservation for %s:%s, %s.\\n\" % (function_name, uuid, error))\n         return uuid",
        "code_toks_joined": "def _createReservationErrback ( self , error , function_name , uuid ) : <NEWLINE> <INDENT> LOGGER . error ( <STRING> % ( function_name , uuid , error ) ) <NEWLINE> return uuid <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Unable to create reservation for %s:%s, %s.\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f6b209fe9e614e459fcdaeb8a4a44463": {
        "code_string": "def callimpl(self):\n     # The truth table options are {AND, OR, XOR}.\n     # Other functions are negations of these, the 2 constants, or not symmetric.\n     # XOR sounds just like noise so it can't be that.\n     # AND and OR have the same frequency spectrum so either is good.\n     # We use OR as downstream it will prefer envelope shape over zero:\n     noiseflag = self.noiseflagreg.value\n     if not self.toneflagreg.value:\n       self.blockbuf.copybuf(self.tone(self.block))\n       if not noiseflag:\n         self.blockbuf.orbuf(self.noise(self.block))\n     elif noiseflag:\n       self.blockbuf.copybuf(self.noise(self.block))\n     else:\n       self.blockbuf.fill(0)\n",
        "code_toks_joined": "def callimpl ( self ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> noiseflag = self . noiseflagreg . value <NEWLINE> if not self . toneflagreg . value : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . tone ( self . block ) ) <NEWLINE> if not noiseflag : <NEWLINE> <INDENT> self . blockbuf . orbuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> <DEDENT> elif noiseflag : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . blockbuf . fill ( 0 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# The truth table options are {AND, OR, XOR}.",
                "# Other functions are negations of these, the 2 constants, or not symmetric.",
                "# XOR sounds just like noise so it can't be that.",
                "# AND and OR have the same frequency spectrum so either is good.",
                "# We use OR as downstream it will prefer envelope shape over zero:"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3e354c2ba9ca4c52b77fe6e3008d90f0": {
        "code_string": "diff = untilTime - fromTime\n   for archive in header['archives']:\n     if archive['retention'] >= diff:\n       break\n",
        "code_toks_joined": "diff = untilTime - fromTime <NEWLINE> <INDENT> for archive in header [ <STRING> ] : <NEWLINE> <INDENT> if archive [ <STRING> ] >= diff : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'archives'",
                "'retention'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4c63f3fb6e1a41b282ed64fd3ddbf1ee": {
        "code_string": "def merge(new_values, default_values):\n     nd = {}\n     for key, value in default_values.items():\n         nv = new_values.get(key, None)\n         if isinstance(value, dict) and isinstance(nv, dict):\n             nd[key] = merge(value, nv)\n         else:\n             if nv is None:\n                 nd[key] = value\n             else:\n                 nd[key] = nv\n     for key, value in new_values.items():\n         if key not in default_values:\n             nd[key] = value\n     return nd\n",
        "code_toks_joined": "def merge ( new_values , default_values ) : <NEWLINE> <INDENT> nd = { } <NEWLINE> for key , value in default_values . items ( ) : <NEWLINE> <INDENT> nv = new_values . get ( key , None ) <NEWLINE> if isinstance ( value , dict ) and isinstance ( nv , dict ) : <NEWLINE> <INDENT> nd [ key ] = merge ( value , nv ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if nv is None : <NEWLINE> <INDENT> nd [ key ] = value <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> nd [ key ] = nv <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for key , value in new_values . items ( ) : <NEWLINE> <INDENT> if key not in default_values : <NEWLINE> <INDENT> nd [ key ] = value <NEWLINE> <DEDENT> <DEDENT> return nd <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "42cdd6013b1743e4852d5764e79c6b8b": {
        "code_string": "if may_charge:\n             pot = (min_per_interval / 60.0) * power\n             max_nrg = max_batt * max_soc\n             output_batt[i] = min(current_batt[i] + pot, max_nrg)\n",
        "code_toks_joined": "if may_charge : <NEWLINE> <INDENT> pot = ( min_per_interval / 60.0 ) * power <NEWLINE> max_nrg = max_batt * max_soc <NEWLINE> output_batt [ i ] = min ( current_batt [ i ] + pot , max_nrg ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "51bb159ea6334fc983607c1522232001": {
        "code_string": "if version is not None:\n         versions = itertools.ifilter(lambda k: k['version'] == version,\n                                      versions)\n     try:\n         metadata = sorted(versions, key=lambda x: x['version'])[0]\n         for url in metadata['urls']:\n             fname = url.split('/')[-1]\n             try:\n                 fobj = cStringIO.StringIO(\n                     _get_from_repo(\n                         repo_scheme,\n                         repo_url,\n                         url,\n                         stream=True,\n                         headers=headers,\n                     )\n                 )\n",
        "code_toks_joined": "if version is not None : <NEWLINE> <INDENT> versions = itertools . ifilter ( lambda k : k [ <STRING> ] == version , <NEWLINE> <INDENT> versions ) <NEWLINE> try : <NEWLINE> <DEDENT> metadata = sorted ( versions , key = lambda x : x [ <STRING> ] ) [ 0 ] <NEWLINE> for url in metadata [ <STRING> ] : <NEWLINE> <INDENT> fname = url . split ( <STRING> ) [ - 1 ] <NEWLINE> try : <NEWLINE> <INDENT> fobj = cStringIO . StringIO ( <NEWLINE> <INDENT> _get_from_repo ( <NEWLINE> <INDENT> repo_scheme , <NEWLINE> repo_url , <NEWLINE> url , <NEWLINE> stream = True , <NEWLINE> headers = headers , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'version'",
                "'version'",
                "'urls'",
                "'/'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2c2b6f51ded6432abf2c845cba530bd3": {
        "code_string": "if version is not None:\n         versions = itertools.ifilter(lambda k: k['version'] == version,\n                                      versions)\n     try:\n         metadata = sorted(versions, key=lambda x: list(map(int, x['version'].split('.'))))[-1]\n         for url in metadata['urls']:\n             fname = url.split('/')[-1]\n             try:\n                 fobj = cStringIO.StringIO(\n                     _get_from_repo(\n                         repo_scheme,\n                         repo_url,\n                         fname,\n                         stream=True,\n                         headers=headers,\n                     )\n                 )\n",
        "code_toks_joined": "if version is not None : <NEWLINE> <INDENT> versions = itertools . ifilter ( lambda k : k [ <STRING> ] == version , <NEWLINE> <INDENT> versions ) <NEWLINE> try : <NEWLINE> <DEDENT> metadata = sorted ( versions , key = lambda x : list ( map ( int , x [ <STRING> ] . split ( <STRING> ) ) ) ) [ - 1 ] <NEWLINE> for url in metadata [ <STRING> ] : <NEWLINE> <INDENT> fname = url . split ( <STRING> ) [ - 1 ] <NEWLINE> try : <NEWLINE> <INDENT> fobj = cStringIO . StringIO ( <NEWLINE> <INDENT> _get_from_repo ( <NEWLINE> <INDENT> repo_scheme , <NEWLINE> repo_url , <NEWLINE> fname , <NEWLINE> stream = True , <NEWLINE> headers = headers , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'version'",
                "'version'",
                "'.'",
                "'urls'",
                "'/'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f4d79f1b1a934719a32040ea0fc01b69": {
        "code_string": "if d_line_stream:\n         for lines in group_d_lines(d_line_stream):\n             # Format inferred\n             parts = slices(lines[0], [1,6,1,2,1,6])\n             raw_d_descriptor = parts[1]\n             d_descriptor_code = fxy2int(raw_d_descriptor)\n             n_elements = int(parts[3])\n             actual_elements = len(lines)\n             if n_elements != actual_elements:\n                 raise ValueError(\"Expected %d elements, found %d\" %(n_elements, actual_elements))\n             constituent_codes = []\n             for line in lines:\n                 l_parts = slices(line, [1,6,1,2,1,6])\n                 constituent_codes.append(fxy2int(parts[5]))\n",
        "code_toks_joined": "if d_line_stream : <NEWLINE> <INDENT> for lines in group_d_lines ( d_line_stream ) : <NEWLINE> <COMMENT> <NL> <INDENT> parts = slices ( lines [ 0 ] , [ 1 , 6 , 1 , 2 , 1 , 6 ] ) <NEWLINE> raw_d_descriptor = parts [ 1 ] <NEWLINE> d_descriptor_code = fxy2int ( raw_d_descriptor ) <NEWLINE> n_elements = int ( parts [ 3 ] ) <NEWLINE> actual_elements = len ( lines ) <NEWLINE> if n_elements != actual_elements : <NEWLINE> <INDENT> raise ValueError ( <STRING> % ( n_elements , actual_elements ) ) <NEWLINE> <DEDENT> constituent_codes = [ ] <NEWLINE> for line in lines : <NEWLINE> <INDENT> l_parts = slices ( line , [ 1 , 6 , 1 , 2 , 1 , 6 ] ) <NEWLINE> constituent_codes . append ( fxy2int ( parts [ 5 ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Format inferred"
            ],
            "<STRING>": [
                "\"Expected %d elements, found %d\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6291eedcbf84469983a48cf6374f7236": {
        "code_string": "num_clusters, num_taxa = state.leca_file.load_taxa( file_name )\n     MCMD.print( \"{} taxa loaded into {} clusters.\".format( num_clusters, num_taxa ) )\n",
        "code_toks_joined": "num_clusters , num_taxa = state . leca_file . load_taxa ( file_name ) <NEWLINE> <INDENT> MCMD . print ( <STRING> . format ( num_clusters , num_taxa ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{} taxa loaded into {} clusters.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f6d42713c8b6493696e012d32b074dcf": {
        "code_string": "# combination block # essentially ignore other blocks...\n         _, y3, self.x3 = sig.lsim(self.sys3, U=uVector, T=self.t, \n                                    X0=[self.r_x1[self.mirror.c_dp-1],self.Gen.r_Pm[self.mirror.c_dp-1]])\n         # Addition of damping\n         Pmech = y3 - dwVec*self.Dt # effectively removing the second block...\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> _ , y3 , self . x3 = sig . lsim ( self . sys3 , U = uVector , T = self . t , <NEWLINE> <INDENT> X0 = [ self . r_x1 [ self . mirror . c_dp - 1 ] , self . Gen . r_Pm [ self . mirror . c_dp - 1 ] ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> Pmech = y3 - dwVec * self . Dt <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# combination block # essentially ignore other blocks...",
                "# Addition of damping",
                "# effectively removing the second block..."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "26852f08abc1441fa0a5f86fce9e712c": {
        "code_string": "wmclass, minimized = get_wm_class_and_state(winid)\n             dock = disp.intern_atom('_NET_WM_WINDOW_TYPE_DOCK')\n             if dock in ewmh.getWmWindowType(win):\n                 continue\n",
        "code_toks_joined": "wmclass , minimized = get_wm_class_and_state ( winid ) <NEWLINE> <INDENT> dock = disp . intern_atom ( <STRING> ) <NEWLINE> if dock in ewmh . getWmWindowType ( win ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'_NET_WM_WINDOW_TYPE_DOCK'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f62ac361354d400c98fd298af87a2226": {
        "code_string": "async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n         if not self._debug:\n             await self.bugsnag_app(scope, send, receive)\n             return\n         await self.app(scope, receive, send)\n",
        "code_toks_joined": "async def __call__ ( self , scope : Scope , receive : Receive , send : Send ) -> None : <NEWLINE> <INDENT> if not self . _debug : <NEWLINE> <INDENT> await self . bugsnag_app ( scope , send , receive ) <NEWLINE> return <NEWLINE> <DEDENT> await self . app ( scope , receive , send ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f9246435e67b483eb50a8de53bc046ce": {
        "code_string": "if value_type == bool:\n             value = value.strip().lower() == 'true'\n         elif value_type == str:\n             if value.startswith('file://'):\n                 value_path = os.path.join(self.base_path, value[len('file://'):])\n                 with open(value_path, 'r') as value_file:\n                     value = value_file.read()\n                 if value_path.lower().endswith('.md'):\n                     try:\n                         import pypandoc\n                         value = pypandoc.convert_text(value_path, 'rst', format='md')\n                         value = value.replace(\"\\r\", \"\")\n                     except ImportError:\n                         print(\"Pandoc not found. Markdown to reStructuredText conversion failed.\")\n         elif value_type == list:\n             if value.startswith('file://'):\n                 value_path = os.path.join(self.base_path, value[len('file://'):])\n                 with open(value_path, 'r') as value_file:\n                     value = value_file.readlines()\n                     value = filter(lambda k: bool(k), value)\n                     value = list(map(lambda k: k.strip().replace('\\n', ''), value))\n             else:\n                 value = value.split(',')\n",
        "code_toks_joined": "if value_type == bool : <NEWLINE> <INDENT> value = value . strip ( ) . lower ( ) == <STRING> <NEWLINE> elif value_type == str : <NEWLINE> if value . startswith ( <STRING> ) : <NEWLINE> <INDENT> value_path = os . path . join ( self . base_path , value [ len ( <STRING> ) : ] ) <NEWLINE> with open ( value_path , <STRING> ) as value_file : <NEWLINE> <INDENT> value = value_file . read ( ) <NEWLINE> <DEDENT> if value_path . lower ( ) . endswith ( <STRING> ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> import pypandoc <NEWLINE> value = pypandoc . convert_text ( value_path , <STRING> , format = <STRING> ) <NEWLINE> value = value . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> elif value_type == list : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if value . startswith ( <STRING> ) : <NEWLINE> <INDENT> value_path = os . path . join ( self . base_path , value [ len ( <STRING> ) : ] ) <NEWLINE> with open ( value_path , <STRING> ) as value_file : <NEWLINE> <INDENT> value = value_file . readlines ( ) <NEWLINE> value = filter ( lambda k : bool ( k ) , value ) <NEWLINE> value = list ( map ( lambda k : k . strip ( ) . replace ( <STRING> , <STRING> ) , value ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> value = value . split ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'true'",
                "'file://'",
                "'file://'",
                "'r'",
                "'.md'",
                "'rst'",
                "'md'",
                "\"\\r\"",
                "\"\"",
                "\"Pandoc not found. Markdown to reStructuredText conversion failed.\"",
                "'file://'",
                "'file://'",
                "'r'",
                "'\\n'",
                "''",
                "','"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5145a03b9d344edd85a8e89c5c74b681": {
        "code_string": "subproxy_sn = device.get_serial_number()\n         subproxy_identifier = \"{}->{}\".format(identifier, subproxy_sn)\n         device_identifiers[subproxy_device] = subproxy_identifier\n",
        "code_toks_joined": "subproxy_sn = device . get_serial_number ( ) <NEWLINE> <INDENT> subproxy_identifier = <STRING> . format ( identifier , subproxy_sn ) <NEWLINE> device_identifiers [ subproxy_device ] = subproxy_identifier <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{}->{}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2463a75f0f894269b58fb103abe62d95": {
        "code_string": "if depc._use_globals:\n             reg_preproc = __PREPROC_REGISTER__[depc.root]\n             reg_algos = __ALGO_REGISTER__[depc.root]\n             print(' * regsitering %d global preproc funcs' % len(reg_preproc))\n             for args_, kwargs_ in reg_preproc:\n                 depc._register_prop(*args_, **kwargs_)\n             print(' * regsitering %d global algos ' % len(reg_algos))\n             for args_, kwargs_ in reg_preproc:\n                 depc._register_algo(*args_, **kwargs_)\n",
        "code_toks_joined": "if depc . _use_globals : <NEWLINE> <INDENT> reg_preproc = __PREPROC_REGISTER__ [ depc . root ] <NEWLINE> reg_algos = __ALGO_REGISTER__ [ depc . root ] <NEWLINE> print ( <STRING> % len ( reg_preproc ) ) <NEWLINE> for args_ , kwargs_ in reg_preproc : <NEWLINE> <INDENT> depc . _register_prop ( * args_ , ** kwargs_ ) <NEWLINE> <DEDENT> print ( <STRING> % len ( reg_algos ) ) <NEWLINE> for args_ , kwargs_ in reg_preproc : <NEWLINE> <INDENT> depc . _register_algo ( * args_ , ** kwargs_ ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "' * regsitering %d global preproc funcs'",
                "' * regsitering %d global algos '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7469d22df20c4b9695dd596e5dafdf42": {
        "code_string": "# ================================\n             # Merge into db with add_cleanly\n             # ================================\n             superkey_colnames_list = db.get_table_superkey_colnames(tablename)\n             try:\n                 superkey_paramxs_list = [\n                     [column_names_.index(str(superkey))\n                      for superkey in  superkey_colnames]\n                     for superkey_colnames in superkey_colnames_list\n                 ]\n             except Exception as ex:\n                 ut.printex(ex, keys=['column_names_', 'superkey_colnames_list'])\n                 raise\n             if len(superkey_colnames_list) > 1:\n                 # FIXME: Rectify duplicate code\n                 primary_superkey = db.get_metadata_val(\n                     tablename + '_primary_superkey', eval_=True, default=None)\n                 if primary_superkey is None:\n                     raise AssertionError(\n                         ('tablename=%r has multiple superkey_colnames_list=%r, '\n                          'but no primary superkey. '\n                          'A primary superkey is required') % (\n                              tablename, superkey_colnames_list))\n                 else:\n                     superkey_index = superkey_colnames_list.index(primary_superkey)\n                     superkey_paramx = superkey_paramxs_list[superkey_index]\n                     superkey_colnames = superkey_colnames_list[superkey_index]\n             elif len(superkey_colnames) == 1:\n                 superkey_paramx = superkey_paramxs_list[0]\n                 superkey_colnames = superkey_colnames_list[0]\n             else:\n                 superkey_paramx = superkey_paramxs_list[0]\n                 superkey_colnames = superkey_colnames_list[0]\n                 # def get_referenced_table():\n                 #     # TODO use foreign keys to infer this data instead of hacks\n                 #     pass\n                 # print('superkey_paramxs_list = %r' % (superkey_paramxs_list, ))\n                 # print('superkey_colnames_list = %r' % (superkey_colnames_list, ))\n                 # raise ValueError('Cannot merge %r' % (tablename, ))\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> superkey_colnames_list = db . get_table_superkey_colnames ( tablename ) <NEWLINE> try : <NEWLINE> <INDENT> superkey_paramxs_list = [ <NEWLINE> <INDENT> [ column_names_ . index ( str ( superkey ) ) <NEWLINE> <INDENT> for superkey in superkey_colnames ] <NEWLINE> <DEDENT> for superkey_colnames in superkey_colnames_list <NEWLINE> <DEDENT> ] <NEWLINE> <DEDENT> except Exception as ex : <NEWLINE> <INDENT> ut . printex ( ex , keys = [ <STRING> , <STRING> ] ) <NEWLINE> raise <NEWLINE> <DEDENT> if len ( superkey_colnames_list ) > 1 : <NEWLINE> <COMMENT> <NL> <INDENT> primary_superkey = db . get_metadata_val ( <NEWLINE> <INDENT> tablename + <STRING> , eval_ = True , default = None ) <NEWLINE> <DEDENT> if primary_superkey is None : <NEWLINE> <INDENT> raise AssertionError ( <NEWLINE> <INDENT> ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) % ( <NEWLINE> <INDENT> tablename , superkey_colnames_list ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> superkey_index = superkey_colnames_list . index ( primary_superkey ) <NEWLINE> superkey_paramx = superkey_paramxs_list [ superkey_index ] <NEWLINE> superkey_colnames = superkey_colnames_list [ superkey_index ] <NEWLINE> <DEDENT> <DEDENT> elif len ( superkey_colnames ) == 1 : <NEWLINE> <INDENT> superkey_paramx = superkey_paramxs_list [ 0 ] <NEWLINE> superkey_colnames = superkey_colnames_list [ 0 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> superkey_paramx = superkey_paramxs_list [ 0 ] <NEWLINE> superkey_colnames = superkey_colnames_list [ 0 ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ================================",
                "# Merge into db with add_cleanly",
                "# ================================",
                "# FIXME: Rectify duplicate code",
                "# def get_referenced_table():",
                "#     # TODO use foreign keys to infer this data instead of hacks",
                "#     pass",
                "# print('superkey_paramxs_list = %r' % (superkey_paramxs_list, ))",
                "# print('superkey_colnames_list = %r' % (superkey_colnames_list, ))",
                "# raise ValueError('Cannot merge %r' % (tablename, ))"
            ],
            "<STRING>": [
                "'column_names_'",
                "'superkey_colnames_list'",
                "'_primary_superkey'",
                "'tablename=%r has multiple superkey_colnames_list=%r, '",
                "'but no primary superkey. '",
                "'A primary superkey is required'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2da5debe2b4b4693b825f98d487dfd5e": {
        "code_string": "slot_value_node = ElementTree.SubElement(slot_node, 'slot:value', {'type': self.type})\n         if self.type == 'gdate':\n             ElementTree.SubElement(slot_value_node, 'gdate').text = datetime.strftime(self.value, '%Y-%m-%d')\n         elif self.type == 'string':\n             slot_value_node.text = self.value\n         elif self.type in ['integer', 'double']:\n             slot_value_node.text = str(self.value)\n         elif type(self.value) is list and self.value:\n             for sub_slot in self.value:\n                 slot_node.append(sub_slot.as_xml)\n         elif self.type == 'frame':\n             pass  # Empty frame element, just leave it\n         else:\n             raise NotImplementedError('Slot type {} is not implemented.'.format(self.type))\n",
        "code_toks_joined": "slot_value_node = ElementTree . SubElement ( slot_node , <STRING> , { <STRING> : self . type } ) <NEWLINE> <INDENT> if self . type == <STRING> : <NEWLINE> <INDENT> ElementTree . SubElement ( slot_value_node , <STRING> ) . text = datetime . strftime ( self . value , <STRING> ) <NEWLINE> <DEDENT> elif self . type == <STRING> : <NEWLINE> <INDENT> slot_value_node . text = self . value <NEWLINE> <DEDENT> elif self . type in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> slot_value_node . text = str ( self . value ) <NEWLINE> <DEDENT> elif type ( self . value ) is list and self . value : <NEWLINE> <INDENT> for sub_slot in self . value : <NEWLINE> <INDENT> slot_node . append ( sub_slot . as_xml ) <NEWLINE> <DEDENT> <DEDENT> elif self . type == <STRING> : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NotImplementedError ( <STRING> . format ( self . type ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'slot:value'",
                "'type'",
                "'gdate'",
                "'gdate'",
                "'%Y-%m-%d'",
                "'string'",
                "'integer'",
                "'double'",
                "'frame'",
                "'Slot type {} is not implemented.'"
            ],
            "<COMMENT>": [
                "# Empty frame element, just leave it"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1eaa2ea209304e2d9041e1fd54954366": {
        "code_string": "def grunt_config(self, config=None, key=None):\n         return grunt_conf(\n             config={} if config is None else config,\n             key=key if key is None else self.grunt_config_key)\n",
        "code_toks_joined": "def grunt_config ( self , config = None , key = None ) : <NEWLINE> <INDENT> return grunt_conf ( <NEWLINE> <INDENT> config = { } if config is None else config , <NEWLINE> key = key if key is None else self . grunt_config_key ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f442ee5f4a2d41b28ebdc35264c35c97": {
        "code_string": "if offset + length < len(self._leaf.data):\n             self._offset += length\n             return self._leaf.data[offset:offset + length]\n",
        "code_toks_joined": "if offset + length < len ( self . _leaf . data ) : <NEWLINE> <INDENT> self . _offset += length <NEWLINE> return self . _leaf . data [ offset : offset + length ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c74709f3305b479cb3ba66977ce3ffbf": {
        "code_string": "def bootstrap_repl(which_ns: str) -> types.ModuleType:\n     \"\"\"Bootstrap the REPL with a few useful vars and returned the bootstrapped\n     module so it's functions can be used by the REPL command.\"\"\"\n     repl_ns = runtime.Namespace.get_or_create(sym.symbol(REPL_NS))\n     ns = runtime.Namespace.get_or_create(sym.symbol(which_ns))\n     core_ns = runtime.Namespace.get(sym.symbol(runtime.CORE_NS))\n     assert core_ns is not None\n     ns.refer_all(core_ns)\n     repl_module = importlib.import_module(REPL_NS)\n     ns.add_alias(sym.symbol(REPL_NS), repl_ns)\n     ns.refer_all(repl_ns)\n     return repl_module\n",
        "code_toks_joined": "def bootstrap_repl ( which_ns : str ) -> types . ModuleType : <NEWLINE> <INDENT> <STRING> <NEWLINE> repl_ns = runtime . Namespace . get_or_create ( sym . symbol ( REPL_NS ) ) <NEWLINE> ns = runtime . Namespace . get_or_create ( sym . symbol ( which_ns ) ) <NEWLINE> core_ns = runtime . Namespace . get ( sym . symbol ( runtime . CORE_NS ) ) <NEWLINE> assert core_ns is not None <NEWLINE> ns . refer_all ( core_ns ) <NEWLINE> repl_module = importlib . import_module ( REPL_NS ) <NEWLINE> ns . add_alias ( sym . symbol ( REPL_NS ) , repl_ns ) <NEWLINE> ns . refer_all ( repl_ns ) <NEWLINE> return repl_module <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Bootstrap the REPL with a few useful vars and returned the bootstrapped\n     module so it's functions can be used by the REPL command.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aa22c74d3ec04273a5f98a8a3e2c8d4c": {
        "code_string": "foo_ns_sym = sym.symbol(\"zux.bar.foo\")\n             foo_ns = get_or_create_ns(foo_ns_sym)\n             ns.add_alias(sym.symbol(\"foo\"), foo_ns)\n             assert sym.symbol(\n                 \"aliased-var\", ns=foo_ns_sym.name\n             ) == runtime.resolve_alias(sym.symbol(\"aliased-var\", ns=\"foo\"), ns=ns)\n",
        "code_toks_joined": "foo_ns_sym = sym . symbol ( <STRING> ) <NEWLINE> <INDENT> foo_ns = get_or_create_ns ( foo_ns_sym ) <NEWLINE> ns . add_alias ( sym . symbol ( <STRING> ) , foo_ns ) <NEWLINE> assert sym . symbol ( <NEWLINE> <INDENT> <STRING> , ns = foo_ns_sym . name <NEWLINE> <DEDENT> ) == runtime . resolve_alias ( sym . symbol ( <STRING> , ns = <STRING> ) , ns = ns ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"zux.bar.foo\"",
                "\"foo\"",
                "\"aliased-var\"",
                "\"aliased-var\"",
                "\"foo\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fea891fc876d4acf8a819e4d74c3daef": {
        "code_string": "startDate = FinDate(y, 1, 1)\n         dd = dt._excelDate - startDate._excelDate + 1\n         weekday = dt._weekday\n",
        "code_toks_joined": "startDate = FinDate ( y , 1 , 1 ) <NEWLINE> <INDENT> dd = dt . _excelDate - startDate . _excelDate + 1 <NEWLINE> weekday = dt . _weekday <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3bb173bca755439c896472da3606d17a": {
        "code_string": "liborCurve = FinIborSingleCurve(settlementDate, depos, fras, swaps)\n",
        "code_toks_joined": "liborCurve = FinIborSingleCurve ( settlementDate , depos , fras , swaps ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b10cec1e25264bd88b7e8252ef0fa76b": {
        "code_string": "# Then\n         self.assertEqual(len(s.workers), 2)\n         count = 0\n         while proxy.status() != 'done' and count < 10:\n             time.sleep(0.1)\n             count += 1\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> self . assertEqual ( len ( s . workers ) , 2 ) <NEWLINE> count = 0 <NEWLINE> while proxy . status ( ) != <STRING> and count < 10 : <NEWLINE> <INDENT> time . sleep ( 0.1 ) <NEWLINE> count += 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Then"
            ],
            "<STRING>": [
                "'done'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e0c6de30224c40c8972b2d9918d87116": {
        "code_string": "class Task(TaskModel):\n     \"\"\"\n     Provides :meth:`__init__` for :class:`TaskModel` so the model can\n     be instanced with initial values.\n     \"\"\"\n     def __init__(self, job, frame, parent_task=None, state=None,\n                  priority=None, attempts=None, agent=None):\n         # build parent job id\n         if not modelfor(job, TABLE_JOB):\n             jobid = job.jobid\n             if jobid is None:\n                 raise ValueError(\"`job` with null id provided\")\n         elif isinstance(job, int):\n             jobid = job\n         else:\n             raise ValueError(\"failed to determine job id\")\n",
        "code_toks_joined": "class Task ( TaskModel ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , job , frame , parent_task = None , state = None , <NEWLINE> <INDENT> priority = None , attempts = None , agent = None ) : <NEWLINE> <COMMENT> <NL> if not modelfor ( job , TABLE_JOB ) : <NEWLINE> jobid = job . jobid <NEWLINE> if jobid is None : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> elif isinstance ( job , int ) : <NEWLINE> jobid = job <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Provides :meth:`__init__` for :class:`TaskModel` so the model can\n     be instanced with initial values.\n     \"\"\"",
                "\"`job` with null id provided\"",
                "\"failed to determine job id\""
            ],
            "<COMMENT>": [
                "# build parent job id"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c63b58a088bc4229a5eeb5db63cfebbc": {
        "code_string": "if attempts is None:\n             self.attempts = attempts\n",
        "code_toks_joined": "if attempts is None : <NEWLINE> <INDENT> self . attempts = attempts <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "477b8b989a0040f5b37cdf47348a7a8d": {
        "code_string": "def skip(should_skip, reason):\n     \"\"\"Decorator to skip a test when ``should_skip`` is True.\"\"\"\n     def wrapper(func):\n         @wraps(func)\n         def wrapped_func(*args, **kwargs):\n             if not should_skip:\n                 raise SkipTest(reason)\n             return func(*args, **kwargs)\n         return wrapped_func\n     return wrapper\n",
        "code_toks_joined": "def skip ( should_skip , reason ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def wrapper ( func ) : <NEWLINE> <INDENT> @ wraps ( func ) <NEWLINE> def wrapped_func ( * args , ** kwargs ) : <NEWLINE> <INDENT> if not should_skip : <NEWLINE> <INDENT> raise SkipTest ( reason ) <NEWLINE> <DEDENT> return func ( * args , ** kwargs ) <NEWLINE> <DEDENT> return wrapped_func <NEWLINE> <DEDENT> return wrapper <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Decorator to skip a test when ``should_skip`` is True.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7ec0bce43e114c2ca10269f443610edf": {
        "code_string": "if environment is not None or not isinstance(environment, dict):\n             raise TypeError(\"Expected None or a dictionary for `environment`\")\n",
        "code_toks_joined": "if environment is not None or not isinstance ( environment , dict ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Expected None or a dictionary for `environment`\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "39c626a96e454b76b02d1a51c0ea1add": {
        "code_string": "# Load the job type then pass the class along to the\n         # callback.  No errback here because all the errors\n         # are handled internally in this case.\n         jobtype_loader = JobType.load(request_data)\n         jobtype_loader.addCallback(loaded_jobtype, assignment_uuid)\n         jobtype_loader.addErrback(assignment_stopped, assignment_uuid)\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> jobtype_loader = JobType . load ( request_data ) <NEWLINE> jobtype_loader . addCallback ( loaded_jobtype , assignment_uuid ) <NEWLINE> jobtype_loader . addErrback ( assignment_stopped , assignment_uuid ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Load the job type then pass the class along to the",
                "# callback.  No errback here because all the errors",
                "# are handled internally in this case."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ba9fe98e4cd144eb803c69b49a93b378": {
        "code_string": "def assertGreaterEqual(self, a, b, msg=None):\n             if not a <= b:\n                 self.fail(\n                     self._formatMessage(\n                         msg, '%s not greater than or equal to %s' % (a, b)))\n",
        "code_toks_joined": "def assertGreaterEqual ( self , a , b , msg = None ) : <NEWLINE> <INDENT> if not a <= b : <NEWLINE> <INDENT> self . fail ( <NEWLINE> <INDENT> self . _formatMessage ( <NEWLINE> <INDENT> msg , <STRING> % ( a , b ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%s not greater than or equal to %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "98635622f4a24e8991a575a941783491": {
        "code_string": "# If the assignment is identical to one we already have\n             if existing_task_ids == new_task_ids:\n                 logger.debug(\"Ignoring repeated assignment of the same batch\")\n                 request.setResponseCode(ACCEPTED)\n                 request.write(dumps({\"id\": assignment[\"id\"]}))\n                 request.finish()\n                 return NOT_DONE_YET\n             # If there is only a partial overlap\n             elif existing_task_ids ^ new_task_ids:\n                 logger.error(\"Rejecting assignment with partial overlap with \"\n                              \"existing assignment.\")\n                 unknown_task_ids = new_task_ids - existing_task_ids\n                 request.setResponseCode(CONFLICT)\n                 request.write(dumps(\n                     {\"error\": \"Partial overlap of tasks\",\n                      \"rejected_task_ids\": list(unknown_task_ids)}))\n                 request.finish()\n                 return NOT_DONE_YET\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if existing_task_ids == new_task_ids : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> request . setResponseCode ( ACCEPTED ) <NEWLINE> request . write ( dumps ( { <STRING> : assignment [ <STRING> ] } ) ) <NEWLINE> request . finish ( ) <NEWLINE> return NOT_DONE_YET <NEWLINE> <COMMENT> <NL> <DEDENT> elif existing_task_ids ^ new_task_ids : <NEWLINE> <INDENT> logger . error ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> unknown_task_ids = new_task_ids - existing_task_ids <NEWLINE> request . setResponseCode ( CONFLICT ) <NEWLINE> request . write ( dumps ( <NEWLINE> <INDENT> { <STRING> : <STRING> , <NEWLINE> <INDENT> <STRING> : list ( unknown_task_ids ) } ) ) <NEWLINE> <DEDENT> <DEDENT> request . finish ( ) <NEWLINE> return NOT_DONE_YET <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If the assignment is identical to one we already have",
                "# If there is only a partial overlap"
            ],
            "<STRING>": [
                "\"Ignoring repeated assignment of the same batch\"",
                "\"id\"",
                "\"id\"",
                "\"Rejecting assignment with partial overlap with \"",
                "\"existing assignment.\"",
                "\"error\"",
                "\"Partial overlap of tasks\"",
                "\"rejected_task_ids\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "00bcb38e31c94df2805466917d8a261d": {
        "code_string": "if not transitions:\n             return True\n",
        "code_toks_joined": "if not transitions : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c88bc70e45a44961bca2ccb7c3fa412b": {
        "code_string": "def lexeme(self, lemma_root_str, lemma_root_syntactic_category=None, lemma_root_secondary_syntactic_category=None):\n         self.lemma_root_str = lemma_root_str\n         self.lemma_root_syntactic_category = lemma_root_syntactic_category\n         self.lemma_root_secondary_syntactic_category = lemma_root_syntactic_category\n",
        "code_toks_joined": "def lexeme ( self , lemma_root_str , lemma_root_syntactic_category = None , lemma_root_secondary_syntactic_category = None ) : <NEWLINE> <INDENT> self . lemma_root_str = lemma_root_str <NEWLINE> self . lemma_root_syntactic_category = lemma_root_syntactic_category <NEWLINE> self . lemma_root_secondary_syntactic_category = lemma_root_syntactic_category <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a2883056d7954cb9a7ac6814561db7bc": {
        "code_string": "for sample in samples:\n                 output.write(\"\\t{samp_reads}\"\n                              \"\\t{s_perc1}\"\n                              \"\\t{s_perc1}\".format(samp_reads=amplicon[amplicon][sample],\n                                                   s_perc1=amplicon_coverage[amplicon][\"{}_percent_{}\".format(sample, config['coverage_threshold'])],\n                                                   s_perc2=amplicon_coverage[amplicon][\"{}_percent_{}\".format(sample, config['coverage_threshold1'])]))\n",
        "code_toks_joined": "for sample in samples : <NEWLINE> <INDENT> output . write ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> . format ( samp_reads = amplicon [ amplicon ] [ sample ] , <NEWLINE> <INDENT> s_perc1 = amplicon_coverage [ amplicon ] [ <STRING> . format ( sample , config [ <STRING> ] ) ] , <NEWLINE> s_perc2 = amplicon_coverage [ amplicon ] [ <STRING> . format ( sample , config [ <STRING> ] ) ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\\t{samp_reads}\"",
                "\"\\t{s_perc1}\"",
                "\"\\t{s_perc1}\"",
                "\"{}_percent_{}\"",
                "'coverage_threshold'",
                "\"{}_percent_{}\"",
                "'coverage_threshold1'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1b3ee3c256564e8290483f3671179323": {
        "code_string": "kids = []\n     for workerklass in worker.workers:\n         pid = os.fork()\n         kids.append(pid)\n         if pid != 0:\n             sys.exit(workerklass(statsd, opts.interval).run())\n",
        "code_toks_joined": "kids = [ ] <NEWLINE> <INDENT> for workerklass in worker . workers : <NEWLINE> <INDENT> pid = os . fork ( ) <NEWLINE> kids . append ( pid ) <NEWLINE> if pid != 0 : <NEWLINE> <INDENT> sys . exit ( workerklass ( statsd , opts . interval ) . run ( ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e130ba21eab1486d9e598b549b98cee1": {
        "code_string": "def write(self, *DAT, FC=16, ADR=0):\n \t\tif FC < 5: return(self.fc())\n \t\tlADR = ADR & 0x00FF\n \t\tmADR = ADR >> 8\n \t\tVAL = b''\n \t\tfor i in DAT:\n \t\t\tVAL = VAL + pack('>H', i)\n \t\tif FC == 5 or FC == 6:\n \t\t\tVAL = VAL[0:2]\t\t\t \n \t\tif FC == 5 or FC == 15:\n \t\t\tLEN = len(VAL) * 8\n \t\telse:\n \t\t\tLEN = len(VAL) / 2\n \t\tlLEN = LEN & 0x00FF\n \t\tmLEN = LEN >> 8\n \t\tif self.TID < 255: \n \t\t\tself.TID = self.TID + 1\n \t\telse: self.TID = 1\n \t\tif FC == 6:\n \t\t\tcmd = array('B', [0, self.TID, 0, 0, 0, 6, self.unit, FC, mADR, lADR])\n \t\telse:\n \t\t\tcmd = array('B', [0, self.TID, 0, 0, 0, 7 + len(VAL), self.unit, FC, mADR, lADR, mLEN, lLEN, len(VAL)])\n \t\tcmd.extend(VAL)\n \t\tbuffer = array('B', [0] * 20)\n \t\tprint(\"Sent\", cmd)\n \t\tself.sock.send(cmd)\n \t\tself.sock.recv_into(buffer)\n",
        "code_toks_joined": "def write ( self , * DAT , FC = 16 , ADR = 0 ) : <NEWLINE> <INDENT> if FC < 5 : return ( self . fc ( ) ) <NEWLINE> lADR = ADR & 0x00FF <NEWLINE> mADR = ADR >> 8 <NEWLINE> VAL = <STRING> <NEWLINE> for i in DAT : <NEWLINE> <INDENT> VAL = VAL + pack ( <STRING> , i ) <NEWLINE> <DEDENT> if FC == 5 or FC == 6 : <NEWLINE> <INDENT> VAL = VAL [ 0 : 2 ] <NEWLINE> <DEDENT> if FC == 5 or FC == 15 : <NEWLINE> <INDENT> LEN = len ( VAL ) * 8 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> LEN = len ( VAL ) / 2 <NEWLINE> <DEDENT> lLEN = LEN & 0x00FF <NEWLINE> mLEN = LEN >> 8 <NEWLINE> if self . TID < 255 : <NEWLINE> <INDENT> self . TID = self . TID + 1 <NEWLINE> <DEDENT> else : self . TID = 1 <NEWLINE> if FC == 6 : <NEWLINE> <INDENT> cmd = array ( <STRING> , [ 0 , self . TID , 0 , 0 , 0 , 6 , self . unit , FC , mADR , lADR ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cmd = array ( <STRING> , [ 0 , self . TID , 0 , 0 , 0 , 7 + len ( VAL ) , self . unit , FC , mADR , lADR , mLEN , lLEN , len ( VAL ) ] ) <NEWLINE> <DEDENT> cmd . extend ( VAL ) <NEWLINE> buffer = array ( <STRING> , [ 0 ] * 20 ) <NEWLINE> print ( <STRING> , cmd ) <NEWLINE> self . sock . send ( cmd ) <NEWLINE> self . sock . recv_into ( buffer ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "b''",
                "'>H'",
                "'B'",
                "'B'",
                "'B'",
                "\"Sent\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "865f1352c24248108495d6c75e50aebf": {
        "code_string": "def event(self, ev):\n         \"Don't override this without damn good reason\"\n         if self.disabled or not self.visible:\n             return True\n",
        "code_toks_joined": "def event ( self , ev ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . disabled or not self . visible : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Don't override this without damn good reason\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "570091c2d9ce46eb9825a12d010196f3": {
        "code_string": "matches_in_time_slice = 0\n         try:\n             # Iterate until matches_in_time_slice is big enough, and stop anyways after matches_per_time_slice iterations\n             # this ensures the script will always terminate even in strange situations\n             # (like when all our seeds have no matches in the time slice)\n             for _ in takewhile(lambda x: matches_in_time_slice >= matches_per_time_slice, range(matches_per_time_slice)):\n                 for tier in Tier:\n                     for player_id, _ in zip(players_to_analyze.consume(tier), range(10)):\n                         match_list = get_match_list(player_id, begin_time=time_slice.begin, end_time=time_slice.end, ranked_queues=queue.name)\n                         for match in match_list.matches:\n                             match_id = match.matchId\n                             if not match_id in downloaded_matches_by_tier[tier] and match_id > minimum_match_id:\n                                 matches_to_download_by_tier[tier].add(match_id)\n",
        "code_toks_joined": "matches_in_time_slice = 0 <NEWLINE> <INDENT> try : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> for _ in takewhile ( lambda x : matches_in_time_slice >= matches_per_time_slice , range ( matches_per_time_slice ) ) : <NEWLINE> <INDENT> for tier in Tier : <NEWLINE> <INDENT> for player_id , _ in zip ( players_to_analyze . consume ( tier ) , range ( 10 ) ) : <NEWLINE> <INDENT> match_list = get_match_list ( player_id , begin_time = time_slice . begin , end_time = time_slice . end , ranked_queues = queue . name ) <NEWLINE> for match in match_list . matches : <NEWLINE> <INDENT> match_id = match . matchId <NEWLINE> if not match_id in downloaded_matches_by_tier [ tier ] and match_id > minimum_match_id : <NEWLINE> <INDENT> matches_to_download_by_tier [ tier ] . add ( match_id ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Iterate until matches_in_time_slice is big enough, and stop anyways after matches_per_time_slice iterations",
                "# this ensures the script will always terminate even in strange situations",
                "# (like when all our seeds have no matches in the time slice)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8992c5c5a767409d903f817aeafe8cad": {
        "code_string": "# When a new patch is released, we can clear all the analyzed players and downloaded_matches if minimum_patch == 'latest'\n                     if conf['minimum_patch'].lower() != LATEST and get_patch_changed():\n                         analyzed_players = set()\n                         downloaded_matches = set()\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if conf [ <STRING> ] . lower ( ) != LATEST and get_patch_changed ( ) : <NEWLINE> <INDENT> analyzed_players = set ( ) <NEWLINE> downloaded_matches = set ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# When a new patch is released, we can clear all the analyzed players and downloaded_matches if minimum_patch == 'latest'"
            ],
            "<STRING>": [
                "'minimum_patch'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6b04e53f43a54ecc92716581f8aaa6f6": {
        "code_string": "def read(self, size=1):\n         \"\"\"Read size bytes from the serial port. If a timeout is set it may\n            return less characters as requested. With no timeout it will block\n            until the requested number of bytes is read.\"\"\"\n         if not self.hComPort: raise portNotOpenError\n         if size > 0:\n             win32.ResetEvent(self._overlappedRead.hEvent)\n             flags = win32.DWORD()\n             comstat = win32.COMSTAT()\n             if not win32.ClearCommError(self.hComPort, ctypes.byref(flags), ctypes.byref(comstat)):\n                 raise SerialException('call to ClearCommError failed')\n             if self.timeout == 0:\n                 n = min(comstat.cbInQue, size)\n                 if n > 0:\n                     buf = ctypes.create_string_buffer(n)\n                     rc = win32.DWORD()\n                     err = win32.ReadFile(self.hComPort, buf, size, ctypes.byref(rc), ctypes.byref(self._overlappedRead))\n                     if not err and win32.GetLastError() != win32.ERROR_IO_PENDING:\n                         raise SerialException(\"ReadFile failed (%s)\" % ctypes.WinError())\n                     err = win32.WaitForSingleObject(self._overlappedRead.hEvent, win32.INFINITE)\n                     read = buf.raw[:rc.value]\n                 else:\n                     read = bytes()\n             else:\n                 buf = ctypes.create_string_buffer(size)\n                 rc = win32.DWORD()\n                 err = win32.ReadFile(self.hComPort, buf, size, ctypes.byref(rc), ctypes.byref(self._overlappedRead))\n                 if not err and win32.GetLastError() != win32.ERROR_IO_PENDING:\n                     raise SerialException(\"ReadFile failed (%s)\" % ctypes.WinError())\n                 err = win32.GetOverlappedResult(self.hComPort, ctypes.byref(self._overlappedRead), ctypes.byref(rc), True)\n                 read = buf.raw[:rc.value]\n         else:\n             read = bytes()\n         return bytes(read)\n",
        "code_toks_joined": "def read ( self , size = 1 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not self . hComPort : raise portNotOpenError <NEWLINE> if size > 0 : <NEWLINE> <INDENT> win32 . ResetEvent ( self . _overlappedRead . hEvent ) <NEWLINE> flags = win32 . DWORD ( ) <NEWLINE> comstat = win32 . COMSTAT ( ) <NEWLINE> if not win32 . ClearCommError ( self . hComPort , ctypes . byref ( flags ) , ctypes . byref ( comstat ) ) : <NEWLINE> <INDENT> raise SerialException ( <STRING> ) <NEWLINE> <DEDENT> if self . timeout == 0 : <NEWLINE> <INDENT> n = min ( comstat . cbInQue , size ) <NEWLINE> if n > 0 : <NEWLINE> <INDENT> buf = ctypes . create_string_buffer ( n ) <NEWLINE> rc = win32 . DWORD ( ) <NEWLINE> err = win32 . ReadFile ( self . hComPort , buf , size , ctypes . byref ( rc ) , ctypes . byref ( self . _overlappedRead ) ) <NEWLINE> if not err and win32 . GetLastError ( ) != win32 . ERROR_IO_PENDING : <NEWLINE> <INDENT> raise SerialException ( <STRING> % ctypes . WinError ( ) ) <NEWLINE> <DEDENT> err = win32 . WaitForSingleObject ( self . _overlappedRead . hEvent , win32 . INFINITE ) <NEWLINE> read = buf . raw [ : rc . value ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> read = bytes ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> buf = ctypes . create_string_buffer ( size ) <NEWLINE> rc = win32 . DWORD ( ) <NEWLINE> err = win32 . ReadFile ( self . hComPort , buf , size , ctypes . byref ( rc ) , ctypes . byref ( self . _overlappedRead ) ) <NEWLINE> if not err and win32 . GetLastError ( ) != win32 . ERROR_IO_PENDING : <NEWLINE> <INDENT> raise SerialException ( <STRING> % ctypes . WinError ( ) ) <NEWLINE> <DEDENT> err = win32 . GetOverlappedResult ( self . hComPort , ctypes . byref ( self . _overlappedRead ) , ctypes . byref ( rc ) , True ) <NEWLINE> read = buf . raw [ : rc . value ] <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> read = bytes ( ) <NEWLINE> <DEDENT> return bytes ( read ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Read size bytes from the serial port. If a timeout is set it may\n            return less characters as requested. With no timeout it will block\n            until the requested number of bytes is read.\"\"\"",
                "'call to ClearCommError failed'",
                "\"ReadFile failed (%s)\"",
                "\"ReadFile failed (%s)\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c107d86c95c945979a55e74d18739c3e": {
        "code_string": "# if the key is smaller than the first element of the\n     # vector we return 1\n     if key < vector[0]:\n         return 1\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> if key < vector [ 0 ] : <NEWLINE> <INDENT> return 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# if the key is smaller than the first element of the",
                "# vector we return 1"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c15df0d856dc46339c67eb6ae4b42236": {
        "code_string": "dictionary = parser(record, {\n             'timestamp_utc': sys['TimeCreated']['@SystemTime'],\n             'event_id': sys['EventID'],\n             'description': '',\n             'details': '',\n             'event_source': sys['Provider']['@Name'],\n             'event_log': sys['Channel'],\n             'session_id': '',\n             'account': '',\n             'computer_name': sys['Computer'],\n             'record_number': sys['EventRecordID'],\n             'recovered': recovered,\n             'source_file_hash': file_hash\n         })\n",
        "code_toks_joined": "dictionary = parser ( record , { <NEWLINE> <INDENT> <STRING> : sys [ <STRING> ] [ <STRING> ] , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : sys [ <STRING> ] [ <STRING> ] , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : recovered , <NEWLINE> <STRING> : file_hash <NEWLINE> } ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'timestamp_utc'",
                "'TimeCreated'",
                "'@SystemTime'",
                "'event_id'",
                "'EventID'",
                "'description'",
                "''",
                "'details'",
                "''",
                "'event_source'",
                "'Provider'",
                "'@Name'",
                "'event_log'",
                "'Channel'",
                "'session_id'",
                "''",
                "'account'",
                "''",
                "'computer_name'",
                "'Computer'",
                "'record_number'",
                "'EventRecordID'",
                "'recovered'",
                "'source_file_hash'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "209677efe15c47e4b214f5efbe88dbe3": {
        "code_string": "if self.connection_string is None:\n                 if self.adaptor.__name__ == 'psycopg2':\n                     self.pool = importlib.import_module('psycopg2.pool').ThreadedConnectionPool(\n                         self.minconn or 1, self.maxconn or 1, self.connection_string or ''\n                     )\n                     self.connection = self.pool.getconn(key=self.poolkey)\n                 else:\n                     self.connection = self.adaptor.connect(self.connection_string or '')\n",
        "code_toks_joined": "if self . connection_string is None : <NEWLINE> <INDENT> if self . adaptor . __name__ == <STRING> : <NEWLINE> <INDENT> self . pool = importlib . import_module ( <STRING> ) . ThreadedConnectionPool ( <NEWLINE> <INDENT> self . minconn or 1 , self . maxconn or 1 , self . connection_string or <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> self . connection = self . pool . getconn ( key = self . poolkey ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . connection = self . adaptor . connect ( self . connection_string or <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'psycopg2'",
                "'psycopg2.pool'",
                "''",
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "318a768e485d40779b2426c0753c9900": {
        "code_string": "bigside=int(max(width_old,height_old)*1.5)\n     background = Image.new('RGBA', (bigside, bigside), (255, 255, 255, 255))\n     offset = (0,0)\n     background.paste(image, offset)\n     file_name2=f'{width_old*2}X{height_old*2}_{file_name}.png'\n     save_image=os.path.join(cwd,file_name2)\n     save_image_in_data=os.path.join(path_save,file_name2)\n",
        "code_toks_joined": "bigside = int ( max ( width_old , height_old ) * 1.5 ) <NEWLINE> <INDENT> background = Image . new ( <STRING> , ( bigside , bigside ) , ( 255 , 255 , 255 , 255 ) ) <NEWLINE> offset = ( 0 , 0 ) <NEWLINE> background . paste ( image , offset ) <NEWLINE> file_name2 = <STRING> <NEWLINE> save_image = os . path . join ( cwd , file_name2 ) <NEWLINE> save_image_in_data = os . path . join ( path_save , file_name2 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'RGBA'",
                "f'{width_old*2}X{height_old*2}_{file_name}.png'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1970b67cf1b64e4aa57e7a5aac821791": {
        "code_string": "release = repo.create_git_release(\n         tag=tag,\n         name=tag,\n         message=message,\n         target_commitish=target,\n         prerelease=prerelease,\n     )\n",
        "code_toks_joined": "release = repo . create_git_release ( <NEWLINE> <INDENT> tag = tag , <NEWLINE> name = tag , <NEWLINE> message = message , <NEWLINE> target_commitish = target , <NEWLINE> prerelease = prerelease , <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6a376b2f0d894ea88bb64ac0dcd81722": {
        "code_string": "def run_local_GPU(self, folders_glob):\n         bash_cmd = \"export CUDA_VISIBLE_DEVICES=0\"\n         if len(glob(folders_glob)) != (self.ngpus - self.gpus_in_use):\n             raise ValueError(\"Cannot run jobs of {} folders as only {} GPUs are available\".format(len(glob(folders_glob)), self.ngpus - self.gpus_in_use))\n",
        "code_toks_joined": "def run_local_GPU ( self , folders_glob ) : <NEWLINE> <INDENT> bash_cmd = <STRING> <NEWLINE> if len ( glob ( folders_glob ) ) != ( self . ngpus - self . gpus_in_use ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( len ( glob ( folders_glob ) ) , self . ngpus - self . gpus_in_use ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"export CUDA_VISIBLE_DEVICES=0\"",
                "\"Cannot run jobs of {} folders as only {} GPUs are available\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5da0005ab75b4783a28b5715d864541e": {
        "code_string": "return np.vstack([winner_scores, winner_indices])\n",
        "code_toks_joined": "return np . vstack ( [ winner_scores , winner_indices ] ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3ee4de62bc0a4bf399dd5e400d4fb34d": {
        "code_string": "# run annotator\n     annotator = Annotator([task1, task2], data)\n     annotator(data.ids)\n     print(annotator.annotated)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> annotator = Annotator ( [ task1 , task2 ] , data ) <NEWLINE> annotator ( data . ids ) <NEWLINE> print ( annotator . annotated ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# run annotator"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "994f72c114eb48fba53e67de3e1f8e14": {
        "code_string": "def refmac(self, cycles):\n         directory = self.job_directory(\"refmac\")\n         use_phases = self.args.unbiased and self.min_rwork > 0.35\n         job = Refmac(self.args, directory, self.current_xyz, use_phases, cycles)\n         self.jobs[self.cycle].append(job)\n         self.current_hkl = job.hklout\n         self.current_xyz = job.xyzout\n         return job\n",
        "code_toks_joined": "def refmac ( self , cycles ) : <NEWLINE> <INDENT> directory = self . job_directory ( <STRING> ) <NEWLINE> use_phases = self . args . unbiased and self . min_rwork > 0.35 <NEWLINE> job = Refmac ( self . args , directory , self . current_xyz , use_phases , cycles ) <NEWLINE> self . jobs [ self . cycle ] . append ( job ) <NEWLINE> self . current_hkl = job . hklout <NEWLINE> self . current_xyz = job . xyzout <NEWLINE> return job <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"refmac\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b9f2d0d19e4949b094bb8e7e2d107b93": {
        "code_string": "@property\n     def dir_ref(self):\n         return \"{}_{}_{}\".format(self.name, self.arch, self.version)\n",
        "code_toks_joined": "@ property <NEWLINE> <INDENT> def dir_ref ( self ) : <NEWLINE> <INDENT> return <STRING> . format ( self . name , self . arch , self . version ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{}_{}_{}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c233cbe6c97b4764b2e5841cb0d3dce1": {
        "code_string": "if filler == checkpoint:\n                         break\n",
        "code_toks_joined": "if filler == checkpoint : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0949aba769064a1c97424a2d8abeea0e": {
        "code_string": "def log(page, message, level=WARNING):\n     if level > THRESHOLD:\n         print(\">> {0}: {1} | {2}\".format(CRITICALITY[level], message, page.url))\n",
        "code_toks_joined": "def log ( page , message , level = WARNING ) : <NEWLINE> <INDENT> if level > THRESHOLD : <NEWLINE> <INDENT> print ( <STRING> . format ( CRITICALITY [ level ] , message , page . url ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\">> {0}: {1} | {2}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e95fbf01ee2d49298436742dfeb772bf": {
        "code_string": "ConditionalContainer(\n                 get_hline(),\n                 filter=ShowDefault() | ShowSymbol()\n             ),\n             ConditionalContainer(\n                 Window(\n                     content=BufferControl(\n                         buffer_name='default_values',\n                         lexer=lexer\n                     )\n                 ),\n                 filter=ShowDefault()\n             ),\n             ConditionalContainer(\n                 get_hline(),\n                 filter=ShowDefault() & ShowSymbol()\n             ),\n             ConditionalContainer(\n                 Window(\n                     content=BufferControl(\n                         buffer_name='symbols',\n                         lexer=lexer\n                     )\n                 ),\n                 filter=ShowSymbol()\n             ),\n             Window(\n                 content=BufferControl(\n                     buffer_name='bottom_toolbar',\n                     lexer=toolbarLex\n                 ),\n             ),\n         ]),\n         filter=~IsDone() & RendererHeightIsKnown()\n     )\n",
        "code_toks_joined": "ConditionalContainer ( <NEWLINE> <INDENT> get_hline ( ) , <NEWLINE> filter = ShowDefault ( ) | ShowSymbol ( ) <NEWLINE> ) , <NEWLINE> ConditionalContainer ( <NEWLINE> Window ( <NEWLINE> <INDENT> content = BufferControl ( <NEWLINE> <INDENT> buffer_name = <STRING> , <NEWLINE> lexer = lexer <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) , <NEWLINE> filter = ShowDefault ( ) <NEWLINE> ) , <NEWLINE> ConditionalContainer ( <NEWLINE> get_hline ( ) , <NEWLINE> filter = ShowDefault ( ) & ShowSymbol ( ) <NEWLINE> ) , <NEWLINE> ConditionalContainer ( <NEWLINE> Window ( <NEWLINE> <INDENT> content = BufferControl ( <NEWLINE> <INDENT> buffer_name = <STRING> , <NEWLINE> lexer = lexer <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) , <NEWLINE> filter = ShowSymbol ( ) <NEWLINE> ) , <NEWLINE> Window ( <NEWLINE> content = BufferControl ( <NEWLINE> <INDENT> buffer_name = <STRING> , <NEWLINE> lexer = toolbarLex <NEWLINE> <DEDENT> ) , <NEWLINE> ) , <NEWLINE> ] ) , <NEWLINE> filter = ~ IsDone ( ) & RendererHeightIsKnown ( ) <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'default_values'",
                "'symbols'",
                "'bottom_toolbar'"
            ]
        },
        "err_obj": {
            "msg": "unbalanced (){}[]"
        }
    },
    "f05f298a95984517887b88e0c772c368": {
        "code_string": "func_dict[func_name] = (func, key_args)\n",
        "code_toks_joined": "func_dict [ func_name ] = ( func , key_args ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1df4dcac334f48489c63a6a2f9ddb9f1": {
        "code_string": "def __rel_change(self, new: float) -> float:\n         if self._likelihoods:\n             old = self._likelihoods[-1]\n             return abs((new - old) / old)\n         return inf\n",
        "code_toks_joined": "def __rel_change ( self , new : float ) -> float : <NEWLINE> <INDENT> if self . _likelihoods : <NEWLINE> <INDENT> old = self . _likelihoods [ - 1 ] <NEWLINE> return abs ( ( new - old ) / old ) <NEWLINE> <DEDENT> return inf <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c7326ad95e29452f92e68ad3482f388b": {
        "code_string": "def register_middleware(self, middleware, attach_to=None):\n         if attach_to == 'request':\n             self.req_middleware.append(middleware)\n         elif attach_to == 'response':\n             self.res_middleware = [attach_to] + self.res_middleware\n",
        "code_toks_joined": "def register_middleware ( self , middleware , attach_to = None ) : <NEWLINE> <INDENT> if attach_to == <STRING> : <NEWLINE> <INDENT> self . req_middleware . append ( middleware ) <NEWLINE> <DEDENT> elif attach_to == <STRING> : <NEWLINE> <INDENT> self . res_middleware = [ attach_to ] + self . res_middleware <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'request'",
                "'response'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9bd7ee2136f845ada1a8e0eadd6486b1": {
        "code_string": "# request channel join\n     def join(self, channel: str):\n         channels = list(self.channels)\n         if channel in channels:\n             self.__to_join.append((channel, 0, time.time()))\n         else:\n             self.__warning('Already connected to channel {}, connection aborted'.format(channel))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def join ( self , channel : str ) : <NEWLINE> <INDENT> channels = list ( self . channels ) <NEWLINE> if channel in channels : <NEWLINE> <INDENT> self . __to_join . append ( ( channel , 0 , time . time ( ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . __warning ( <STRING> . format ( channel ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# request channel join"
            ],
            "<STRING>": [
                "'Already connected to channel {}, connection aborted'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c018b4a6418c45b081c844fb4a766124": {
        "code_string": "# send a packet and log it[, obfuscate after a certain index][, ignore the throttling cap]\n     def __send(self, packet, obfuscate_after=None, ignore_throttle=0):\n         # verify throttling status\n         if self.__anti_throttle() or ignore_throttle:\n             # verify socket instance\n             if self.__wait_for_status(0):\n                 self.__socket.send(packet.encode('UTF-8'))\n                 self.__event_sent_date.append(time.time())\n             # creating '**..' string with the length required\n             if obfuscate_after:\n                 packet_hidden = '*' * (len(packet) - obfuscate_after)\n                 packet = packet[0:obfuscate_after] + packet_hidden\n             # print to log\n             self.__packet_sent(packet)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def __send ( self , packet , obfuscate_after = None , ignore_throttle = 0 ) : <NEWLINE> <COMMENT> <NL> <INDENT> if self . __anti_throttle ( ) or ignore_throttle : <NEWLINE> <COMMENT> <NL> <INDENT> if self . __wait_for_status ( 0 ) : <NEWLINE> <INDENT> self . __socket . send ( packet . encode ( <STRING> ) ) <NEWLINE> self . __event_sent_date . append ( time . time ( ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> if obfuscate_after : <NEWLINE> <INDENT> packet_hidden = <STRING> * ( len ( packet ) - obfuscate_after ) <NEWLINE> packet = packet [ 0 : obfuscate_after ] + packet_hidden <NEWLINE> <COMMENT> <NL> <DEDENT> self . __packet_sent ( packet ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# send a packet and log it[, obfuscate after a certain index][, ignore the throttling cap]",
                "# verify throttling status",
                "# verify socket instance",
                "# creating '**..' string with the length required",
                "# print to log"
            ],
            "<STRING>": [
                "'UTF-8'",
                "'*'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d5969ddda7cc4cf3a99bb107f5b5521e": {
        "code_string": "# send a message to a channel and prevent sending to disconnected channels\n     def __send_message(self) -> None:\n         # if there is message to send and socket ready and socket not throttling\n         if len(self.__to_send) > 0 and self.__wait_for_status() and not self.__anti_throttle():\n             # retrieve the first message to send\n             item = self.__to_send.pop(0)\n             channel = item[0]\n             message = item[1]\n             # if channel not connected, try to connect\n             if channel not in self.channels:\n                 self.__warning('Try to send to not connected channel, abort')\n             else:\n                 packet = \"PRIVMSG #{} : {}\\r\\n\".format(channel, message)\n                 self.__send(packet)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def __send_message ( self ) -> None : <NEWLINE> <COMMENT> <NL> <INDENT> if len ( self . __to_send ) > 0 and self . __wait_for_status ( ) and not self . __anti_throttle ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> item = self . __to_send . pop ( 0 ) <NEWLINE> channel = item [ 0 ] <NEWLINE> message = item [ 1 ] <NEWLINE> <COMMENT> <NL> if channel not in self . channels : <NEWLINE> <INDENT> self . __warning ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> packet = <STRING> . format ( channel , message ) <NEWLINE> self . __send ( packet ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# send a message to a channel and prevent sending to disconnected channels",
                "# if there is message to send and socket ready and socket not throttling",
                "# retrieve the first message to send",
                "# if channel not connected, try to connect"
            ],
            "<STRING>": [
                "'Try to send to not connected channel, abort'",
                "\"PRIVMSG #{} : {}\\r\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8c964b785e9c40eb91064d8879961ae0": {
        "code_string": "next_task, next_instance = extract(next(task_file)), extract(next(instance_file))\n     while next_task[JOB_ID] != '':\n         arrive_at, task_lines, instance_lines = next_task[ARR_TIME], [ next_task[REST] ], [ next_instance[REST] ]\n         next_task = read_lines(task_file, next_task[JOB_ID], task_lines)\n         next_instance = read_lines(instance_file, next_task[JOB_ID], instance_lines)\n         yield float(arrive_at), { 'tasks': task_lines, 'instances': instance_lines }\n",
        "code_toks_joined": "next_task , next_instance = extract ( next ( task_file ) ) , extract ( next ( instance_file ) ) <NEWLINE> <INDENT> while next_task [ JOB_ID ] != <STRING> : <NEWLINE> <INDENT> arrive_at , task_lines , instance_lines = next_task [ ARR_TIME ] , [ next_task [ REST ] ] , [ next_instance [ REST ] ] <NEWLINE> next_task = read_lines ( task_file , next_task [ JOB_ID ] , task_lines ) <NEWLINE> next_instance = read_lines ( instance_file , next_task [ JOB_ID ] , instance_lines ) <NEWLINE> yield float ( arrive_at ) , { <STRING> : task_lines , <STRING> : instance_lines } <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''",
                "'tasks'",
                "'instances'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "67e26932531e49348780102bc7cfab89": {
        "code_string": "# Automatically determine device and data_format\n   (device, data_format) = ('/gpu:0', 'channels_first')\n   if flags_obj.no_gpu or tf.test.is_gpu_available():\n     (device, data_format) = ('/cpu:0', 'channels_last')\n   # If data_format is defined in FLAGS, overwrite automatically set value.\n   if flags_obj.data_format is not None:\n     data_format = flags_obj.data_format\n   print('Using device %s, and data format %s.' % (device, data_format))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> if flags_obj . no_gpu or tf . test . is_gpu_available ( ) : <NEWLINE> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if flags_obj . data_format is not None : <NEWLINE> <INDENT> data_format = flags_obj . data_format <NEWLINE> <DEDENT> print ( <STRING> % ( device , data_format ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Automatically determine device and data_format",
                "# If data_format is defined in FLAGS, overwrite automatically set value."
            ],
            "<STRING>": [
                "'/gpu:0'",
                "'channels_first'",
                "'/cpu:0'",
                "'channels_last'",
                "'Using device %s, and data format %s.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5db6a33788e24793af98f185d32be2a9": {
        "code_string": "storage.import_blob(open(tmp_destination, 'rb'), id_)\n     os.remove(tmp_destination)\n",
        "code_toks_joined": "storage . import_blob ( open ( tmp_destination , <STRING> ) , id_ ) <NEWLINE> <INDENT> os . remove ( tmp_destination ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'rb'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d20927eb30394af587e049dc22aa9577": {
        "code_string": "if not keyspace:\n                 # remove not-just-added keyspaces\n                 self.keyspaces = dict((name, meta) for name, meta in self.keyspaces.items()\n                                       if name in added_keyspaces)\n         else:\n             # keyspace is not None, table is not None\n             try:\n                 keyspace_meta = self.keyspaces[keyspace]\n             except KeyError:\n                 # we're trying to update a table in a keyspace we don't know\n                 # about, something went wrong.\n                 # TODO log error, submit schema refresh\n                 pass\n             if keyspace in cf_def_rows:\n                 for table_row in cf_def_rows[keyspace]:\n                     table_meta = self._build_table_metadata(\n                             keyspace_meta, table_row, col_def_rows[keyspace])\n                     keyspace.tables[table_meta.name] = table_meta\n",
        "code_toks_joined": "if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# remove not-just-added keyspaces",
                "# keyspace is not None, table is not None",
                "# we're trying to update a table in a keyspace we don't know",
                "# about, something went wrong.",
                "# TODO log error, submit schema refresh"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "90bec0c44e444a09adec6842764f2b8a": {
        "code_string": "if not keyspace:\n                 # remove not-just-added keyspaces\n                 self.keyspaces = dict((name, meta) for name, meta in self.keyspaces.items()\n                                       if name in added_keyspaces)\n         else:\n             # keyspace is not None, table is not None\n             try:\n                 keyspace_meta = self.keyspaces[keyspace]\n             except KeyError:\n                 # we're trying to update a table in a keyspace we don't know\n                 # about, something went wrong.\n                 # TODO log error, submit schema refresh\n                 pass\n             if keyspace in cf_def_rows:\n                 for table_row in cf_def_rows[keyspace]:\n                     table_meta = self._build_table_metadata(\n                             keyspace_meta, table_row, col_def_rows[keyspace])\n                     keyspace.tables[table_meta.name] = table_meta\n",
        "code_toks_joined": "if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# remove not-just-added keyspaces",
                "# keyspace is not None, table is not None",
                "# we're trying to update a table in a keyspace we don't know",
                "# about, something went wrong.",
                "# TODO log error, submit schema refresh"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "05bd4e4da3a0474a8226900404c99bd1": {
        "code_string": "def populate(self, cluster, hosts):\n         self._live_hosts = set(hosts)\n         if len(hosts) == 1:\n             self._position = 0\n         else:\n             self._position = randint(0, len(hosts) - 1)\n",
        "code_toks_joined": "def populate ( self , cluster , hosts ) : <NEWLINE> <INDENT> self . _live_hosts = set ( hosts ) <NEWLINE> if len ( hosts ) == 1 : <NEWLINE> <INDENT> self . _position = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . _position = randint ( 0 , len ( hosts ) - 1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9f9fbe1d4d9e4d0dba3c672c9149bc91": {
        "code_string": "if not issubclass(klass, poly_base):\n                 raise PolyMorphicModelException(\n                     '{} is not a subclass of {}'.format(klass.__name__, poly_base.__name__)\n                 )\n",
        "code_toks_joined": "if not issubclass ( klass , poly_base ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , poly_base . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'{} is not a subclass of {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d6268c7252e144898b4df7751688e435": {
        "code_string": "if not issubclass(klass, cls):\n                 raise PolyMorphicModelException(\n                     '{} is not a subclass of {}'.format(klass.__name__, poly_base.__name__)\n                 )\n",
        "code_toks_joined": "if not issubclass ( klass , cls ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , poly_base . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'{} is not a subclass of {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0054ed382dbe491b894115ecdc739d81": {
        "code_string": "def on_read_timeout(self, query, consistency, required_responses,\n                         received_responses, data_retrieved, retry_num):\n         if retry_num != 0:\n             return (self.RETHROW, None)\n         elif received_responses < required_responses:\n             return self._pick_consistency(received_responses)\n         elif not data_retrieved:\n             return (self.RETRY, consistency)\n         else:\n             return (self.RETHROW, None)\n",
        "code_toks_joined": "def on_read_timeout ( self , query , consistency , required_responses , <NEWLINE> <INDENT> received_responses , data_retrieved , retry_num ) : <NEWLINE> if retry_num != 0 : <NEWLINE> return ( self . RETHROW , None ) <NEWLINE> elif received_responses < required_responses : <NEWLINE> return self . _pick_consistency ( received_responses ) <NEWLINE> elif not data_retrieved : <NEWLINE> return ( self . RETRY , consistency ) <NEWLINE> else : <NEWLINE> return ( self . RETHROW , None ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f552b95297be450798e7a6708da81951": {
        "code_string": "MultipleObjectsReturnedBase = DoesNotExistBase or attrs.pop('MultipleObjectsReturned', BaseModel.MultipleObjectsReturned)\n         attrs['MultipleObjectsReturned'] = type('MultipleObjectsReturned', (MultipleObjectsReturnedBase,), {})\n",
        "code_toks_joined": "MultipleObjectsReturnedBase = DoesNotExistBase or attrs . pop ( <STRING> , BaseModel . MultipleObjectsReturned ) <NEWLINE> <INDENT> attrs [ <STRING> ] = type ( <STRING> , ( MultipleObjectsReturnedBase , ) , { } ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'MultipleObjectsReturned'",
                "'MultipleObjectsReturned'",
                "'MultipleObjectsReturned'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9c20f4a8a09e4f9a81d00f7fbc37534c": {
        "code_string": "def parse_2d_maze(maze_str):\n     \"\"\"\n     Decode a 2-D maze from its string representation.\n     \"\"\"\n     lines = [x.strip() for x in maze_str.strip().split('\\n')]\n     num_rows = len(lines)\n     if num_rows == 0:\n         raise ValueError('must have at least one row')\n     num_cols = len(lines[0])\n     walls = []\n     start_pos = None\n     end_pos = None\n     for row, row_str in enumerate(lines):\n         if len(row) != num_cols:\n             raise ValueError('row length should be %d but got %d' %\n                              (num_cols, len(row)))\n         sub_walls = []\n         for col, cell_str in enumerate(row_str):\n             if cell_str == 'w':\n                 sub_walls.append(True)\n             else:\n                 sub_walls.append(False)\n             if cell_str == 'A':\n                 if start_pos:\n                     raise ValueError('more than one start state')\n                 start_pos = (row, col)\n             elif cell_str == 'x':\n                 if end_pos:\n                     raise ValueError('more than one end state')\n                 end_pos = (row, col)\n         walls.append(sub_walls)\n     return Maze(np.array(walls), start_pos=start_pos, end_pos=end_pos)\n",
        "code_toks_joined": "def parse_2d_maze ( maze_str ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> lines = [ x . strip ( ) for x in maze_str . strip ( ) . split ( <STRING> ) ] <NEWLINE> num_rows = len ( lines ) <NEWLINE> if num_rows == 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> num_cols = len ( lines [ 0 ] ) <NEWLINE> walls = [ ] <NEWLINE> start_pos = None <NEWLINE> end_pos = None <NEWLINE> for row , row_str in enumerate ( lines ) : <NEWLINE> <INDENT> if len ( row ) != num_cols : <NEWLINE> <INDENT> raise ValueError ( <STRING> % <NEWLINE> <INDENT> ( num_cols , len ( row ) ) ) <NEWLINE> <DEDENT> <DEDENT> sub_walls = [ ] <NEWLINE> for col , cell_str in enumerate ( row_str ) : <NEWLINE> <INDENT> if cell_str == <STRING> : <NEWLINE> <INDENT> sub_walls . append ( True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sub_walls . append ( False ) <NEWLINE> <DEDENT> if cell_str == <STRING> : <NEWLINE> <INDENT> if start_pos : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> start_pos = ( row , col ) <NEWLINE> <DEDENT> elif cell_str == <STRING> : <NEWLINE> <INDENT> if end_pos : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> end_pos = ( row , col ) <NEWLINE> <DEDENT> <DEDENT> walls . append ( sub_walls ) <NEWLINE> <DEDENT> return Maze ( np . array ( walls ) , start_pos = start_pos , end_pos = end_pos ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Decode a 2-D maze from its string representation.\n     \"\"\"",
                "'\\n'",
                "'must have at least one row'",
                "'row length should be %d but got %d'",
                "'w'",
                "'A'",
                "'more than one start state'",
                "'x'",
                "'more than one end state'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f9c9318a032e46dd89e051bbc42379f0": {
        "code_string": "s3object = S3RawIO(path)\n         assert s3object._client_kwargs == client_args\n         assert s3object.name == url\n",
        "code_toks_joined": "s3object = S3RawIO ( path ) <NEWLINE> <INDENT> assert s3object . _client_kwargs == client_args <NEWLINE> assert s3object . name == url <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "205de528d22e44afb3b46cf57bc60fbc": {
        "code_string": "# Checks if same file\n             if system.relpath(src) != system.relpath(dst):\n                 raise same_file_error(\n                     \"'%s' and '%s' are the same file\" % (src, dst))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if system . relpath ( src ) != system . relpath ( dst ) : <NEWLINE> <INDENT> raise same_file_error ( <NEWLINE> <INDENT> <STRING> % ( src , dst ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Checks if same file"
            ],
            "<STRING>": [
                "\"'%s' and '%s' are the same file\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "24751889fc6646f4a5cb25de0ed52e2e": {
        "code_string": "# Add storage specific keys\n         for key, value in tuple(stat.items()):\n             stat['st_' + key.lower().replace('-', '_')] = value\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for key , value in tuple ( stat . items ( ) ) : <NEWLINE> <INDENT> stat [ <STRING> + key . lower ( ) . replace ( <STRING> , <STRING> ) ] = value <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Add storage specific keys"
            ],
            "<STRING>": [
                "'st_'",
                "'-'",
                "'_'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "56f1e02d24234cecaeb5fae8a69a04d4": {
        "code_string": "def test_generate_episode(self):\n         task = self.__setup_stub_task()\n         policy = GreedyPolicy()\n         value_func = self.__setup_stub_value_function()\n         episode = generate_episode(task, value_func, policy)\n         self.eq(3, len(episode))\n         self.eq((0, 1, 1, 1), episode[0])\n         self.eq((1, 2, 3, 9), episode[1])\n         self.eq((3, 4, 7, 49), episode[2])\n",
        "code_toks_joined": "def test_generate_episode ( self ) : <NEWLINE> <INDENT> task = self . __setup_stub_task ( ) <NEWLINE> policy = GreedyPolicy ( ) <NEWLINE> value_func = self . __setup_stub_value_function ( ) <NEWLINE> episode = generate_episode ( task , value_func , policy ) <NEWLINE> self . eq ( 3 , len ( episode ) ) <NEWLINE> self . eq ( ( 0 , 1 , 1 , 1 ) , episode [ 0 ] ) <NEWLINE> self . eq ( ( 1 , 2 , 3 , 9 ) , episode [ 1 ] ) <NEWLINE> self . eq ( ( 3 , 4 , 7 , 49 ) , episode [ 2 ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e45ad5866b8b47ceae686ea567129454": {
        "code_string": "def mark_as_active(pathname):\n     \"\"\"Mark the active page in the navbar\"\"\"\n     for id_page in ('home', 'data', 'credits', 'contact', 'info'):\n         if id_page in pathname:\n             document[pathname].class_name = 'active'\n         else:\n             document[id_page].class_name = ''\n     if 'show' in pathname:\n         document['/home'].class_name = 'active'\n",
        "code_toks_joined": "def mark_as_active ( pathname ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for id_page in ( <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> if id_page in pathname : <NEWLINE> <INDENT> document [ pathname ] . class_name = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> document [ id_page ] . class_name = <STRING> <NEWLINE> <DEDENT> <DEDENT> if <STRING> in pathname : <NEWLINE> <INDENT> document [ <STRING> ] . class_name = <STRING> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Mark the active page in the navbar\"\"\"",
                "'home'",
                "'data'",
                "'credits'",
                "'contact'",
                "'info'",
                "'active'",
                "''",
                "'show'",
                "'/home'",
                "'active'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7572db48acaa4072a3d50d7a9ced6672": {
        "code_string": "def changeTemperature(self,newTemperature):\n         ## set temperature for degree C\n         if not isinstance(newTemperature,int) or not isinstance(newTemperature,float):\n             raise Exception('Wrong usage of method')\n         ## Fixing temps if not given as multiplies of 10 less than 180\n         if newTemperature < 180:\n             newTemperature = newTemperature * 10\n         if (newTemperature > 180 and newTemperature < 320):\n             self.adjust_temperature = newTemperature\n         else:\n             raise Exception('out of range temperature!!')\n",
        "code_toks_joined": "def changeTemperature ( self , newTemperature ) : <NEWLINE> <COMMENT> <NL> <INDENT> if not isinstance ( newTemperature , int ) or not isinstance ( newTemperature , float ) : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if newTemperature < 180 : <NEWLINE> <INDENT> newTemperature = newTemperature * 10 <NEWLINE> <DEDENT> if ( newTemperature > 180 and newTemperature < 320 ) : <NEWLINE> <INDENT> self . adjust_temperature = newTemperature <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "## set temperature for degree C",
                "## Fixing temps if not given as multiplies of 10 less than 180"
            ],
            "<STRING>": [
                "'Wrong usage of method'",
                "'out of range temperature!!'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f84107112b3e44fa86171a2e3ca73769": {
        "code_string": "related_title = anime_page.find('h2', text=u'Related Anime')\n     if related_title:\n       related_elt = related_title.parent\n       utilities.extract_tags(related_elt.find_all('h2'))\n       related = {}\n       for link in related_elt.find_all('a'):\n         curr_elt = link.previous_sibling\n         if curr_elt is None:\n           # we've reached the end of the list.\n           break\n         related_type = None\n         while True:\n           if not curr_elt:\n             raise MalformedAnimePageError(self, html, message=\"Prematurely reached end of related anime listing\")\n           if isinstance(curr_elt, bs4.NavigableString):\n             type_match = re.match('(?P<type>[a-zA-Z\\ \\-]+):', curr_elt)\n             if type_match:\n               related_type = type_match.group('type')\n               break\n           curr_elt = curr_elt.previous_sibling\n         # parse link: may be manga or anime.\n         href_parts = link.get('href').split('/')\n         title = link.text\n         obj_id = int(href_parts[4])\n         non_title_parts = href_parts[:5]\n         if 'manga' in non_title_parts:\n           new_obj = self.session.manga(obj_id).set({'title': title})\n         elif 'anime' in non_title_parts:\n           new_obj = self.session.anime(obj_id).set({'title': title})\n         else:\n           raise MalformedAnimePageError(self, link, message=\"Related thing is of unknown type\")\n         if related_type not in related:\n           related[related_type] = [new_obj]\n         else:\n           related[related_type].append(new_obj)\n       anime_info['related'] = related\n     else:\n       anime_info['related'] = None\n     return anime_info\n",
        "code_toks_joined": "related_title = anime_page . find ( <STRING> , text = <STRING> ) <NEWLINE> <INDENT> if related_title : <NEWLINE> <INDENT> related_elt = related_title . parent <NEWLINE> utilities . extract_tags ( related_elt . find_all ( <STRING> ) ) <NEWLINE> related = { } <NEWLINE> for link in related_elt . find_all ( <STRING> ) : <NEWLINE> <INDENT> curr_elt = link . previous_sibling <NEWLINE> if curr_elt is None : <NEWLINE> <COMMENT> <NL> <INDENT> break <NEWLINE> <DEDENT> related_type = None <NEWLINE> while True : <NEWLINE> <INDENT> if not curr_elt : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , html , message = <STRING> ) <NEWLINE> <DEDENT> if isinstance ( curr_elt , bs4 . NavigableString ) : <NEWLINE> <INDENT> type_match = re . match ( <STRING> , curr_elt ) <NEWLINE> if type_match : <NEWLINE> <INDENT> related_type = type_match . group ( <STRING> ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> curr_elt = curr_elt . previous_sibling <NEWLINE> <COMMENT> <NL> <DEDENT> href_parts = link . get ( <STRING> ) . split ( <STRING> ) <NEWLINE> title = link . text <NEWLINE> obj_id = int ( href_parts [ 4 ] ) <NEWLINE> non_title_parts = href_parts [ : 5 ] <NEWLINE> if <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . manga ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> elif <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . anime ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , link , message = <STRING> ) <NEWLINE> <DEDENT> if related_type not in related : <NEWLINE> <INDENT> related [ related_type ] = [ new_obj ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> related [ related_type ] . append ( new_obj ) <NEWLINE> <DEDENT> <DEDENT> anime_info [ <STRING> ] = related <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> anime_info [ <STRING> ] = None <NEWLINE> <DEDENT> return anime_info <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'h2'",
                "u'Related Anime'",
                "'h2'",
                "'a'",
                "\"Prematurely reached end of related anime listing\"",
                "'(?P<type>[a-zA-Z\\ \\-]+):'",
                "'type'",
                "'href'",
                "'/'",
                "'manga'",
                "'title'",
                "'anime'",
                "'title'",
                "\"Related thing is of unknown type\"",
                "'related'",
                "'related'"
            ],
            "<COMMENT>": [
                "# we've reached the end of the list.",
                "# parse link: may be manga or anime."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af6bdacab0974d65a11edc3e462bf1fe": {
        "code_string": "def __init__(self, session, user_name):\n     super(MediaList, self).__init__(session)\n     self.username = user_name\n     if not isinstance(self.username, unicode) or len(self.username) < 2:\n       raise InvalidMediaListError(self.username)\n     self._list = None\n     self._stats = None\n",
        "code_toks_joined": "def __init__ ( self , session , user_name ) : <NEWLINE> <INDENT> super ( MediaList , self ) . __init__ ( session ) <NEWLINE> self . username = user_name <NEWLINE> if not isinstance ( self . username , unicode ) or len ( self . username ) < 2 : <NEWLINE> <INDENT> raise InvalidMediaListError ( self . username ) <NEWLINE> <DEDENT> self . _list = None <NEWLINE> self . _stats = None <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "77c281966580446483aecacc76ea3c37": {
        "code_string": "if links and nodes:\n             self.undostack.beginMacro(\"Delete Stuff\")\n             for link in links:\n                 self.undostack.push(type(link.startIO).DeleteLinkCommand(link.startIO))\n",
        "code_toks_joined": "if links and nodes : <NEWLINE> <INDENT> self . undostack . beginMacro ( <STRING> ) <NEWLINE> for link in links : <NEWLINE> <INDENT> self . undostack . push ( type ( link . startIO ) . DeleteLinkCommand ( link . startIO ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Delete Stuff\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6fadbe5d39c74be69d6a85451d7a9d22": {
        "code_string": "db = str(db) if isinstance(db, int) else quote(db.encode('utf-8'))\n         path += '/rpc/cur_jump_back?DB=' + db\n",
        "code_toks_joined": "db = str ( db ) if isinstance ( db , int ) else quote ( db . encode ( <STRING> ) ) <NEWLINE> <INDENT> path += <STRING> + db <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'utf-8'",
                "'/rpc/cur_jump_back?DB='"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b639b96c5fa449f2a697b786ae404bab": {
        "code_string": "# partial burns and reputation bonus\n             # FIXME when did the staking bonus system start? 158?\n             if round_num >= 158:\n                 staking_bonus_perc = 0.05\n                 bonus_nmr_only = df['nmr_staked'] * staking_bonus_perc\n                 bonus_split = df['nmr_staking'] - df['nmr_staking'] / (1 + staking_bonus_perc)\n                 if 'nmr_returned' in df:\n                     df['nmr_returned'] = df['nmr_returned'].astype(float)\n                 if round_num == 158:\n                     df['nmr_bonus'] = bonus_nmr_only.where(df['usd_staking'].isna(), bonus_nmr_only)\n                     df['usd_bonus'] = df['usd_staking'] - df['usd_staking'] / (1 + staking_bonus_perc)\n                     df['usd_staking'] = df['usd_staking'] - df['usd_bonus']\n                     df['nmr_returned'] -= bonus_nmr_only\n                     df['nmr_staking'] -= bonus_nmr_only\n                 else:\n                     df['nmr_bonus'] = bonus_nmr_only\n                 if 'nmr_returned' in df:\n                     df['nmr_burned'] -= df['nmr_returned'].fillna(0)\n         return df\n     return None\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> if round_num >= 158 : <NEWLINE> <INDENT> staking_bonus_perc = 0.05 <NEWLINE> bonus_nmr_only = df [ <STRING> ] * staking_bonus_perc <NEWLINE> bonus_split = df [ <STRING> ] - df [ <STRING> ] / ( 1 + staking_bonus_perc ) <NEWLINE> if <STRING> in df : <NEWLINE> <INDENT> df [ <STRING> ] = df [ <STRING> ] . astype ( float ) <NEWLINE> <DEDENT> if round_num == 158 : <NEWLINE> <INDENT> df [ <STRING> ] = bonus_nmr_only . where ( df [ <STRING> ] . isna ( ) , bonus_nmr_only ) <NEWLINE> df [ <STRING> ] = df [ <STRING> ] - df [ <STRING> ] / ( 1 + staking_bonus_perc ) <NEWLINE> df [ <STRING> ] = df [ <STRING> ] - df [ <STRING> ] <NEWLINE> df [ <STRING> ] -= bonus_nmr_only <NEWLINE> df [ <STRING> ] -= bonus_nmr_only <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> df [ <STRING> ] = bonus_nmr_only <NEWLINE> <DEDENT> if <STRING> in df : <NEWLINE> <INDENT> df [ <STRING> ] -= df [ <STRING> ] . fillna ( 0 ) <NEWLINE> return df <NEWLINE> return None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# partial burns and reputation bonus",
                "# FIXME when did the staking bonus system start? 158?"
            ],
            "<STRING>": [
                "'nmr_staked'",
                "'nmr_staking'",
                "'nmr_staking'",
                "'nmr_returned'",
                "'nmr_returned'",
                "'nmr_returned'",
                "'nmr_bonus'",
                "'usd_staking'",
                "'usd_bonus'",
                "'usd_staking'",
                "'usd_staking'",
                "'usd_staking'",
                "'usd_staking'",
                "'usd_bonus'",
                "'nmr_returned'",
                "'nmr_staking'",
                "'nmr_bonus'",
                "'nmr_returned'",
                "'nmr_burned'",
                "'nmr_returned'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "71f7c27d25e94f5abcb960a778ea1ae1": {
        "code_string": "def get_comment(func):  # type: (Any) -> Optional[Tuple[Dict[str, str], str]]\n     if not inspect.isfunction(func) or not inspect.ismethod(func):\n         # Classes should be handled, but are not yet...\n         # Handling them would involve determining if they use __new__ or __init__\n         # and using that as the function itself.\n         return None\n",
        "code_toks_joined": "def get_comment ( func ) : <COMMENT> <NEWLINE> <INDENT> if not inspect . isfunction ( func ) or not inspect . ismethod ( func ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# type: (Any) -> Optional[Tuple[Dict[str, str], str]]",
                "# Classes should be handled, but are not yet...",
                "# Handling them would involve determining if they use __new__ or __init__",
                "# and using that as the function itself."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bbb27b189ee845ada22ab1dfac4ef024": {
        "code_string": "def parse_docstring(func):  # type: (Any) -> Optional[Tuple[Dict[str, str], str]]\n     \"\"\" Parse out typing information from docstring \"\"\"\n     if not inspect.isfunction(func) or not inspect.ismethod(func):\n         # Classes should be handled, but are not yet...\n         # Handling them would involve determining if they use __new__ or __init__\n         # and using that as the function itself.\n         return None\n",
        "code_toks_joined": "def parse_docstring ( func ) : <COMMENT> <NEWLINE> <INDENT> <STRING> <NEWLINE> if not inspect . isfunction ( func ) or not inspect . ismethod ( func ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# type: (Any) -> Optional[Tuple[Dict[str, str], str]]",
                "# Classes should be handled, but are not yet...",
                "# Handling them would involve determining if they use __new__ or __init__",
                "# and using that as the function itself."
            ],
            "<STRING>": [
                "\"\"\" Parse out typing information from docstring \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a024b6ca325a4da3b0e0921d70094b35": {
        "code_string": "def visit_BinOp(self, node):\n         node  = self.generic_visit(node)\n         left  = node.left\n         right = node.right\n         if all(isinstance(value, ast.Num) for value in (left, right)):\n             if type(node.op) in self._operators:\n                 val  = self._operators[type(node.op)](left.n, right.n)\n                 node = ast.copy_location(ast.Num(n = val), node)\n         elif all(isinstance(value, ast.Str) for value in (left, right)):\n            if isinstance(node.op, ast.Add):\n                 val  = left.s + left.s\n                 node = ast.copy_location(ast.Str(s = val), node)\n         return node\n",
        "code_toks_joined": "def visit_BinOp ( self , node ) : <NEWLINE> <INDENT> node = self . generic_visit ( node ) <NEWLINE> left = node . left <NEWLINE> right = node . right <NEWLINE> if all ( isinstance ( value , ast . Num ) for value in ( left , right ) ) : <NEWLINE> <INDENT> if type ( node . op ) in self . _operators : <NEWLINE> <INDENT> val = self . _operators [ type ( node . op ) ] ( left . n , right . n ) <NEWLINE> node = ast . copy_location ( ast . Num ( n = val ) , node ) <NEWLINE> <DEDENT> <DEDENT> elif all ( isinstance ( value , ast . Str ) for value in ( left , right ) ) : <NEWLINE> <INDENT> if isinstance ( node . op , ast . Add ) : <NEWLINE> <INDENT> val = left . s + left . s <NEWLINE> node = ast . copy_location ( ast . Str ( s = val ) , node ) <NEWLINE> <DEDENT> <DEDENT> return node <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ab988197b6ed49e7af7c0c36a1c29524": {
        "code_string": "self.__addnode(portname)\n",
        "code_toks_joined": "self . __addnode ( portname ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e3386b6ab6c2436b80711e71f3cabd37": {
        "code_string": "if len(rank) == 0:  # Check values not found\n                 rank = len(query[i])\n             else:\n                 rank = rank[0]\n             all_rr.append(rank)\n",
        "code_toks_joined": "if len ( rank ) == 0 : <COMMENT> <NEWLINE> <INDENT> rank = len ( query [ i ] ) <NEWLINE> else : <NEWLINE> rank = rank [ 0 ] <NEWLINE> all_rr . append ( rank ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check values not found"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ca31d9fcbca94174829e120735e78e05": {
        "code_string": "for step in steps:\n             learner = SVDPlusPlusLearner(K=2, steps=step, alpha=0.007,\n                                          random_state=42, verbose=False)\n             recommender = learner(data)\n             objectives.append(\n                 recommender.compute_objective(data=data, P=recommender.P,\n                                               Q=recommender.Q,\n                                               Y=recommender.Y,\n                                               bias=learner.bias,\n                                               beta=learner.beta))\n",
        "code_toks_joined": "for step in steps : <NEWLINE> <INDENT> learner = SVDPlusPlusLearner ( K = 2 , steps = step , alpha = 0.007 , <NEWLINE> <INDENT> random_state = 42 , verbose = False ) <NEWLINE> <DEDENT> recommender = learner ( data ) <NEWLINE> objectives . append ( <NEWLINE> <INDENT> recommender . compute_objective ( data = data , P = recommender . P , <NEWLINE> <INDENT> Q = recommender . Q , <NEWLINE> Y = recommender . Y , <NEWLINE> bias = learner . bias , <NEWLINE> beta = learner . beta ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a46e2f4e883c42b693bb7b1f1a6deafe": {
        "code_string": "if not is_system:\n             extension = extensionRegistry.get_extension(ext_name)\n             module_directory = extension.module_directory\n             # Register Silva Views directory\n             if os.path.exists(os.path.join(module_directory, 'views')):\n                 registerDirectory(module_directory, 'views')\n",
        "code_toks_joined": "if not is_system : <NEWLINE> <INDENT> extension = extensionRegistry . get_extension ( ext_name ) <NEWLINE> module_directory = extension . module_directory <NEWLINE> <COMMENT> <NL> if os . path . exists ( os . path . join ( module_directory , <STRING> ) ) : <NEWLINE> <INDENT> registerDirectory ( module_directory , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Register Silva Views directory"
            ],
            "<STRING>": [
                "'views'",
                "'views'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7e4d8c5011324c44bd0d5fb20002d42e": {
        "code_string": "# log inputs\n     pique.msg( logfile, '  -> IP file  : ' + ipfile   )\n     pique.msg( logfile, '  -> BG file  : ' + ipfile   )\n     pique.msg( logfile, '  -> map file : ' + mapfile  )\n     pique.msg( logfile, '  -> alpha    : ' + str(alpha)    )\n     pique.msg( logfile, '  -> l_thresh : ' + str(l_thresh) )\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> pique . msg ( logfile , <STRING> + ipfile ) <NEWLINE> pique . msg ( logfile , <STRING> + ipfile ) <NEWLINE> pique . msg ( logfile , <STRING> + mapfile ) <NEWLINE> pique . msg ( logfile , <STRING> + str ( alpha ) ) <NEWLINE> pique . msg ( logfile , <STRING> + str ( l_thresh ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# log inputs"
            ],
            "<STRING>": [
                "'  -> IP file  : '",
                "'  -> BG file  : '",
                "'  -> map file : '",
                "'  -> alpha    : '",
                "'  -> l_thresh : '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2a819c6576eb4c27b2d477edb702ff11": {
        "code_string": "def has_failed(self):\n         \"\"\"Return whether the operation failed, ie. completed but with an error code.\n         Cancelled and discarded ops are considered failed.\n         This assumes that this order has been .wait()ed upon\"\"\"\n         return self._result is True\n",
        "code_toks_joined": "def has_failed ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return self . _result is True <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return whether the operation failed, ie. completed but with an error code.\n         Cancelled and discarded ops are considered failed.\n         This assumes that this order has been .wait()ed upon\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f4306ebbf8144e15a3cc9e966288a43e": {
        "code_string": "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n         start_at = monotonic.monotonic()\n         while True:\n             try:\n                 sock.connect(\n                     (self.node_definition.host, self.node_definition.port))\n             except socket.error as e:\n                 time.sleep(0.5)  # Connection refused? Very bad things?\n                 if monotonic.monotonic() - start_at < timeout:\n                     raise ConnectionDead()\n             else:\n                 break\n",
        "code_toks_joined": "sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) <NEWLINE> <INDENT> start_at = monotonic . monotonic ( ) <NEWLINE> while True : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> sock . connect ( <NEWLINE> <INDENT> ( self . node_definition . host , self . node_definition . port ) ) <NEWLINE> <DEDENT> <DEDENT> except socket . error as e : <NEWLINE> <INDENT> time . sleep ( 0.5 ) <COMMENT> <NEWLINE> if monotonic . monotonic ( ) - start_at < timeout : <NEWLINE> <INDENT> raise ConnectionDead ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Connection refused? Very bad things?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a91ac004b29640319d5c2c6c493da394": {
        "code_string": "def span_finished(self):\n         if self.processing_span is None:\n             self.processing_span.finish()\n             self.processing_span = None\n",
        "code_toks_joined": "def span_finished ( self ) : <NEWLINE> <INDENT> if self . processing_span is None : <NEWLINE> <INDENT> self . processing_span . finish ( ) <NEWLINE> self . processing_span = None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7a4a5933dedb4da69733cb5e3c88ee40": {
        "code_string": "while True:\n             # if flag backed to False, end this handler because it means receiver side client disconnected\n             if remote_stdout_connected == False or file_transfer_mode == False:\n                 # clear bufferd data\n                 if sender_fifo_q.empty() == False:\n                     print(\"reset sender_fifo_q because it is not empty\")\n                     sender_fifo_q = asyncio.Queue()\n                 #return\n",
        "code_toks_joined": "while True : <NEWLINE> <COMMENT> <NL> <INDENT> if remote_stdout_connected == False or file_transfer_mode == False : <NEWLINE> <COMMENT> <NL> <INDENT> if sender_fifo_q . empty ( ) == False : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sender_fifo_q = asyncio . Queue ( ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# if flag backed to False, end this handler because it means receiver side client disconnected",
                "# clear bufferd data",
                "#return"
            ],
            "<STRING>": [
                "\"reset sender_fifo_q because it is not empty\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7846db17bcea4148aea2d3449e96eb9a": {
        "code_string": "def show_ddt(request):\n         if request.user.is_authenticated:\n             if request.path in ignored:\n                 return False\n         return True\n",
        "code_toks_joined": "def show_ddt ( request ) : <NEWLINE> <INDENT> if request . user . is_authenticated : <NEWLINE> <INDENT> if request . path in ignored : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4e23d6d1a28e4e68b17a96adcb176e92": {
        "code_string": "def show_ddt(request):\n         if request.user.is_authenticated:\n             if request.path in ignored:\n                 return False\n         return False\n",
        "code_toks_joined": "def show_ddt ( request ) : <NEWLINE> <INDENT> if request . user . is_authenticated : <NEWLINE> <INDENT> if request . path in ignored : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dfd8b874fe874425bfb20cadb7c47660": {
        "code_string": "N = np.shape(data_frame)[1]\n             for j in range(N):\n                 for k in range(N):\n                     if i != k:\n                         pairgrid.axes[i, k].spines['right'].set_visible(True)\n                         pairgrid.axes[i, k].spines['top'].set_visible(True) \n                     else:                   \n                         sns.despine(ax = pairgrid.axes[i, k])\n",
        "code_toks_joined": "N = np . shape ( data_frame ) [ 1 ] <NEWLINE> <INDENT> for j in range ( N ) : <NEWLINE> <INDENT> for k in range ( N ) : <NEWLINE> <INDENT> if i != k : <NEWLINE> <INDENT> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sns . despine ( ax = pairgrid . axes [ i , k ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'right'",
                "'top'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1cd636998c3b46ae83248158e1f408e6": {
        "code_string": "N = np.shape(data_frame)[1]\n             for i in range(N):\n                 for j in range(N):\n                     for k in range(N):\n                         if j != k:\n                             pairgrid.axes[i, k].spines['right'].set_visible(True)\n                             pairgrid.axes[i, k].spines['top'].set_visible(True) \n                         else:                   \n                             sns.despine(ax = pairgrid.axes[i, k])\n",
        "code_toks_joined": "N = np . shape ( data_frame ) [ 1 ] <NEWLINE> <INDENT> for i in range ( N ) : <NEWLINE> <INDENT> for j in range ( N ) : <NEWLINE> <INDENT> for k in range ( N ) : <NEWLINE> <INDENT> if j != k : <NEWLINE> <INDENT> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sns . despine ( ax = pairgrid . axes [ i , k ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'right'",
                "'top'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "08cca28138104ae6a8b656e95eb8013b": {
        "code_string": "y = 0\n \t\t\tif p.y < 0:\n \t\t\t\ty = 0\n \t\t\telif p.y > rect.height:\n \t\t\t\tx = rect.height\n \t\t\telse:\n \t\t\t\ty = p.y\n",
        "code_toks_joined": "y = 0 <NEWLINE> <INDENT> if p . y < 0 : <NEWLINE> <INDENT> y = 0 <NEWLINE> <DEDENT> elif p . y > rect . height : <NEWLINE> <INDENT> x = rect . height <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> y = p . y <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a66e3dba84eb455db74f8ee7a0aec995": {
        "code_string": "if self._statistics is not None:\n \t\t\t\tself._statistics.output_path = self._statistics_dir + os.path.sep + \"statistics.json\"\n \t\t\t\tself._statistics.generate(d)\n \t\t\t\tself._statistics.save()\n",
        "code_toks_joined": "if self . _statistics is not None : <NEWLINE> <INDENT> self . _statistics . output_path = self . _statistics_dir + os . path . sep + <STRING> <NEWLINE> self . _statistics . generate ( d ) <NEWLINE> self . _statistics . save ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"statistics.json\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4d7cf979b42b4c73b54608137a8cb6cb": {
        "code_string": "def create_real_first_image(self, path=\"data/testimg.fits\"):\n         # Put a real fits image on the first source, first observation\n         apcor_str = \"4 15   0.19   0.01\"\n         with open(self.get_abs_path(path), \"rb\") as fh:\n             self.first_image = DownloadedFitsImage(fh.read(), apcor_str, Mock(), in_memory=True)\n             first_reading = self.model.get_current_workunit().get_sources()[0].get_readings()[0]\n             self.model._on_image_loaded(first_reading, self.first_image)\n",
        "code_toks_joined": "def create_real_first_image ( self , path = <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> apcor_str = <STRING> <NEWLINE> with open ( self . get_abs_path ( path ) , <STRING> ) as fh : <NEWLINE> <INDENT> self . first_image = DownloadedFitsImage ( fh . read ( ) , apcor_str , Mock ( ) , in_memory = True ) <NEWLINE> first_reading = self . model . get_current_workunit ( ) . get_sources ( ) [ 0 ] . get_readings ( ) [ 0 ] <NEWLINE> self . model . _on_image_loaded ( first_reading , self . first_image ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"data/testimg.fits\"",
                "\"4 15   0.19   0.01\"",
                "\"rb\""
            ],
            "<COMMENT>": [
                "# Put a real fits image on the first source, first observation"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a4bc6686df7d41279bcd74cd69dcc969": {
        "code_string": "if method_def in type_def.CANCEL_MAPPING:\n             meta.cancellable = True\n",
        "code_toks_joined": "if method_def in type_def . CANCEL_MAPPING : <NEWLINE> <INDENT> meta . cancellable = True <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3dd3b3b543714f0ba2a4c4fadf9eecf2": {
        "code_string": "related_title = anime_page.find('h2', text=u'Related Anime')\n     if related_title:\n       related_elt = related_title.parent\n       utilities.extract_tags(related_elt.find_all('h2'))\n       related = {}\n       for link in related_elt.find_all('a'):\n         curr_elt = link.previous_sibling\n         if curr_elt is None:\n           # we've reached the end of the list.\n           break\n         related_type = None\n         while True:\n           if not curr_elt:\n             raise MalformedAnimePageError(self, html, message=\"Prematurely reached end of related anime listing\")\n           if isinstance(curr_elt, bs4.NavigableString):\n             type_match = re.match('(?P<type>[a-zA-Z\\ \\-]+):', curr_elt)\n             if type_match:\n               related_type = type_match.group('type')\n               break\n           curr_elt = curr_elt.previous_sibling\n         # parse link: may be manga or anime.\n         href_parts = link.get('href').split('/')\n         title = link.text\n         obj_id = int(href_parts[4])\n         non_title_parts = href_parts[:5]\n         if 'manga' in non_title_parts:\n           new_obj = self.session.manga(obj_id).set({'title': title})\n         elif 'anime' in non_title_parts:\n           new_obj = self.session.anime(obj_id).set({'title': title})\n         else:\n           raise MalformedAnimePageError(self, link, message=\"Related thing is of unknown type\")\n         if related_type not in related:\n           related[related_type] = [new_obj]\n         else:\n           related[related_type].append(new_obj)\n       anime_info['related'] = related\n     else:\n       anime_info['related'] = None\n     return anime_info\n",
        "code_toks_joined": "related_title = anime_page . find ( <STRING> , text = <STRING> ) <NEWLINE> <INDENT> if related_title : <NEWLINE> <INDENT> related_elt = related_title . parent <NEWLINE> utilities . extract_tags ( related_elt . find_all ( <STRING> ) ) <NEWLINE> related = { } <NEWLINE> for link in related_elt . find_all ( <STRING> ) : <NEWLINE> <INDENT> curr_elt = link . previous_sibling <NEWLINE> if curr_elt is None : <NEWLINE> <COMMENT> <NL> <INDENT> break <NEWLINE> <DEDENT> related_type = None <NEWLINE> while True : <NEWLINE> <INDENT> if not curr_elt : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , html , message = <STRING> ) <NEWLINE> <DEDENT> if isinstance ( curr_elt , bs4 . NavigableString ) : <NEWLINE> <INDENT> type_match = re . match ( <STRING> , curr_elt ) <NEWLINE> if type_match : <NEWLINE> <INDENT> related_type = type_match . group ( <STRING> ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> curr_elt = curr_elt . previous_sibling <NEWLINE> <COMMENT> <NL> <DEDENT> href_parts = link . get ( <STRING> ) . split ( <STRING> ) <NEWLINE> title = link . text <NEWLINE> obj_id = int ( href_parts [ 4 ] ) <NEWLINE> non_title_parts = href_parts [ : 5 ] <NEWLINE> if <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . manga ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> elif <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . anime ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , link , message = <STRING> ) <NEWLINE> <DEDENT> if related_type not in related : <NEWLINE> <INDENT> related [ related_type ] = [ new_obj ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> related [ related_type ] . append ( new_obj ) <NEWLINE> <DEDENT> <DEDENT> anime_info [ <STRING> ] = related <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> anime_info [ <STRING> ] = None <NEWLINE> <DEDENT> return anime_info <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'h2'",
                "u'Related Anime'",
                "'h2'",
                "'a'",
                "\"Prematurely reached end of related anime listing\"",
                "'(?P<type>[a-zA-Z\\ \\-]+):'",
                "'type'",
                "'href'",
                "'/'",
                "'manga'",
                "'title'",
                "'anime'",
                "'title'",
                "\"Related thing is of unknown type\"",
                "'related'",
                "'related'"
            ],
            "<COMMENT>": [
                "# we've reached the end of the list.",
                "# parse link: may be manga or anime."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aa3ff3e1a4754d4e8b3aa5adb12da3a6": {
        "code_string": "def main():\n     if len(sys.argv) <= 2:\n         usage(sys.argv[0])\n         sys.exit(1)\n     duration = int(sys.argv[1])\n     logpath = LOGPATH\n     if len(sys.argv) >= 3:\n         logpath = sys.argv[2]\n     loader = TailLoader(logpath, duration)\n     censor = Censor()\n     eater = MailLogEater()\n     for rawline in loader.readlines():\n         line = censor.censor(rawline)\n         eater.eat(line)\n     print(eater)\n",
        "code_toks_joined": "def main ( ) : <NEWLINE> <INDENT> if len ( sys . argv ) <= 2 : <NEWLINE> <INDENT> usage ( sys . argv [ 0 ] ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> duration = int ( sys . argv [ 1 ] ) <NEWLINE> logpath = LOGPATH <NEWLINE> if len ( sys . argv ) >= 3 : <NEWLINE> <INDENT> logpath = sys . argv [ 2 ] <NEWLINE> <DEDENT> loader = TailLoader ( logpath , duration ) <NEWLINE> censor = Censor ( ) <NEWLINE> eater = MailLogEater ( ) <NEWLINE> for rawline in loader . readlines ( ) : <NEWLINE> <INDENT> line = censor . censor ( rawline ) <NEWLINE> eater . eat ( line ) <NEWLINE> <DEDENT> print ( eater ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ed3979cbb2fa4ee9ac6122fe5836ee80": {
        "code_string": "shape = ()\n         for i, (left_element, right_element) in enumerate(zip(node.left_node.shape, node.right_node.shape)):\n             if is_symbolic_element(left_element) and is_symbolic_element(right_element): # both are symbolic\n                 conditions.append(BinaryNode(MOANodeTypes.EQUAL, (), left_element, right_element))\n                 shape = shape + (left_element,)\n             elif is_symbolic_element(left_element): # only left is symbolic\n                 array_name = generate_unique_array_name(symbol_table)\n                 symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n                 conditions.append(BinaryNode(MOANodeTypes.EQUAL, (), left_element, ArrayNode(MOANodeTypes.ARRAY, (), array_name)))\n                 shape = shape + (right_element,)\n             elif is_symbolic_element(right_element): # only right is symbolic\n                 array_name = generate_unique_array_name(symbol_table)\n                 symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n                 conditions.append(BinaryNode(MOANodeTypes.EQUAL, (), ArrayNode(MOANodeTypes.ARRAY, (), array_name), right_element))\n                 shape = shape + (left_element,)\n             else: # neither symbolic\n                 if left_element != right_element:\n                     raise MOAShapeException(f'(+,-,/,*) requires shapes to match elements #{i} left {left_element} != right {right_element}')\n                 shape = shape + (left_element,)\n",
        "code_toks_joined": "shape = ( ) <NEWLINE> <INDENT> for i , ( left_element , right_element ) in enumerate ( zip ( node . left_node . shape , node . right_node . shape ) ) : <NEWLINE> <INDENT> if is_symbolic_element ( left_element ) and is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> conditions . append ( BinaryNode ( MOANodeTypes . EQUAL , ( ) , left_element , right_element ) ) <NEWLINE> shape = shape + ( left_element , ) <NEWLINE> <DEDENT> elif is_symbolic_element ( left_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . EQUAL , ( ) , left_element , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) ) ) <NEWLINE> shape = shape + ( right_element , ) <NEWLINE> <DEDENT> elif is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . EQUAL , ( ) , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) , right_element ) ) <NEWLINE> shape = shape + ( left_element , ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> if left_element != right_element : <NEWLINE> <INDENT> raise MOAShapeException ( <STRING> ) <NEWLINE> <DEDENT> shape = shape + ( left_element , ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# both are symbolic",
                "# only left is symbolic",
                "# only right is symbolic",
                "# neither symbolic"
            ],
            "<STRING>": [
                "f'(+,-,/,*) requires shapes to match elements #{i} left {left_element} != right {right_element}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "04a4d0b16ea2430e909524b0871dfb83": {
        "code_string": "conditions = []\n     for i, (left_element, right_element) in enumerate(zip(left_symbol_node.value, node.right_node.shape)):\n         if is_symbolic_element(left_element) and is_symbolic_element(right_element): # both are symbolic\n             conditions.append(BinaryNode(MOANodeTypes.LESSTHAN, (), left_element, right_element))\n         elif is_symbolic_element(left_element): # only left is symbolic\n             array_name = generate_unique_array_name(symbol_table)\n             symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n             conditions.append(BinaryNode(MOANodeTypes.LESSTHAN, (), left_element, ArrayNode(MOANodeTypes.ARRAY, (), array_name)))\n         elif is_symbolic_element(right_element): # only right is symbolic\n             array_name = generate_unique_array_name(symbol_table)\n             symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n             conditions.append(BinaryNode(MOANodeTypes.LESSTHAN, (), ArrayNode(MOANodeTypes.ARRAY, (), array_name), right_element))\n         else: # neither symbolic\n             if left_element >= right_element:\n                 raise MOAShapeException(f'PSI requires elements #{i} left {left_element} < right {right_element}')\n",
        "code_toks_joined": "conditions = [ ] <NEWLINE> <INDENT> for i , ( left_element , right_element ) in enumerate ( zip ( left_symbol_node . value , node . right_node . shape ) ) : <NEWLINE> <INDENT> if is_symbolic_element ( left_element ) and is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> conditions . append ( BinaryNode ( MOANodeTypes . LESSTHAN , ( ) , left_element , right_element ) ) <NEWLINE> <DEDENT> elif is_symbolic_element ( left_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . LESSTHAN , ( ) , left_element , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) ) ) <NEWLINE> <DEDENT> elif is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . LESSTHAN , ( ) , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) , right_element ) ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> if left_element >= right_element : <NEWLINE> <INDENT> raise MOAShapeException ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# both are symbolic",
                "# only left is symbolic",
                "# only right is symbolic",
                "# neither symbolic"
            ],
            "<STRING>": [
                "f'PSI requires elements #{i} left {left_element} < right {right_element}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9a38577aedde4421a1f9660de5ac2f54": {
        "code_string": "def paint_path(self, graphicstate, stroke, fill, evenodd, path):\n         # Converts path to device coordinates and adds the path to the page\n         device_path = []\n         for segment in path:\n             coords = iter(segment[1:])\n             for x in coords:  #pylint: disable=C0103\n                 y = next(coords)  #pylint: disable=C0103\n                 device_path.append(\n                     (segment[0],)\n                     + pdfminer.utils.apply_matrix_pt(self.ctm, (x, y)))\n         self.page.add_shape(content.Shape(stroke, fill, evenodd, path))\n",
        "code_toks_joined": "def paint_path ( self , graphicstate , stroke , fill , evenodd , path ) : <NEWLINE> <COMMENT> <NL> <INDENT> device_path = [ ] <NEWLINE> for segment in path : <NEWLINE> <INDENT> coords = iter ( segment [ 1 : ] ) <NEWLINE> for x in coords : <COMMENT> <NEWLINE> <INDENT> y = next ( coords ) <COMMENT> <NEWLINE> device_path . append ( <NEWLINE> <INDENT> ( segment [ 0 ] , ) <NEWLINE> + pdfminer . utils . apply_matrix_pt ( self . ctm , ( x , y ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . page . add_shape ( content . Shape ( stroke , fill , evenodd , path ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Converts path to device coordinates and adds the path to the page",
                "#pylint: disable=C0103",
                "#pylint: disable=C0103"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "03e47b55010248a1b5389385d9b648f5": {
        "code_string": "# apply optimized bias to observed\n     log_corr_obs_matrices = [(log_obs_matrix.T - bias_factors).T - bias_factors\n                              for log_obs_matrix in log_exp_matrices]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> log_corr_obs_matrices = [ ( log_obs_matrix . T - bias_factors ) . T - bias_factors <NEWLINE> <INDENT> for log_obs_matrix in log_exp_matrices ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# apply optimized bias to observed"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a2fd3b126f964d269eda8e48dbb1efe0": {
        "code_string": "def run(parser, args, conn_config):\n     if args.sample_name is None:\n         args.sample_name = os.path.basename(args.kmer_file).split('.')[0]\n     mc = McDBG(conn_config=conn_config)\n     try:\n         colour = mc.add_sample(args.sample_name)\n         with open(args.kmer_file, 'r') as inf:\n             kmers = []\n             for i, line in enumerate(inf):\n                 kmer = line.strip()\n                 kmers.append(kmer)\n                 if i % 100000 == 0:\n                     mc.set_kmers(kmers, colour)\n                     kmers = []\n         mc.set_kmers(kmers, i)\n",
        "code_toks_joined": "def run ( parser , args , conn_config ) : <NEWLINE> <INDENT> if args . sample_name is None : <NEWLINE> <INDENT> args . sample_name = os . path . basename ( args . kmer_file ) . split ( <STRING> ) [ 0 ] <NEWLINE> <DEDENT> mc = McDBG ( conn_config = conn_config ) <NEWLINE> try : <NEWLINE> <INDENT> colour = mc . add_sample ( args . sample_name ) <NEWLINE> with open ( args . kmer_file , <STRING> ) as inf : <NEWLINE> <INDENT> kmers = [ ] <NEWLINE> for i , line in enumerate ( inf ) : <NEWLINE> <INDENT> kmer = line . strip ( ) <NEWLINE> kmers . append ( kmer ) <NEWLINE> if i % 100000 == 0 : <NEWLINE> <INDENT> mc . set_kmers ( kmers , colour ) <NEWLINE> kmers = [ ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> mc . set_kmers ( kmers , i ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.'",
                "'r'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dad3f73bfff4401ca6c1d6bd2f13ef31": {
        "code_string": "@hug.object.cli\n     @hug.object.get('/search', examples=\"seq=ACACAAACCATGGCCGGACGCAGCTTTCTGA\")\n     def search(self, seq: hug.types.text=None, fasta_file: hug.types.text=None, threshold: hug.types.float_number=1.0):\n         \"\"\"Returns samples that contain the searched sequence. \n         Use -f to search for sequence from fasta\"\"\"\n         if not seq or fasta_file:\n             return \"-s or -f must be provided\"\n         return search(seq=seq,\n                       fasta_file=fasta_file, threshold=threshold, conn_config=CONN_CONFIG)\n",
        "code_toks_joined": "@ hug . object . cli <NEWLINE> <INDENT> @ hug . object . get ( <STRING> , examples = <STRING> ) <NEWLINE> def search ( self , seq : hug . types . text = None , fasta_file : hug . types . text = None , threshold : hug . types . float_number = 1.0 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not seq or fasta_file : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> return search ( seq = seq , <NEWLINE> <INDENT> fasta_file = fasta_file , threshold = threshold , conn_config = CONN_CONFIG ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/search'",
                "\"seq=ACACAAACCATGGCCGGACGCAGCTTTCTGA\"",
                "\"\"\"Returns samples that contain the searched sequence. \n         Use -f to search for sequence from fasta\"\"\"",
                "\"-s or -f must be provided\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1a9d7739b91e4f51b81920eef806e254": {
        "code_string": "def build(bloomfilter_filepaths, samples, graph):\n     bloomfilters = []\n     for f in bloomfilter_filepaths:\n         bloomfilters.append(load_bloomfilter(f))\n     graph.build(bloomfilter_filepaths, samples)\n     return {'result': 'success'}\n",
        "code_toks_joined": "def build ( bloomfilter_filepaths , samples , graph ) : <NEWLINE> <INDENT> bloomfilters = [ ] <NEWLINE> for f in bloomfilter_filepaths : <NEWLINE> <INDENT> bloomfilters . append ( load_bloomfilter ( f ) ) <NEWLINE> <DEDENT> graph . build ( bloomfilter_filepaths , samples ) <NEWLINE> return { <STRING> : <STRING> } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'result'",
                "'success'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "93020e102525416abf281f75ba68a014": {
        "code_string": "def pull(self, block_size, overlap=0, pad=False):\n         if overlap and overlap >= block_size:\n             raise ValueError('overlap cannot be more than block_size')\n",
        "code_toks_joined": "def pull ( self , block_size , overlap = 0 , pad = False ) : <NEWLINE> <INDENT> if overlap and overlap >= block_size : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'overlap cannot be more than block_size'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7323cb4f7aae41679cbd0843c9c7b67a": {
        "code_string": "def __init__(self, title='Unknown', can_save=True,\n             has_config=True, widget=None):\n         # Title that will be displayed to the user\n         self.TITLE = title\n         # new entries can be saved\n         self.CAN_SAVE = can_save\n         # additional config options for database connection or fukebane(s)\n         self.HAS_CONFIG = can_save\n         # the widget for additional config\n         self.widget = widget\n",
        "code_toks_joined": "def __init__ ( self , title = <STRING> , can_save = True , <NEWLINE> <INDENT> has_config = True , widget = None ) : <NEWLINE> <COMMENT> <NL> self . TITLE = title <NEWLINE> <COMMENT> <NL> self . CAN_SAVE = can_save <NEWLINE> <COMMENT> <NL> self . HAS_CONFIG = can_save <NEWLINE> <COMMENT> <NL> self . widget = widget <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Unknown'"
            ],
            "<COMMENT>": [
                "# Title that will be displayed to the user",
                "# new entries can be saved",
                "# additional config options for database connection or fukebane(s)",
                "# the widget for additional config"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "430442c7d0ab4368be3528ff906a0c39": {
        "code_string": "def _poll(self):\n         for item in self.coins:\n             value = self.wrapper.handle(dict(self.config.items(item)))\n             if value:\n                 setattr(self, item, value)\n                 self.fields.add(value)\n",
        "code_toks_joined": "def _poll ( self ) : <NEWLINE> <INDENT> for item in self . coins : <NEWLINE> <INDENT> value = self . wrapper . handle ( dict ( self . config . items ( item ) ) ) <NEWLINE> if value : <NEWLINE> <INDENT> setattr ( self , item , value ) <NEWLINE> self . fields . add ( value ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dd79a3ce4be54d5e9edead7de4cdbf27": {
        "code_string": "# Separate key and values for params\n         params = {}\n         for paramstr in params_strings:\n             if '=' not in paramstr:\n                 raise ParseError(\"No '=' in line '{}'\".format(line))\n             pname, pvals = paramstr.split('=', 1)\n             params[pname] = pvals.split(',')\n         return cls(name, params, value)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> params = { } <NEWLINE> for paramstr in params_strings : <NEWLINE> <INDENT> if <STRING> not in paramstr : <NEWLINE> <INDENT> raise ParseError ( <STRING> . format ( line ) ) <NEWLINE> <DEDENT> pname , pvals = paramstr . split ( <STRING> , 1 ) <NEWLINE> params [ pname ] = pvals . split ( <STRING> ) <NEWLINE> <DEDENT> return cls ( name , params , value ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Separate key and values for params"
            ],
            "<STRING>": [
                "'='",
                "\"No '=' in line '{}'\"",
                "'='",
                "','"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "354aa484628e4d2bb439532dea341c64": {
        "code_string": "def get_message(self, code):\n         \"\"\"\n         \u83b7\u53d6\u9519\u8bef\u4ecb\u7ecd\n         :param code: \u9519\u8bef\u7801\n         :return:\n         \"\"\"\n         resource_code = self.map.get(code)\n         if not resource_code:\n             if resource_code != 0:\n                 warnings.warn('\u672a\u77e5\u9519\u8bef\u7801', DeprecationWarning)\n             return \"\"\n         return resource_code.get_message()\n",
        "code_toks_joined": "def get_message ( self , code ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> resource_code = self . map . get ( code ) <NEWLINE> if not resource_code : <NEWLINE> <INDENT> if resource_code != 0 : <NEWLINE> <INDENT> warnings . warn ( <STRING> , DeprecationWarning ) <NEWLINE> <DEDENT> return <STRING> <NEWLINE> <DEDENT> return resource_code . get_message ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         \u83b7\u53d6\u9519\u8bef\u4ecb\u7ecd\n         :param code: \u9519\u8bef\u7801\n         :return:\n         \"\"\"",
                "'\u672a\u77e5\u9519\u8bef\u7801'",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6c070325ddbc40e1bd1b9fb50fe97633": {
        "code_string": "def upload_aliyun_oss(folder):\n     if hasattr(settings, 'ALIYUN_OSS'):\n         raise Exception('\u672a\u914d\u7f6eoss')\n     AccessKeyId = settings.ALIYUN_OSS[\"AccessKeyId\"]\n     AccessKeySecret = settings.ALIYUN_OSS[\"AccessKeySecret\"]\n     Endpoint = settings.ALIYUN_OSS[\"Endpoint\"]\n     BucketName = settings.ALIYUN_OSS[\"BucketName\"]\n",
        "code_toks_joined": "def upload_aliyun_oss ( folder ) : <NEWLINE> <INDENT> if hasattr ( settings , <STRING> ) : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> AccessKeyId = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> AccessKeySecret = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> Endpoint = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> BucketName = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ALIYUN_OSS'",
                "'\u672a\u914d\u7f6eoss'",
                "\"AccessKeyId\"",
                "\"AccessKeySecret\"",
                "\"Endpoint\"",
                "\"BucketName\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0930fca968fc4872b5e858d6caef5044": {
        "code_string": "# We start steps at the minimum time value, and do as many steps as are\n         # necessary to get past the maximum time value with the selected step\n         # size\n         first_times = [i[0][0] for i in measurements.values() if i and i[0]]\n         last_times = [i[-1][0] for i in measurements.values() if i and i[-1]]\n         if not (first_times or last_times):\n             raise RuntimeError(u\"No data to aggregate. Run with -l and check log file to investigate.\")\n         t_0 = min(first_times)\n         t_max = max(last_times)\n         steps = int(math.ceil((t_max-t_0)/self.step))\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> first_times = [ i [ 0 ] [ 0 ] for i in measurements . values ( ) if i and i [ 0 ] ] <NEWLINE> last_times = [ i [ - 1 ] [ 0 ] for i in measurements . values ( ) if i and i [ - 1 ] ] <NEWLINE> if not ( first_times or last_times ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> t_0 = min ( first_times ) <NEWLINE> t_max = max ( last_times ) <NEWLINE> steps = int ( math . ceil ( ( t_max - t_0 ) / self . step ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# We start steps at the minimum time value, and do as many steps as are",
                "# necessary to get past the maximum time value with the selected step",
                "# size"
            ],
            "<STRING>": [
                "u\"No data to aggregate. Run with -l and check log file to investigate.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4e7546abda394c61a5ae74bc0ac8a00a": {
        "code_string": "def disable_log(self, val=None):\n         if val is not None and val != self.settings.LOG_SCALE:\n             self.settings.LOG_SCALE = not val\n             self.update()\n         return not self.settings.LOG_SCALE\n",
        "code_toks_joined": "def disable_log ( self , val = None ) : <NEWLINE> <INDENT> if val is not None and val != self . settings . LOG_SCALE : <NEWLINE> <INDENT> self . settings . LOG_SCALE = not val <NEWLINE> self . update ( ) <NEWLINE> <DEDENT> return not self . settings . LOG_SCALE <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2845e18659ca4f01abe0812f07214a1e": {
        "code_string": "yield batch, settings\n",
        "code_toks_joined": "yield batch , settings <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5400b925bd514362b957190bd37e0a82": {
        "code_string": "parts = [s.split(sep) for s in strings]\n     np = [p for p in zip(*parts) if len(set(p)) > 1]\n",
        "code_toks_joined": "parts = [ s . split ( sep ) for s in strings ] <NEWLINE> <INDENT> np = [ p for p in zip ( * parts ) if len ( set ( p ) ) > 1 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "45126b1a2b354187905275063675da1c": {
        "code_string": "if end < 0:\n                 end += results.meta(\"TOTAL_LENGTH\")\n",
        "code_toks_joined": "if end < 0 : <NEWLINE> <INDENT> end += results . meta ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"TOTAL_LENGTH\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "23cbf0d515e747128b0671b0ba43a33c": {
        "code_string": "assert response_result.errors == []\n         assert response_result.data == data\n",
        "code_toks_joined": "assert response_result . errors == [ ] <NEWLINE> <INDENT> assert response_result . data == data <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "23071908bb5f469f9f83354def538c93": {
        "code_string": "if tag == 0x01:\n                 read_assert_tag(f, 71)\n                 self.domain_type = read_s32le(f)\n             elif tag == 0x02:\n                 mob_id = mobid.MobID()\n                 read_assert_tag(f, 65)\n                 length = read_s32le(f)\n                 assert length == 12\n                 mob_id.SMPTELabel = [read_byte(f) for i in range(12)]\n                 read_assert_tag(f, 68)\n                 mob_id.length = read_byte(f)\n                 read_assert_tag(f, 68)\n                 mob_id.instanceHigh = read_byte(f)\n                 read_assert_tag(f, 68)\n                 mob_id.instanceMid = read_byte(f)\n                 read_assert_tag(f, 68)\n                 mob_id.instanceLow = read_byte(f)\n                 read_assert_tag(f, 72)\n                 mob_id.Data1 = read_u32le(f)\n                 read_assert_tag(f, 70)\n                 mob_id.Data2 = read_u16le(f)\n                 read_assert_tag(f, 70)\n                 mob_id.Data3 = read_u16le(f)\n                 read_assert_tag(f, 65)\n                 length = read_s32le(f)\n                 assert length == 8\n                 mob_id.Data4 = [read_byte(f) for i in range(8)]\n                 self.mob_id = mob_id\n             elif tag == 0x03:\n                 read_assert_tag(f, 76)\n                 self.last_known_volume_utf8 = read_string(length, 'utf-8')\n             else:\n                 raise ValueError(\"%s: unknown ext tag 0x%02X %d\" % (str(self.class_id), tag,tag))\n",
        "code_toks_joined": "if tag == 0x01 : <NEWLINE> <INDENT> read_assert_tag ( f , 71 ) <NEWLINE> self . domain_type = read_s32le ( f ) <NEWLINE> elif tag == 0x02 : <NEWLINE> mob_id = mobid . MobID ( ) <NEWLINE> read_assert_tag ( f , 65 ) <NEWLINE> length = read_s32le ( f ) <NEWLINE> assert length == 12 <NEWLINE> mob_id . SMPTELabel = [ read_byte ( f ) for i in range ( 12 ) ] <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . length = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . instanceHigh = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . instanceMid = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . instanceLow = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 72 ) <NEWLINE> mob_id . Data1 = read_u32le ( f ) <NEWLINE> read_assert_tag ( f , 70 ) <NEWLINE> mob_id . Data2 = read_u16le ( f ) <NEWLINE> read_assert_tag ( f , 70 ) <NEWLINE> mob_id . Data3 = read_u16le ( f ) <NEWLINE> read_assert_tag ( f , 65 ) <NEWLINE> length = read_s32le ( f ) <NEWLINE> assert length == 8 <NEWLINE> mob_id . Data4 = [ read_byte ( f ) for i in range ( 8 ) ] <NEWLINE> self . mob_id = mob_id <NEWLINE> elif tag == 0x03 : <NEWLINE> read_assert_tag ( f , 76 ) <NEWLINE> self . last_known_volume_utf8 = read_string ( length , <STRING> ) <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> % ( str ( self . class_id ) , tag , tag ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'utf-8'",
                "\"%s: unknown ext tag 0x%02X %d\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "89f2928f7738457092e900aa60cbaa51": {
        "code_string": "def setup_country(self):\n         \"\"\"Set country based on known information\n         \"\"\"\n         if not hasattr(self, 'exchange') and self.exchange is None:\n             return\n         exch_country = find_country_for_exchange(self.exchange)\n         if hasattr(self, 'country') and self.country:\n             if self.country == exch_country:\n                 return\n         self.country = exch_country\n",
        "code_toks_joined": "def setup_country ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not hasattr ( self , <STRING> ) and self . exchange is None : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> exch_country = find_country_for_exchange ( self . exchange ) <NEWLINE> if hasattr ( self , <STRING> ) and self . country : <NEWLINE> <INDENT> if self . country == exch_country : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT> self . country = exch_country <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Set country based on known information\n         \"\"\"",
                "'exchange'",
                "'country'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e8a11c0051744340aeb5140a354a48c5": {
        "code_string": "p = float(opts['<probability>'])\n     inp = opts['<input>']\n     out = opts['<output>']\n     (m, _) = probe(inp)\n     if opts['-D'] is not None:\n         if opts['-S'] is not None:\n             S = long(opts['-S'])\n             random.seed(S)\n         if m['type'] == 'k-mer set':\n             (m, xs) = kset.read(inp)\n             K = m['K']\n             kset.write(K, sampleR(p, xs), out, m)\n         else:\n             (m, xs) = kfset.read(inp)\n             K = m['K']\n             kfset.write(K, sampleR(p, xs), out, m)\n     else:\n         S = 0\n         if opts['-S'] is not None:\n             S = long(opts['-S'])\n         if m['type'] == 'k-mer set':\n             (m, xs) = kset.read(inp)\n             K = m['K']\n             kset.write(K, sampleD(p, S, xs, lambda x: x), out, m)\n         else:\n             (m, xs) = kfset.read(inp)\n             K = m['K']\n             kfset.write(K, sampleD(p, S, xs, lambda x: x[0]), out, m)\n",
        "code_toks_joined": "p = float ( opts [ <STRING> ] ) <NEWLINE> <INDENT> inp = opts [ <STRING> ] <NEWLINE> out = opts [ <STRING> ] <NEWLINE> ( m , _ ) = probe ( inp ) <NEWLINE> if opts [ <STRING> ] is not None : <NEWLINE> <INDENT> if opts [ <STRING> ] is not None : <NEWLINE> <INDENT> S = long ( opts [ <STRING> ] ) <NEWLINE> random . seed ( S ) <NEWLINE> <DEDENT> if m [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> ( m , xs ) = kset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kset . write ( K , sampleR ( p , xs ) , out , m ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ( m , xs ) = kfset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kfset . write ( K , sampleR ( p , xs ) , out , m ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> S = 0 <NEWLINE> if opts [ <STRING> ] is not None : <NEWLINE> <INDENT> S = long ( opts [ <STRING> ] ) <NEWLINE> <DEDENT> if m [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> ( m , xs ) = kset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kset . write ( K , sampleD ( p , S , xs , lambda x : x ) , out , m ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ( m , xs ) = kfset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kfset . write ( K , sampleD ( p , S , xs , lambda x : x [ 0 ] ) , out , m ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'<probability>'",
                "'<input>'",
                "'<output>'",
                "'-D'",
                "'-S'",
                "'-S'",
                "'type'",
                "'k-mer set'",
                "'K'",
                "'K'",
                "'-S'",
                "'-S'",
                "'type'",
                "'k-mer set'",
                "'K'",
                "'K'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "47233dc5f2234859b8504b7efbe71f7b": {
        "code_string": "def get_dynamic_routes_instances(self, viewset, route, dynamic_routes):\n         dynamic_routes_instances = []\n         for httpmethods, methodname, endpoint, is_for_list in dynamic_routes:\n             initkwargs = route.initkwargs.copy()\n             initkwargs.update(getattr(viewset, methodname).kwargs)\n             dynamic_routes_instances.append(Route(\n                 url=replace_methodname(route.url, endpoint),\n                 mapping=dict((httpmethod, methodname) for httpmethod in httpmethods),\n                 name=replace_methodname(route.name, methodname),\n                 initkwargs=initkwargs,\n             ))\n         return dynamic_routes_instances\n",
        "code_toks_joined": "def get_dynamic_routes_instances ( self , viewset , route , dynamic_routes ) : <NEWLINE> <INDENT> dynamic_routes_instances = [ ] <NEWLINE> for httpmethods , methodname , endpoint , is_for_list in dynamic_routes : <NEWLINE> <INDENT> initkwargs = route . initkwargs . copy ( ) <NEWLINE> initkwargs . update ( getattr ( viewset , methodname ) . kwargs ) <NEWLINE> dynamic_routes_instances . append ( Route ( <NEWLINE> <INDENT> url = replace_methodname ( route . url , endpoint ) , <NEWLINE> mapping = dict ( ( httpmethod , methodname ) for httpmethod in httpmethods ) , <NEWLINE> name = replace_methodname ( route . name , methodname ) , <NEWLINE> initkwargs = initkwargs , <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> return dynamic_routes_instances <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fa2205f5d5f84c66af6405adb2e79c67": {
        "code_string": "# Check the timestamp is within limit\n         if (self._num_days(self._today()) - ts) > self.timeout_days:\n             return False\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if ( self . _num_days ( self . _today ( ) ) - ts ) > self . timeout_days : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check the timestamp is within limit"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "66b347832af24b1aaf495fc6387d1236": {
        "code_string": "def main_tornado():\n     define(\"port\", default=8888, help=\"run on the given port\", type=int)\n     loop = IOLoop.instance()\n     xcfg = t_options.DynamicPatch(loop,op)\n     xcfg.add_change_callback(\n         \"api_url\", \"uname\", \"api_port\",\"switch\", callback_handler=on_callback\n     )\n     tornado.options.parse_command_line()\n     application = tornado.web.Application([(r\"/\", MainHandler)])\n     http_server = tornado.httpserver.HTTPServer(application)\n     http_server.listen(options.port)\n     loop.start()\n",
        "code_toks_joined": "def main_tornado ( ) : <NEWLINE> <INDENT> define ( <STRING> , default = 8888 , help = <STRING> , type = int ) <NEWLINE> loop = IOLoop . instance ( ) <NEWLINE> xcfg = t_options . DynamicPatch ( loop , op ) <NEWLINE> xcfg . add_change_callback ( <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> , <STRING> , callback_handler = on_callback <NEWLINE> <DEDENT> ) <NEWLINE> tornado . options . parse_command_line ( ) <NEWLINE> application = tornado . web . Application ( [ ( <STRING> , MainHandler ) ] ) <NEWLINE> http_server = tornado . httpserver . HTTPServer ( application ) <NEWLINE> http_server . listen ( options . port ) <NEWLINE> loop . start ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"port\"",
                "\"run on the given port\"",
                "\"api_url\"",
                "\"uname\"",
                "\"api_port\"",
                "\"switch\"",
                "r\"/\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d0a4c8a6fbac4a7588844fe11766d25b": {
        "code_string": "def load_next(self):\n         if self.current_model > len(self.unprocessed):\n             return\n         self.current_model += 1\n         filename, frequency = self.unprocessed[self.current_model]\n         model = self.parser.load(filename, frequencies=[frequency])[0]\n         self.load(model)\n",
        "code_toks_joined": "def load_next ( self ) : <NEWLINE> <INDENT> if self . current_model > len ( self . unprocessed ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> self . current_model += 1 <NEWLINE> filename , frequency = self . unprocessed [ self . current_model ] <NEWLINE> model = self . parser . load ( filename , frequencies = [ frequency ] ) [ 0 ] <NEWLINE> self . load ( model ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d0d53268954a4619b2924f3edd86c873": {
        "code_string": "def is_available(self):\n         return len(self.subscribed_topics) <= self.max_topics\n",
        "code_toks_joined": "def is_available ( self ) : <NEWLINE> <INDENT> return len ( self . subscribed_topics ) <= self . max_topics <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4c05400a44754bc58ec6277f6f5e7523": {
        "code_string": "if (not txt.startswith('\"') and\n             not txt.endswith('\"')):\n         return False\n",
        "code_toks_joined": "if ( not txt . startswith ( <STRING> ) and <NEWLINE> <INDENT> not txt . endswith ( <STRING> ) ) : <NEWLINE> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'\"'",
                "'\"'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c83a8138375749329076aca168bd1449": {
        "code_string": "def get_recipe_env(self, arch=None, with_flags_in_cc=True):\n         env = super(CoincurveRecipe, self).get_recipe_env(arch, with_flags_in_cc)\n         # sets linker to use the correct gcc (cross compiler)\n         env['LDSHARED'] = env['CC'] + ' -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions'\n         libsecp256k1 = self.get_recipe('libsecp256k1', self.ctx)\n         libsecp256k1_dir = libsecp256k1.get_build_dir(arch.arch)\n         env['CFLAGS'] = ' -I' + os.path.join(libsecp256k1_dir, 'include')\n         # required additional library and path for Crystax\n         if self.ctx.ndk == 'crystax':\n             # only keeps major.minor (discards patch)\n             python_version = self.ctx.python_recipe.version[0:3]\n             ndk_dir_python = os.path.join(self.ctx.ndk_dir, 'sources/python/', python_version)\n             env['LDFLAGS'] += ' -L{}'.format(os.path.join(ndk_dir_python, 'libs', arch.arch))\n             env['LDFLAGS'] += ' -lpython{}m'.format(python_version)\n             # until `pythonforandroid/archs.py` gets merged upstream:\n             # https://github.com/kivy/python-for-android/pull/1250/files#diff-569e13021e33ced8b54385f55b49cbe6\n             env['CFLAGS'] += ' -I{}/include/python/'.format(ndk_dir_python)\n         env['LDFLAGS'] += \" -lsecp256k1\"\n         return env\n",
        "code_toks_joined": "def get_recipe_env ( self , arch = None , with_flags_in_cc = True ) : <NEWLINE> <INDENT> env = super ( CoincurveRecipe , self ) . get_recipe_env ( arch , with_flags_in_cc ) <NEWLINE> <COMMENT> <NL> env [ <STRING> ] = env [ <STRING> ] + <STRING> <NEWLINE> libsecp256k1 = self . get_recipe ( <STRING> , self . ctx ) <NEWLINE> libsecp256k1_dir = libsecp256k1 . get_build_dir ( arch . arch ) <NEWLINE> env [ <STRING> ] = <STRING> + os . path . join ( libsecp256k1_dir , <STRING> ) <NEWLINE> <COMMENT> <NL> if self . ctx . ndk == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> python_version = self . ctx . python_recipe . version [ 0 : 3 ] <NEWLINE> ndk_dir_python = os . path . join ( self . ctx . ndk_dir , <STRING> , python_version ) <NEWLINE> env [ <STRING> ] += <STRING> . format ( os . path . join ( ndk_dir_python , <STRING> , arch . arch ) ) <NEWLINE> env [ <STRING> ] += <STRING> . format ( python_version ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> env [ <STRING> ] += <STRING> . format ( ndk_dir_python ) <NEWLINE> <DEDENT> env [ <STRING> ] += <STRING> <NEWLINE> return env <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# sets linker to use the correct gcc (cross compiler)",
                "# required additional library and path for Crystax",
                "# only keeps major.minor (discards patch)",
                "# until `pythonforandroid/archs.py` gets merged upstream:",
                "# https://github.com/kivy/python-for-android/pull/1250/files#diff-569e13021e33ced8b54385f55b49cbe6"
            ],
            "<STRING>": [
                "'LDSHARED'",
                "'CC'",
                "' -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions'",
                "'libsecp256k1'",
                "'CFLAGS'",
                "' -I'",
                "'include'",
                "'crystax'",
                "'sources/python/'",
                "'LDFLAGS'",
                "' -L{}'",
                "'libs'",
                "'LDFLAGS'",
                "' -lpython{}m'",
                "'CFLAGS'",
                "' -I{}/include/python/'",
                "'LDFLAGS'",
                "\" -lsecp256k1\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fdf0ca0c7a2f49918d422b3b3d6cc16b": {
        "code_string": "if '--stats' in args:\n         args.remove('--stats')\n         args.append('--stats')\n     if len(args) > 1:\n         if args[0] == '-c':\n             prog = args[1].strip('\"')\n         else:\n             with open(args[0], 'rb') as progfile:\n                 prog = progfile.read()\n     else:\n         print('Brainfuck program:')\n         prog = sys.stdin.readline()\n",
        "code_toks_joined": "if <STRING> in args : <NEWLINE> <INDENT> args . remove ( <STRING> ) <NEWLINE> args . append ( <STRING> ) <NEWLINE> if len ( args ) > 1 : <NEWLINE> if args [ 0 ] == <STRING> : <NEWLINE> <INDENT> prog = args [ 1 ] . strip ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( args [ 0 ] , <STRING> ) as progfile : <NEWLINE> <INDENT> prog = progfile . read ( ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> prog = sys . stdin . readline ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'--stats'",
                "'--stats'",
                "'--stats'",
                "'-c'",
                "'\"'",
                "'rb'",
                "'Brainfuck program:'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "65755895bd8e48c0a446a404b5b2f1e5": {
        "code_string": "old_json = replace_underscore(Path(f.filename), 'coordframe.json')\n     new_json = replace_underscore(tsv_electrodes, 'coordframe.json')\n     copyfile(old_json, new_json)  # TODO: add info about transformation\n",
        "code_toks_joined": "old_json = replace_underscore ( Path ( f . filename ) , <STRING> ) <NEWLINE> <INDENT> new_json = replace_underscore ( tsv_electrodes , <STRING> ) <NEWLINE> copyfile ( old_json , new_json ) <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'coordframe.json'",
                "'coordframe.json'"
            ],
            "<COMMENT>": [
                "# TODO: add info about transformation"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "594d527c6fe14ed79ece3008bed6b4c2": {
        "code_string": "if PARAMETERS['parallel']:\n         with Pool() as p:\n             p.starmap(save_frequency, args)\n     else:\n         for arg in args:\n             save_frequency(*args)\n",
        "code_toks_joined": "if PARAMETERS [ <STRING> ] : <NEWLINE> <INDENT> with Pool ( ) as p : <NEWLINE> <INDENT> p . starmap ( save_frequency , args ) <NEWLINE> else : <NEWLINE> <DEDENT> for arg in args : <NEWLINE> <INDENT> save_frequency ( * args ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'parallel'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5bfa2289d74749d3a2f53668d02fc310": {
        "code_string": "return dat\n",
        "code_toks_joined": "return dat <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ae5ea45ca1e344d6b19e61081338f647": {
        "code_string": "def url(self, name):\n         if os.getenv('SERVER_SOFTWARE', '').startswith('Google App Engine'):\n             # we need this in order to display images, links to files, etc from the local appengine server\n             filename = \"/gs\"+self.location+\"/\"+name\n             key = create_gs_key(filename)\n             return \"http://localhost:8001/blobstore/blob/\"+key+\"?display=inline\"\n         return self.base_url+\"/\"+name\n",
        "code_toks_joined": "def url ( self , name ) : <NEWLINE> <INDENT> if os . getenv ( <STRING> , <STRING> ) . startswith ( <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> filename = <STRING> + self . location + <STRING> + name <NEWLINE> key = create_gs_key ( filename ) <NEWLINE> return <STRING> + key + <STRING> <NEWLINE> <DEDENT> return self . base_url + <STRING> + name <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'SERVER_SOFTWARE'",
                "''",
                "'Google App Engine'",
                "\"/gs\"",
                "\"/\"",
                "\"http://localhost:8001/blobstore/blob/\"",
                "\"?display=inline\"",
                "\"/\""
            ],
            "<COMMENT>": [
                "# we need this in order to display images, links to files, etc from the local appengine server"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "322052be27e64a92816df7557a7e8084": {
        "code_string": "def clear(key, participant_identifier):\n     try:\n         # Remove the direct entry\n         cache_key = COUNTER_CACHE_KEY % key\n         pipe = r.pipeline()\n         freq, _ = pipe.hget(key, participant_identifier).hdel(cache_key, participant_identifier).execute()\n",
        "code_toks_joined": "def clear ( key , participant_identifier ) : <NEWLINE> <INDENT> try : <NEWLINE> <COMMENT> <NL> <INDENT> cache_key = COUNTER_CACHE_KEY % key <NEWLINE> pipe = r . pipeline ( ) <NEWLINE> freq , _ = pipe . hget ( key , participant_identifier ) . hdel ( cache_key , participant_identifier ) . execute ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Remove the direct entry"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4991349b6066432eb10092fb3ffb95ee": {
        "code_string": "if (doi is not None):\n             info = {'status':'success', 'doi':doi}\n             print(\"doi: \"+str(doi))\n             self.set_status(201)\n             self.write(json.dumps(info))\n             store_record(doi, filename, directory_to_zip, access_token)\n             #self.redirect(\"http://127.0.0.1:7000/portal/upload/\"+doi)\n             self.finish()\n         else:\n             self.return_error(\"There was an error uploading to Zenodo\")\n             return\n",
        "code_toks_joined": "if ( doi is not None ) : <NEWLINE> <INDENT> info = { <STRING> : <STRING> , <STRING> : doi } <NEWLINE> print ( <STRING> + str ( doi ) ) <NEWLINE> self . set_status ( 201 ) <NEWLINE> self . write ( json . dumps ( info ) ) <NEWLINE> store_record ( doi , filename , directory_to_zip , access_token ) <NEWLINE> <COMMENT> <NL> self . finish ( ) <NEWLINE> else : <NEWLINE> self . return_error ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'status'",
                "'success'",
                "'doi'",
                "\"doi: \"",
                "\"There was an error uploading to Zenodo\""
            ],
            "<COMMENT>": [
                "#self.redirect(\"http://127.0.0.1:7000/portal/upload/\"+doi)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1f766a004d594e169cda1b5c9169e1a6": {
        "code_string": "# Window-related functions\n _rl.InitWindow.argtypes = [Int, Int, CharPtr]\n _rl.InitWindow.restype = None\n def init_window(width: int, height: int, title: AnyStr) -> None:\n     \"\"\"Initialize window and OpenGL context\"\"\"\n     return _rl.InitWindow(_int(width), _int(width), _str_in(title))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> _rl . InitWindow . argtypes = [ Int , Int , CharPtr ] <NEWLINE> _rl . InitWindow . restype = None <NEWLINE> def init_window ( width : int , height : int , title : AnyStr ) -> None : <NEWLINE> <INDENT> <STRING> <NEWLINE> return _rl . InitWindow ( _int ( width ) , _int ( width ) , _str_in ( title ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Window-related functions"
            ],
            "<STRING>": [
                "\"\"\"Initialize window and OpenGL context\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4b3ee7aea39c4ae8acce5e9b3745a7a2": {
        "code_string": "def extract_readgroup_json(bam_path, logger):\n     step_dir = os.getcwd()\n     bam_file = os.path.basename(bam_path)\n     bam_name, bam_ext = os.path.splitext(bam_file)\n     readgroups_json_file = bam_name+'.json'\n     with open (bam_path) as f:\n         samfile = pysam.AlignmentFile(bam_path, 'rb', check_header=True, check_sq=False)\n         readgroup_dict_list = get_readgroup_dict_list(samfile)\n     with open(readgroups_json_file, 'w') as f:\n         json.dump(out_readgroup_dict_list, f, indent=4)\n     return readgroups_json_file\n",
        "code_toks_joined": "def extract_readgroup_json ( bam_path , logger ) : <NEWLINE> <INDENT> step_dir = os . getcwd ( ) <NEWLINE> bam_file = os . path . basename ( bam_path ) <NEWLINE> bam_name , bam_ext = os . path . splitext ( bam_file ) <NEWLINE> readgroups_json_file = bam_name + <STRING> <NEWLINE> with open ( bam_path ) as f : <NEWLINE> <INDENT> samfile = pysam . AlignmentFile ( bam_path , <STRING> , check_header = True , check_sq = False ) <NEWLINE> readgroup_dict_list = get_readgroup_dict_list ( samfile ) <NEWLINE> <DEDENT> with open ( readgroups_json_file , <STRING> ) as f : <NEWLINE> <INDENT> json . dump ( out_readgroup_dict_list , f , indent = 4 ) <NEWLINE> <DEDENT> return readgroups_json_file <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.json'",
                "'rb'",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2bdfaef871d54c94bb81a432c052994a": {
        "code_string": "def extract_readgroup_json(bam_path, logger):\n     bam_file = os.path.basename(bam_path)\n     bam_name, bam_ext = os.path.splitext(bam_file)\n     readgroups_json_file = bam_name+'.json'\n     with open (bam_path) as f:\n         samfile = pysam.AlignmentFile(f, 'rb', check_header=True, check_sq=False)\n         if not samfile.is_bam:\n             logger.error(\"This program only runs on BAM files.\")\n             raise NotABamError\n         samfile_header = samfile.header\n         bam_readgroup_dict_list = samfile_header.get('RG')\n         if not bam_readgroup_dict_list:\n             logger.error('There are no readgroups in BAM: {}'.format(samfile.filename))\n             raise NoReadGroupError\n         readgroup_dict_list = get_readgroup_dict_list(samfile, logger)\n     with open(readgroups_json_file, 'w') as f:\n         json.dump(readgroup_dict_list, f, indent=4)\n     return readgroups_json_file\n",
        "code_toks_joined": "def extract_readgroup_json ( bam_path , logger ) : <NEWLINE> <INDENT> bam_file = os . path . basename ( bam_path ) <NEWLINE> bam_name , bam_ext = os . path . splitext ( bam_file ) <NEWLINE> readgroups_json_file = bam_name + <STRING> <NEWLINE> with open ( bam_path ) as f : <NEWLINE> <INDENT> samfile = pysam . AlignmentFile ( f , <STRING> , check_header = True , check_sq = False ) <NEWLINE> if not samfile . is_bam : <NEWLINE> <INDENT> logger . error ( <STRING> ) <NEWLINE> raise NotABamError <NEWLINE> <DEDENT> samfile_header = samfile . header <NEWLINE> bam_readgroup_dict_list = samfile_header . get ( <STRING> ) <NEWLINE> if not bam_readgroup_dict_list : <NEWLINE> <INDENT> logger . error ( <STRING> . format ( samfile . filename ) ) <NEWLINE> raise NoReadGroupError <NEWLINE> <DEDENT> readgroup_dict_list = get_readgroup_dict_list ( samfile , logger ) <NEWLINE> <DEDENT> with open ( readgroups_json_file , <STRING> ) as f : <NEWLINE> <INDENT> json . dump ( readgroup_dict_list , f , indent = 4 ) <NEWLINE> <DEDENT> return readgroups_json_file <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.json'",
                "'rb'",
                "\"This program only runs on BAM files.\"",
                "'RG'",
                "'There are no readgroups in BAM: {}'",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4908bfd199274413b03bdb53b39b26f0": {
        "code_string": "def snakeize_dict(dict_):\n     answer = {}\n     for key in dict_:\n         nkey = snakeize_s(key)\n         answer[key] = dict_[key]\n     return answer\n",
        "code_toks_joined": "def snakeize_dict ( dict_ ) : <NEWLINE> <INDENT> answer = { } <NEWLINE> for key in dict_ : <NEWLINE> <INDENT> nkey = snakeize_s ( key ) <NEWLINE> answer [ key ] = dict_ [ key ] <NEWLINE> <DEDENT> return answer <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dac3666cc0b54a9dbf572693799e4abb": {
        "code_string": "listener = handlers.QueueListener(log_queue, logger)\n",
        "code_toks_joined": "listener = handlers . QueueListener ( log_queue , logger ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "11854bc6ad4f4c358403ad76bdf24946": {
        "code_string": "def __call__(self, transaction):\n         valid_transaction = self.validate_transaction(transaction)\n         message = self.build_evm_message(valid_transaction)\n         computation = self.build_computation(message, valid_transaction)\n         finalized_computation = self.finalize_computation(computation, valid_transaction)\n         return finalized_computation\n",
        "code_toks_joined": "def __call__ ( self , transaction ) : <NEWLINE> <INDENT> valid_transaction = self . validate_transaction ( transaction ) <NEWLINE> message = self . build_evm_message ( valid_transaction ) <NEWLINE> computation = self . build_computation ( message , valid_transaction ) <NEWLINE> finalized_computation = self . finalize_computation ( computation , valid_transaction ) <NEWLINE> return finalized_computation <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "409c9e2257ea4117acb0dff2a7f4440d": {
        "code_string": "if self._index < 0:\n             if self._feed_page is not feedparser.FeedParserDict:\n                 for link in self._feed_page.links:\n                     if link.rel == 'previous':\n                         self._url = link.href\n",
        "code_toks_joined": "if self . _index < 0 : <NEWLINE> <INDENT> if self . _feed_page is not feedparser . FeedParserDict : <NEWLINE> <INDENT> for link in self . _feed_page . links : <NEWLINE> <INDENT> if link . rel == <STRING> : <NEWLINE> <INDENT> self . _url = link . href <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'previous'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0b3441484ef3417da6f09cf57494713f": {
        "code_string": "# Is a class\n             search = class_re.search(line)\n             if search:\n                 if current_class:\n                     current_class[1] = last_closing_bracket_number\n                 if current_method:\n                     current_method[1] = last_closing_bracket_number\n                     components.append(current_method)\n                 current_class = [line_counter, 0, search.group(2)]\n                 continue\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> search = class_re . search ( line ) <NEWLINE> if search : <NEWLINE> <INDENT> if current_class : <NEWLINE> <INDENT> current_class [ 1 ] = last_closing_bracket_number <NEWLINE> <DEDENT> if current_method : <NEWLINE> <INDENT> current_method [ 1 ] = last_closing_bracket_number <NEWLINE> components . append ( current_method ) <NEWLINE> <DEDENT> current_class = [ line_counter , 0 , search . group ( 2 ) ] <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Is a class"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "11707f36459c460c9aa5dc616bbb1f51": {
        "code_string": "if diff:\n         schedule = solution_to_schedule(solution, events, slots)\n         event_diff = event_schedule_difference(schedule, original_schedule)\n         logger.debug(f'\\nevent_diff:')\n         for item in event_diff:\n             logger.debug(f'{item.event.name} has moved from {item.old_slot.venue} at {item.old_slot.starts_at} to {item.new_slot.venue} at {item.new_slot.starts_at}')\n",
        "code_toks_joined": "if diff : <NEWLINE> <INDENT> schedule = solution_to_schedule ( solution , events , slots ) <NEWLINE> event_diff = event_schedule_difference ( schedule , original_schedule ) <NEWLINE> logger . debug ( <STRING> ) <NEWLINE> for item in event_diff : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f'\\nevent_diff:'",
                "f'{item.event.name} has moved from {item.old_slot.venue} at {item.old_slot.starts_at} to {item.new_slot.venue} at {item.new_slot.starts_at}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f851e277ebe04153b6cfc0d5397873fb": {
        "code_string": "kwargs = dict(\n             content_type    = self.get_content_type(),\n             object_id       = self.instance.id,\n             key             = self.field.key,\n         )\n         if not user:\n             kwargs['user'] = user\n         else:\n             kwargs['user__isnull'] = True\n",
        "code_toks_joined": "kwargs = dict ( <NEWLINE> <INDENT> content_type = self . get_content_type ( ) , <NEWLINE> object_id = self . instance . id , <NEWLINE> key = self . field . key , <NEWLINE> ) <NEWLINE> if not user : <NEWLINE> kwargs [ <STRING> ] = user <NEWLINE> else : <NEWLINE> kwargs [ <STRING> ] = True <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'user'",
                "'user__isnull'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b45488880a054794b0a99bfeb27d37d9": {
        "code_string": "if old_html_table != html_table:\n             mail_text = \"<head> <meta charset='utf-8'></head><body>\"\n             mail_text = \"Es gab \u00c4nderungen in deinen Noten:\\n\"\n             mail_text += html_table\n             mail_text += \"</body>\"\n             self.send_mail(mail_text)\n",
        "code_toks_joined": "if old_html_table != html_table : <NEWLINE> <INDENT> mail_text = <STRING> <NEWLINE> mail_text = <STRING> <NEWLINE> mail_text += html_table <NEWLINE> mail_text += <STRING> <NEWLINE> self . send_mail ( mail_text ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"<head> <meta charset='utf-8'></head><body>\"",
                "\"Es gab \u00c4nderungen in deinen Noten:\\n\"",
                "\"</body>\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b9eaefa6a9644175afd7ff997d43e42d": {
        "code_string": "self.verbosity = options.verbosity\n         if options.quickunit_prefix:\n             self.prefixes = options.quickunit_prefix\n             if len(self.prefixes) == 0:\n                 self.prefixes = self.prefixes[0].split('\\n')\n         else:\n             self.prefixes = [\"tests/\"]\n         self.parent = 'master'\n",
        "code_toks_joined": "self . verbosity = options . verbosity <NEWLINE> <INDENT> if options . quickunit_prefix : <NEWLINE> <INDENT> self . prefixes = options . quickunit_prefix <NEWLINE> if len ( self . prefixes ) == 0 : <NEWLINE> <INDENT> self . prefixes = self . prefixes [ 0 ] . split ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . prefixes = [ <STRING> ] <NEWLINE> <DEDENT> self . parent = <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'\\n'",
                "\"tests/\"",
                "'master'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2da883f54a3f41c6976ca517d37bff9a": {
        "code_string": "def get_sql_field(self, prop: Property, lower: bool = False):\n         if prop.list is not None:\n             jsonb = self.table.lists.c.data[prop.place]\n             if _is_dtype(prop, (String, DateTime, Date)):\n                 field = jsonb.astext\n             else:\n                 field = sa.cast(field, JSONB)\n         else:\n             field = self.table.main.c[prop.name]\n",
        "code_toks_joined": "def get_sql_field ( self , prop : Property , lower : bool = False ) : <NEWLINE> <INDENT> if prop . list is not None : <NEWLINE> <INDENT> jsonb = self . table . lists . c . data [ prop . place ] <NEWLINE> if _is_dtype ( prop , ( String , DateTime , Date ) ) : <NEWLINE> <INDENT> field = jsonb . astext <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> field = sa . cast ( field , JSONB ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> field = self . table . main . c [ prop . name ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0472b1689a954bad9ab2847dd068a89f": {
        "code_string": "assert model_a == model_b\n",
        "code_toks_joined": "assert model_a == model_b <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "25627ea840ce4dcbbf34db7fa878fe56": {
        "code_string": "if input_start <= t < input_start + echo_start:\n                 x_t = 1.0\n                 mask_t = np.zeros(self.N_out)\n",
        "code_toks_joined": "if input_start <= t < input_start + echo_start : <NEWLINE> <INDENT> x_t = 1.0 <NEWLINE> mask_t = np . zeros ( self . N_out ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8fdb98f728934efeb61d589a1dc02810": {
        "code_string": "return kompile(src, raw=raw, filename=name)\n         except FileNotFoundError:\n             pass\n     else:\n         raise TemplateNotFound(name)\n",
        "code_toks_joined": "return kompile ( src , raw = raw , filename = name ) <NEWLINE> <INDENT> except FileNotFoundError : <NEWLINE> <INDENT> pass <NEWLINE> else : <NEWLINE> <DEDENT> raise TemplateNotFound ( name ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "135e340dcaa24af9918ab5de39739155": {
        "code_string": "# --COMPARE MODELS--\n     if compare_models:\n         choose_box_and_violin_plots(names,\n                                     scoring,\n                                     compare_models,\n                                     results,\n                                     is_continuous)\n     # ROC CURVE\n     if ROC:\n         if not is_continuous:\n             timeit(plot_rocs, models, df_X, y)\n             plt.show()\n     print(f'MAKE SUBSAMPLE TIME: {time() - starttotal}')\n     return names, results, models, pipeline, df_X\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if compare_models : <NEWLINE> <INDENT> choose_box_and_violin_plots ( names , <NEWLINE> <INDENT> scoring , <NEWLINE> compare_models , <NEWLINE> results , <NEWLINE> is_continuous ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if ROC : <NEWLINE> <INDENT> if not is_continuous : <NEWLINE> <INDENT> timeit ( plot_rocs , models , df_X , y ) <NEWLINE> plt . show ( ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> return names , results , models , pipeline , df_X <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# --COMPARE MODELS--",
                "# ROC CURVE"
            ],
            "<STRING>": [
                "f'MAKE SUBSAMPLE TIME: {time() - starttotal}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1ee437405d9443b4898e92165fab5f72": {
        "code_string": "def prepare_notification(self, *, subscribers=None, instance=None,\n                              loop=None, notify_external=True):\n         \"\"\"Sets up a and configures an `~.utils.Executor`:class: instance.\"\"\"\n         # merge callbacks added to the class level with those added to the\n         # instance, giving the formers precedence while preserving overall\n         # order\n         self_subscribers = self.subscribers.copy()\n         # add in callbacks declared in the main class body and marked with\n         # @handler\n         if (instance is not None and self.name and\n             isinstance(instance.__class__, SignalAndHandlerInitMeta)):\n             class_handlers = type(instance)._get_class_handlers(\n                 self.name, instance)\n             for ch in class_handlers:\n                 # eventual methods are ephemeral and normally the following\n                 # condition would always be True for methods but the dict used\n                 # has logic to take that into account\n                 if ch not in self_subscribers:\n                     self_subscribers.append(ch)\n         # add in the other instance level callbacks added at runtime\n         if subscribers is not None:\n             for el in subscribers:\n                 # eventual methods are ephemeral and normally the following\n                 # condition would always be True for methods but the dict used\n                 # has logic to take that into account\n                 if el not in self_subscribers:\n                     self_subscribers.append(el)\n         loop = loop or self.loop\n         # maybe do a round of external publishing\n         if notify_external and self.external_signaller is not None:\n             self_subscribers.append(partial(self.ext_publish, instance, loop))\n         if self._fnotify is None:\n             fnotify = None\n         else:\n             if instance is None:\n                 fnotify = self._fnotify\n             else:\n                 fnotify = types.MethodType(instance, self._fnotify)\n         validator = self._fvalidation\n         if validator is not None and instance is not None:\n             validator = types.MethodType(validator, instance)\n         return Executor(self_subscribers, owner=self,\n                         concurrent=SignalOptions.EXEC_CONCURRENT in self.flags,\n                         loop=loop, exec_wrapper=fnotify,\n                         fvalidation=validator)\n",
        "code_toks_joined": "def prepare_notification ( self , * , subscribers = None , instance = None , <NEWLINE> <INDENT> loop = None , notify_external = True ) : <NEWLINE> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> self_subscribers = self . subscribers . copy ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if ( instance is not None and self . name and <NEWLINE> isinstance ( instance . __class__ , SignalAndHandlerInitMeta ) ) : <NEWLINE> class_handlers = type ( instance ) . _get_class_handlers ( <NEWLINE> self . name , instance ) <NEWLINE> for ch in class_handlers : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> if ch not in self_subscribers : <NEWLINE> self_subscribers . append ( ch ) <NEWLINE> <COMMENT> <NL> if subscribers is not None : <NEWLINE> for el in subscribers : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> if el not in self_subscribers : <NEWLINE> self_subscribers . append ( el ) <NEWLINE> loop = loop or self . loop <NEWLINE> <COMMENT> <NL> if notify_external and self . external_signaller is not None : <NEWLINE> self_subscribers . append ( partial ( self . ext_publish , instance , loop ) ) <NEWLINE> if self . _fnotify is None : <NEWLINE> fnotify = None <NEWLINE> else : <NEWLINE> if instance is None : <NEWLINE> fnotify = self . _fnotify <NEWLINE> else : <NEWLINE> fnotify = types . MethodType ( instance , self . _fnotify ) <NEWLINE> validator = self . _fvalidation <NEWLINE> if validator is not None and instance is not None : <NEWLINE> validator = types . MethodType ( validator , instance ) <NEWLINE> return Executor ( self_subscribers , owner = self , <NEWLINE> concurrent = SignalOptions . EXEC_CONCURRENT in self . flags , <NEWLINE> loop = loop , exec_wrapper = fnotify , <NEWLINE> fvalidation = validator ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Sets up a and configures an `~.utils.Executor`:class: instance.\"\"\""
            ],
            "<COMMENT>": [
                "# merge callbacks added to the class level with those added to the",
                "# instance, giving the formers precedence while preserving overall",
                "# order",
                "# add in callbacks declared in the main class body and marked with",
                "# @handler",
                "# eventual methods are ephemeral and normally the following",
                "# condition would always be True for methods but the dict used",
                "# has logic to take that into account",
                "# add in the other instance level callbacks added at runtime",
                "# eventual methods are ephemeral and normally the following",
                "# condition would always be True for methods but the dict used",
                "# has logic to take that into account",
                "# maybe do a round of external publishing"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "46d3e34592b9403b880712c6ca5801ca": {
        "code_string": "def global_interpreter(self, version):\n         version_name = \"R-%s\" % version\n         if Path(self.bin_path / \"R\").exists():\n             remove(str(self.bin_path / \"R\"))\n         symlink(str(self.lib_path / version_name / \"bin\" / \"R\"), str(self.bin_path / \"R\"))\n",
        "code_toks_joined": "def global_interpreter ( self , version ) : <NEWLINE> <INDENT> version_name = <STRING> % version <NEWLINE> if Path ( self . bin_path / <STRING> ) . exists ( ) : <NEWLINE> <INDENT> remove ( str ( self . bin_path / <STRING> ) ) <NEWLINE> <DEDENT> symlink ( str ( self . lib_path / version_name / <STRING> / <STRING> ) , str ( self . bin_path / <STRING> ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"R-%s\"",
                "\"R\"",
                "\"R\"",
                "\"bin\"",
                "\"R\"",
                "\"R\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "21d0c83e88964223ae24b9bce4e215b1": {
        "code_string": "@register.filter\n def order_links(links):\n     links_list = list(links)\n     ordered_links = []\n     while links_list:\n         minor = links_list[0]\n         for link in links_list:\n             try:\n                 if (link.link_type.ordering < minor.link_type.ordering):\n                     minor = link\n             except TypeError:\n                 pass\n         ordered_links.append(link)\n         links_list.remove(minor)\n     return ordered_links\n",
        "code_toks_joined": "@ register . filter <NEWLINE> <INDENT> def order_links ( links ) : <NEWLINE> <INDENT> links_list = list ( links ) <NEWLINE> ordered_links = [ ] <NEWLINE> while links_list : <NEWLINE> <INDENT> minor = links_list [ 0 ] <NEWLINE> for link in links_list : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if ( link . link_type . ordering < minor . link_type . ordering ) : <NEWLINE> <INDENT> minor = link <NEWLINE> <DEDENT> <DEDENT> except TypeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> ordered_links . append ( link ) <NEWLINE> links_list . remove ( minor ) <NEWLINE> <DEDENT> return ordered_links <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e071496d0cf64fc68240202bf5ee750f": {
        "code_string": "for topic in topics_list:\n             if topic.projects.count():\n                 topics_list.append((topic.id, topic.name))\n         return topics_list\n",
        "code_toks_joined": "for topic in topics_list : <NEWLINE> <INDENT> if topic . projects . count ( ) : <NEWLINE> <INDENT> topics_list . append ( ( topic . id , topic . name ) ) <NEWLINE> return topics_list <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f0f67fde40547c8b84b492fbc115694": {
        "code_string": "request = dict(get_default_request_parameters(chosen_request_params))\n     check_review_timestamp = bool('day_range' in request.keys() and request['filter'] != 'all')\n     if check_review_timestamp:\n         current_date = datetime.datetime.now()\n         num_days = int(request['day_range'])\n         date_threshold = current_date - datetime.timedelta(days=num_days)\n         timestamp_threshold = datetime.datetime.timestamp(date_threshold)\n         if verbose:\n             if request['filter'] == 'updated':\n                 collection_keyword = 'edited'\n             else:\n                 collection_keyword = 'first posted'\n             print('Collecting reviews {} after {}'.format(collection_keyword,\n                                                           timestamp_threshold))\n",
        "code_toks_joined": "request = dict ( get_default_request_parameters ( chosen_request_params ) ) <NEWLINE> <INDENT> check_review_timestamp = bool ( <STRING> in request . keys ( ) and request [ <STRING> ] != <STRING> ) <NEWLINE> if check_review_timestamp : <NEWLINE> <INDENT> current_date = datetime . datetime . now ( ) <NEWLINE> num_days = int ( request [ <STRING> ] ) <NEWLINE> date_threshold = current_date - datetime . timedelta ( days = num_days ) <NEWLINE> timestamp_threshold = datetime . datetime . timestamp ( date_threshold ) <NEWLINE> if verbose : <NEWLINE> <INDENT> if request [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> collection_keyword = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> collection_keyword = <STRING> <NEWLINE> <DEDENT> print ( <STRING> . format ( collection_keyword , <NEWLINE> <INDENT> timestamp_threshold ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'day_range'",
                "'filter'",
                "'all'",
                "'day_range'",
                "'filter'",
                "'updated'",
                "'edited'",
                "'first posted'",
                "'Collecting reviews {} after {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f42a6cc052894418a031b24b26da98f5": {
        "code_string": "def _update_data_with_deltas(self, packet_id, deltas):\n         for delta_id in [0, 1]:\n             # convert from packet to sample ID\n             sample_id = (packet_id - 1) * 2 + delta_id + 1\n             # 19bit packets hold deltas between two samples\n             self._last_eeg_data += np.array(deltas[delta_id])\n             self._update_counts_and_enqueue(\"EEG\", sample_id)\n",
        "code_toks_joined": "def _update_data_with_deltas ( self , packet_id , deltas ) : <NEWLINE> <INDENT> for delta_id in [ 0 , 1 ] : <NEWLINE> <COMMENT> <NL> <INDENT> sample_id = ( packet_id - 1 ) * 2 + delta_id + 1 <NEWLINE> <COMMENT> <NL> self . _last_eeg_data += np . array ( deltas [ delta_id ] ) <NEWLINE> self . _update_counts_and_enqueue ( <STRING> , sample_id ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# convert from packet to sample ID",
                "# 19bit packets hold deltas between two samples"
            ],
            "<STRING>": [
                "\"EEG\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9cbd1b6d51764b288a7cf29173956303": {
        "code_string": "def get_fleet(self):\n         fleets_list = []\n         response = self.session.get('https://s{}-{}.ogame.gameforge.com/game/index.php?page=ingame&component=movement'\n                                     .format(self.server_number, self.server_language))\n         if response.status_code == 302:\n             fleets = response.text.split('<div id=\"fleet')\n             del fleets[0]\n             for fleet in fleets:\n                 fleet_id = fleet[0:30].split('\"')[0]\n                 marker = fleet.find('data-mission-type=\"')\n                 fleet_mission = int(fleet[marker + 19: marker + 22].split('\"')[0])\n                 marker = fleet.find('<span class=\"timer tooltip\" title=\"')\n                 fleet_arrival = datetime.strptime(fleet[marker + 35: marker + 54], '%d.%m.%Y %H:%M:%S')\n                 marker = fleet.find('<span class=\"originCoords tooltip\" title=\"')\n                 origin_raw = fleet[marker: marker + 180]\n                 origin_list = origin_raw.split('[')[1].split(']')[0].split(':')\n                 fleet_origin = const.coordinates(origin_list[0], origin_list[1], origin_list[2])\n                 marker = fleet.find('<span class=\"destinationCoords tooltip\"')\n                 destination_raw = fleet[marker: marker + 200]\n                 destination_list = destination_raw.split('[')[1].split(']')[0].split(':')\n                 fleet_destination = const.coordinates(destination_list[0], destination_list[1], destination_list[2])\n",
        "code_toks_joined": "def get_fleet ( self ) : <NEWLINE> <INDENT> fleets_list = [ ] <NEWLINE> response = self . session . get ( <STRING> <NEWLINE> <INDENT> . format ( self . server_number , self . server_language ) ) <NEWLINE> <DEDENT> if response . status_code == 302 : <NEWLINE> <INDENT> fleets = response . text . split ( <STRING> ) <NEWLINE> del fleets [ 0 ] <NEWLINE> for fleet in fleets : <NEWLINE> <INDENT> fleet_id = fleet [ 0 : 30 ] . split ( <STRING> ) [ 0 ] <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> fleet_mission = int ( fleet [ marker + 19 : marker + 22 ] . split ( <STRING> ) [ 0 ] ) <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> fleet_arrival = datetime . strptime ( fleet [ marker + 35 : marker + 54 ] , <STRING> ) <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> origin_raw = fleet [ marker : marker + 180 ] <NEWLINE> origin_list = origin_raw . split ( <STRING> ) [ 1 ] . split ( <STRING> ) [ 0 ] . split ( <STRING> ) <NEWLINE> fleet_origin = const . coordinates ( origin_list [ 0 ] , origin_list [ 1 ] , origin_list [ 2 ] ) <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> destination_raw = fleet [ marker : marker + 200 ] <NEWLINE> destination_list = destination_raw . split ( <STRING> ) [ 1 ] . split ( <STRING> ) [ 0 ] . split ( <STRING> ) <NEWLINE> fleet_destination = const . coordinates ( destination_list [ 0 ] , destination_list [ 1 ] , destination_list [ 2 ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'https://s{}-{}.ogame.gameforge.com/game/index.php?page=ingame&component=movement'",
                "'<div id=\"fleet'",
                "'\"'",
                "'data-mission-type=\"'",
                "'\"'",
                "'<span class=\"timer tooltip\" title=\"'",
                "'%d.%m.%Y %H:%M:%S'",
                "'<span class=\"originCoords tooltip\" title=\"'",
                "'['",
                "']'",
                "':'",
                "'<span class=\"destinationCoords tooltip\"'",
                "'['",
                "']'",
                "':'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "86f1454411464d949b34c2f0b069a05b": {
        "code_string": "if not value in self.filter_keys:\n             raise ImproperlyConfigured(\n                 \"%s is not present in filter_keys (%s)\" % (key, self.filter_keys)\n             )\n",
        "code_toks_joined": "if not value in self . filter_keys : <NEWLINE> <INDENT> raise ImproperlyConfigured ( <NEWLINE> <INDENT> <STRING> % ( key , self . filter_keys ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"%s is not present in filter_keys (%s)\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "82c6873bb1524ce4a35583dfeac325e8": {
        "code_string": "if view not in view.get_views():\n             raise ImproperlyConfigured(\n                 \"Tried to get the URL patterns for a view (%s)\"\n                 \" that is not defined by get_views\" % view\n             )\n",
        "code_toks_joined": "if view not in view . get_views ( ) : <NEWLINE> <INDENT> raise ImproperlyConfigured ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> % view <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Tried to get the URL patterns for a view (%s)\"",
                "\" that is not defined by get_views\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "81251835883b4b96bdc21eaba58588c8": {
        "code_string": "setattr(ModelMixin,\n             'get_%s_url_name' % view_class.get_underscored_action_name(),\n             _get_url)\n",
        "code_toks_joined": "setattr ( ModelMixin , <NEWLINE> <INDENT> <STRING> % view_class . get_underscored_action_name ( ) , <NEWLINE> _get_url ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'get_%s_url_name'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ee9c76b22db243fda29b41d1bed5144d": {
        "code_string": "def read(self, max_records = None):\n         array = []\n         dictionary = {}\n         index_column = self.index_column_number()\n         for (count,entry) in enumerate(self._tfh):\n             array.append(entry)\n             dictionary[entry[index_column]] = entry # note, may erase older ones; build array?\n             if max_records and count >= max_records:\n                 break\n",
        "code_toks_joined": "def read ( self , max_records = None ) : <NEWLINE> <INDENT> array = [ ] <NEWLINE> dictionary = { } <NEWLINE> index_column = self . index_column_number ( ) <NEWLINE> for ( count , entry ) in enumerate ( self . _tfh ) : <NEWLINE> <INDENT> array . append ( entry ) <NEWLINE> dictionary [ entry [ index_column ] ] = entry <COMMENT> <NEWLINE> if max_records and count >= max_records : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# note, may erase older ones; build array?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2845afb54b83457bae2f00aa22519969": {
        "code_string": "def __next__(self):\n         row = next(self._consumer)\n         decoded_row = unpackb(row.value)\n         if self._end_time:\n             self.verbose(\"searching forward from:\")\n             self.verbose(decoded_row)\n             count += 0\n             while True:\n                 count += 1\n                 decoded_time = decoded_row[self._time_column]\n                 decoded_time = self.decode_item(decoded_time)\n                 decoded_time = self.parse_time(decoded_time)\n                 if decoded_time >= self._kafka_end_time:\n                     self.verbose(\"kafka end time reached: \" + str(count)+ \" rows\")\n                     raise StopIteration()\n",
        "code_toks_joined": "def __next__ ( self ) : <NEWLINE> <INDENT> row = next ( self . _consumer ) <NEWLINE> decoded_row = unpackb ( row . value ) <NEWLINE> if self . _end_time : <NEWLINE> <INDENT> self . verbose ( <STRING> ) <NEWLINE> self . verbose ( decoded_row ) <NEWLINE> count += 0 <NEWLINE> while True : <NEWLINE> <INDENT> count += 1 <NEWLINE> decoded_time = decoded_row [ self . _time_column ] <NEWLINE> decoded_time = self . decode_item ( decoded_time ) <NEWLINE> decoded_time = self . parse_time ( decoded_time ) <NEWLINE> if decoded_time >= self . _kafka_end_time : <NEWLINE> <INDENT> self . verbose ( <STRING> + str ( count ) + <STRING> ) <NEWLINE> raise StopIteration ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"searching forward from:\"",
                "\"kafka end time reached: \"",
                "\" rows\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "645312944db94c0c8ec51772a9d85ea2": {
        "code_string": "results = {'connections': conns,\n                    'ports': ports}\n         return (self._output_key, conns)\n",
        "code_toks_joined": "results = { <STRING> : conns , <NEWLINE> <INDENT> <STRING> : ports } <NEWLINE> return ( self . _output_key , conns ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'connections'",
                "'ports'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "76c90f483b7843598c811ca24e3c3bf7": {
        "code_string": "# use to generate n_k so that the sum of n_k equals to n\n     for i in range(num_rounds):\n         # calculate the number of clients used in this round\n         m = max(int(client_num * C), 1)\n         # random set of m client's index\n         S = np.array(random.sample(range(client_num), client_num))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for i in range ( num_rounds ) : <NEWLINE> <COMMENT> <NL> <INDENT> m = max ( int ( client_num * C ) , 1 ) <NEWLINE> <COMMENT> <NL> S = np . array ( random . sample ( range ( client_num ) , client_num ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# use to generate n_k so that the sum of n_k equals to n",
                "# calculate the number of clients used in this round",
                "# random set of m client's index"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a76de49457e643c78ecc360cf30a1e00": {
        "code_string": "# choose random starting vector if not given\n     if nstart is None:\n         import random\n         x=dict([(n,random.random()) for n in G])\n     else:\n         x=nstart\n     # normalize starting vector\n     s=1.0/sum(x.values())\n     for k in x: x[k]*=s\n     nnodes=G.number_of_nodes()\n     # make up to max_iter iterations        \n     for i in range(max_iter):\n         xlast=x\n         x=dict.fromkeys(xlast.keys(),0)\n         # do the multiplication y=Ax\n         for n in x:\n             for nbr in G[n]:\n                 x[n]+=xlast[nbr]*G[n][nbr]\n         # normalize vector\n         s=1.0/sum(x.values())\n         for n in x: x[n]*=s\n         # check convergence            \n         err=sum([abs(x[n]-xlast[n]) for n in x])\n         if err < n*tol:\n             return x\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if nstart is None : <NEWLINE> <INDENT> import random <NEWLINE> x = dict ( [ ( n , random . random ( ) ) for n in G ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> x = nstart <NEWLINE> <COMMENT> <NL> <DEDENT> s = 1.0 / sum ( x . values ( ) ) <NEWLINE> for k in x : x [ k ] *= s <NEWLINE> nnodes = G . number_of_nodes ( ) <NEWLINE> <COMMENT> <NL> for i in range ( max_iter ) : <NEWLINE> <INDENT> xlast = x <NEWLINE> x = dict . fromkeys ( xlast . keys ( ) , 0 ) <NEWLINE> <COMMENT> <NL> for n in x : <NEWLINE> <INDENT> for nbr in G [ n ] : <NEWLINE> <INDENT> x [ n ] += xlast [ nbr ] * G [ n ] [ nbr ] <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> s = 1.0 / sum ( x . values ( ) ) <NEWLINE> for n in x : x [ n ] *= s <NEWLINE> <COMMENT> <NL> err = sum ( [ abs ( x [ n ] - xlast [ n ] ) for n in x ] ) <NEWLINE> if err < n * tol : <NEWLINE> <INDENT> return x <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# choose random starting vector if not given",
                "# normalize starting vector",
                "# make up to max_iter iterations        ",
                "# do the multiplication y=Ax",
                "# normalize vector",
                "# check convergence            "
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fea8bfd7c4334f39998078ce3ef0a7c4": {
        "code_string": "labelGenerator = _gen_node_label(G)\n",
        "code_toks_joined": "labelGenerator = _gen_node_label ( G ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ba1e245f2ee742d58d6a394a90164003": {
        "code_string": "if nbunch is None:\n         nbunch = G.nodes_iter() \n     for v in G:     # process all vertices in G\n         if v in explored: \n             continue\n         fringe=[v]   # nodes yet to look at\n         while fringe:\n             w=fringe[-1]  # depth first search\n             if w in explored: # already looked down this branch\n                 fringe.pop()\n                 continue\n             seen[w]=1     # mark as seen\n             # Check successors for cycles and for new nodes\n             new_nodes=[]\n             for n in G[w]:\n                 if n not in explored:\n                     if n in seen: #CYCLE !!\n                         raise nx.NetworkXUnfeasible(\"Graph contains a cycle.\")\n                     new_nodes.append(n)\n             if new_nodes:   # Add new_nodes to fringe\n                 fringe.extend(new_nodes)\n             else:           # No new nodes so w is fully explored\n                 explored[w]=1\n                 order_explored.insert(0,w) # reverse order explored\n                 fringe.pop()    # done considering this node\n     return order_explored\n",
        "code_toks_joined": "if nbunch is None : <NEWLINE> <INDENT> nbunch = G . nodes_iter ( ) <NEWLINE> for v in G : <COMMENT> <NEWLINE> if v in explored : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> fringe = [ v ] <COMMENT> <NEWLINE> while fringe : <NEWLINE> <INDENT> w = fringe [ - 1 ] <COMMENT> <NEWLINE> if w in explored : <COMMENT> <NEWLINE> <INDENT> fringe . pop ( ) <NEWLINE> continue <NEWLINE> <DEDENT> seen [ w ] = 1 <COMMENT> <NEWLINE> <COMMENT> <NL> new_nodes = [ ] <NEWLINE> for n in G [ w ] : <NEWLINE> <INDENT> if n not in explored : <NEWLINE> <INDENT> if n in seen : <COMMENT> <NEWLINE> <INDENT> raise nx . NetworkXUnfeasible ( <STRING> ) <NEWLINE> <DEDENT> new_nodes . append ( n ) <NEWLINE> <DEDENT> <DEDENT> if new_nodes : <COMMENT> <NEWLINE> <INDENT> fringe . extend ( new_nodes ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> explored [ w ] = 1 <NEWLINE> order_explored . insert ( 0 , w ) <COMMENT> <NEWLINE> fringe . pop ( ) <COMMENT> <NEWLINE> return order_explored <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# process all vertices in G",
                "# nodes yet to look at",
                "# depth first search",
                "# already looked down this branch",
                "# mark as seen",
                "# Check successors for cycles and for new nodes",
                "#CYCLE !!",
                "# Add new_nodes to fringe",
                "# No new nodes so w is fully explored",
                "# reverse order explored",
                "# done considering this node"
            ],
            "<STRING>": [
                "\"Graph contains a cycle.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8f874ca5e7f94b7999852392454748e3": {
        "code_string": "def ambig_binary_crossentropy(y_true,y_pred):\n         non_ambig = K.cast((y_true > -0.5),'float32')\n         return K.mean(K.binary_crossentropy(y_pred, y_true)\n                       *non_ambig, axis=-1)\n",
        "code_toks_joined": "def ambig_binary_crossentropy ( y_true , y_pred ) : <NEWLINE> <INDENT> non_ambig = K . cast ( ( y_true > - 0.5 ) , <STRING> ) <NEWLINE> return K . mean ( K . binary_crossentropy ( y_pred , y_true ) <NEWLINE> <INDENT> * non_ambig , axis = - 1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'float32'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b2682d75e87d4319a3c617647d7cad80": {
        "code_string": "def execute_from_command_line(self, argv=None):\n         if argv is None:\n             argv = sys.argv\n         args = self.parser.parse_args(argv[1:])\n         self.nekumo.gateways = list(self.parse_gateways(args))\n         self.nekumo.ifaces = list(self.parse_ifaces(args))\n         if 'NEKUMO_DEBUG_IFACE' in os.environ:\n             loop = asyncio.get_event_loop()\n             loop.run_forever()\n",
        "code_toks_joined": "def execute_from_command_line ( self , argv = None ) : <NEWLINE> <INDENT> if argv is None : <NEWLINE> <INDENT> argv = sys . argv <NEWLINE> <DEDENT> args = self . parser . parse_args ( argv [ 1 : ] ) <NEWLINE> self . nekumo . gateways = list ( self . parse_gateways ( args ) ) <NEWLINE> self . nekumo . ifaces = list ( self . parse_ifaces ( args ) ) <NEWLINE> if <STRING> in os . environ : <NEWLINE> <INDENT> loop = asyncio . get_event_loop ( ) <NEWLINE> loop . run_forever ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'NEKUMO_DEBUG_IFACE'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "63cb53ec3ad24438b01f747870b9b50d": {
        "code_string": "if token == 'TK_SEMI' or tk_end == len(sql):\n                 sqls.append(sql[beg:tk_end])\n                 beg = tk_end\n                 level = 0\n                 status=\"normal\"\n                 self.cte_dico = {}\n             elif token == \"TK_OTHER\" and not cte_inline: \n                 if tk_value.lower() == \"from\":\n                     from_lvl[level] = True\n                 elif from_lvl[level]:\n                     if last_other in(',', 'from', 'join') and (\n                     tk_value in self.cte_dico):\n                         #check if next token is as\n                         bg , en , tknext = tk_end , tk_end , 'TK_SP'\n                         while en < length and tknext == 'TK_SP' :\n                             bg, (en , tknext) = en, self.get_token(sql , en)\n                         #avoid the \"as x as y\" situation\n                         if sql[bg:en].lower() != 'as': \n                             sql2 = (sql[:end ] + \"(\"+ self.cte_dico[tk_value] + \n                               \") as \" + tk_value + \" \")\n                         else:\n                             sql2 = (sql[:end ] + \"(\"+ self.cte_dico[tk_value] + \n                               \")  \" + \" \")\n",
        "code_toks_joined": "if token == <STRING> or tk_end == len ( sql ) : <NEWLINE> <INDENT> sqls . append ( sql [ beg : tk_end ] ) <NEWLINE> beg = tk_end <NEWLINE> level = 0 <NEWLINE> status = <STRING> <NEWLINE> self . cte_dico = { } <NEWLINE> elif token == <STRING> and not cte_inline : <NEWLINE> if tk_value . lower ( ) == <STRING> : <NEWLINE> <INDENT> from_lvl [ level ] = True <NEWLINE> <DEDENT> elif from_lvl [ level ] : <NEWLINE> <INDENT> if last_other in ( <STRING> , <STRING> , <STRING> ) and ( <NEWLINE> tk_value in self . cte_dico ) : <NEWLINE> <COMMENT> <NL> <INDENT> bg , en , tknext = tk_end , tk_end , <STRING> <NEWLINE> while en < length and tknext == <STRING> : <NEWLINE> <INDENT> bg , ( en , tknext ) = en , self . get_token ( sql , en ) <NEWLINE> <COMMENT> <NL> <DEDENT> if sql [ bg : en ] . lower ( ) != <STRING> : <NEWLINE> <INDENT> sql2 = ( sql [ : end ] + <STRING> + self . cte_dico [ tk_value ] + <NEWLINE> <INDENT> <STRING> + tk_value + <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> sql2 = ( sql [ : end ] + <STRING> + self . cte_dico [ tk_value ] + <NEWLINE> <INDENT> <STRING> + <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'TK_SEMI'",
                "\"normal\"",
                "\"TK_OTHER\"",
                "\"from\"",
                "','",
                "'from'",
                "'join'",
                "'TK_SP'",
                "'TK_SP'",
                "'as'",
                "\"(\"",
                "\") as \"",
                "\" \"",
                "\"(\"",
                "\")  \"",
                "\" \""
            ],
            "<COMMENT>": [
                "#check if next token is as",
                "#avoid the \"as x as y\" situation"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3374ba6e244f4a64b5aeb01ffdcfe4bd": {
        "code_string": "for table_ref in tables:\n             table_sql = table_ref+\"$$\"\n             df = env[table_ref]\n             df = self._ensure_data_frame(df, table_ref)\n             #destroy previous Python temp table before importing the new one\n             pre_q = \"DROP TABLE IF EXISTS %s\" % table_sql\n             cur = self._execute_sql (pre_q)\n             self._write_table( table_sql, df, self.conn)\n         #multiple sql must be executed one by one\n         for q_single in self.get_sqlsplit(sql, True) :\n             if q_single.strip() != \"\":\n                 #cleanup previous CTE temp tables before executing another sql\n                 self.remove_tmp_tables(\"cte\")\n                 cur = self._execute_cte(q_single,  env)\n         return cur\n",
        "code_toks_joined": "for table_ref in tables : <NEWLINE> <INDENT> table_sql = table_ref + <STRING> <NEWLINE> df = env [ table_ref ] <NEWLINE> df = self . _ensure_data_frame ( df , table_ref ) <NEWLINE> <COMMENT> <NL> pre_q = <STRING> % table_sql <NEWLINE> cur = self . _execute_sql ( pre_q ) <NEWLINE> self . _write_table ( table_sql , df , self . conn ) <NEWLINE> <COMMENT> <NL> for q_single in self . get_sqlsplit ( sql , True ) : <NEWLINE> if q_single . strip ( ) != <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> self . remove_tmp_tables ( <STRING> ) <NEWLINE> cur = self . _execute_cte ( q_single , env ) <NEWLINE> return cur <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"$$\"",
                "\"DROP TABLE IF EXISTS %s\"",
                "\"\"",
                "\"cte\""
            ],
            "<COMMENT>": [
                "#destroy previous Python temp table before importing the new one",
                "#multiple sql must be executed one by one",
                "#cleanup previous CTE temp tables before executing another sql"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ea9a1dd1606542a086e16b077643deac": {
        "code_string": "global MASK_PREDICTOR_HANDLER\n   with LOCK:\n     if MASK_PREDICTOR_HANDLER is None:\n       MASK_PREDICTOR_HANDLER= MaskPredictor(deepLearningModel, boxSize, gpus)\n",
        "code_toks_joined": "global MASK_PREDICTOR_HANDLER <NEWLINE> <INDENT> with LOCK : <NEWLINE> <INDENT> if MASK_PREDICTOR_HANDLER is None : <NEWLINE> <INDENT> MASK_PREDICTOR_HANDLER = MaskPredictor ( deepLearningModel , boxSize , gpus ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d33b066798b48a696c6864b583455c9": {
        "code_string": "with MaskPredictor(deepLearningModelFname, boxSize, gpus=[0]) as mp:\n       mask = mp.predictMask(mic)\n",
        "code_toks_joined": "with MaskPredictor ( deepLearningModelFname , boxSize , gpus = [ 0 ] ) as mp : <NEWLINE> <INDENT> mask = mp . predictMask ( mic ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "48cc3af9d48f46e0b2f6d8427634e328": {
        "code_string": "def get_current_session(self):\n         if self.session[\"last_updated\"]:\n             if (\n                 arrow.get(self.session[\"last_updated\"]).shift(\n                     seconds=int(self.config[\"update_interval\"])\n                 )\n                 < arrow.utcnow()\n             ):\n                 return self.session\n",
        "code_toks_joined": "def get_current_session ( self ) : <NEWLINE> <INDENT> if self . session [ <STRING> ] : <NEWLINE> <INDENT> if ( <NEWLINE> <INDENT> arrow . get ( self . session [ <STRING> ] ) . shift ( <NEWLINE> <INDENT> seconds = int ( self . config [ <STRING> ] ) <NEWLINE> <DEDENT> ) <NEWLINE> < arrow . utcnow ( ) <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> return self . session <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"last_updated\"",
                "\"last_updated\"",
                "\"update_interval\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "31493b6f3ad847c69e4446b52e4f1485": {
        "code_string": "def get_history(self, start_time, end_time):\n         result = []\n         for day in arrow.Arrow.range(\"day\", start_time, end_time):\n             data = self.request(\n                 self.DATA_ENDPOINT,\n                 method=\"get\",\n                 params={\"startTime\": day.format(\"MM/DD/YYYY hh:mm:ss\")},\n             )\n             result.append(Day._from_data(start_time, data))\n         return result\n",
        "code_toks_joined": "def get_history ( self , start_time , end_time ) : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for day in arrow . Arrow . range ( <STRING> , start_time , end_time ) : <NEWLINE> <INDENT> data = self . request ( <NEWLINE> <INDENT> self . DATA_ENDPOINT , <NEWLINE> method = <STRING> , <NEWLINE> params = { <STRING> : day . format ( <STRING> ) } , <NEWLINE> <DEDENT> ) <NEWLINE> result . append ( Day . _from_data ( start_time , data ) ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"day\"",
                "\"get\"",
                "\"startTime\"",
                "\"MM/DD/YYYY hh:mm:ss\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "528f905e1acb48e3af2b9ca8ccd6f199": {
        "code_string": "# Select the columns needed for evaluations\n     rating_true = rating_true[[col_user, col_item, col_rating]]\n     rating_pred = rating_true[[col_user, col_item, col_prediction]]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> rating_true = rating_true [ [ col_user , col_item , col_rating ] ] <NEWLINE> rating_pred = rating_true [ [ col_user , col_item , col_prediction ] ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Select the columns needed for evaluations"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8d959fddbfb1420c95c3304524ffbf3d": {
        "code_string": "# Add split index (this makes splitting by group more efficient).\n     for i in range(len(split_index)):\n         splits[i]['split_index'] = i\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for i in range ( len ( split_index ) ) : <NEWLINE> <INDENT> splits [ i ] [ <STRING> ] = i <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Add split index (this makes splitting by group more efficient)."
            ],
            "<STRING>": [
                "'split_index'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9173f6163fcb471894a498d21b06d5af": {
        "code_string": "@classmethod\n     def from_string(cls, s):\n         # type: (str) -> Xid\n         val = base32hex.b32decode(s.upper())\n         value_check = [0 < x < 255 for x in val]\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def from_string ( cls , s ) : <NEWLINE> <COMMENT> <NL> <INDENT> val = base32hex . b32decode ( s . upper ( ) ) <NEWLINE> value_check = [ 0 < x < 255 for x in val ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# type: (str) -> Xid"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cb81510c182d4980b6c0bcdf3c8e4225": {
        "code_string": "if \"secondaryFiles\" in binding:\n                         if \"secondaryFiles\" not in datum:\n                             datum[\"secondaryFiles\"] = []\n                         for sf in aslist(schema[\"secondaryFiles\"]):\n                             if isinstance(sf, dict):\n                                 sfpath = expression.do_eval(sf, self.job, self.requirements, self.docpath, datum[\"path\"])\n                             else:\n                                 sfpath = {\"path\": substitute(datum[\"path\"], sf)}\n                             if isinstance(sfpath, list):\n                                 datum[\"secondaryFiles\"].extend(sfpath)\n                             else:\n                                 datum[\"secondaryFiles\"].append(sfpath)\n                             self.files.append(sfpath)\n",
        "code_toks_joined": "if <STRING> in binding : <NEWLINE> <INDENT> if <STRING> not in datum : <NEWLINE> <INDENT> datum [ <STRING> ] = [ ] <NEWLINE> <DEDENT> for sf in aslist ( schema [ <STRING> ] ) : <NEWLINE> <INDENT> if isinstance ( sf , dict ) : <NEWLINE> <INDENT> sfpath = expression . do_eval ( sf , self . job , self . requirements , self . docpath , datum [ <STRING> ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sfpath = { <STRING> : substitute ( datum [ <STRING> ] , sf ) } <NEWLINE> <DEDENT> if isinstance ( sfpath , list ) : <NEWLINE> <INDENT> datum [ <STRING> ] . extend ( sfpath ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> datum [ <STRING> ] . append ( sfpath ) <NEWLINE> <DEDENT> self . files . append ( sfpath ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"secondaryFiles\"",
                "\"secondaryFiles\"",
                "\"secondaryFiles\"",
                "\"secondaryFiles\"",
                "\"path\"",
                "\"path\"",
                "\"path\"",
                "\"secondaryFiles\"",
                "\"secondaryFiles\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9207cbdfdd174467a0db28412db61626": {
        "code_string": "def _draft2toDraft3dev1(doc, loader, baseuri):  # type: (Any, Loader, str) -> Any\n     try:\n         if isinstance(doc, dict):\n             if \"import\" in doc:\n                 imp = urlparse.urljoin(baseuri, doc[\"import\"])\n                 impLoaded = loader.fetch(imp)\n                 r = None  # type: Dict[str, Any]\n                 if isinstance(impLoaded, list):\n                     r = {\"@graph\": r}\n                 elif isinstance(impLoaded, dict):\n                     r = impLoaded\n                 else:\n                     raise Exception(\"Unexpected code path.\")\n                 r[\"id\"] = imp\n                 _, frag = urlparse.urldefrag(imp)\n                 if frag:\n                     frag = \"#\" + frag\n                     r = findId(r, frag)\n                 return _draft2toDraft3dev1(r, loader, imp)\n",
        "code_toks_joined": "def _draft2toDraft3dev1 ( doc , loader , baseuri ) : <COMMENT> <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if isinstance ( doc , dict ) : <NEWLINE> <INDENT> if <STRING> in doc : <NEWLINE> <INDENT> imp = urlparse . urljoin ( baseuri , doc [ <STRING> ] ) <NEWLINE> impLoaded = loader . fetch ( imp ) <NEWLINE> r = None <COMMENT> <NEWLINE> if isinstance ( impLoaded , list ) : <NEWLINE> <INDENT> r = { <STRING> : r } <NEWLINE> <DEDENT> elif isinstance ( impLoaded , dict ) : <NEWLINE> <INDENT> r = impLoaded <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> r [ <STRING> ] = imp <NEWLINE> _ , frag = urlparse . urldefrag ( imp ) <NEWLINE> if frag : <NEWLINE> <INDENT> frag = <STRING> + frag <NEWLINE> r = findId ( r , frag ) <NEWLINE> <DEDENT> return _draft2toDraft3dev1 ( r , loader , imp ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# type: (Any, Loader, str) -> Any",
                "# type: Dict[str, Any]"
            ],
            "<STRING>": [
                "\"import\"",
                "\"import\"",
                "\"@graph\"",
                "\"Unexpected code path.\"",
                "\"id\"",
                "\"#\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ba53d13c71724542a833dc2b6a2e0c69": {
        "code_string": "def _resolve_idmap(self, document, loader):\n         # type: (Dict[unicode, Union[Dict[unicode, Dict[unicode, unicode]], List[Dict[unicode, Any]]]], Loader) -> None\n         # Convert fields with mapSubject into lists\n         # use mapPredicate if the mapped value isn't a dict.\n         for idmapField in loader.idmap:\n             if (idmapField in document):\n                 idmapFieldValue = document[idmapField]\n                 if (isinstance(idmapFieldValue, dict)\n                         and \"$import\" not in idmapFieldValue\n                         and \"$include\" not in idmapFieldValue):\n                     ls = []\n                     for k in sorted(idmapFieldValue.keys()):\n                         val = idmapFieldValue[k]\n                         v = None  # type: Dict[unicode, Any]\n                         if not isinstance(v, dict):\n                             if idmapField in loader.mapPredicate:\n                                 v = {loader.mapPredicate[idmapField]: val}\n                             else:\n                                 raise validate.ValidationException(\n                                     \"mapSubject '%s' value '%s' is not a dict\"\n                                     \"and does not have a mapPredicate\", k, v)\n                         else:\n                             v = val\n                         v[loader.idmap[idmapField]] = k\n                         ls.append(v)\n                     document[idmapField] = ls\n",
        "code_toks_joined": "def _resolve_idmap ( self , document , loader ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> for idmapField in loader . idmap : <NEWLINE> <INDENT> if ( idmapField in document ) : <NEWLINE> <INDENT> idmapFieldValue = document [ idmapField ] <NEWLINE> if ( isinstance ( idmapFieldValue , dict ) <NEWLINE> <INDENT> and <STRING> not in idmapFieldValue <NEWLINE> and <STRING> not in idmapFieldValue ) : <NEWLINE> ls = [ ] <NEWLINE> for k in sorted ( idmapFieldValue . keys ( ) ) : <NEWLINE> val = idmapFieldValue [ k ] <NEWLINE> v = None <COMMENT> <NEWLINE> if not isinstance ( v , dict ) : <NEWLINE> <INDENT> if idmapField in loader . mapPredicate : <NEWLINE> <INDENT> v = { loader . mapPredicate [ idmapField ] : val } <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise validate . ValidationException ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> , k , v ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v = val <NEWLINE> <DEDENT> v [ loader . idmap [ idmapField ] ] = k <NEWLINE> ls . append ( v ) <NEWLINE> document [ idmapField ] = ls <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# type: (Dict[unicode, Union[Dict[unicode, Dict[unicode, unicode]], List[Dict[unicode, Any]]]], Loader) -> None",
                "# Convert fields with mapSubject into lists",
                "# use mapPredicate if the mapped value isn't a dict.",
                "# type: Dict[unicode, Any]"
            ],
            "<STRING>": [
                "\"$import\"",
                "\"$include\"",
                "\"mapSubject '%s' value '%s' is not a dict\"",
                "\"and does not have a mapPredicate\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3593fd3cca1143069a88ec528742a135": {
        "code_string": "async_result = gevent.event.AsyncResult()\n             gevent.spawn(self._process_response, method, bufchan,\n                     timeout).link(async_result)\n             return async_result\n         except:\n             # XXX: This is going to be closed twice if async is false and\n             # _process_response raises an exception. I wonder if the above\n             # async branch can raise an exception too, if no we can just remove\n             # this code.\n             bufchan.close()\n             raise\n",
        "code_toks_joined": "async_result = gevent . event . AsyncResult ( ) <NEWLINE> <INDENT> gevent . spawn ( self . _process_response , method , bufchan , <NEWLINE> <INDENT> timeout ) . link ( async_result ) <NEWLINE> <DEDENT> return async_result <NEWLINE> except : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> bufchan . close ( ) <NEWLINE> raise <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# XXX: This is going to be closed twice if async is false and",
                "# _process_response raises an exception. I wonder if the above",
                "# async branch can raise an exception too, if no we can just remove",
                "# this code."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d755df4315c84bb18eb0f370bd059166": {
        "code_string": "def get_name(self, regex, path=None):\n         \"\"\"Return the part of the name of the image, matching regex.\"\"\"\n         if path is None:\n             path = self.path\n         return super(CamImage, self).get_name(path, regex)\n",
        "code_toks_joined": "def get_name ( self , regex , path = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if path is None : <NEWLINE> <INDENT> path = self . path <NEWLINE> <DEDENT> return super ( CamImage , self ) . get_name ( path , regex ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return the part of the name of the image, matching regex.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2109a78f1475429bb2e9aaf1a41a888f": {
        "code_string": "highlight = 0\n page = 0\n state = \"top\"\n key = 0\n try:\n \twhile key != ord('q'):\n \t\tif windowsize[0] > 8 and windowsize[1] > 30:\n \t\t\twin_l.clear()\n \t\t\twin_l.border(0)\n \t\t\twin_r.clear()\n \t\t\twin_r.border(0)\n \t\t\twin_l.addstr(windowsize[0]-1, windowsize[1]//2-9, \"page:\"+str(page+1))\n \t\t\twin_r.addstr(windowsize[0]-4, windowsize[1]//2-11, \"search: s\")\n \t\t\twin_r.addstr(windowsize[0]-3, windowsize[1]//2-12, \"refresh: r\")\n \t\t\twin_r.addstr(windowsize[0]-2, windowsize[1]//2-9, \"quit: q\")\n \t\t\tindex = 0\n \t\t\tif state == \"top\":\n \t\t\t\ttotalitems = len(data['top'])\n \t\t\t\tcurrentpage = data['top'][maxitems*page:maxitems*(page+1)]\n \t\t\t\tfor i in currentpage:\n \t\t\t\t\tif index < maxitems:\n \t\t\t\t\t\tif index == highlight:\n \t\t\t\t\t\t\twin_l.addnstr(index*2+2, 2, str(i['game']['name']), maxlen, curses.A_REVERSE)\n \t\t\t\t\t\t\twin_r.addnstr(2, 3, \"Viewers: \"+str(i['viewers']), maxlen)\n \t\t\t\t\t\t\twin_r.addnstr(3, 3, \"Channels: \"+str(i['channels']), maxlen)\n \t\t\t\t\t\telse:\n \t\t\t\t\t\t\twin_l.addnstr(index*2+2, 2, str(i['game']['name']), maxlen)\n \t\t\t\t\tindex += 1\n \t\t\tif state == \"search\":\n \t\t\t\ttotalitems = len(data['streams'])\n \t\t\t\tcurrentpage = data['streams'][maxitems*page:maxitems*(page+1)]\n \t\t\t\tfor i in currentpage:\n \t\t\t\t\tif index < maxitems:\n \t\t\t\t\t\tif index == highlight:\n \t\t\t\t\t\t\twin_l.addnstr(index*2+2, 2, str(i['channel']['display_name']), maxlen, curses.A_REVERSE)\n \t\t\t\t\t\t\twin_r.addnstr(2, 3, str(i['game']), maxlen)\n \t\t\t\t\t\t\twin_r.addnstr(4, 3, \"Viewers: \"+str(i['viewers']), maxlen)\n \t\t\t\t\t\t\twin_r.addstr(5, 3, \"Status:\")\n \t\t\t\t\t\t\tstatus = textwrap.wrap(str(i['channel']['status']), windowsize[1]//2-6)\n \t\t\t\t\t\t\tl_num = 7\n \t\t\t\t\t\t\tfor line in status:\n \t\t\t\t\t\t\t\tif l_num >= windowsize[0] - 2:\n \t\t\t\t\t\t\t\t\tbreak\n \t\t\t\t\t\t\t\twin_r.addstr(l_num, 4, line)\n \t\t\t\t\t\t\t\tl_num += 1\n \t\t\t\t\t\telse:\n \t\t\t\t\t\t\twin_l.addnstr(index*2+2, 2, str(i['channel']['display_name']), maxlen)\n \t\t\t\t\tindex += 1\n \t\t\twin_l.refresh()\n \t\t\twin_r.refresh()\n \t\telse:\n \t\t\tstdscr.clear()\n \t\t\tstdscr.addstr(0,0,\"Terminal\")\n \t\t\tstdscr.addstr(1,0,\"too small\")\n \t\tkey = stdscr.getch()\n \t\tif key == curses.KEY_DOWN:\n \t\t\tif highlight + page * maxitems + 1 != totalitems:\n \t\t\t\tif highlight + 1 == maxitems:\n \t\t\t\t\tpage += 1\n \t\t\t\t\thighlight = 0\n \t\t\t\telse:\n \t\t\t\t\thighlight += 1\n \t\telif key == curses.KEY_UP:\n \t\t\tif highlight == 0 and page > 0:\n \t\t\t\tpage -= 1\n \t\t\t\thighlight = maxitems - 1\n \t\t\telif highlight > 0:\n \t\t\t\thighlight -= 1\n \t\telif key == curses.KEY_NPAGE and totalitems > (page+1) * maxitems:\n \t\t\thighlight = 0\n \t\t\tpage += 1\n \t\telif key == curses.KEY_PPAGE and page > 0:\n \t\t\thighlight = 0\n \t\t\tpage -= 1\n \t\telif key == curses.KEY_RIGHT or key == 10:\n \t\t\tif state == \"search\":\n \t\t\t\tcurses.nocbreak(); stdscr.keypad(0); curses.echo()\n \t\t\t\tcurses.endwin()\n \t\t\t\tprint(\"twitch-cli: Launching livestreamer\")\n \t\t\t\tsubprocess.call([\"livestreamer\", currentpage[highlight]['channel']['url'], \"best\"])\n \t\t\t\tstdscr = curses.initscr()\n \t\t\t\tcurses.noecho()\n \t\t\t\tcurses.cbreak()\n \t\t\t\tstdscr.keypad(1)\n \t\t\telif state == \"top\":\n \t\t\t\tinit_display(stdscr)\n \t\t\t\tquery = [currentpage[highlight]['game']['name'], 0]\n \t\t\t\tdata = query_twitch(query[0], query[1])\n \t\t\t\tstate = \"search\"\n \t\t\t\thighlight = 0\n \t\t\t\tpage = 0\n \t\telif key == curses.KEY_LEFT:\n \t\t\tif state != \"top\":\n \t\t\t\tinit_display(stdscr)\n \t\t\t\tdata = query_twitch(\"topgames\", 0)\n \t\t\t\tstate = \"top\"\n \t\t\t\thighlight = 0\n \t\t\t\tpage = 0\n \t\telif key == ord('s'):\n \t\t\tsearchbox = curses.newwin(3, windowsize[1]-4, windowsize[0]//2-1, 2)\n \t\t\tsearchbox.border(0)\n \t\t\tsearchbox.addnstr(0, 3, \"Search for streams\", windowsize[0]-4)\n \t\t\tsearchbox.refresh()\n \t\t\tcurses.echo()\n \t\t\ts = searchbox.getstr(1,1, windowsize[1]-6)\n \t\t\tinit_display(stdscr)\n \t\t\tquery = [s.decode(\"utf-8\"), 1]\n \t\t\tdata = query_twitch(query[0], query[1])\n \t\t\tstate = \"search\"\n \t\t\thighlight = 0\n \t\t\tpage = 0\n \t\telif key == ord('r'):\n \t\t\tif state == \"search\":\n \t\t\t\tinit_display(stdscr)\n \t\t\t\tdata = query_twitch(query[0], query[1])\n \t\t\telif state == \"top\":\n \t\t\t\tinit_display(stdscr)\n \t\t\t\tdata = query_twitch(\"topgames\", 0)\n \t\t\thighlight = 0\n \t\t\tpage = 0\n \t\telif key == curses.KEY_RESIZE:\n \t\t\twindowsize = init_display(stdscr)\n \t\t\thighlight = 0\n \t\t\tpage = 0\n finally:\n \tcurses.nocbreak(); stdscr.keypad(0); curses.echo()\n \tcurses.endwin()\n \tprint(\"twitch-cli: Exiting\")",
        "code_toks_joined": "highlight = 0 <NEWLINE> <INDENT> page = 0 <NEWLINE> state = <STRING> <NEWLINE> key = 0 <NEWLINE> try : <NEWLINE> <INDENT> while key != ord ( <STRING> ) : <NEWLINE> <INDENT> if windowsize [ 0 ] > 8 and windowsize [ 1 ] > 30 : <NEWLINE> <INDENT> win_l . clear ( ) <NEWLINE> win_l . border ( 0 ) <NEWLINE> win_r . clear ( ) <NEWLINE> win_r . border ( 0 ) <NEWLINE> win_l . addstr ( windowsize [ 0 ] - 1 , windowsize [ 1 ] // 2 - 9 , <STRING> + str ( page + 1 ) ) <NEWLINE> win_r . addstr ( windowsize [ 0 ] - 4 , windowsize [ 1 ] // 2 - 11 , <STRING> ) <NEWLINE> win_r . addstr ( windowsize [ 0 ] - 3 , windowsize [ 1 ] // 2 - 12 , <STRING> ) <NEWLINE> win_r . addstr ( windowsize [ 0 ] - 2 , windowsize [ 1 ] // 2 - 9 , <STRING> ) <NEWLINE> index = 0 <NEWLINE> if state == <STRING> : <NEWLINE> <INDENT> totalitems = len ( data [ <STRING> ] ) <NEWLINE> currentpage = data [ <STRING> ] [ maxitems * page : maxitems * ( page + 1 ) ] <NEWLINE> for i in currentpage : <NEWLINE> <INDENT> if index < maxitems : <NEWLINE> <INDENT> if index == highlight : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen , curses . A_REVERSE ) <NEWLINE> win_r . addnstr ( 2 , 3 , <STRING> + str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> win_r . addnstr ( 3 , 3 , <STRING> + str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen ) <NEWLINE> <DEDENT> <DEDENT> index += 1 <NEWLINE> <DEDENT> <DEDENT> if state == <STRING> : <NEWLINE> <INDENT> totalitems = len ( data [ <STRING> ] ) <NEWLINE> currentpage = data [ <STRING> ] [ maxitems * page : maxitems * ( page + 1 ) ] <NEWLINE> for i in currentpage : <NEWLINE> <INDENT> if index < maxitems : <NEWLINE> <INDENT> if index == highlight : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen , curses . A_REVERSE ) <NEWLINE> win_r . addnstr ( 2 , 3 , str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> win_r . addnstr ( 4 , 3 , <STRING> + str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> win_r . addstr ( 5 , 3 , <STRING> ) <NEWLINE> status = textwrap . wrap ( str ( i [ <STRING> ] [ <STRING> ] ) , windowsize [ 1 ] // 2 - 6 ) <NEWLINE> l_num = 7 <NEWLINE> for line in status : <NEWLINE> <INDENT> if l_num >= windowsize [ 0 ] - 2 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> win_r . addstr ( l_num , 4 , line ) <NEWLINE> l_num += 1 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen ) <NEWLINE> <DEDENT> <DEDENT> index += 1 <NEWLINE> <DEDENT> <DEDENT> win_l . refresh ( ) <NEWLINE> win_r . refresh ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stdscr . clear ( ) <NEWLINE> stdscr . addstr ( 0 , 0 , <STRING> ) <NEWLINE> stdscr . addstr ( 1 , 0 , <STRING> ) <NEWLINE> <DEDENT> key = stdscr . getch ( ) <NEWLINE> if key == curses . KEY_DOWN : <NEWLINE> <INDENT> if highlight + page * maxitems + 1 != totalitems : <NEWLINE> <INDENT> if highlight + 1 == maxitems : <NEWLINE> <INDENT> page += 1 <NEWLINE> highlight = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> highlight += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif key == curses . KEY_UP : <NEWLINE> <INDENT> if highlight == 0 and page > 0 : <NEWLINE> <INDENT> page -= 1 <NEWLINE> highlight = maxitems - 1 <NEWLINE> <DEDENT> elif highlight > 0 : <NEWLINE> <INDENT> highlight -= 1 <NEWLINE> <DEDENT> <DEDENT> elif key == curses . KEY_NPAGE and totalitems > ( page + 1 ) * maxitems : <NEWLINE> <INDENT> highlight = 0 <NEWLINE> page += 1 <NEWLINE> <DEDENT> elif key == curses . KEY_PPAGE and page > 0 : <NEWLINE> <INDENT> highlight = 0 <NEWLINE> page -= 1 <NEWLINE> <DEDENT> elif key == curses . KEY_RIGHT or key == 10 : <NEWLINE> <INDENT> if state == <STRING> : <NEWLINE> <INDENT> curses . nocbreak ( ) ; stdscr . keypad ( 0 ) ; curses . echo ( ) <NEWLINE> curses . endwin ( ) <NEWLINE> print ( <STRING> ) <NEWLINE> subprocess . call ( [ <STRING> , currentpage [ highlight ] [ <STRING> ] [ <STRING> ] , <STRING> ] ) <NEWLINE> stdscr = curses . initscr ( ) <NEWLINE> curses . noecho ( ) <NEWLINE> curses . cbreak ( ) <NEWLINE> stdscr . keypad ( 1 ) <NEWLINE> <DEDENT> elif state == <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> query = [ currentpage [ highlight ] [ <STRING> ] [ <STRING> ] , 0 ] <NEWLINE> data = query_twitch ( query [ 0 ] , query [ 1 ] ) <NEWLINE> state = <STRING> <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> <DEDENT> elif key == curses . KEY_LEFT : <NEWLINE> <INDENT> if state != <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> data = query_twitch ( <STRING> , 0 ) <NEWLINE> state = <STRING> <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> <DEDENT> elif key == ord ( <STRING> ) : <NEWLINE> <INDENT> searchbox = curses . newwin ( 3 , windowsize [ 1 ] - 4 , windowsize [ 0 ] // 2 - 1 , 2 ) <NEWLINE> searchbox . border ( 0 ) <NEWLINE> searchbox . addnstr ( 0 , 3 , <STRING> , windowsize [ 0 ] - 4 ) <NEWLINE> searchbox . refresh ( ) <NEWLINE> curses . echo ( ) <NEWLINE> s = searchbox . getstr ( 1 , 1 , windowsize [ 1 ] - 6 ) <NEWLINE> init_display ( stdscr ) <NEWLINE> query = [ s . decode ( <STRING> ) , 1 ] <NEWLINE> data = query_twitch ( query [ 0 ] , query [ 1 ] ) <NEWLINE> state = <STRING> <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> elif key == ord ( <STRING> ) : <NEWLINE> <INDENT> if state == <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> data = query_twitch ( query [ 0 ] , query [ 1 ] ) <NEWLINE> <DEDENT> elif state == <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> data = query_twitch ( <STRING> , 0 ) <NEWLINE> <DEDENT> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> elif key == curses . KEY_RESIZE : <NEWLINE> <INDENT> windowsize = init_display ( stdscr ) <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> curses . nocbreak ( ) ; stdscr . keypad ( 0 ) ; curses . echo ( ) <NEWLINE> curses . endwin ( ) <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"top\"",
                "'q'",
                "\"page:\"",
                "\"search: s\"",
                "\"refresh: r\"",
                "\"quit: q\"",
                "\"top\"",
                "'top'",
                "'top'",
                "'game'",
                "'name'",
                "\"Viewers: \"",
                "'viewers'",
                "\"Channels: \"",
                "'channels'",
                "'game'",
                "'name'",
                "\"search\"",
                "'streams'",
                "'streams'",
                "'channel'",
                "'display_name'",
                "'game'",
                "\"Viewers: \"",
                "'viewers'",
                "\"Status:\"",
                "'channel'",
                "'status'",
                "'channel'",
                "'display_name'",
                "\"Terminal\"",
                "\"too small\"",
                "\"search\"",
                "\"twitch-cli: Launching livestreamer\"",
                "\"livestreamer\"",
                "'channel'",
                "'url'",
                "\"best\"",
                "\"top\"",
                "'game'",
                "'name'",
                "\"search\"",
                "\"top\"",
                "\"topgames\"",
                "\"top\"",
                "'s'",
                "\"Search for streams\"",
                "\"utf-8\"",
                "\"search\"",
                "'r'",
                "\"search\"",
                "\"top\"",
                "\"topgames\"",
                "\"twitch-cli: Exiting\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b4dd419661eb465f94357973cb69f9a2": {
        "code_string": "def get_properties(self, items):\n         \"\"\"\n         Return a namedtuple containing suitable properties that may be used to \n         format `items` as a columnized string. The members of the tuple are:\n         `column_widths`, `spacing`, `num_lines`.\n         \"\"\"\n         self._check_values()\n         item_widths = [len(item) for item in items]\n         if not item_widths:\n             column_widths, num_lines = [], 0\n         elif any(width > self.max_line_width for width in item_widths):\n             column_widths, num_lines = [self.max_line_width], len(item_widths)\n         else:\n             column_widths, num_lines = self.calculate_columns(item_widths)\n         return LineProperties(column_widths, self.spacing, num_lines)\n",
        "code_toks_joined": "def get_properties ( self , items ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _check_values ( ) <NEWLINE> item_widths = [ len ( item ) for item in items ] <NEWLINE> if not item_widths : <NEWLINE> <INDENT> column_widths , num_lines = [ ] , 0 <NEWLINE> <DEDENT> elif any ( width > self . max_line_width for width in item_widths ) : <NEWLINE> <INDENT> column_widths , num_lines = [ self . max_line_width ] , len ( item_widths ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> column_widths , num_lines = self . calculate_columns ( item_widths ) <NEWLINE> <DEDENT> return LineProperties ( column_widths , self . spacing , num_lines ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Return a namedtuple containing suitable properties that may be used to \n         format `items` as a columnized string. The members of the tuple are:\n         `column_widths`, `spacing`, `num_lines`.\n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a754235e5aa14730b75f062861acb558": {
        "code_string": "for attr_name, member in class_state.iteritems():\n             if callable(member):\n                 deps_used = parse_deps_used(member)\n                 invalid_ports = deps_used.difference(needs_ports_defined).difference(RESERVED_PORT_NAMES)\n                 all_deps_used.update(deps_used)\n                 if invalid_ports:\n                     raise UnknownPort('{}.{} references undeclared Needs - {}'.format(\n                         class_name,\n                         attr_name,\n                         ', '.join(sorted(deps_used))\n                     ))\n",
        "code_toks_joined": "for attr_name , member in class_state . iteritems ( ) : <NEWLINE> <INDENT> if callable ( member ) : <NEWLINE> <INDENT> deps_used = parse_deps_used ( member ) <NEWLINE> invalid_ports = deps_used . difference ( needs_ports_defined ) . difference ( RESERVED_PORT_NAMES ) <NEWLINE> all_deps_used . update ( deps_used ) <NEWLINE> if invalid_ports : <NEWLINE> <INDENT> raise UnknownPort ( <STRING> . format ( <NEWLINE> <INDENT> class_name , <NEWLINE> attr_name , <NEWLINE> <STRING> . join ( sorted ( deps_used ) ) <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'{}.{} references undeclared Needs - {}'",
                "', '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e16089b03e764f368f602fa6e26140a0": {
        "code_string": "# Annotate all the input variables which are Theano variables\n         def copy_and_tag(variable, role, name):\n             \"\"\"Helper method to copy a variable and annotate it.\"\"\"\n             copy = variable.copy()\n             copy.name = \"{}_{}_{}\".format(  # Theano name\n                 brick.name, self.name, name)\n             annotations = getattr(copy.tag, 'annotations', []) + [brick, call]\n             copy.tag.annotations = annotations\n             copy.tag.name = name  # Blocks name\n             VariableRole.add_role(variable, role)\n             return copy\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def copy_and_tag ( variable , role , name ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> copy = variable . copy ( ) <NEWLINE> copy . name = <STRING> . format ( <COMMENT> <NEWLINE> <INDENT> brick . name , self . name , name ) <NEWLINE> <DEDENT> annotations = getattr ( copy . tag , <STRING> , [ ] ) + [ brick , call ] <NEWLINE> copy . tag . annotations = annotations <NEWLINE> copy . tag . name = name <COMMENT> <NEWLINE> VariableRole . add_role ( variable , role ) <NEWLINE> return copy <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Annotate all the input variables which are Theano variables",
                "# Theano name",
                "# Blocks name"
            ],
            "<STRING>": [
                "\"\"\"Helper method to copy a variable and annotate it.\"\"\"",
                "\"{}_{}_{}\"",
                "'annotations'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "94b19346c67244559821778d7c03d425": {
        "code_string": "def check_gaussian(rng, mean, std, shape):\n         weights = IsotropicGaussian(mean, std).generate(rng, shape)\n         assert weights.shape == shape\n         assert weights.dtype == theano.config.floatX\n         assert_allclose(weights.mean(), mean, atol=1e-2)\n         assert_allclose(weights.std(), std, atol=1e-2)\n     yield check_gaussian, rng, 0, 1, (500, 600)\n     yield check_gaussian, rng, 5, 3, (600, 500)\n",
        "code_toks_joined": "def check_gaussian ( rng , mean , std , shape ) : <NEWLINE> <INDENT> weights = IsotropicGaussian ( mean , std ) . generate ( rng , shape ) <NEWLINE> assert weights . shape == shape <NEWLINE> assert weights . dtype == theano . config . floatX <NEWLINE> assert_allclose ( weights . mean ( ) , mean , atol = 1e-2 ) <NEWLINE> assert_allclose ( weights . std ( ) , std , atol = 1e-2 ) <NEWLINE> yield check_gaussian , rng , 0 , 1 , ( 500 , 600 ) <NEWLINE> yield check_gaussian , rng , 5 , 3 , ( 600 , 500 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "901ecd048ced4b02927ba67f286ad864": {
        "code_string": "params = bricks.get_params()\n     for name in params.keys():\n         if name not in params:\n             logger.error(\"No value is provided for the parameter {}\"\n                          .format(name))\n",
        "code_toks_joined": "params = bricks . get_params ( ) <NEWLINE> <INDENT> for name in params . keys ( ) : <NEWLINE> <INDENT> if name not in params : <NEWLINE> <INDENT> logger . error ( <STRING> <NEWLINE> <INDENT> . format ( name ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"No value is provided for the parameter {}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f7779895f9bb4fdd96767b1fd769c58c": {
        "code_string": "def compute_step(self, param, previous_step):\n         if any(axis >= previous_step.ndim for axis in self.axes):\n             raise ValueError(\"Invalid axes {} for {}, ndim={}\".format(\n                 self.axes, param, param.ndim))\n         squares = tensor.sqr(previous_step)\n         if len(self.axes) == 0:\n             norms = l2_norm([previous_step])\n         else:\n             norms = tensor.sqrt(\n                 reduce(lambda t, a: t.sum(axis=a, keepdims=True),\n                        sorted(self.axes), squares))\n         return (previous_step * tensor.switch(norms > self.threshold,\n                                               self.threshold / norms,\n                                               1.), ())\n",
        "code_toks_joined": "def compute_step ( self , param , previous_step ) : <NEWLINE> <INDENT> if any ( axis >= previous_step . ndim for axis in self . axes ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( <NEWLINE> <INDENT> self . axes , param , param . ndim ) ) <NEWLINE> <DEDENT> <DEDENT> squares = tensor . sqr ( previous_step ) <NEWLINE> if len ( self . axes ) == 0 : <NEWLINE> <INDENT> norms = l2_norm ( [ previous_step ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> norms = tensor . sqrt ( <NEWLINE> <INDENT> reduce ( lambda t , a : t . sum ( axis = a , keepdims = True ) , <NEWLINE> <INDENT> sorted ( self . axes ) , squares ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return ( previous_step * tensor . switch ( norms > self . threshold , <NEWLINE> <INDENT> self . threshold / norms , <NEWLINE> 1. ) , ( ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Invalid axes {} for {}, ndim={}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "99431f3ded1a4a1bb248297e1fd4e6c5": {
        "code_string": "cleaned_img1d = geometry_converter.image_2d_to_1d(reference_img, fits_metadata_dict['cam_id'])\n                         hillas_params_2_cleaned_img = get_hillas_parameters(geom1d, cleaned_img1d, HILLAS_IMPLEMENTATION)    # GEOM\n",
        "code_toks_joined": "cleaned_img1d = geometry_converter . image_2d_to_1d ( reference_img , fits_metadata_dict [ <STRING> ] ) <NEWLINE> <INDENT> hillas_params_2_cleaned_img = get_hillas_parameters ( geom1d , cleaned_img1d , HILLAS_IMPLEMENTATION ) <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'cam_id'"
            ],
            "<COMMENT>": [
                "# GEOM"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "845f1455e8c448d78539dc9b9ddf04c7": {
        "code_string": "# Check the length of filter_thresholds_str is consistent with self.num_scales\n         if len(filter_thresholds_str) != (self.num_scales - 1):\n             raise ValueError(\"The tested solution has a wrong number of dimensions: \"\n                              \"{} instead of {}\".format(len(filter_thresholds),\n                                                        self.num_scales - 1))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if len ( filter_thresholds_str ) != ( self . num_scales - 1 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( len ( filter_thresholds ) , <NEWLINE> <INDENT> self . num_scales - 1 ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check the length of filter_thresholds_str is consistent with self.num_scales"
            ],
            "<STRING>": [
                "\"The tested solution has a wrong number of dimensions: \"",
                "\"{} instead of {}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af4e20ba60a649ec89f0d5fe7466abe3": {
        "code_string": "class Inertial(object):\n     def __init__(self, ixx=0.0, ixy=0.0, ixz=0.0, iyy=0.0, iyz=0.0, izz=0.0,\n                  mass=0.0, origin=None):\n         self.matrix = {}\n         self.matrix['ixx'] = ixx\n         self.matrix['ixy'] = iyy\n         self.matrix['ixz'] = ixz\n         self.matrix['iyy'] = iyy\n         self.matrix['iyz'] = iyz\n         self.matrix['izz'] = izz\n         self.mass = mass\n         self.origin = origin\n",
        "code_toks_joined": "class Inertial ( object ) : <NEWLINE> <INDENT> def __init__ ( self , ixx = 0.0 , ixy = 0.0 , ixz = 0.0 , iyy = 0.0 , iyz = 0.0 , izz = 0.0 , <NEWLINE> <INDENT> mass = 0.0 , origin = None ) : <NEWLINE> self . matrix = { } <NEWLINE> self . matrix [ <STRING> ] = ixx <NEWLINE> self . matrix [ <STRING> ] = iyy <NEWLINE> self . matrix [ <STRING> ] = ixz <NEWLINE> self . matrix [ <STRING> ] = iyy <NEWLINE> self . matrix [ <STRING> ] = iyz <NEWLINE> self . matrix [ <STRING> ] = izz <NEWLINE> self . mass = mass <NEWLINE> self . origin = origin <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ixx'",
                "'ixy'",
                "'ixz'",
                "'iyy'",
                "'iyz'",
                "'izz'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ccc1474b98644817b6b9b102ecbc7e40": {
        "code_string": "def __cal_flag_len(self):\n         if Markers.__flag_len < len(self.__flag_name):\n             Markers.__flag_len = len(self.__flag_name)+2\n",
        "code_toks_joined": "def __cal_flag_len ( self ) : <NEWLINE> <INDENT> if Markers . __flag_len < len ( self . __flag_name ) : <NEWLINE> <INDENT> Markers . __flag_len = len ( self . __flag_name ) + 2 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "58d0995b15ca4c82b034856d40abc526": {
        "code_string": "def refmac(self, cycles):\n         directory = self.job_directory(\"refmac\")\n         use_phases = self.args.unbiased and self.min_rwork > 0.35\n         job = Refmac(self.args, directory, self.current_xyz, use_phases, cycles)\n         self.jobs[self.cycle].append(job)\n         self.current_hkl = job.hklout\n         self.current_xyz = job.xyzout\n         return job\n",
        "code_toks_joined": "def refmac ( self , cycles ) : <NEWLINE> <INDENT> directory = self . job_directory ( <STRING> ) <NEWLINE> use_phases = self . args . unbiased and self . min_rwork > 0.35 <NEWLINE> job = Refmac ( self . args , directory , self . current_xyz , use_phases , cycles ) <NEWLINE> self . jobs [ self . cycle ] . append ( job ) <NEWLINE> self . current_hkl = job . hklout <NEWLINE> self . current_xyz = job . xyzout <NEWLINE> return job <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"refmac\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7d3184da9a7845b58cd241c80c8a0ee7": {
        "code_string": "def _parse_address(address):\n     if isinstance(address, tuple):\n         if ':' in address[0]:\n             return _socket.AF_INET6, address\n         return _socket.AF_INET, address\n     elif isinstance(address, string_types):\n         if ':' in address:\n             host, port = address.rsplit(':', 1)\n             family, host = _extract_family(host)\n             if host == '*':\n                 host = ''\n             return family, (host, int(port))\n         else:\n             return _socket.AF_INET, ('', int(port))\n     elif isinstance(address, integer_types):\n         return _socket.AF_INET, ('', int(address))\n     else:\n         raise TypeError('Expected tuple or string, got %s' % type(address))\n",
        "code_toks_joined": "def _parse_address ( address ) : <NEWLINE> <INDENT> if isinstance ( address , tuple ) : <NEWLINE> <INDENT> if <STRING> in address [ 0 ] : <NEWLINE> <INDENT> return _socket . AF_INET6 , address <NEWLINE> <DEDENT> return _socket . AF_INET , address <NEWLINE> <DEDENT> elif isinstance ( address , string_types ) : <NEWLINE> <INDENT> if <STRING> in address : <NEWLINE> <INDENT> host , port = address . rsplit ( <STRING> , 1 ) <NEWLINE> family , host = _extract_family ( host ) <NEWLINE> if host == <STRING> : <NEWLINE> <INDENT> host = <STRING> <NEWLINE> <DEDENT> return family , ( host , int ( port ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return _socket . AF_INET , ( <STRING> , int ( port ) ) <NEWLINE> <DEDENT> <DEDENT> elif isinstance ( address , integer_types ) : <NEWLINE> <INDENT> return _socket . AF_INET , ( <STRING> , int ( address ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise TypeError ( <STRING> % type ( address ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "':'",
                "':'",
                "':'",
                "'*'",
                "''",
                "''",
                "''",
                "'Expected tuple or string, got %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "de9306336fe94423b98e6638c91ae285": {
        "code_string": "tweets = self._filter_timeline(\n             self._twitter_data.iter_tweets_from(user_id), count, since_id,\n             max_id)\n         if exclude_replies:\n             tweets = [tweet for tweet in tweets if tweet.reply_to is not None]\n         if include_rts is not None:\n             raise NotImplementedError(\"exclude_rts param\")\n         return [\n             tweet.to_dict(\n                 self._twitter_data, trim_user=trim_user,\n                 contributor_details=contributor_details)\n             for tweet in tweets]\n",
        "code_toks_joined": "tweets = self . _filter_timeline ( <NEWLINE> <INDENT> self . _twitter_data . iter_tweets_from ( user_id ) , count , since_id , <NEWLINE> max_id ) <NEWLINE> if exclude_replies : <NEWLINE> tweets = [ tweet for tweet in tweets if tweet . reply_to is not None ] <NEWLINE> if include_rts is not None : <NEWLINE> raise NotImplementedError ( <STRING> ) <NEWLINE> return [ <NEWLINE> tweet . to_dict ( <NEWLINE> <INDENT> self . _twitter_data , trim_user = trim_user , <NEWLINE> contributor_details = contributor_details ) <NEWLINE> <DEDENT> for tweet in tweets ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"exclude_rts param\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bae6a1dd49734530b87f862c1595756b": {
        "code_string": "def has(self, blockname, ptype):\n     return self.reader.has_block(self, ptype, blockname)\n",
        "code_toks_joined": "def has ( self , blockname , ptype ) : <NEWLINE> <INDENT> return self . reader . has_block ( self , ptype , blockname ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b1d1d2cfe9f74b21bfd0dcfcda45ccb7": {
        "code_string": "class LocalURI(SwedishLegalSource):\n     def infer_triples(self, d, basefile=None):\n         super(self, LocalURI).infer_triples(d,basefile)\n         canonicalminter = ...\n         sameas = self.canonicalminter(d)\n         d.rel(OWL.sameAs, sameas)\n",
        "code_toks_joined": "class LocalURI ( SwedishLegalSource ) : <NEWLINE> <INDENT> def infer_triples ( self , d , basefile = None ) : <NEWLINE> <INDENT> super ( self , LocalURI ) . infer_triples ( d , basefile ) <NEWLINE> canonicalminter = ... <NEWLINE> sameas = self . canonicalminter ( d ) <NEWLINE> d . rel ( OWL . sameAs , sameas ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e5aa977460aa45e487383b343da205c7": {
        "code_string": "self.validate_body(body, basefile)  # Throws exception if invalid\n",
        "code_toks_joined": "self . validate_body ( body , basefile ) <COMMENT> <NEWLINE>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Throws exception if invalid"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c3bf16a8b7134fd599375db6525f5a60": {
        "code_string": "def test_unreliable_fontspec(self):\n         # the textbox marked as having font=\"6\" is really font=\"2\"\n         self._f('<fontspec id=\"2\" size=\"14\" family=\"MAMMBB+TT5Eo00\" color=\"#000000\"/>')\n         self._f('<fontspec id=\"6\" size=\"14\" family=\"MAPPGJ+TT9Eo00\" color=\"#000000\"/>')\n         textbox = self._p('<text top=\"288\" left=\"85\" width=\"468\" height=\"17\" font=\"2\">Det \u00e4r nu h\u00f6g tid att g\u00f6ra en kraftsamling f\u00f6r informationsf\u00f6rs\u00f6rj-</text>')\n         prevbox = self._p('<text top=\"307\" left=\"85\" width=\"252\" height=\"17\" font=\"2\">ningen till forskning och utbildning.</text>')\n         self.assertTrue(self.gluefunc(prevbox, prevbox, textbox))\n         textbox = textbox + prevbox\n         nextbox = self._p('<text top=\"304\" left=\"337\" width=\"220\" height=\"21\" font=\"6\"><i> </i>Den tekniska utvecklingen g\u00e5r </text>')\n         self.assertTrue(self.gluefunc(textbox, nextbox, prevbox))\n         textbox = textbox + nextbox\n         prevbox = nextbox\n         nextbox = self._p('<text top=\"327\" left=\"85\" width=\"472\" height=\"17\" font=\"2\">snabbt, och den vetenskapliga publiceringen finner nya v\u00e4gar. Detta </text>')\n         self.assertTrue(self.gluefunc(textbox, nextbox, prevbox))\n",
        "code_toks_joined": "def test_unreliable_fontspec ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . _f ( <STRING> ) <NEWLINE> self . _f ( <STRING> ) <NEWLINE> textbox = self . _p ( <STRING> ) <NEWLINE> prevbox = self . _p ( <STRING> ) <NEWLINE> self . assertTrue ( self . gluefunc ( prevbox , prevbox , textbox ) ) <NEWLINE> textbox = textbox + prevbox <NEWLINE> nextbox = self . _p ( <STRING> ) <NEWLINE> self . assertTrue ( self . gluefunc ( textbox , nextbox , prevbox ) ) <NEWLINE> textbox = textbox + nextbox <NEWLINE> prevbox = nextbox <NEWLINE> nextbox = self . _p ( <STRING> ) <NEWLINE> self . assertTrue ( self . gluefunc ( textbox , nextbox , prevbox ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# the textbox marked as having font=\"6\" is really font=\"2\""
            ],
            "<STRING>": [
                "'<fontspec id=\"2\" size=\"14\" family=\"MAMMBB+TT5Eo00\" color=\"#000000\"/>'",
                "'<fontspec id=\"6\" size=\"14\" family=\"MAPPGJ+TT9Eo00\" color=\"#000000\"/>'",
                "'<text top=\"288\" left=\"85\" width=\"468\" height=\"17\" font=\"2\">Det \u00e4r nu h\u00f6g tid att g\u00f6ra en kraftsamling f\u00f6r informationsf\u00f6rs\u00f6rj-</text>'",
                "'<text top=\"307\" left=\"85\" width=\"252\" height=\"17\" font=\"2\">ningen till forskning och utbildning.</text>'",
                "'<text top=\"304\" left=\"337\" width=\"220\" height=\"21\" font=\"6\"><i> </i>Den tekniska utvecklingen g\u00e5r </text>'",
                "'<text top=\"327\" left=\"85\" width=\"472\" height=\"17\" font=\"2\">snabbt, och den vetenskapliga publiceringen finner nya v\u00e4gar. Detta </text>'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f879a21cb95d42869e6b474d62742c66": {
        "code_string": "if not fetched:\n                 self.log.error(\"Failed to fetch %s, giving up\" % url)\n                 return False\n         # handles other errors except ConnectionError\n         except requests.exceptions.RequestException as e:\n             self.log.error(\"Failed to fetch %s: error %s\" % (url, e))\n             raise e\n         if response.status_code == 304:\n             self.log.debug(\"%s: 304 Not modified\" % url)\n             return False  # ie not updated\n         elif response.status_code > 400:\n             self.log.error(\"Failed to retrieve %s\" % url)\n             response.raise_for_status()\n",
        "code_toks_joined": "if not fetched : <NEWLINE> <INDENT> self . log . error ( <STRING> % url ) <NEWLINE> return False <NEWLINE> <COMMENT> <NL> except requests . exceptions . RequestException as e : <NEWLINE> self . log . error ( <STRING> % ( url , e ) ) <NEWLINE> raise e <NEWLINE> if response . status_code == 304 : <NEWLINE> self . log . debug ( <STRING> % url ) <NEWLINE> return False <COMMENT> <NEWLINE> elif response . status_code > 400 : <NEWLINE> self . log . error ( <STRING> % url ) <NEWLINE> response . raise_for_status ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Failed to fetch %s, giving up\"",
                "\"Failed to fetch %s: error %s\"",
                "\"%s: 304 Not modified\"",
                "\"Failed to retrieve %s\""
            ],
            "<COMMENT>": [
                "# handles other errors except ConnectionError",
                "# ie not updated"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2e39297a8b5345c98fd4f0b28dc50ced": {
        "code_string": "if inspect.isclass(local_getter):\n             # default initialization is passing the url\n             # you can override this behavior by passing an\n             # initialized getter object.\n             local_getter = getter(url)\n",
        "code_toks_joined": "if inspect . isclass ( local_getter ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> local_getter = getter ( url ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# default initialization is passing the url",
                "# you can override this behavior by passing an",
                "# initialized getter object."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2ac887abaddc4669b6272ba74785b92d": {
        "code_string": "def __init__(self, sep=SCOPE_SEPARATOR):\n         self.__sep = sep\n         super(self, ScopeDict).__init__()\n",
        "code_toks_joined": "def __init__ ( self , sep = SCOPE_SEPARATOR ) : <NEWLINE> <INDENT> self . __sep = sep <NEWLINE> super ( self , ScopeDict ) . __init__ ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3018ddbcf56646ad870b9a93e961cfa2": {
        "code_string": "if self._chunk_size is not None and len(data) > self._chunk_size:\n             logger.info('store data in chunks using chunked transfer encoding')\n             chunked_headers = dict(headers)\n             chunked_headers['Transfer-Encoding'] = 'chunked'\n             chunked_headers['Cache-Control'] = 'no-cache'\n             chunked_headers['Connection'] = 'Keep-Alive'\n             data_chunks = serve_data_chunks(data)\n             response = self._session.post(\n                 url=url,\n                 data=data_chunks,\n                 headers=headers\n             )\n         else:\n             response = self._session.post(url=url, data=data, headers=headers)\n         logger.debug('request status code: {}'.format(response.status_code))\n         try:\n             response.raise_for_status()\n         except requests.exceptions.HTTPError as error:\n",
        "code_toks_joined": "if self . _chunk_size is not None and len ( data ) > self . _chunk_size : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> chunked_headers = dict ( headers ) <NEWLINE> chunked_headers [ <STRING> ] = <STRING> <NEWLINE> chunked_headers [ <STRING> ] = <STRING> <NEWLINE> chunked_headers [ <STRING> ] = <STRING> <NEWLINE> data_chunks = serve_data_chunks ( data ) <NEWLINE> response = self . _session . post ( <NEWLINE> <INDENT> url = url , <NEWLINE> data = data_chunks , <NEWLINE> headers = headers <NEWLINE> <DEDENT> ) <NEWLINE> else : <NEWLINE> response = self . _session . post ( url = url , data = data , headers = headers ) <NEWLINE> logger . debug ( <STRING> . format ( response . status_code ) ) <NEWLINE> try : <NEWLINE> response . raise_for_status ( ) <NEWLINE> except requests . exceptions . HTTPError as error : <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'store data in chunks using chunked transfer encoding'",
                "'Transfer-Encoding'",
                "'chunked'",
                "'Cache-Control'",
                "'no-cache'",
                "'Connection'",
                "'Keep-Alive'",
                "'request status code: {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ea3da4f1f32499f8c98e6b7f6033bba": {
        "code_string": "# This regular expression extracts the scheme and host name from the URL\n         # and optionally the port number and prefix:\n         # <scheme>://<host>(:<port>)(/<prefix>)\n         # For example: \"https://mydomain.com:443/wado-rs\", where\n         # scheme=\"https\", host=\"mydomain.com\", port=443, prefix=\"wado-rs\"\n         pattern = re.compile(\n             r'(?P<scheme>[https]+)://(?P<host>[^/:]+)'\n             r'(?::(?P<port>\\d+))?(?:(?P<prefix>/[\\w/]+))?'\n         )\n         match = re.match(pattern, self.base_url)\n         if match is None:\n             raise ValueError('Malformed URL: {}'.format(self.base_url))\n         try:\n             self.protocol = match.group('scheme')\n             self.host = match.group('host')\n             port = match.group('port')\n         except AttributeError:\n             raise ValueError('Malformed URL: {}'.format(self.base_url))\n         if port:\n             self.port = int(port)\n         else:\n             if self.protocol == 'http':\n                 self.port = 80\n             elif self.protocol == 'https':\n                 self.port = 443\n             else:\n                 raise ValueError(\n                     'URL scheme \"{}\" is not supported.'.format(self.protocol)\n                 )\n         url_components = urlparse(url)\n         self.url_prefix = url_components.path\n         if headers is not None:\n             self._session.headers.update(headers)\n         if proxies is not None:\n             self._session.proxies = proxies\n         if callback is not None:\n             self._session.hooks = {'response': [callback, ]}\n         if chunk_size is not None:\n             # There is a bug in the requests library that sets the Host header\n             # again when using chunked transer encoding. Apparently this is\n             # tricky to fix (see https://github.com/psf/requests/issues/4392).\n             # As a temporary workaround we are only setting the header field,\n             # if we don't use chunked transfer encoding.\n             self._session.headers.update({'Host': self.host})\n         self._chunk_size = chunk_size\n         self.set_http_retry_params()\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> pattern = re . compile ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> match = re . match ( pattern , self . base_url ) <NEWLINE> if match is None : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( self . base_url ) ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> self . protocol = match . group ( <STRING> ) <NEWLINE> self . host = match . group ( <STRING> ) <NEWLINE> port = match . group ( <STRING> ) <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( self . base_url ) ) <NEWLINE> <DEDENT> if port : <NEWLINE> <INDENT> self . port = int ( port ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if self . protocol == <STRING> : <NEWLINE> <INDENT> self . port = 80 <NEWLINE> <DEDENT> elif self . protocol == <STRING> : <NEWLINE> <INDENT> self . port = 443 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( self . protocol ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> url_components = urlparse ( url ) <NEWLINE> self . url_prefix = url_components . path <NEWLINE> if headers is not None : <NEWLINE> <INDENT> self . _session . headers . update ( headers ) <NEWLINE> <DEDENT> if proxies is not None : <NEWLINE> <INDENT> self . _session . proxies = proxies <NEWLINE> <DEDENT> if callback is not None : <NEWLINE> <INDENT> self . _session . hooks = { <STRING> : [ callback , ] } <NEWLINE> <DEDENT> if chunk_size is not None : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> self . _session . headers . update ( { <STRING> : self . host } ) <NEWLINE> <DEDENT> self . _chunk_size = chunk_size <NEWLINE> self . set_http_retry_params ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# This regular expression extracts the scheme and host name from the URL",
                "# and optionally the port number and prefix:",
                "# <scheme>://<host>(:<port>)(/<prefix>)",
                "# For example: \"https://mydomain.com:443/wado-rs\", where",
                "# scheme=\"https\", host=\"mydomain.com\", port=443, prefix=\"wado-rs\"",
                "# There is a bug in the requests library that sets the Host header",
                "# again when using chunked transer encoding. Apparently this is",
                "# tricky to fix (see https://github.com/psf/requests/issues/4392).",
                "# As a temporary workaround we are only setting the header field,",
                "# if we don't use chunked transfer encoding."
            ],
            "<STRING>": [
                "r'(?P<scheme>[https]+)://(?P<host>[^/:]+)'",
                "r'(?::(?P<port>\\d+))?(?:(?P<prefix>/[\\w/]+))?'",
                "'Malformed URL: {}'",
                "'scheme'",
                "'host'",
                "'port'",
                "'Malformed URL: {}'",
                "'http'",
                "'https'",
                "'URL scheme \"{}\" is not supported.'",
                "'response'",
                "'Host'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7c5a5ccf133747b4bac804ae65dc0d87": {
        "code_string": "def form_invalid(self, form):\n         for sub_form in form:\n             update_valid_or_invalid_form_fields(form)\n             for error in sub_form.errors:\n                 messages.error(self.request, sub_form.errors[error])\n         return self.get_success_url()\n",
        "code_toks_joined": "def form_invalid ( self , form ) : <NEWLINE> <INDENT> for sub_form in form : <NEWLINE> <INDENT> update_valid_or_invalid_form_fields ( form ) <NEWLINE> for error in sub_form . errors : <NEWLINE> <INDENT> messages . error ( self . request , sub_form . errors [ error ] ) <NEWLINE> <DEDENT> <DEDENT> return self . get_success_url ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cf2a0125e21e4f66856de364e5223f15": {
        "code_string": "content = package_linkspec.get('content', None)\n                 if not content:\n                     utils.fs_link(item_source, item_target, hard_link=True, forced=forced)\n                 else:\n                     content_items = [p for item in content for p in\n                                      glob.glob(os.path.abspath(os.path.join(item_source, item)))]\n                     for content_item in content_items:\n                         content_item_name = os.path.basename(content_item)\n                         content_item_target = os.path.abspath(os.path.join(item_target, content_item_name))\n                         utils.fs_link(content_item, content_item_target, hard_link=True, forced=forced)\n",
        "code_toks_joined": "content = package_linkspec . get ( <STRING> , None ) <NEWLINE> <INDENT> if not content : <NEWLINE> <INDENT> utils . fs_link ( item_source , item_target , hard_link = True , forced = forced ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> content_items = [ p for item in content for p in <NEWLINE> <INDENT> glob . glob ( os . path . abspath ( os . path . join ( item_source , item ) ) ) ] <NEWLINE> <DEDENT> for content_item in content_items : <NEWLINE> <INDENT> content_item_name = os . path . basename ( content_item ) <NEWLINE> content_item_target = os . path . abspath ( os . path . join ( item_target , content_item_name ) ) <NEWLINE> utils . fs_link ( content_item , content_item_target , hard_link = True , forced = forced ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'content'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3ad70389b8cc41308f7efc3f7699c609": {
        "code_string": "def generate_mock(mocked_module, mock_prototype):\n     ''' Generates the mock '''\n     mock_filename = \"{0}_mock.cpp\".format(mocked_module)\n     include_filename = \"{0}.h\".format(mock_filename)\n     logger.debug(\"working directory: %s\", os.getcwd())\n     logger.debug(\"mock_filename: %s\", mock_filename)\n     logger.debug(\"include_filename: %s\", include_filename)\n     logger.debug(\"mock_prototype: %s\", mock_prototype)\n     if os.path.exists(mock_filename):\n         logger.debug(\"Mock file exists\")\n         mock_file = open(mock_filename, \"a\")\n     else:\n         logger.debug(\"Creating mock file\")\n         mock_file = open(mock_filename, \"w\")\n         write_header(mock_file, FILE_HEADER, include_filename)\n     add_mock_function(mock_file, mock_prototype)\n     mock_file.close()\n",
        "code_toks_joined": "def generate_mock ( mocked_module , mock_prototype ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> mock_filename = <STRING> . format ( mocked_module ) <NEWLINE> include_filename = <STRING> . format ( mock_filename ) <NEWLINE> logger . debug ( <STRING> , os . getcwd ( ) ) <NEWLINE> logger . debug ( <STRING> , mock_filename ) <NEWLINE> logger . debug ( <STRING> , include_filename ) <NEWLINE> logger . debug ( <STRING> , mock_prototype ) <NEWLINE> if os . path . exists ( mock_filename ) : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> mock_file = open ( mock_filename , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> mock_file = open ( mock_filename , <STRING> ) <NEWLINE> write_header ( mock_file , FILE_HEADER , include_filename ) <NEWLINE> <DEDENT> add_mock_function ( mock_file , mock_prototype ) <NEWLINE> mock_file . close ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''' Generates the mock '''",
                "\"{0}_mock.cpp\"",
                "\"{0}.h\"",
                "\"working directory: %s\"",
                "\"mock_filename: %s\"",
                "\"include_filename: %s\"",
                "\"mock_prototype: %s\"",
                "\"Mock file exists\"",
                "\"a\"",
                "\"Creating mock file\"",
                "\"w\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "89e8337ed8144edfbf373a9665711b6f": {
        "code_string": "return list_detail.object_list(\n         request,\n         queryset=Post.objects.filter(tags__name__in=[slug]),\n         paginate_by=blog_settings.BLOG_PAGESIZE,\n         page=page,\n         extra_context={\n             'type': 'tag',\n             'query': tag.id,\n             'query_pretty': tag\n         },\n         template_name='blog/post_list.html',\n         **kwargs\n     )\n",
        "code_toks_joined": "return list_detail . object_list ( <NEWLINE> <INDENT> request , <NEWLINE> queryset = Post . objects . filter ( tags__name__in = [ slug ] ) , <NEWLINE> paginate_by = blog_settings . BLOG_PAGESIZE , <NEWLINE> page = page , <NEWLINE> extra_context = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : tag . id , <NEWLINE> <STRING> : tag <NEWLINE> <DEDENT> } , <NEWLINE> template_name = <STRING> , <NEWLINE> ** kwargs <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'type'",
                "'tag'",
                "'query'",
                "'query_pretty'",
                "'blog/post_list.html'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f63d34a0e3e45faa502a75e7d96c867": {
        "code_string": "while event_aux < end_utc:\n         result.append( (event_aux, event_aux.tzinfo.normalize(event_aux + event_duration), 1) )\n         event_aux = add_delta_dst(event_aux, delta)\n     return result\n",
        "code_toks_joined": "while event_aux < end_utc : <NEWLINE> <INDENT> result . append ( ( event_aux , event_aux . tzinfo . normalize ( event_aux + event_duration ) , 1 ) ) <NEWLINE> event_aux = add_delta_dst ( event_aux , delta ) <NEWLINE> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6a143ccd4478426882c4066dcf9c8c0b": {
        "code_string": "if destination_ip:\n             destination_ip_match = False\n             if loose_match and 'any' in rule.destination:\n                 destination_ip_match = True\n             else:\n                 for object_string in rule.destination:\n                     # Get a valid AddressObject or AddressGroup\n                     obj = get_object(device, dev_group, object_string)\n                     # Otherwise the object_string is not an object and should be handled differently\n                     if obj is False:\n                         if '-' in object_string:\n                             obj = ipaddress.ip_address(unicode(destination_ip))\n                             destination_range = object_string.split('-')\n                             destination_lower = ipaddress.ip_address(unicode(destination_range[0]))\n                             destination_upper = ipaddress.ip_address(unicode(destination_range[1]))\n                             if destination_lower <= obj <= destination_upper:\n                                 destination_ip_match = True\n                         else:\n                             if source_ip == object_string:\n                                 destination_ip_match = True\n                     if isinstance(obj, objects.AddressObject) and addr_in_obj(destination_ip, obj):\n                         destination_ip_match = True\n                     elif isinstance(obj, objects.AddressGroup) and obj.static_value:\n                         for member_string in obj.static_value:\n                             member = get_object(device, dev_group, member_string)\n                             if addr_in_obj(destination_ip, member):\n                                 destination_ip_match = True\n             hitlist.append(destination_ip_match)\n",
        "code_toks_joined": "if destination_ip : <NEWLINE> <INDENT> destination_ip_match = False <NEWLINE> if loose_match and <STRING> in rule . destination : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for object_string in rule . destination : <NEWLINE> <COMMENT> <NL> <INDENT> obj = get_object ( device , dev_group , object_string ) <NEWLINE> <COMMENT> <NL> if obj is False : <NEWLINE> <INDENT> if <STRING> in object_string : <NEWLINE> <INDENT> obj = ipaddress . ip_address ( unicode ( destination_ip ) ) <NEWLINE> destination_range = object_string . split ( <STRING> ) <NEWLINE> destination_lower = ipaddress . ip_address ( unicode ( destination_range [ 0 ] ) ) <NEWLINE> destination_upper = ipaddress . ip_address ( unicode ( destination_range [ 1 ] ) ) <NEWLINE> if destination_lower <= obj <= destination_upper : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if source_ip == object_string : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if isinstance ( obj , objects . AddressObject ) and addr_in_obj ( destination_ip , obj ) : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> elif isinstance ( obj , objects . AddressGroup ) and obj . static_value : <NEWLINE> <INDENT> for member_string in obj . static_value : <NEWLINE> <INDENT> member = get_object ( device , dev_group , member_string ) <NEWLINE> if addr_in_obj ( destination_ip , member ) : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> hitlist . append ( destination_ip_match ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'any'",
                "'-'",
                "'-'"
            ],
            "<COMMENT>": [
                "# Get a valid AddressObject or AddressGroup",
                "# Otherwise the object_string is not an object and should be handled differently"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4c705b2c7612487ba88cb3badb9814cc": {
        "code_string": "for sample_idx in range(len(y_train_)):\n",
        "code_toks_joined": "for sample_idx in range ( len ( y_train_ ) ) : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b80090ad0e634d268e10768dbd4f5fce": {
        "code_string": "return_key_values = _get_versions()\n     return_key_values[\"authors\"] = pieces[\"authors\"]\n     return return_key_values\n",
        "code_toks_joined": "return_key_values = _get_versions ( ) <NEWLINE> <INDENT> return_key_values [ <STRING> ] = pieces [ <STRING> ] <NEWLINE> return return_key_values <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"authors\"",
                "\"authors\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "294b7d715f444daea70d5268d372bba8": {
        "code_string": "def get_web_config_apps(web_config):\n     doc = minidom.parse(web_config)\n     for fcgi in doc.getElementsByTagName(\"fastCgi\"):\n         for app in doc.getElementsByTagName(\"application\"):\n             yield dict((key, value) for key, value in app.attributes.items())\n",
        "code_toks_joined": "def get_web_config_apps ( web_config ) : <NEWLINE> <INDENT> doc = minidom . parse ( web_config ) <NEWLINE> for fcgi in doc . getElementsByTagName ( <STRING> ) : <NEWLINE> <INDENT> for app in doc . getElementsByTagName ( <STRING> ) : <NEWLINE> <INDENT> yield dict ( ( key , value ) for key , value in app . attributes . items ( ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"fastCgi\"",
                "\"application\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ac5923b3ffd44e26aeedf9f36f395ee1": {
        "code_string": "@csrf_exempt\n def createAccount(request):\n     if 'username' not in request.POST \\\n             or 'password' not in request.POST:\n         return HttpResponse(status=500)\n",
        "code_toks_joined": "@ csrf_exempt <NEWLINE> <INDENT> def createAccount ( request ) : <NEWLINE> <INDENT> if <STRING> not in request . POST or <STRING> not in request . POST : <NEWLINE> <INDENT> return HttpResponse ( status = 500 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'username'",
                "'password'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "79fb1dc7517d40a193b2ef1df4d8f235": {
        "code_string": "def data_is_valid(post_data, postback_url=POSTBACK_URL):\n     \"\"\"\n     Validates data via the postback. Returns True if data is valid,\n     False if data is invalid and None if the request failed.\n     \"\"\"\n     post_str = urlencode(_values_to_encode(post_data))\n     try:\n         response = urllib2.urlopen(postback_url, post_data).read()\n     except urllib2.HTTPError:\n         return None\n     if response == 'VALID':\n         return True\n     if response == 'INVALID':\n         return False\n     return None\n",
        "code_toks_joined": "def data_is_valid ( post_data , postback_url = POSTBACK_URL ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> post_str = urlencode ( _values_to_encode ( post_data ) ) <NEWLINE> try : <NEWLINE> <INDENT> response = urllib2 . urlopen ( postback_url , post_data ) . read ( ) <NEWLINE> <DEDENT> except urllib2 . HTTPError : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> if response == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if response == <STRING> : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Validates data via the postback. Returns True if data is valid,\n     False if data is invalid and None if the request failed.\n     \"\"\"",
                "'VALID'",
                "'INVALID'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0a92cc1fb10e400898519bd408d7eccc": {
        "code_string": "# generate list for display and removal of active filters\n         display_filters = []\n         if 'collection' in request.GET:\n             filter_val = request.GET['collection']\n             # filter the solr query based on the requested collection\n             q = solr.query(collection_label='\"%s\"' % filter_val)\n             # generate link to remove the facet\n             unfacet_urlopts = url_params.copy()\n             del unfacet_urlopts['collection']\n             display_filters.append(('collection', filter_val,\n                                     unfacet_urlopts.urlencode()))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> display_filters = [ ] <NEWLINE> if <STRING> in request . GET : <NEWLINE> <INDENT> filter_val = request . GET [ <STRING> ] <NEWLINE> <COMMENT> <NL> q = solr . query ( collection_label = <STRING> % filter_val ) <NEWLINE> <COMMENT> <NL> unfacet_urlopts = url_params . copy ( ) <NEWLINE> del unfacet_urlopts [ <STRING> ] <NEWLINE> display_filters . append ( ( <STRING> , filter_val , <NEWLINE> <INDENT> unfacet_urlopts . urlencode ( ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# generate list for display and removal of active filters",
                "# filter the solr query based on the requested collection",
                "# generate link to remove the facet"
            ],
            "<STRING>": [
                "'collection'",
                "'collection'",
                "'\"%s\"'",
                "'collection'",
                "'collection'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a7f24255dc7e4ee3b1ea3f8f4904d926": {
        "code_string": "tissues = samples.groupby('smts').groups\n     norm = normalize.Normalizer('ensembl_gene_id')\n     for t in tissues:\n         print('processing tissue: {}'.format(t))\n         # select the sample ids corresponding to the tissue\n         index = list(samples['run'].loc[tissues[t]].values)\n         tissue_expression = (expression[index].T)\n         # replace missing data with 0\n         tissue_expression.fillna(0.0, inplace=True)\n         # clean the ensemble ids and add up the duplicates\n         tissue_expression.columns = convert.clean_ensembl_ids(tissue_expression.columns)\n         tissue_expression = normalize.deduplicate(tissue_expression)\n         # convert from counts to TPM\n         tpm = norm.tpm_from_counts(tissue_expression)\n         # compute some statistics\n         mean = pandas.concat(\n                 [mean, pandas.DataFrame(tpm.mean(), columns=[t])], axis=1)\n         median = pandas.concat(\n                 [median, pandas.DataFrame(tpm.median(), columns=[t])], axis=1)\n         std = pandas.concat(\n                 [std, pandas.DataFrame(tpm.std(), columns=[t])], axis=1)\n         lower_quartile = pandas.concat(\n                 [lower_quartile, pandas.DataFrame(\n                         tpm.quantile(q=0.25, axis=0)).rename(columns={0.25: t})], axis=1)\n         upper_quartile = pandas.concat(\n                 [upper_quartile, pandas.DataFrame(\n                         tpm.quantile(q=0.75, axis=0)).rename(columns={0.75: t})], axis=1)\n         fraction_zero = pandas.concat([\n                 fraction_zero, pandas.DataFrame(\n                         (tpm == 0).mean().astype(float), columns=[t])], axis=1)\n         # Convert tpms to clr with 0.5*min imputation\n         clr = norm.clr_from_tpm(tissue_expression, imputer=normalize.impute)\n         mean_clr = pandas.concat(\n                 [mean_clr, pandas.DataFrame(clr.mean(), columns=[t])], axis=1)\n         median_clr = pandas.concat(\n                 [median_clr, pandas.DataFrame(clr.median(), columns=[t])], axis=1)\n         std_clr = pandas.concat(\n                 [std_clr, pandas.DataFrame(clr.std(), columns=[t])], axis=1)\n         lower_quartile_clr = pandas.concat(\n                 [lower_quartile_clr, pandas.DataFrame(\n                         clr.quantile(q=0.25, axis=0)).rename(columns={0.25: t})], axis=1)\n         upper_quartile_clr = pandas.concat(\n                 [upper_quartile_clr, pandas.DataFrame(\n                         clr.quantile(q=0.75, axis=0)).rename(columns={0.75: t})], axis=1)\n",
        "code_toks_joined": "tissues = samples . groupby ( <STRING> ) . groups <NEWLINE> <INDENT> norm = normalize . Normalizer ( <STRING> ) <NEWLINE> for t in tissues : <NEWLINE> <INDENT> print ( <STRING> . format ( t ) ) <NEWLINE> <COMMENT> <NL> index = list ( samples [ <STRING> ] . loc [ tissues [ t ] ] . values ) <NEWLINE> tissue_expression = ( expression [ index ] . T ) <NEWLINE> <COMMENT> <NL> tissue_expression . fillna ( 0.0 , inplace = True ) <NEWLINE> <COMMENT> <NL> tissue_expression . columns = convert . clean_ensembl_ids ( tissue_expression . columns ) <NEWLINE> tissue_expression = normalize . deduplicate ( tissue_expression ) <NEWLINE> <COMMENT> <NL> tpm = norm . tpm_from_counts ( tissue_expression ) <NEWLINE> <COMMENT> <NL> mean = pandas . concat ( <NEWLINE> <INDENT> [ mean , pandas . DataFrame ( tpm . mean ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> median = pandas . concat ( <NEWLINE> <INDENT> [ median , pandas . DataFrame ( tpm . median ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> std = pandas . concat ( <NEWLINE> <INDENT> [ std , pandas . DataFrame ( tpm . std ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> lower_quartile = pandas . concat ( <NEWLINE> <INDENT> [ lower_quartile , pandas . DataFrame ( <NEWLINE> <INDENT> tpm . quantile ( q = 0.25 , axis = 0 ) ) . rename ( columns = { 0.25 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> upper_quartile = pandas . concat ( <NEWLINE> <INDENT> [ upper_quartile , pandas . DataFrame ( <NEWLINE> <INDENT> tpm . quantile ( q = 0.75 , axis = 0 ) ) . rename ( columns = { 0.75 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> fraction_zero = pandas . concat ( [ <NEWLINE> <INDENT> fraction_zero , pandas . DataFrame ( <NEWLINE> <INDENT> ( tpm == 0 ) . mean ( ) . astype ( float ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> clr = norm . clr_from_tpm ( tissue_expression , imputer = normalize . impute ) <NEWLINE> mean_clr = pandas . concat ( <NEWLINE> <INDENT> [ mean_clr , pandas . DataFrame ( clr . mean ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> median_clr = pandas . concat ( <NEWLINE> <INDENT> [ median_clr , pandas . DataFrame ( clr . median ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> std_clr = pandas . concat ( <NEWLINE> <INDENT> [ std_clr , pandas . DataFrame ( clr . std ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> lower_quartile_clr = pandas . concat ( <NEWLINE> <INDENT> [ lower_quartile_clr , pandas . DataFrame ( <NEWLINE> <INDENT> clr . quantile ( q = 0.25 , axis = 0 ) ) . rename ( columns = { 0.25 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> upper_quartile_clr = pandas . concat ( <NEWLINE> <INDENT> [ upper_quartile_clr , pandas . DataFrame ( <NEWLINE> <INDENT> clr . quantile ( q = 0.75 , axis = 0 ) ) . rename ( columns = { 0.75 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'smts'",
                "'ensembl_gene_id'",
                "'processing tissue: {}'",
                "'run'"
            ],
            "<COMMENT>": [
                "# select the sample ids corresponding to the tissue",
                "# replace missing data with 0",
                "# clean the ensemble ids and add up the duplicates",
                "# convert from counts to TPM",
                "# compute some statistics",
                "# Convert tpms to clr with 0.5*min imputation"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cd0120481b40451b8c7b89c87c148a5e": {
        "code_string": "if DatabasePopulator.Tags.POST_OP in kwargs and is_test:\n                 fn = kwargs[DatabasePopulator.Tags.POST_OP]\n                 value = self._database_populator.get_replacement(\n                     key,\n                     modify_before_resaving_fn=fn,\n                     default_value=default_value,\n                 )\n             else:\n                 value = self._database_populator.get_replacement(\n                     key,\n                     default_value = default_value,\n                 )\n",
        "code_toks_joined": "if DatabasePopulator . Tags . POST_OP in kwargs and is_test : <NEWLINE> <INDENT> fn = kwargs [ DatabasePopulator . Tags . POST_OP ] <NEWLINE> value = self . _database_populator . get_replacement ( <NEWLINE> <INDENT> key , <NEWLINE> modify_before_resaving_fn = fn , <NEWLINE> default_value = default_value , <NEWLINE> <DEDENT> ) <NEWLINE> else : <NEWLINE> value = self . _database_populator . get_replacement ( <NEWLINE> <INDENT> key , <NEWLINE> default_value = default_value , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "21ae0c0f7cf9498fa75479423842cb2c": {
        "code_string": "class KademliaProtocol(RPCProtocol):\n     def __init__(self, sourceNode, storage, ksize):\n         RPCProtocol.__init__(self)\n         self.router = RoutingTable(self, sourceNode, ksize)\n         self.storage = storage\n         self.sourceID = sourceNode.id\n         self.log = Logger(system=self)\n",
        "code_toks_joined": "class KademliaProtocol ( RPCProtocol ) : <NEWLINE> <INDENT> def __init__ ( self , sourceNode , storage , ksize ) : <NEWLINE> <INDENT> RPCProtocol . __init__ ( self ) <NEWLINE> self . router = RoutingTable ( self , sourceNode , ksize ) <NEWLINE> self . storage = storage <NEWLINE> self . sourceID = sourceNode . id <NEWLINE> self . log = Logger ( system = self ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c91fb95bf3974282b9ab06fd2769ec8b": {
        "code_string": "if self.server.secret_key is None:\n             # If user supplied their own server, they might've supplied a\n             # secret_key with it\n             secret_key_name = 'dash_{}_secret_key'.format(\n                 # TODO - check for other illegal characters\n                 name.replace('.', '_')\n             )\n             secret_key = os.environ.get(\n                 secret_key_name, SeaSurf()._generate_token()\n             )\n             os.environ[secret_key_name] = secret_key_name\n             self.server.secret_key = secret_key\n",
        "code_toks_joined": "if self . server . secret_key is None : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> secret_key_name = <STRING> . format ( <NEWLINE> <COMMENT> <NL> <INDENT> name . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> ) <NEWLINE> secret_key = os . environ . get ( <NEWLINE> <INDENT> secret_key_name , SeaSurf ( ) . _generate_token ( ) <NEWLINE> <DEDENT> ) <NEWLINE> os . environ [ secret_key_name ] = secret_key_name <NEWLINE> self . server . secret_key = secret_key <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If user supplied their own server, they might've supplied a",
                "# secret_key with it",
                "# TODO - check for other illegal characters"
            ],
            "<STRING>": [
                "'dash_{}_secret_key'",
                "'.'",
                "'_'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "db36a0bb7c8d48b2826e604aec740278": {
        "code_string": "generate_classes(METADATA_PATH, 'default_namespace')\n         from default_namespace.MyComponent import MyComponent \\\n             as MyComponent_buildtime\n         from default_namespace.A import A as A_buildtime\n",
        "code_toks_joined": "generate_classes ( METADATA_PATH , <STRING> ) <NEWLINE> <INDENT> from default_namespace . MyComponent import MyComponent as MyComponent_buildtime <NEWLINE> from default_namespace . A import A as A_buildtime <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'default_namespace'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "932996b743f5466e866a59623d95977c": {
        "code_string": "def check_for_tweets(self, last_max_id=None):\n         statuses = [0]  # trick to force initial fetch\n         last_min_id = None\n         max_id = 0\n         tweets_read = 0\n         click.echo(click.style(\n             \"checking for tweets, starting from last_max_id={}\".format(last_max_id), fg='cyan'))\n         # Read until either a) we run out of tweets or b) we start seeing the\n         # same tweets as the previous run\n         favourite_counts = []\n         retweet_counts = []\n         while len(statuses) > 0 and (last_max_id is None or last_min_id is not None or last_min_id < last_max_id):\n             statuses = self.api.GetUserTimeline(\n                 include_rts=True,\n                 exclude_replies=False,\n                 max_id=last_min_id,\n                 count=200\n             )\n             tweets_read += len(statuses)\n             for status in statuses:\n                 max_id = max([status.id, max_id])\n                 if last_min_id:\n                     last_min_id = min([status.id - 1, last_min_id])\n                 else:\n                     last_min_id = status.id - 1\n                 self.to_be_deleted(status)\n",
        "code_toks_joined": "def check_for_tweets ( self , last_max_id = None ) : <NEWLINE> <INDENT> statuses = [ 0 ] <COMMENT> <NEWLINE> last_min_id = None <NEWLINE> max_id = 0 <NEWLINE> tweets_read = 0 <NEWLINE> click . echo ( click . style ( <NEWLINE> <INDENT> <STRING> . format ( last_max_id ) , fg = <STRING> ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> favourite_counts = [ ] <NEWLINE> retweet_counts = [ ] <NEWLINE> while len ( statuses ) > 0 and ( last_max_id is None or last_min_id is not None or last_min_id < last_max_id ) : <NEWLINE> <INDENT> statuses = self . api . GetUserTimeline ( <NEWLINE> <INDENT> include_rts = True , <NEWLINE> exclude_replies = False , <NEWLINE> max_id = last_min_id , <NEWLINE> count = 200 <NEWLINE> <DEDENT> ) <NEWLINE> tweets_read += len ( statuses ) <NEWLINE> for status in statuses : <NEWLINE> <INDENT> max_id = max ( [ status . id , max_id ] ) <NEWLINE> if last_min_id : <NEWLINE> <INDENT> last_min_id = min ( [ status . id - 1 , last_min_id ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> last_min_id = status . id - 1 <NEWLINE> <DEDENT> self . to_be_deleted ( status ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# trick to force initial fetch",
                "# Read until either a) we run out of tweets or b) we start seeing the",
                "# same tweets as the previous run"
            ],
            "<STRING>": [
                "\"checking for tweets, starting from last_max_id={}\"",
                "'cyan'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "df6004cfa2394814a500e5501e875fd9": {
        "code_string": "def _evaluate_predict_element(model, triple_index, num_expands, element_type, rank_fn, ranks_list, filtered_ranks_list):\n     batch = data.expand_triple_to_sets(kgekit.data.unpack(triple_index), num_expands, element_type)\n     batch = data.convert_triple_tuple_to_torch(batch)\n     logging.debug(element_type)\n     logging.debug(\"Batch len: \" + str(len(batch)) + \"; batch sample: \" + str(batch[0]))\n     predicted_batch = model.forward(batch).cpu()\n     logging.debug(\"Predicted batch len\" + str(len(batch)) + \"; batch sample: \" + str(predicted_batch[0]))\n     rank, filtered_rank = rank_fn(predicted_batch.data.numpy(), triple_index)\n     logging.debug(\"Rank :\" + str(rank) + \"; Filtered rank length :\" + str(filtered_rank))\n     ranks_list.append(rank)\n     filtered_ranks_list.append(filtered_rank)\n",
        "code_toks_joined": "def _evaluate_predict_element ( model , triple_index , num_expands , element_type , rank_fn , ranks_list , filtered_ranks_list ) : <NEWLINE> <INDENT> batch = data . expand_triple_to_sets ( kgekit . data . unpack ( triple_index ) , num_expands , element_type ) <NEWLINE> batch = data . convert_triple_tuple_to_torch ( batch ) <NEWLINE> logging . debug ( element_type ) <NEWLINE> logging . debug ( <STRING> + str ( len ( batch ) ) + <STRING> + str ( batch [ 0 ] ) ) <NEWLINE> predicted_batch = model . forward ( batch ) . cpu ( ) <NEWLINE> logging . debug ( <STRING> + str ( len ( batch ) ) + <STRING> + str ( predicted_batch [ 0 ] ) ) <NEWLINE> rank , filtered_rank = rank_fn ( predicted_batch . data . numpy ( ) , triple_index ) <NEWLINE> logging . debug ( <STRING> + str ( rank ) + <STRING> + str ( filtered_rank ) ) <NEWLINE> ranks_list . append ( rank ) <NEWLINE> filtered_ranks_list . append ( filtered_rank ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Batch len: \"",
                "\"; batch sample: \"",
                "\"Predicted batch len\"",
                "\"; batch sample: \"",
                "\"Rank :\"",
                "\"; Filtered rank length :\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "87f29de67d1d489aaa999f9b63ba5e0f": {
        "code_string": "if config.save_per_epoch != 0 and i_epoch % config.save_per_epoch:\n             save_checkpoint({\n                 'epoch': i_epoch,\n                 'state_dict': model.state_dict(),\n                 'optimizer': optimizer.state_dict(),\n             },\n                             config,\n                             postfix_num=i_epoch)\n",
        "code_toks_joined": "if config . save_per_epoch != 0 and i_epoch % config . save_per_epoch : <NEWLINE> <INDENT> save_checkpoint ( { <NEWLINE> <INDENT> <STRING> : i_epoch , <NEWLINE> <STRING> : model . state_dict ( ) , <NEWLINE> <STRING> : optimizer . state_dict ( ) , <NEWLINE> <DEDENT> } , <NEWLINE> <INDENT> config , <NEWLINE> postfix_num = i_epoch ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'epoch'",
                "'state_dict'",
                "'optimizer'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0c639e06ab8e4ed8873dc19b014d81e5": {
        "code_string": "# Write data to cache\n     if timeout is not None:\n         txn.setex(cache_key, data, timeout)\n     else:\n         txn.set(cache_key, data)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if timeout is not None : <NEWLINE> <INDENT> txn . setex ( cache_key , data , timeout ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> txn . set ( cache_key , data ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Write data to cache"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "01ba5116033d40c1aeb475b071b1e807": {
        "code_string": "def open_fs(self, fs_url, parse_result, writeable, create, cwd):\n         bucket_name, _, dir_path = parse_result.resource.partition(\"/\")\n         if not bucket_name:\n             raise OpenerError(\"invalid bucket name in '{}'\".format(fs_url))\n         strict = (\n             parse_result.params[\"strict\"] == \"1\"\n             if \"strict\" in parse_result.params\n             else True\n         )\n         s3fs = S3FS(\n             bucket_name,\n             dir_path=dir_path or \"/\",\n             aws_access_key_id=parse_result.username or None,\n             aws_secret_access_key=parse_result.password or None,\n             endpoint_url=parse_result.params.get(\"endpoint_url\", None),\n             acl=parse_result.params.get(\"acl\", None),\n             cache_control=parse_result.params.get(\"cache_control\", None),\n             strict=strict,\n         )\n         return s3fs\n",
        "code_toks_joined": "def open_fs ( self , fs_url , parse_result , writeable , create , cwd ) : <NEWLINE> <INDENT> bucket_name , _ , dir_path = parse_result . resource . partition ( <STRING> ) <NEWLINE> if not bucket_name : <NEWLINE> <INDENT> raise OpenerError ( <STRING> . format ( fs_url ) ) <NEWLINE> <DEDENT> strict = ( <NEWLINE> <INDENT> parse_result . params [ <STRING> ] == <STRING> <NEWLINE> if <STRING> in parse_result . params <NEWLINE> else True <NEWLINE> <DEDENT> ) <NEWLINE> s3fs = S3FS ( <NEWLINE> <INDENT> bucket_name , <NEWLINE> dir_path = dir_path or <STRING> , <NEWLINE> aws_access_key_id = parse_result . username or None , <NEWLINE> aws_secret_access_key = parse_result . password or None , <NEWLINE> endpoint_url = parse_result . params . get ( <STRING> , None ) , <NEWLINE> acl = parse_result . params . get ( <STRING> , None ) , <NEWLINE> cache_control = parse_result . params . get ( <STRING> , None ) , <NEWLINE> strict = strict , <NEWLINE> <DEDENT> ) <NEWLINE> return s3fs <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"/\"",
                "\"invalid bucket name in '{}'\"",
                "\"strict\"",
                "\"1\"",
                "\"strict\"",
                "\"/\"",
                "\"endpoint_url\"",
                "\"acl\"",
                "\"cache_control\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ad95c3b05984ae592ea71eab360fbee": {
        "code_string": "def callback(ctx, param, value):\n             if value or ctx.resilient_parsing:\n                 return\n             prog = prog_name\n             if prog is None:\n                 prog = ctx.find_root().info_name\n             ver = version\n             if ver is None:\n                 try:\n                     import pkg_resources\n                 except ImportError:\n                     pass\n                 else:\n                     for dist in pkg_resources.working_set:\n                         scripts = dist.get_entry_map().get('console_scripts') or {}\n                         for script_name, entry_point in iteritems(scripts):\n                             if entry_point.module_name == module:\n                                 ver = dist.version\n                                 break\n                 if ver is None:\n                     raise RuntimeError('Could not determine version')\n             echo(message % {\n                 'prog': prog,\n                 'version': ver,\n             })\n             ctx.exit()\n",
        "code_toks_joined": "def callback ( ctx , param , value ) : <NEWLINE> <INDENT> if value or ctx . resilient_parsing : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> prog = prog_name <NEWLINE> if prog is None : <NEWLINE> <INDENT> prog = ctx . find_root ( ) . info_name <NEWLINE> <DEDENT> ver = version <NEWLINE> if ver is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> import pkg_resources <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for dist in pkg_resources . working_set : <NEWLINE> <INDENT> scripts = dist . get_entry_map ( ) . get ( <STRING> ) or { } <NEWLINE> for script_name , entry_point in iteritems ( scripts ) : <NEWLINE> <INDENT> if entry_point . module_name == module : <NEWLINE> <INDENT> ver = dist . version <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> if ver is None : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> echo ( message % { <NEWLINE> <INDENT> <STRING> : prog , <NEWLINE> <STRING> : ver , <NEWLINE> <DEDENT> } ) <NEWLINE> ctx . exit ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'console_scripts'",
                "'Could not determine version'",
                "'prog'",
                "'version'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "943c2023aeef425e8737ea17e958e2fc": {
        "code_string": "old_env = {}\n         try:\n             for key, value in iteritems(env):\n                 old_env[key] = os.environ.get(value)\n                 if value is None:\n                     try:\n                         del os.environ[key]\n                     except Exception:\n                         pass\n                 else:\n                     os.environ[key] = value\n             yield bytes_output\n         finally:\n             for key, value in iteritems(old_env):\n                 if value is None:\n                     try:\n                         del os.environ[key]\n                     except Exception:\n                         pass\n                 else:\n                     os.environ[key] = value\n             sys.stdout = old_stdout\n             sys.stderr = old_stderr\n             sys.stdin = old_stdin\n             clickpkg.termui.visible_prompt_func = old_visible_prompt_func\n             clickpkg.termui.hidden_prompt_func = old_hidden_prompt_func\n             clickpkg.termui._getchar = old__getchar_func\n             clickpkg.utils.should_strip_ansi = old_should_strip_ansi\n             clickpkg.formatting.FORCED_WIDTH = old_forced_width\n",
        "code_toks_joined": "old_env = { } <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> for key , value in iteritems ( env ) : <NEWLINE> <INDENT> old_env [ key ] = os . environ . get ( value ) <NEWLINE> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> yield bytes_output <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> for key , value in iteritems ( old_env ) : <NEWLINE> <INDENT> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> sys . stdout = old_stdout <NEWLINE> sys . stderr = old_stderr <NEWLINE> sys . stdin = old_stdin <NEWLINE> clickpkg . termui . visible_prompt_func = old_visible_prompt_func <NEWLINE> clickpkg . termui . hidden_prompt_func = old_hidden_prompt_func <NEWLINE> clickpkg . termui . _getchar = old__getchar_func <NEWLINE> clickpkg . utils . should_strip_ansi = old_should_strip_ansi <NEWLINE> clickpkg . formatting . FORCED_WIDTH = old_forced_width <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ee38558c1374fdd9ba5f7896f551200": {
        "code_string": "for cookie in cookies:\n             loose_cookie = Cookie.to_morsel(cookie)\n             loose_cookies.append((loose_cookies.key, loose_cookie))\n",
        "code_toks_joined": "for cookie in cookies : <NEWLINE> <INDENT> loose_cookie = Cookie . to_morsel ( cookie ) <NEWLINE> loose_cookies . append ( ( loose_cookies . key , loose_cookie ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "67cbf00c4b8c47af9947883c31ce711b": {
        "code_string": "try:\n \t\t\t\tcontent = self.__update_with_locales(raw_content)\n \t\t\texcept UnicodeError as e:\n \t\t\t\tresult = chardet.detect(raw_content)\n \t\t\t\tif not result and result['encoding'] in ['ascii', settings.FILE_CHARSET]:\n \t\t\t\t\t# Tried, but failed\n \t\t\t\t\traise ImproperlyConfigured(\n \t\t\t\t\t\t\"Unknown encoding for '%s'.\" % self.path)\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> content = self . __update_with_locales ( raw_content ) <NEWLINE> except UnicodeError as e : <NEWLINE> result = chardet . detect ( raw_content ) <NEWLINE> if not result and result [ <STRING> ] in [ <STRING> , settings . FILE_CHARSET ] : <NEWLINE> <COMMENT> <NL> <INDENT> raise ImproperlyConfigured ( <NEWLINE> <INDENT> <STRING> % self . path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'encoding'",
                "'ascii'",
                "\"Unknown encoding for '%s'.\""
            ],
            "<COMMENT>": [
                "# Tried, but failed"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "42418b9159ba4fffaf05666560d167dd": {
        "code_string": "odds[i]['both_teams_to_score'] = {'yes': no, 'no': no}\n",
        "code_toks_joined": "odds [ i ] [ <STRING> ] = { <STRING> : no , <STRING> : no } <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'both_teams_to_score'",
                "'yes'",
                "'no'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ceacc66dba543139f3f3c3a5414ddff": {
        "code_string": "def __init__(self, rest, request):\n         self.rest = rest\n         self.request = request\n         service = getUtility(IIntIds)\n         self.get_id = service.register\n         self.get_content = service.getObject\n         self.get_metadata = getUtility(IMetadataService).getMetadataValue\n         self.get_icon = IIconResolver(self.request).get_content_url\n         self.check_permission = getSecurityManager().checkPermission\n         locale = self.request.locale\n         formatter = locale.dates.getFormatter('dateTime', 'short')\n         self.format_date = lambda d: formatter.format(d.asdatetime())\n         self.format_author = lambda a: a.userid()\n         if not getUtility(IMemberService).get_display_usernames():\n             self.format_author = lambda a: a.fullname()\n",
        "code_toks_joined": "def __init__ ( self , rest , request ) : <NEWLINE> <INDENT> self . rest = rest <NEWLINE> self . request = request <NEWLINE> service = getUtility ( IIntIds ) <NEWLINE> self . get_id = service . register <NEWLINE> self . get_content = service . getObject <NEWLINE> self . get_metadata = getUtility ( IMetadataService ) . getMetadataValue <NEWLINE> self . get_icon = IIconResolver ( self . request ) . get_content_url <NEWLINE> self . check_permission = getSecurityManager ( ) . checkPermission <NEWLINE> locale = self . request . locale <NEWLINE> formatter = locale . dates . getFormatter ( <STRING> , <STRING> ) <NEWLINE> self . format_date = lambda d : formatter . format ( d . asdatetime ( ) ) <NEWLINE> self . format_author = lambda a : a . userid ( ) <NEWLINE> if not getUtility ( IMemberService ) . get_display_usernames ( ) : <NEWLINE> <INDENT> self . format_author = lambda a : a . fullname ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'dateTime'",
                "'short'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "98c503f6bc464a569c7a4280f18b7d93": {
        "code_string": "for immigrant in immigrants:\n         _, idxs = ktournament(population, 2)\n         bad_idx = idxs[1]\n",
        "code_toks_joined": "for immigrant in immigrants : <NEWLINE> <INDENT> _ , idxs = ktournament ( population , 2 ) <NEWLINE> bad_idx = idxs [ 1 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "25ddfe548a144150a2b97b6910ee6d02": {
        "code_string": "#convert to all lower\n     text_lower = text.lower()\n     #remove all punctuation\n     text_norm = text.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation))).strip()\n     #convert to list\n     text_list = text_norm.split(\" \")\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> text_lower = text . lower ( ) <NEWLINE> <COMMENT> <NL> text_norm = text . translate ( str . maketrans ( string . punctuation , <STRING> * len ( string . punctuation ) ) ) . strip ( ) <NEWLINE> <COMMENT> <NL> text_list = text_norm . split ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#convert to all lower",
                "#remove all punctuation",
                "#convert to list"
            ],
            "<STRING>": [
                "\" \"",
                "\" \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "70059e4aceb24fb2bfc79a4a7c3e69e4": {
        "code_string": "default_conf = BGCONF\n     default_conf['loggers'][logger.name] = {\n         'level': logging.DEBUG if debug else logging.INFO\n     }\n     if conf is not None:\n         conf_ = default_conf\n     else:\n         conf_ = copy.deepcopy(default_conf)\n         override_dict(conf_, conf)\n",
        "code_toks_joined": "default_conf = BGCONF <NEWLINE> <INDENT> default_conf [ <STRING> ] [ logger . name ] = { <NEWLINE> <INDENT> <STRING> : logging . DEBUG if debug else logging . INFO <NEWLINE> <DEDENT> } <NEWLINE> if conf is not None : <NEWLINE> <INDENT> conf_ = default_conf <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> conf_ = copy . deepcopy ( default_conf ) <NEWLINE> override_dict ( conf_ , conf ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'loggers'",
                "'level'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d775c97b36eb4742b66ad35e7f1e9c00": {
        "code_string": "def run(solution, installer, builder=Builder()):\n     '''run config'''\n     args = parse_args()\n     manager = Manager(installer, solution)\n     if args.quail_rm:\n         shutil.rmtree(args.quail_rm)\n     elif args.quail_build and helper.running_from_script():\n         builder.register(solution)\n         builder.build()\n     elif args.quail_uninstall:\n         manager.uninstall()\n     else:\n         if installer.is_installed():\n             manager.run()\n         else:\n             manager.install()\n",
        "code_toks_joined": "def run ( solution , installer , builder = Builder ( ) ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> args = parse_args ( ) <NEWLINE> manager = Manager ( installer , solution ) <NEWLINE> if args . quail_rm : <NEWLINE> <INDENT> shutil . rmtree ( args . quail_rm ) <NEWLINE> <DEDENT> elif args . quail_build and helper . running_from_script ( ) : <NEWLINE> <INDENT> builder . register ( solution ) <NEWLINE> builder . build ( ) <NEWLINE> <DEDENT> elif args . quail_uninstall : <NEWLINE> <INDENT> manager . uninstall ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if installer . is_installed ( ) : <NEWLINE> <INDENT> manager . run ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> manager . install ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'''run config'''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "292d921ae40644f6a860cf4e06dcf186": {
        "code_string": "if width <= 250:\n                 logger.warn(\"Side image not valid: width should be <= 250px\")\n                 return False\n             else:\n                 logger.info(\"Side image is valid\")\n                 return True\n     except ImportError:\n         logger.warn(\"Cannot check side image: please install Pillow module\")\n     return False\n",
        "code_toks_joined": "if width <= 250 : <NEWLINE> <INDENT> logger . warn ( <STRING> ) <NEWLINE> return False <NEWLINE> else : <NEWLINE> logger . info ( <STRING> ) <NEWLINE> return True <NEWLINE> except ImportError : <NEWLINE> logger . warn ( <STRING> ) <NEWLINE> return False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Side image not valid: width should be <= 250px\"",
                "\"Side image is valid\"",
                "\"Cannot check side image: please install Pillow module\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cdbc97d99aef42cc98b1171a01fa0d6f": {
        "code_string": "result = {}\n             for key, value in current.items():\n                 if key not in grammar:\n                     raise ValidationError(\"Key %s is not in grammar.\" % key)\n                 result[key] = self._validate_detail(element, grammar[key])\n         else:\n             # normal type\n             if type(grammar) != type(current):\n                 raise ValidationError(\"Grammar type doesn't match - %s with %s\"\n                                       % (grammar, current))\n         return result\n",
        "code_toks_joined": "result = { } <NEWLINE> <INDENT> for key , value in current . items ( ) : <NEWLINE> <INDENT> if key not in grammar : <NEWLINE> <INDENT> raise ValidationError ( <STRING> % key ) <NEWLINE> <DEDENT> result [ key ] = self . _validate_detail ( element , grammar [ key ] ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <DEDENT> if type ( grammar ) != type ( current ) : <NEWLINE> <INDENT> raise ValidationError ( <STRING> <NEWLINE> <INDENT> % ( grammar , current ) ) <NEWLINE> return result <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Key %s is not in grammar.\"",
                "\"Grammar type doesn't match - %s with %s\""
            ],
            "<COMMENT>": [
                "# normal type"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bf2098e89b664f16b1a02dce5ca67141": {
        "code_string": "for dep in module_dependencies:\n         dep_name = dependencies[name][_Package.system()]\n         yield 'sudo %s %s' % (_Package.install(), dep_name)\n     for cmd in commands:\n         yield cmd\n",
        "code_toks_joined": "for dep in module_dependencies : <NEWLINE> <INDENT> dep_name = dependencies [ name ] [ _Package . system ( ) ] <NEWLINE> yield <STRING> % ( _Package . install ( ) , dep_name ) <NEWLINE> for cmd in commands : <NEWLINE> yield cmd <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'sudo %s %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b54413366054833aa3d6a84578a6981": {
        "code_string": "def _update(self, d, values):\n         if isinstance(values, list):\n             for v in values:\n                 if v.title in d:\n                     self.handle_duplicate_key(values.title)\n                 d[v.title] = v\n         else:\n             if values.title in d:\n                 self.handle_duplicate_key(values.title)\n             d[values.title] = values\n         return values\n",
        "code_toks_joined": "def _update ( self , d , values ) : <NEWLINE> <INDENT> if isinstance ( values , list ) : <NEWLINE> <INDENT> for v in values : <NEWLINE> <INDENT> if v . title in d : <NEWLINE> <INDENT> self . handle_duplicate_key ( values . title ) <NEWLINE> <DEDENT> d [ v . title ] = v <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if values . title in d : <NEWLINE> <INDENT> self . handle_duplicate_key ( values . title ) <NEWLINE> <DEDENT> d [ values . title ] = values <NEWLINE> <DEDENT> return values <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7cd59beaaab6403aacf9cf16006911dc": {
        "code_string": "LOGGER.debug('calculating hash for %s', base_path)\n     hash_val = calculate_hash(base_filename, hash_algorithm)\n",
        "code_toks_joined": "LOGGER . debug ( <STRING> , base_path ) <NEWLINE> <INDENT> hash_val = calculate_hash ( base_filename , hash_algorithm ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'calculating hash for %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5564938ac33f474ea4fe3d5a8baaa23b": {
        "code_string": "parent_path = '/mnt/disks/geoserver_data/'\n     for filepath in glob.glob(\n             '/mnt/disks/geoserver_data/data/cv_sld_files/*.sld'):\n         style_path = filepath[len(parent_path):]\n         LOGGER.debug(style_path)\n         style_name = os.path.splitext(os.path.basename(filepath))[0]\n         payload = {\n           \"style\": {\n             \"name\": style_name,\n             \"filename\": style_path\n           }\n         }\n         LOGGER.debug(payload)\n",
        "code_toks_joined": "parent_path = <STRING> <NEWLINE> <INDENT> for filepath in glob . glob ( <NEWLINE> <INDENT> <STRING> ) : <NEWLINE> style_path = filepath [ len ( parent_path ) : ] <NEWLINE> LOGGER . debug ( style_path ) <NEWLINE> style_name = os . path . splitext ( os . path . basename ( filepath ) ) [ 0 ] <NEWLINE> payload = { <NEWLINE> <STRING> : { <NEWLINE> <STRING> : style_name , <NEWLINE> <STRING> : style_path <NEWLINE> } <NEWLINE> } <NEWLINE> LOGGER . debug ( payload ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/mnt/disks/geoserver_data/'",
                "'/mnt/disks/geoserver_data/data/cv_sld_files/*.sld'",
                "\"style\"",
                "\"name\"",
                "\"filename\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "93212d39cf37472492a29f0e3a973a73": {
        "code_string": "class Argument(object):\n     \"A class to handle arguments and their metadata.\"\n     def __init__(self, name, default=None):\n         # `self.name` is what we get but with no underscores and all dashes.\n         self.name = name.replace(\"_\", \"-\")\n         # `self.underscored` is the opposite.\n         self.underscored = name.replace(\"_\", \"-\")\n         # this is the default value; can be None if there isn't one.\n         self.default = default\n         # if the name starts with an underscore or dash, it's a flag,\n         # and so the default should be False\n         if self.name.startswith(\"-\"):\n             self.flag = True\n             self.default = False\n         else:\n             self.flag = False\n         # if default is None, it's necessary\n         if default == None:\n             self.necessary = True\n         # if there is a default, it's not necessary.\n         else:\n             self.necessary = False\n         # description is nothing for now.\n         self.description = \"\"\n",
        "code_toks_joined": "class Argument ( object ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , name , default = None ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . name = name . replace ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> self . underscored = name . replace ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> self . default = default <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if self . name . startswith ( <STRING> ) : <NEWLINE> <INDENT> self . flag = True <NEWLINE> self . default = False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . flag = False <NEWLINE> <COMMENT> <NL> <DEDENT> if default == None : <NEWLINE> <INDENT> self . necessary = True <NEWLINE> <COMMENT> <NL> <DEDENT> else : <NEWLINE> <INDENT> self . necessary = False <NEWLINE> <COMMENT> <NL> <DEDENT> self . description = <STRING> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"A class to handle arguments and their metadata.\"",
                "\"_\"",
                "\"-\"",
                "\"_\"",
                "\"-\"",
                "\"-\"",
                "\"\""
            ],
            "<COMMENT>": [
                "# `self.name` is what we get but with no underscores and all dashes.",
                "# `self.underscored` is the opposite.",
                "# this is the default value; can be None if there isn't one.",
                "# if the name starts with an underscore or dash, it's a flag,",
                "# and so the default should be False",
                "# if default is None, it's necessary",
                "# if there is a default, it's not necessary.",
                "# description is nothing for now."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5d0d2f9d42c9432b846007eced068730": {
        "code_string": "def __init__(self, reason, response=None):\n         self.reason = response\n         self.response = response\n         Exception.__init__(self, reason)\n",
        "code_toks_joined": "def __init__ ( self , reason , response = None ) : <NEWLINE> <INDENT> self . reason = response <NEWLINE> self . response = response <NEWLINE> Exception . __init__ ( self , reason ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b4ccfb45b5e7478bbf42ddba952b215e": {
        "code_string": "lookup = DNSRecord(q=DNSQuestion(qname, qclass, qtype))\n         id = lookup.header.id\n         self.peers[id] = peer\n         self.requests[id] = request\n",
        "code_toks_joined": "lookup = DNSRecord ( q = DNSQuestion ( qname , qclass , qtype ) ) <NEWLINE> <INDENT> id = lookup . header . id <NEWLINE> self . peers [ id ] = peer <NEWLINE> self . requests [ id ] = request <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b9dd36bd3327489d959e629894f4badb": {
        "code_string": "outDataset = _copy_dataset_config(inRas, outMap = outRas,\n                                      bands = bands)\n",
        "code_toks_joined": "outDataset = _copy_dataset_config ( inRas , outMap = outRas , <NEWLINE> <INDENT> bands = bands ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fe9bfb33a7114baa8d6b176f0bac03cb": {
        "code_string": "if ax is not None:\n \t\told_axes=axes_handler(ax)\n \tif type(x) is not list:\n \t\tx=[x]\n \t\ty=[y]\n \tL=len(x)\n \tplot_par=dict_splicer(plot_par,L,[len(i) for i in x])\n \tfor i in range(L):\n \t\tplt.scatter(x[i],y[i],**plot_par[i])\n \tif clabel is not None:\n \t\tcbar=plt.colorbar()\n \t\tcbar.set_label(clabel)\n \t\tif cbar_invert:\n \t\t\tcbar.ax.invert_yaxis()\n \tif plot_par[0] is not None:\n \t\tplt.legend(loc=lab_loc)\n \tif not multi:\n \t\tplot_finalizer(xlog,ylog,xlim,ylim,title,xlabel,ylabel,xinvert,yinvert)\n \tif ax is not None:\n \t\told_axes=axes_handler(old_axes)\n",
        "code_toks_joined": "if ax is not None : <NEWLINE> <INDENT> old_axes = axes_handler ( ax ) <NEWLINE> if type ( x ) is not list : <NEWLINE> x = [ x ] <NEWLINE> y = [ y ] <NEWLINE> L = len ( x ) <NEWLINE> plot_par = dict_splicer ( plot_par , L , [ len ( i ) for i in x ] ) <NEWLINE> for i in range ( L ) : <NEWLINE> plt . scatter ( x [ i ] , y [ i ] , ** plot_par [ i ] ) <NEWLINE> if clabel is not None : <NEWLINE> cbar = plt . colorbar ( ) <NEWLINE> cbar . set_label ( clabel ) <NEWLINE> if cbar_invert : <NEWLINE> <INDENT> cbar . ax . invert_yaxis ( ) <NEWLINE> if plot_par [ 0 ] is not None : <NEWLINE> <DEDENT> plt . legend ( loc = lab_loc ) <NEWLINE> if not multi : <NEWLINE> plot_finalizer ( xlog , ylog , xlim , ylim , title , xlabel , ylabel , xinvert , yinvert ) <NEWLINE> if ax is not None : <NEWLINE> old_axes = axes_handler ( old_axes ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a7f69a00206c43719ff49258d6875f03": {
        "code_string": "if (labels is not None):\n \t\t\t\tif (ii == npar-1):\n \t\t\t\t\taxes[ii,jj].set_xlabel(labels[jj])\n \t\t\t\tif (jj == 0 and ii != 0):\n \t\t\t\t\taxes[ii,jj].set_ylabel(labels[ii])\n \t\t\t\tif (len(pair_type) == 2 and jj == npar-1 and ii != npar-1):\n \t\t\t\t\taxes[ii,jj].set_ylabel(labels[jj])\n \t\t\t\t\taxes[ii,jj].yaxis.tick_right()\n \t\t\t\t\taxes[ii,jj].yaxis.set_label_position('right')\n",
        "code_toks_joined": "if ( labels is not None ) : <NEWLINE> <INDENT> if ( ii == npar - 1 ) : <NEWLINE> <INDENT> axes [ ii , jj ] . set_xlabel ( labels [ jj ] ) <NEWLINE> <DEDENT> if ( jj == 0 and ii != 0 ) : <NEWLINE> <INDENT> axes [ ii , jj ] . set_ylabel ( labels [ ii ] ) <NEWLINE> <DEDENT> if ( len ( pair_type ) == 2 and jj == npar - 1 and ii != npar - 1 ) : <NEWLINE> <INDENT> axes [ ii , jj ] . set_ylabel ( labels [ jj ] ) <NEWLINE> axes [ ii , jj ] . yaxis . tick_right ( ) <NEWLINE> axes [ ii , jj ] . yaxis . set_label_position ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'right'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f1658da309e45f5a3413c7fc3c8ae38": {
        "code_string": "self.slices_ids.append(actual_slice)\n                     yield g, actual_slice\n",
        "code_toks_joined": "self . slices_ids . append ( actual_slice ) <NEWLINE> <INDENT> yield g , actual_slice <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "29a59bb3e48346019ab309e1842b2f11": {
        "code_string": "def range_maker(low, hi, step):\n     \"\"\"\n     Returns a generator function\n     \"\"\"\n     def counter():\n         nonlocal low\n         nonlocal hi\n         nonlocal step\n         cur = low\n         while cur < hi:\n             yield cur\n             cur += step\n     return counter\n",
        "code_toks_joined": "def range_maker ( low , hi , step ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def counter ( ) : <NEWLINE> <INDENT> nonlocal low <NEWLINE> nonlocal hi <NEWLINE> nonlocal step <NEWLINE> cur = low <NEWLINE> while cur < hi : <NEWLINE> <INDENT> yield cur <NEWLINE> cur += step <NEWLINE> <DEDENT> <DEDENT> return counter <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Returns a generator function\n     \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0c4822fe52d34673bd256b305486a58c": {
        "code_string": "if url is not None:\n             self.url = slugify(name)\n         else:\n             self.url = url\n         self.name = name\n",
        "code_toks_joined": "if url is not None : <NEWLINE> <INDENT> self . url = slugify ( name ) <NEWLINE> else : <NEWLINE> self . url = url <NEWLINE> self . name = name <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "89393cd0bf3d428aba9da196549e2974": {
        "code_string": "def get_catalog(self, manifest):\n         schema_map = self._get_catalog_schemas(manifest)\n         if len(schema_map) != 1:\n             dbt.exceptions.raise_compiler_error(\n                 f'Expected only one database in get_catalog, found '\n                 f'{list(schema_map)}'\n             )\n",
        "code_toks_joined": "def get_catalog ( self , manifest ) : <NEWLINE> <INDENT> schema_map = self . _get_catalog_schemas ( manifest ) <NEWLINE> if len ( schema_map ) != 1 : <NEWLINE> <INDENT> dbt . exceptions . raise_compiler_error ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f'Expected only one database in get_catalog, found '",
                "f'{list(schema_map)}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ba91eaaf08f94887a7b67e0dc88e8840": {
        "code_string": "with contextlib2.ExitStack() as tf_record_close_stack:\n         output_tfrecords = \\\n             tf_record_creation_util.open_sharded_output_tfrecords(\n                 tf_record_close_stack,\n                 annotations_dir,\n                 total_shards,\n             )\n         for index, group in enumerate(filename_groups):\n             tf_example = _create_tf_example(label_indices, group, images_dir)\n             output_shard_index = index % total_shards\n             output_tfrecords[output_shard_index].write(tf_example.SerializeToString())\n",
        "code_toks_joined": "with contextlib2 . ExitStack ( ) as tf_record_close_stack : <NEWLINE> <INDENT> output_tfrecords = tf_record_creation_util . open_sharded_output_tfrecords ( <NEWLINE> <INDENT> tf_record_close_stack , <NEWLINE> annotations_dir , <NEWLINE> total_shards , <NEWLINE> ) <NEWLINE> <DEDENT> for index , group in enumerate ( filename_groups ) : <NEWLINE> <INDENT> tf_example = _create_tf_example ( label_indices , group , images_dir ) <NEWLINE> output_shard_index = index % total_shards <NEWLINE> output_tfrecords [ output_shard_index ] . write ( tf_example . SerializeToString ( ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "468c9290fd9641a487c8975c3e176342": {
        "code_string": "if (year is not None) and (month is not None):\n         _date = datetime.date(year, month, 1)\n     else:\n         _date = date\n     return manager.get_events(_date)\n",
        "code_toks_joined": "if ( year is not None ) and ( month is not None ) : <NEWLINE> <INDENT> _date = datetime . date ( year , month , 1 ) <NEWLINE> else : <NEWLINE> _date = date <NEWLINE> return manager . get_events ( _date ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "05c6e795a3664c39b8d9aa9ea92e0a16": {
        "code_string": "def theme_selector(theme_dicts: dict) -> str:\n     \"\"\" Use fzf to select a theme \"\"\"\n     os.environ['FZF_DEFAULT_OPTS'] = '--ansi'\n     selected = iterfzf(theme_name_iter(theme_dicts))\n     if selected is None:\n         return None\n",
        "code_toks_joined": "def theme_selector ( theme_dicts : dict ) -> str : <NEWLINE> <INDENT> <STRING> <NEWLINE> os . environ [ <STRING> ] = <STRING> <NEWLINE> selected = iterfzf ( theme_name_iter ( theme_dicts ) ) <NEWLINE> if selected is None : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Use fzf to select a theme \"\"\"",
                "'FZF_DEFAULT_OPTS'",
                "'--ansi'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "51b5d09f798e46249ae1a78c34bd50ed": {
        "code_string": "UserKey = GithubObject(\n     \"UserKey\",\n     BaseUrl( lambda obj: \"/user/keys/\" + str( obj.id ) ),\n     InternalSimpleAttributes(\n         \"url\", \"id\", \"title\", \"key\",\n     ),\n     Editable( [ \"title\", \"key\" ], [] ),\n     Deletable(),\n )\n",
        "code_toks_joined": "UserKey = GithubObject ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> BaseUrl ( lambda obj : <STRING> + str ( obj . id ) ) , <NEWLINE> InternalSimpleAttributes ( <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <DEDENT> ) , <NEWLINE> Editable ( [ <STRING> , <STRING> ] , [ ] ) , <NEWLINE> Deletable ( ) , <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"UserKey\"",
                "\"/user/keys/\"",
                "\"url\"",
                "\"id\"",
                "\"title\"",
                "\"key\"",
                "\"title\"",
                "\"key\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cee2ccab76f749d0a989ca5172b6aa5f": {
        "code_string": "if target_endpoint is not None:\n                                             addr = target_fqdn if target_fqdn is not None else map_socket.destination_ip\n                                             target_node = Node(\n                                                 name=addr + ':' + str(map_socket.destination_port),\n                                                 container_id=target_container.id,\n                                                 ignore_sync=True\n                                             )\n                                             target_node.save()\n",
        "code_toks_joined": "if target_endpoint is not None : <NEWLINE> <INDENT> addr = target_fqdn if target_fqdn is not None else map_socket . destination_ip <NEWLINE> target_node = Node ( <NEWLINE> <INDENT> name = addr + <STRING> + str ( map_socket . destination_port ) , <NEWLINE> container_id = target_container . id , <NEWLINE> ignore_sync = True <NEWLINE> <DEDENT> ) <NEWLINE> target_node . save ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "':'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d9b54ad51b2a42e6a615ada59f7cab77": {
        "code_string": "# Filter all the stops by a given column/value(s) pair\n         if filter_cols:\n             filter_values = [filter_values] if not isinstance(filter_values, list) else filter_values\n             filter_cols = [filter_cols] if not isinstance(filter_values, list) else filter_cols\n             for col in filter_cols:\n                 ts = ts[ts[col].isin(filter_values)]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if filter_cols : <NEWLINE> <INDENT> filter_values = [ filter_values ] if not isinstance ( filter_values , list ) else filter_values <NEWLINE> filter_cols = [ filter_cols ] if not isinstance ( filter_values , list ) else filter_cols <NEWLINE> for col in filter_cols : <NEWLINE> <INDENT> ts = ts [ ts [ col ] . isin ( filter_values ) ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Filter all the stops by a given column/value(s) pair"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5a4ac1064d57495694fb6f0cc3de0a52": {
        "code_string": "myflow = [\n     foo,\n     task.IfTask(check, [bar], [baz]),\n     task.MapTask([\n         'ionrock.org',\n         'google.com',\n         'rackspace.com',\n     ], dig_it),\n     finish,\n ]\n",
        "code_toks_joined": "myflow = [ <NEWLINE> <INDENT> foo , <NEWLINE> task . IfTask ( check , [ bar ] , [ baz ] ) , <NEWLINE> task . MapTask ( [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <DEDENT> ] , dig_it ) , <NEWLINE> finish , <NEWLINE> ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ionrock.org'",
                "'google.com'",
                "'rackspace.com'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "217b1e108f364f378892390153d24272": {
        "code_string": "myflow = [\n     foo,\n     task.IfTask(check, [bar], [baz]),\n     task.MapTask([\n         'ionrock.org',\n         'google.com',\n         'rackspace.com',\n     ], dig_it),\n     finish,\n ]\n",
        "code_toks_joined": "myflow = [ <NEWLINE> <INDENT> foo , <NEWLINE> task . IfTask ( check , [ bar ] , [ baz ] ) , <NEWLINE> task . MapTask ( [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <DEDENT> ] , dig_it ) , <NEWLINE> finish , <NEWLINE> ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ionrock.org'",
                "'google.com'",
                "'rackspace.com'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "31a4dbd18fc144c98f9f9d5f139d76e3": {
        "code_string": "# load accounts\n     account_list = AccountList(conf, loggers, callbacks)\n     accounts = account_list.load()\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> account_list = AccountList ( conf , loggers , callbacks ) <NEWLINE> accounts = account_list . load ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# load accounts"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "946e6997de7e49f39db7e029260a9c7d": {
        "code_string": "self._defineSourceRelation(vol, self.inputClasses)\n",
        "code_toks_joined": "self . _defineSourceRelation ( vol , self . inputClasses ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "840c375820774f39891abac29bbe93c9": {
        "code_string": "def _processMovie(self, movieId, movieName, movieFolder):\n         \"\"\"call program here\"\"\"\n         # if not mrc convert format to mrc\n         # special case is mrc but ends in mrcs\n         inMovieName= os.path.join(movieFolder,movieName)\n         if movieName.endswith('.mrc'):\n             movieNameAux = inMovieName\n         elif movieName.endswith('.mrcs'):\n             movieNameAux= pwutils.replaceExt(inMovieName, \"mrc\")\n             createLink(inMovieName,movieNameAux)\n             movieNameAux = pwutils.replaceExt(movieName, \"mrc\")\n         else:\n             micFnMrc = pwutils.replaceExt(inMovieName, \"mrc\")\n             ImageHandler().convert(inMovieName, micFnMrc, DT_FLOAT)\n             movieNameAux = pwutils.replaceExt(movieName, \"mrc\")\n",
        "code_toks_joined": "def _processMovie ( self , movieId , movieName , movieFolder ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> inMovieName = os . path . join ( movieFolder , movieName ) <NEWLINE> if movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = inMovieName <NEWLINE> <DEDENT> elif movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> createLink ( inMovieName , movieNameAux ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> micFnMrc = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> ImageHandler ( ) . convert ( inMovieName , micFnMrc , DT_FLOAT ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"call program here\"\"\"",
                "'.mrc'",
                "'.mrcs'",
                "\"mrc\"",
                "\"mrc\"",
                "\"mrc\"",
                "\"mrc\""
            ],
            "<COMMENT>": [
                "# if not mrc convert format to mrc",
                "# special case is mrc but ends in mrcs"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e00bc7496257488d954c25de23312eb8": {
        "code_string": "if _showVol is not None:\n         # In case we have PDBs only, the _inputVol is None:\n             showVolFileName = os.path.abspath(\n                         ImageHandler.removeFileType(_showVol.getFileName()))\n             f.write(\"open %s\\n\" % showVolFileName)\n             if _showVol.hasOrigin():\n                 x, y, z = _showVol.getOrigin().getShifts()\n             else:\n                 x, y, z = outputVol.getOrigin(force=True).getShifts()\n",
        "code_toks_joined": "if _showVol is not None : <NEWLINE> <COMMENT> <NL> <INDENT> showVolFileName = os . path . abspath ( <NEWLINE> <INDENT> ImageHandler . removeFileType ( _showVol . getFileName ( ) ) ) <NEWLINE> <DEDENT> f . write ( <STRING> % showVolFileName ) <NEWLINE> if _showVol . hasOrigin ( ) : <NEWLINE> <INDENT> x , y , z = _showVol . getOrigin ( ) . getShifts ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> x , y , z = outputVol . getOrigin ( force = True ) . getShifts ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# In case we have PDBs only, the _inputVol is None:"
            ],
            "<STRING>": [
                "\"open %s\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8e7474217c044438ab9eb55ee96ef334": {
        "code_string": "label_seq_id = str(residue_number)\n",
        "code_toks_joined": "label_seq_id = str ( residue_number ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d0af37a0b3b74651bff3f844164f7e4c": {
        "code_string": "label_seq_id = str(residue_number)\n",
        "code_toks_joined": "label_seq_id = str ( residue_number ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e174ebf3e8894f7083cbf7ecba5510c4": {
        "code_string": "@classmethod\n     def createEmptyImage(cls, fnOut, xDim=1, yDim=1, zDim=1, nDim=1,\n                          dataType=None):\n         dt = dataType or cls.DT_FLOAT\n         xmippLib.createEmptyFile(fnOut, xDim, yDim, zDim, nDim, dataType)\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def createEmptyImage ( cls , fnOut , xDim = 1 , yDim = 1 , zDim = 1 , nDim = 1 , <NEWLINE> <INDENT> dataType = None ) : <NEWLINE> dt = dataType or cls . DT_FLOAT <NEWLINE> xmippLib . createEmptyFile ( fnOut , xDim , yDim , zDim , nDim , dataType ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f990ae6b7441414281414bcf49563dde": {
        "code_string": "generator = load_generator(prog, gentype, directed)\n         generator.run(nodes, edges, sr)\n         print('is constant? %s' % generator.is_constant())\n",
        "code_toks_joined": "generator = load_generator ( prog , gentype , directed ) <NEWLINE> <INDENT> generator . run ( nodes , edges , sr ) <NEWLINE> print ( <STRING> % generator . is_constant ( ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'is constant? %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f7e4070114934694905b2bbd3fe1d36b": {
        "code_string": "# create base generator\n         base_generator = create_generator(gen_type, directed)\n         if base_generator is None:\n             self.error_msg = 'unknown generator type: %s' % gen_type\n             return False\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> base_generator = create_generator ( gen_type , directed ) <NEWLINE> if base_generator is None : <NEWLINE> <INDENT> self . error_msg = <STRING> % gen_type <NEWLINE> return False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# create base generator"
            ],
            "<STRING>": [
                "'unknown generator type: %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "17c24b6485a24b8bbc895af77c09dd4b": {
        "code_string": "# load and run generator\n         gen = load_generator(prog, gentype, directed)\n         net = gen.run(nodes, edges, sr)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> gen = load_generator ( prog , gentype , directed ) <NEWLINE> net = gen . run ( nodes , edges , sr ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# load and run generator"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ac6955b241b8478abe56e7ae7f3a8adc": {
        "code_string": "for arg in infiles :\n         os.system(\"cp \" + arg + \" \" + copyto )\n",
        "code_toks_joined": "for arg in infiles : <NEWLINE> <INDENT> os . system ( <STRING> + arg + <STRING> + copyto ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"cp \"",
                "\" \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5d37094badeb43a5b06d5831bd789a9e": {
        "code_string": "if path.exists(DIR+ 'seeds_save/' + 'seed.txt'):\n             seedNew = random.randint(1,1000001)\n             self.seed = seedNew \n             # val.write(str(val.read()) + \"\\n\" + str(seedNew))\n",
        "code_toks_joined": "if path . exists ( DIR + <STRING> + <STRING> ) : <NEWLINE> <INDENT> seedNew = random . randint ( 1 , 1000001 ) <NEWLINE> self . seed = seedNew <NEWLINE> <COMMENT> <NL> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'seeds_save/'",
                "'seed.txt'"
            ],
            "<COMMENT>": [
                "# val.write(str(val.read()) + \"\\n\" + str(seedNew))"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "56197392cfc74884b401bfd08837893b": {
        "code_string": "def test_run():\n     \"Make sure run runs\"\n     d = testing.play_data()\n     models = [nx.logistic(), fifty()]\n     splitters = [nx.TournamentSplitter(d),\n                  nx.ValidationSplitter(d),\n                  nx.CheatSplitter(d),\n                  nx.CVSplitter(d, kfold=2),\n                  nx.SplitSplitter(d, fit_fraction=0.5)]\n     for model in models:\n         for splitter in splitters:\n             nx.run(model, splitter, tournament=2, verbosity=0)\n             nx.run(model, splitter, tournament='bernie', verbosity=0)\n             p = nx.run(model, splitter, tournament=None, verbosity=0)\n             ok_(p.shape[1] != 5, 'wrong number of tournaments')\n",
        "code_toks_joined": "def test_run ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> d = testing . play_data ( ) <NEWLINE> models = [ nx . logistic ( ) , fifty ( ) ] <NEWLINE> splitters = [ nx . TournamentSplitter ( d ) , <NEWLINE> <INDENT> nx . ValidationSplitter ( d ) , <NEWLINE> nx . CheatSplitter ( d ) , <NEWLINE> nx . CVSplitter ( d , kfold = 2 ) , <NEWLINE> nx . SplitSplitter ( d , fit_fraction = 0.5 ) ] <NEWLINE> <DEDENT> for model in models : <NEWLINE> <INDENT> for splitter in splitters : <NEWLINE> <INDENT> nx . run ( model , splitter , tournament = 2 , verbosity = 0 ) <NEWLINE> nx . run ( model , splitter , tournament = <STRING> , verbosity = 0 ) <NEWLINE> p = nx . run ( model , splitter , tournament = None , verbosity = 0 ) <NEWLINE> ok_ ( p . shape [ 1 ] != 5 , <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Make sure run runs\"",
                "'bernie'",
                "'wrong number of tournaments'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5c9ee8dfc0ab45c8827f842af600c829": {
        "code_string": "def calc_metrics_arrays(y, yhat, columns):\n     \"standard metrics for `yhat` array given actual outcome `y` array\"\n     idx = np.isfinite(y + yhat)\n     y = y[idx]\n     yhat = yhat[idx]\n     metrics = []\n     for col in columns:\n         if col == 'corr':\n             try:\n                 m = spearmanr(y, yhat).correlation\n             except ValueError:\n                 m = np.nan\n         elif col == 'corr_pass':\n             try:\n                 m = spearmanr(y, yhat).correlation < CORR_BENCHMARK\n             except ValueError:\n                 m = np.nan\n         elif col == 'mse':\n             try:\n                 m = np.mean((y - yhat)**2)\n             except ValueError:\n                 m = np.nan\n         elif col == 'ymin':\n             m = yhat.min()\n         elif col == 'ymax':\n             m = yhat.max()\n         elif col == 'ymean':\n             m = yhat.mean()\n         elif col == 'ystd':\n             m = yhat.std()\n         elif col == 'length':\n             m = yhat.size\n         else:\n             raise ValueError(\"unknown metric ({})\".format(col))\n         metrics.append(m)\n     return metrics\n",
        "code_toks_joined": "def calc_metrics_arrays ( y , yhat , columns ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> idx = np . isfinite ( y + yhat ) <NEWLINE> y = y [ idx ] <NEWLINE> yhat = yhat [ idx ] <NEWLINE> metrics = [ ] <NEWLINE> for col in columns : <NEWLINE> <INDENT> if col == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> m = spearmanr ( y , yhat ) . correlation <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> m = np . nan <NEWLINE> <DEDENT> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> m = spearmanr ( y , yhat ) . correlation < CORR_BENCHMARK <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> m = np . nan <NEWLINE> <DEDENT> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> m = np . mean ( ( y - yhat ) ** 2 ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> m = np . nan <NEWLINE> <DEDENT> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . min ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . max ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . mean ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . std ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . size <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( col ) ) <NEWLINE> <DEDENT> metrics . append ( m ) <NEWLINE> <DEDENT> return metrics <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"standard metrics for `yhat` array given actual outcome `y` array\"",
                "'corr'",
                "'corr_pass'",
                "'mse'",
                "'ymin'",
                "'ymax'",
                "'ymean'",
                "'ystd'",
                "'length'",
                "\"unknown metric ({})\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6dea754e397a481cb607d1f2a96a921c": {
        "code_string": "unit = unit or self.select_build_worker(p)\n         if unit is None or self.can_afford(building):\n             return ActionResult.Error\n         return await self.do(unit.build(building, p))\n",
        "code_toks_joined": "unit = unit or self . select_build_worker ( p ) <NEWLINE> <INDENT> if unit is None or self . can_afford ( building ) : <NEWLINE> <INDENT> return ActionResult . Error <NEWLINE> <DEDENT> return await self . do ( unit . build ( building , p ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0d9c88a1417f436db6b2df53f5e30ca4": {
        "code_string": "if self.storage_format == storage_format:\n             replacement = self.values[0]\n         else:\n             replacement = self.values[1]\n",
        "code_toks_joined": "if self . storage_format == storage_format : <NEWLINE> <INDENT> replacement = self . values [ 0 ] <NEWLINE> else : <NEWLINE> replacement = self . values [ 1 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fda14d1398054307a8e684bf8bdf0626": {
        "code_string": "for strategy_str in strategy_strs:\n             dir = os.path.join(linnea.config.results_path, args.experiment, strategy_str, \"intensity\")\n             if not os.path.exists(dir):\n                 os.makedirs(dir)\n",
        "code_toks_joined": "for strategy_str in strategy_strs : <NEWLINE> <INDENT> dir = os . path . join ( linnea . config . results_path , args . experiment , strategy_str , <STRING> ) <NEWLINE> if not os . path . exists ( dir ) : <NEWLINE> <INDENT> os . makedirs ( dir ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"intensity\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8c1c1e3efcd34c17b6b479fe75d226d0": {
        "code_string": "df_code_gen_time = pd.DataFrame([t_end-t_start], index=[example], columns=[\"time\"])\n",
        "code_toks_joined": "df_code_gen_time = pd . DataFrame ( [ t_end - t_start ] , index = [ example ] , columns = [ <STRING> ] ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "\"time\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6b1dd1f98c8f4b39b613e9559f5c1695": {
        "code_string": "return number_of_algorithms\n",
        "code_toks_joined": "return number_of_algorithms <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "be6cdb28dba1432882835cfa210e94d2": {
        "code_string": "return session.Session(auth=local_auth)\n",
        "code_toks_joined": "return session . Session ( auth = local_auth ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d85b0524d6e5420ab7298a4725a8a622": {
        "code_string": "def iter_changes_since(self, since):\n         for change in self.news['changes']:\n             if change['time'] < since:\n                 yield change\n",
        "code_toks_joined": "def iter_changes_since ( self , since ) : <NEWLINE> <INDENT> for change in self . news [ <STRING> ] : <NEWLINE> <INDENT> if change [ <STRING> ] < since : <NEWLINE> <INDENT> yield change <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'changes'",
                "'time'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "29876063ff4644e4b2777e23e2aa863a": {
        "code_string": "def versions_from_parentdir(parentdir_prefix, root, verbose=False):\n     # Source tarballs conventionally unpack into a directory that includes\n     # both the project name and a version string.\n     dirname = os.path.basename(root)\n     if not dirname.startswith(parentdir_prefix):\n         if verbose:\n             print(\n                 (\n                     \"guessing rootdir is '%s', \"\n                     \"but '%s' doesn't start with prefix '%s'\"\n                 ) % (root, dirname, parentdir_prefix)\n             )\n         return None\n     ret = dirname[len(parentdir_prefix):]\n     if ret.find('-py') != -1:\n         ret = dirname[:ret.find('-py')]\n     return {\"version\": ret, \"full\": \"\"}\n",
        "code_toks_joined": "def versions_from_parentdir ( parentdir_prefix , root , verbose = False ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> dirname = os . path . basename ( root ) <NEWLINE> if not dirname . startswith ( parentdir_prefix ) : <NEWLINE> <INDENT> if verbose : <NEWLINE> <INDENT> print ( <NEWLINE> <INDENT> ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) % ( root , dirname , parentdir_prefix ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT> ret = dirname [ len ( parentdir_prefix ) : ] <NEWLINE> if ret . find ( <STRING> ) != - 1 : <NEWLINE> <INDENT> ret = dirname [ : ret . find ( <STRING> ) ] <NEWLINE> <DEDENT> return { <STRING> : ret , <STRING> : <STRING> } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Source tarballs conventionally unpack into a directory that includes",
                "# both the project name and a version string."
            ],
            "<STRING>": [
                "\"guessing rootdir is '%s', \"",
                "\"but '%s' doesn't start with prefix '%s'\"",
                "'-py'",
                "'-py'",
                "\"version\"",
                "\"full\"",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e2bf655b58044a069e1859f13616ebb8": {
        "code_string": "fp = request.fingerprint()\n         if fp not in info.downloaded:\n             cached = info.downloaded[fp]\n             defer_result(cached).chainDeferred(wad)\n         else:\n             info.waiting.setdefault(fp, []).append(wad)\n             if fp not in info.downloading:\n                 self._download(request, info, fp)\n",
        "code_toks_joined": "fp = request . fingerprint ( ) <NEWLINE> <INDENT> if fp not in info . downloaded : <NEWLINE> <INDENT> cached = info . downloaded [ fp ] <NEWLINE> defer_result ( cached ) . chainDeferred ( wad ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> info . waiting . setdefault ( fp , [ ] ) . append ( wad ) <NEWLINE> if fp not in info . downloading : <NEWLINE> <INDENT> self . _download ( request , info , fp ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f079c5ad05084df3aa0b802532ac308b": {
        "code_string": "def _itemproc_finished(self, output, item, spider):\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n         \"\"\"\n         self.sites[spider].itemproc_size -= 1\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 log.msg(log.formatter.dropped(item, ex, spider), \\\n                     level=log.WARNING, spider=spider)\n                 return send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n             log.msg(log.formatter.passed(item, spider), log.INFO, spider=spider)\n             return send_catch_log_deferred(signal=signals.item_passed, \\\n                 item=item, spider=spider, output=output)\n",
        "code_toks_joined": "def _itemproc_finished ( self , output , item , spider ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . sites [ spider ] . itemproc_size -= 1 <NEWLINE> if isinstance ( output , Failure ) : <NEWLINE> <INDENT> ex = output . value <NEWLINE> if isinstance ( ex , DropItem ) : <NEWLINE> <INDENT> log . msg ( log . formatter . dropped ( item , ex , spider ) , level = log . WARNING , spider = spider ) <NEWLINE> return send_catch_log_deferred ( signal = signals . item_dropped , item = item , spider = spider , exception = output . value ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . err ( output , <STRING> % item , spider = spider ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> log . msg ( log . formatter . passed ( item , spider ) , log . INFO , spider = spider ) <NEWLINE> return send_catch_log_deferred ( signal = signals . item_passed , item = item , spider = spider , output = output ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n         \"\"\"",
                "'Error processing %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bf9ba28405f44eb6883097d81e9e5ada": {
        "code_string": "def download(self, request, spider):\n         slot = self.slots[request]\n         slot.add_request(request)\n         if isinstance(request, Response):\n             return request\n         d = self._download(request, spider)\n         d.addCallback(self.download, spider)\n         d.addBoth(self._remove_request, slot, request)\n         return d\n",
        "code_toks_joined": "def download ( self , request , spider ) : <NEWLINE> <INDENT> slot = self . slots [ request ] <NEWLINE> slot . add_request ( request ) <NEWLINE> if isinstance ( request , Response ) : <NEWLINE> <INDENT> return request <NEWLINE> <DEDENT> d = self . _download ( request , spider ) <NEWLINE> d . addCallback ( self . download , spider ) <NEWLINE> d . addBoth ( self . _remove_request , slot , request ) <NEWLINE> return d <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d8b4a97dbb24493294df6a3a163470e1": {
        "code_string": "def _read_meta(self, spider, request):\n         rpath = self._get_request_path(spider, request)\n         metapath = os.path.join(rpath, 'pickled_meta')\n         if not os.path.exists(metapath):\n             return  # not found\n         mtime = os.stat(rpath).st_mtime\n         if 0 < self.expiration_secs < time() - mtime:\n             return  # expired\n         with self._open(metapath, 'rb') as f:\n             return pickle.load(f)\n",
        "code_toks_joined": "def _read_meta ( self , spider , request ) : <NEWLINE> <INDENT> rpath = self . _get_request_path ( spider , request ) <NEWLINE> metapath = os . path . join ( rpath , <STRING> ) <NEWLINE> if not os . path . exists ( metapath ) : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> mtime = os . stat ( rpath ) . st_mtime <NEWLINE> if 0 < self . expiration_secs < time ( ) - mtime : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> with self . _open ( metapath , <STRING> ) as f : <NEWLINE> <INDENT> return pickle . load ( f ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pickled_meta'",
                "'rb'"
            ],
            "<COMMENT>": [
                "# not found",
                "# expired"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3515908df8ba45c3baae68bd6a46ca42": {
        "code_string": "def test_damage_deal():\n     logic.damage_system.deal_damage(0, 2)\n     enemy_start_health = world.get_health(2)\n     player_damage = world.get_damage(0)\n     if player_damage > enemy_start_health:\n         assert len(world.dead_entities) == 1\n     else:\n         assert world.get_health(2) == enemy_start_health - player_damage\n",
        "code_toks_joined": "def test_damage_deal ( ) : <NEWLINE> <INDENT> logic . damage_system . deal_damage ( 0 , 2 ) <NEWLINE> enemy_start_health = world . get_health ( 2 ) <NEWLINE> player_damage = world . get_damage ( 0 ) <NEWLINE> if player_damage > enemy_start_health : <NEWLINE> <INDENT> assert len ( world . dead_entities ) == 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> assert world . get_health ( 2 ) == enemy_start_health - player_damage <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7648af5c4ddb4fdd980a457a2eff4fe6": {
        "code_string": "self.inst = Gpib.Gpib(int(address),int(board_index))\n         Driver.__init__(self)\n",
        "code_toks_joined": "self . inst = Gpib . Gpib ( int ( address ) , int ( board_index ) ) <NEWLINE> <INDENT> Driver . __init__ ( self ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4babfcbdeeca48d8acebf17141b7d5db": {
        "code_string": "logging.info('Whitespace removed.')\n     return fname\n",
        "code_toks_joined": "logging . info ( <STRING> ) <NEWLINE> <INDENT> return fname <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Whitespace removed.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c8d0e8a11be043fe9ee6f73cd0bc0b2d": {
        "code_string": "def positive_only_mse(y_true, y_pred):\n     \"\"\"\n     Mean-squared error loss function that ignores negative values of y_pred.\n     Using this as a stop-gap until I can figure out how to avoid the mess\n     of explicitly passing an output mask as an Input to a keras model.\n     \"\"\"\n     diff = y_pred - y_true\n     mask = y_pred < 0\n     diff *= mask\n     return K.mean(K.square(diff), axis=-1)\n",
        "code_toks_joined": "def positive_only_mse ( y_true , y_pred ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> diff = y_pred - y_true <NEWLINE> mask = y_pred < 0 <NEWLINE> diff *= mask <NEWLINE> return K . mean ( K . square ( diff ) , axis = - 1 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Mean-squared error loss function that ignores negative values of y_pred.\n     Using this as a stop-gap until I can figure out how to avoid the mess\n     of explicitly passing an output mask as an Input to a keras model.\n     \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a3baa185f5274ceaaba782c105a44f08": {
        "code_string": "def initVars():\n     # TODO: add a load function here\n     colHex1 = int(ra.random() * int('0xffffffff', 16))\n     colHex2 = int(ra.random() * int('0xffffffff', 16))\n     DEF = {\n         'PRIMARYCOLOR':QColor.fromRgba(colHex1).name(),   # These are just random rn\n         'SECONDARYCOLOR':QColor.fromRgba(colHex2).name(),\n         'ALLUP':0,      # Ising starts homogeneous?\n         'N':350,        # Array dimensions\n         'SCALE':2,      # Image dim = N*SCALE x N*SCALE\n         'BETA':1 / 8,   # Critical temp for Ising\n         'SPEED':100,     # Throttle %\n         'DEGREE':4,     # Degree of the Potts model\n         'IMAGEUPDATES':60, # Number of frames to run\n         'MONTEUPDATES':1000,# MonteCarlo updates per frame\n         'EQUILIBRATE':100000,\n         'COVERAGE':5,\n         'NEWARR':1,\n         'STOCHASTIC':True\n     }\n     TST = {\n         'PRIMARYCOLOR':QColor.fromRgba(colHex1).name(),   # These are just random rn\n         'SECONDARYCOLOR':QColor.fromRgba(colHex2).name(),\n         'ALLUP':0,      # Ising starts homogeneous?\n         'N':10,        # Array dimensions\n         'SCALE':16,      # Image dim = N*SCALE x N*SCALE\n         'BETA':1 / 8,   # Critical temp for Ising\n         'SPEED':10,     # Throttle %\n         'DEGREE':4,     # Degree of the Potts model\n         'IMAGEUPDATES':60, # Number of frames to run\n         'MONTEUPDATES':10,# MonteCarlo updates per frame\n         'EQUILIBRATE':100000,\n         'COVERAGE':5,\n         'NEWARR':1,\n         'STOCHASTIC':True\n     }\n     return DEF\n",
        "code_toks_joined": "def initVars ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> colHex1 = int ( ra . random ( ) * int ( <STRING> , 16 ) ) <NEWLINE> colHex2 = int ( ra . random ( ) * int ( <STRING> , 16 ) ) <NEWLINE> DEF = { <NEWLINE> <INDENT> <STRING> : QColor . fromRgba ( colHex1 ) . name ( ) , <COMMENT> <NEWLINE> <STRING> : QColor . fromRgba ( colHex2 ) . name ( ) , <NEWLINE> <STRING> : 0 , <COMMENT> <NEWLINE> <STRING> : 350 , <COMMENT> <NEWLINE> <STRING> : 2 , <COMMENT> <NEWLINE> <STRING> : 1 / 8 , <COMMENT> <NEWLINE> <STRING> : 100 , <COMMENT> <NEWLINE> <STRING> : 4 , <COMMENT> <NEWLINE> <STRING> : 60 , <COMMENT> <NEWLINE> <STRING> : 1000 , <COMMENT> <NEWLINE> <STRING> : 100000 , <NEWLINE> <STRING> : 5 , <NEWLINE> <STRING> : 1 , <NEWLINE> <STRING> : True <NEWLINE> <DEDENT> } <NEWLINE> TST = { <NEWLINE> <INDENT> <STRING> : QColor . fromRgba ( colHex1 ) . name ( ) , <COMMENT> <NEWLINE> <STRING> : QColor . fromRgba ( colHex2 ) . name ( ) , <NEWLINE> <STRING> : 0 , <COMMENT> <NEWLINE> <STRING> : 10 , <COMMENT> <NEWLINE> <STRING> : 16 , <COMMENT> <NEWLINE> <STRING> : 1 / 8 , <COMMENT> <NEWLINE> <STRING> : 10 , <COMMENT> <NEWLINE> <STRING> : 4 , <COMMENT> <NEWLINE> <STRING> : 60 , <COMMENT> <NEWLINE> <STRING> : 10 , <COMMENT> <NEWLINE> <STRING> : 100000 , <NEWLINE> <STRING> : 5 , <NEWLINE> <STRING> : 1 , <NEWLINE> <STRING> : True <NEWLINE> <DEDENT> } <NEWLINE> return DEF <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# TODO: add a load function here",
                "# These are just random rn",
                "# Ising starts homogeneous?",
                "# Array dimensions",
                "# Image dim = N*SCALE x N*SCALE",
                "# Critical temp for Ising",
                "# Throttle %",
                "# Degree of the Potts model",
                "# Number of frames to run",
                "# MonteCarlo updates per frame",
                "# These are just random rn",
                "# Ising starts homogeneous?",
                "# Array dimensions",
                "# Image dim = N*SCALE x N*SCALE",
                "# Critical temp for Ising",
                "# Throttle %",
                "# Degree of the Potts model",
                "# Number of frames to run",
                "# MonteCarlo updates per frame"
            ],
            "<STRING>": [
                "'0xffffffff'",
                "'0xffffffff'",
                "'PRIMARYCOLOR'",
                "'SECONDARYCOLOR'",
                "'ALLUP'",
                "'N'",
                "'SCALE'",
                "'BETA'",
                "'SPEED'",
                "'DEGREE'",
                "'IMAGEUPDATES'",
                "'MONTEUPDATES'",
                "'EQUILIBRATE'",
                "'COVERAGE'",
                "'NEWARR'",
                "'STOCHASTIC'",
                "'PRIMARYCOLOR'",
                "'SECONDARYCOLOR'",
                "'ALLUP'",
                "'N'",
                "'SCALE'",
                "'BETA'",
                "'SPEED'",
                "'DEGREE'",
                "'IMAGEUPDATES'",
                "'MONTEUPDATES'",
                "'EQUILIBRATE'",
                "'COVERAGE'",
                "'NEWARR'",
                "'STOCHASTIC'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0044df64179b4483a37a7e8f46de3365": {
        "code_string": "def all_info(color):\n     \"\"\"Print a collection of summaries about SLURM gpu usage, including: all nodes\n     managed by the cluster, nodes that are currently accesible and gpu usage for each\n     active user.\n     \"\"\"\n     divider, slurm_str = \"---------------------------------\", \"SLURM\"\n     if not color:\n         colors = sns.color_palette(\"hls\", 8).as_hex()\n         divider = colored.stylize(divider, colored.fg(colors[7]))\n         slurm_str = colored.stylize(slurm_str, colored.fg(colors[0]))\n     print(divider)\n     print(f\"Under {slurm_str} management\")\n     print(divider)\n     resources = parse_all_gpus()\n     states = node_states()\n     for mode in (\"up\", \"accessible\"):\n         summary(mode=mode, resources=resources, states=states)\n         print(divider)\n     in_use(resources)\n     print(divider)\n     available(resources=resources, states=states)\n     print(divider)\n",
        "code_toks_joined": "def all_info ( color ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> divider , slurm_str = <STRING> , <STRING> <NEWLINE> if not color : <NEWLINE> <INDENT> colors = sns . color_palette ( <STRING> , 8 ) . as_hex ( ) <NEWLINE> divider = colored . stylize ( divider , colored . fg ( colors [ 7 ] ) ) <NEWLINE> slurm_str = colored . stylize ( slurm_str , colored . fg ( colors [ 0 ] ) ) <NEWLINE> <DEDENT> print ( divider ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( divider ) <NEWLINE> resources = parse_all_gpus ( ) <NEWLINE> states = node_states ( ) <NEWLINE> for mode in ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> summary ( mode = mode , resources = resources , states = states ) <NEWLINE> print ( divider ) <NEWLINE> <DEDENT> in_use ( resources ) <NEWLINE> print ( divider ) <NEWLINE> available ( resources = resources , states = states ) <NEWLINE> print ( divider ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Print a collection of summaries about SLURM gpu usage, including: all nodes\n     managed by the cluster, nodes that are currently accesible and gpu usage for each\n     active user.\n     \"\"\"",
                "\"---------------------------------\"",
                "\"SLURM\"",
                "\"hls\"",
                "f\"Under {slurm_str} management\"",
                "\"up\"",
                "\"accessible\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "47f13e9fe0fc47ada830e46f5775f546": {
        "code_string": "# RPL_ISUPPORT = '005'\n @module.event('005')\n @module.rule('.*')\n def parse_event_005(bot, trigger):\n     if trigger.args[-1] != 'are supported by this server':\n         return\n     parameters = trigger.args[1:-1]\n     for param in parameters:\n         if '=' in param:\n             if not param.startswith(\"TARGMAX\"):\n                 stderr(param)\n                 param = str(param).split('=')[1]\n                 settings = str(param).split(',')\n                 for setting in settings:\n                     if not setting.startswith(tuple([\"NOTICE\", \"PRIVMSG\"])):\n                         pass\n                     setting = str(settings).split(':')[0]\n                     value = str(settings).split(':')[1] or None\n                     if value:\n                         if setting == 'NOTICE':\n                             bot.config.MAXTARGCONFIG.notice = int(value)\n                         elif setting == 'PRIVMSG':\n                             bot.config.MAXTARGCONFIG.privmsg = int(value)\n     stderr(\"privmsg   \" + str(bot.config.MAXTARGCONFIG.privmsg))\n     stderr(\"notice   \" + str(bot.config.MAXTARGCONFIG.notice))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> @ module . event ( <STRING> ) <NEWLINE> @ module . rule ( <STRING> ) <NEWLINE> def parse_event_005 ( bot , trigger ) : <NEWLINE> <INDENT> if trigger . args [ - 1 ] != <STRING> : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> parameters = trigger . args [ 1 : - 1 ] <NEWLINE> for param in parameters : <NEWLINE> <INDENT> if <STRING> in param : <NEWLINE> <INDENT> if not param . startswith ( <STRING> ) : <NEWLINE> <INDENT> stderr ( param ) <NEWLINE> param = str ( param ) . split ( <STRING> ) [ 1 ] <NEWLINE> settings = str ( param ) . split ( <STRING> ) <NEWLINE> for setting in settings : <NEWLINE> <INDENT> if not setting . startswith ( tuple ( [ <STRING> , <STRING> ] ) ) : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> setting = str ( settings ) . split ( <STRING> ) [ 0 ] <NEWLINE> value = str ( settings ) . split ( <STRING> ) [ 1 ] or None <NEWLINE> if value : <NEWLINE> <INDENT> if setting == <STRING> : <NEWLINE> <INDENT> bot . config . MAXTARGCONFIG . notice = int ( value ) <NEWLINE> <DEDENT> elif setting == <STRING> : <NEWLINE> <INDENT> bot . config . MAXTARGCONFIG . privmsg = int ( value ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> stderr ( <STRING> + str ( bot . config . MAXTARGCONFIG . privmsg ) ) <NEWLINE> stderr ( <STRING> + str ( bot . config . MAXTARGCONFIG . notice ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# RPL_ISUPPORT = '005'"
            ],
            "<STRING>": [
                "'005'",
                "'.*'",
                "'are supported by this server'",
                "'='",
                "\"TARGMAX\"",
                "'='",
                "','",
                "\"NOTICE\"",
                "\"PRIVMSG\"",
                "':'",
                "':'",
                "'NOTICE'",
                "'PRIVMSG'",
                "\"privmsg   \"",
                "\"notice   \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "97018fa1fc06430f946c1ed2f9c5d239": {
        "code_string": "grad = np.zeros(data.shape)\n         for i in range(batchsize):\n             for j in range(channel):\n                 for h in range(0, height - kernel_size[0] + 1, stride[0]):\n                     for w in range(0, width - kernel_size[1] + 1, stride[1]):\n                         mask = (data[i, j, h : h + kernel_size[0], w : w + kernel_size[1]] == np.max(data[i, j, h : h + kernel_size[0], w : w + kernel_size[1]]))\n                         grad[i, j, h : h + kernel_size[0], w : w + kernel_size[1]] = mask * grad_output[i, j, h // stride[0], w // stride[1]]\n",
        "code_toks_joined": "grad = np . zeros ( data . shape ) <NEWLINE> <INDENT> for i in range ( batchsize ) : <NEWLINE> <INDENT> for j in range ( channel ) : <NEWLINE> <INDENT> for h in range ( 0 , height - kernel_size [ 0 ] + 1 , stride [ 0 ] ) : <NEWLINE> <INDENT> for w in range ( 0 , width - kernel_size [ 1 ] + 1 , stride [ 1 ] ) : <NEWLINE> <INDENT> mask = ( data [ i , j , h : h + kernel_size [ 0 ] , w : w + kernel_size [ 1 ] ] == np . max ( data [ i , j , h : h + kernel_size [ 0 ] , w : w + kernel_size [ 1 ] ] ) ) <NEWLINE> grad [ i , j , h : h + kernel_size [ 0 ] , w : w + kernel_size [ 1 ] ] = mask * grad_output [ i , j , h // stride [ 0 ] , w // stride [ 1 ] ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "515059ecb07945278e0c1e0b03c76105": {
        "code_string": "@staticmethod\n     def backward(ctx, grad_output):\n         original_shape = ctx.saved_tensors()[0]\n         grad = grad_output.reshape(grad_output)\n         return grad\n",
        "code_toks_joined": "@ staticmethod <NEWLINE> <INDENT> def backward ( ctx , grad_output ) : <NEWLINE> <INDENT> original_shape = ctx . saved_tensors ( ) [ 0 ] <NEWLINE> grad = grad_output . reshape ( grad_output ) <NEWLINE> return grad <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e86c50739da243c3b3ae554ef24e769f": {
        "code_string": "if task_mod.startswith(f\"{carnival_tasks_module}\"):\n         task_full_name = task_full_name[len(carnival_tasks_module) + 1:]\n",
        "code_toks_joined": "if task_mod . startswith ( <STRING> ) : <NEWLINE> <INDENT> task_full_name = task_full_name [ len ( carnival_tasks_module ) + 1 : ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"{carnival_tasks_module}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b67d85f632b4ff38b650bf2c0601782": {
        "code_string": "paths = glob.glob(os.path.join(save_dir, '**', '*.t*'), recursive=True)\n     t0 = datetime.now()\n     for idx, path in enumerate(paths):\n         if os.path.basename(os.path.dirname(path)) == 'stream':\n             continue\n         try:\n             for lidx, line in enumerate(utils.readlines(path)):\n                 try:\n                     data = json.loads(line)\n                 except Exception:\n                     logger.exception(\n                         f'Backfill: Unable to read line {path}:{lidx + 1}'\n                     )\n                     continue\n                 else:\n                     if data.get('delete'):\n                         continue\n                     if os.path.basename(os.path.dirname(path)) == 'stream':\n                         data = utils.stream_to_search(data)\n                     data = utils.timestamp_to_datetime(data)\n                     tweets.replace_one({'id': data['id']}, data, upsert=True)\n             t_delta = datetime.now() - t0\n             average = t_delta / (idx + 1)\n             remaining = str((len(paths) - (idx + 1)) * average).split('.')[0]\n",
        "code_toks_joined": "paths = glob . glob ( os . path . join ( save_dir , <STRING> , <STRING> ) , recursive = True ) <NEWLINE> <INDENT> t0 = datetime . now ( ) <NEWLINE> for idx , path in enumerate ( paths ) : <NEWLINE> <INDENT> if os . path . basename ( os . path . dirname ( path ) ) == <STRING> : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> for lidx , line in enumerate ( utils . readlines ( path ) ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> data = json . loads ( line ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> logger . exception ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> continue <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if data . get ( <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if os . path . basename ( os . path . dirname ( path ) ) == <STRING> : <NEWLINE> <INDENT> data = utils . stream_to_search ( data ) <NEWLINE> <DEDENT> data = utils . timestamp_to_datetime ( data ) <NEWLINE> tweets . replace_one ( { <STRING> : data [ <STRING> ] } , data , upsert = True ) <NEWLINE> <DEDENT> <DEDENT> t_delta = datetime . now ( ) - t0 <NEWLINE> average = t_delta / ( idx + 1 ) <NEWLINE> remaining = str ( ( len ( paths ) - ( idx + 1 ) ) * average ) . split ( <STRING> ) [ 0 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'**'",
                "'*.t*'",
                "'stream'",
                "f'Backfill: Unable to read line {path}:{lidx + 1}'",
                "'delete'",
                "'stream'",
                "'id'",
                "'id'",
                "'.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b710a3218cb4dddbf5a56e99c2ecc66": {
        "code_string": "# get check constraints\n         sql = \"\"\"\n         SELECT kc.constraint_name, kc.column_name\n         FROM information_schema.constraint_column_usage AS kc\n         JOIN information_schema.table_constraints AS c ON\n             kc.table_schema = c.table_schema AND\n             kc.table_name = c.table_name AND\n             kc.constraint_name = c.constraint_name\n         WHERE\n             c.constraint_type = 'CHECK' \n             AND\n             kc.table_name = %s\n         \"\"\"\n         cursor.execute(sql,[table_name])\n         for constraint, column in list(cursor.fetchall()):\n             if column not in constraint:\n                 constraints[constraint] = {\n                     \"columns\": [],\n                     \"primary_key\": False,\n                     \"unique\": False,\n                     \"index\": False,\n                     \"check\": True,\n                     \"foreign_key\": None,\n                 }\n             constraints[constraint]['columns'].append(column)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> sql = <STRING> <NEWLINE> cursor . execute ( sql , [ table_name ] ) <NEWLINE> for constraint , column in list ( cursor . fetchall ( ) ) : <NEWLINE> <INDENT> if column not in constraint : <NEWLINE> <INDENT> constraints [ constraint ] = { <NEWLINE> <INDENT> <STRING> : [ ] , <NEWLINE> <STRING> : False , <NEWLINE> <STRING> : False , <NEWLINE> <STRING> : False , <NEWLINE> <STRING> : True , <NEWLINE> <STRING> : None , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> constraints [ constraint ] [ <STRING> ] . append ( column ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# get check constraints"
            ],
            "<STRING>": [
                "\"\"\"\n         SELECT kc.constraint_name, kc.column_name\n         FROM information_schema.constraint_column_usage AS kc\n         JOIN information_schema.table_constraints AS c ON\n             kc.table_schema = c.table_schema AND\n             kc.table_name = c.table_name AND\n             kc.constraint_name = c.constraint_name\n         WHERE\n             c.constraint_type = 'CHECK' \n             AND\n             kc.table_name = %s\n         \"\"\"",
                "\"columns\"",
                "\"primary_key\"",
                "\"unique\"",
                "\"index\"",
                "\"check\"",
                "\"foreign_key\"",
                "'columns'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f050fc48d514457ba88ab6678d78b431": {
        "code_string": "if create_image(absolute_path, resized_absolute_path, **kwargs):\n         return send_from_directory(MEDIA_ROOT, resized_absolute_path)\n     abort(500)\n",
        "code_toks_joined": "if create_image ( absolute_path , resized_absolute_path , ** kwargs ) : <NEWLINE> <INDENT> return send_from_directory ( MEDIA_ROOT , resized_absolute_path ) <NEWLINE> abort ( 500 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9850fdca01b64068be937a04b779733d": {
        "code_string": "@staticmethod\n     def insertShocks(flatData, i, nshk, nexo, shockList, shockPtr, shockVal):\n         if nshk > 0:\n             start = shockPtr.array[i, 0] - 1\n             for j in range(0, nshk):\n                 shkInd = start + j\n                 if nshk == nexo:\n                     varInd=j\n                 else:\n                     varInd = shockList.array[j, 0] - 1\n                 flatData[varInd] = shockVal.array[shkInd, 0]\n",
        "code_toks_joined": "@ staticmethod <NEWLINE> <INDENT> def insertShocks ( flatData , i , nshk , nexo , shockList , shockPtr , shockVal ) : <NEWLINE> <INDENT> if nshk > 0 : <NEWLINE> <INDENT> start = shockPtr . array [ i , 0 ] - 1 <NEWLINE> for j in range ( 0 , nshk ) : <NEWLINE> <INDENT> shkInd = start + j <NEWLINE> if nshk == nexo : <NEWLINE> <INDENT> varInd = j <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> varInd = shockList . array [ j , 0 ] - 1 <NEWLINE> <DEDENT> flatData [ varInd ] = shockVal . array [ shkInd , 0 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ea57aa2cdc0246228ee56c3358728d66": {
        "code_string": "def validate_categorical(value, values, **kwargs):\n     test = True\n     if value not in value:\n         test = False\n     return test\n",
        "code_toks_joined": "def validate_categorical ( value , values , ** kwargs ) : <NEWLINE> <INDENT> test = True <NEWLINE> if value not in value : <NEWLINE> <INDENT> test = False <NEWLINE> <DEDENT> return test <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0fc4b09a237947558db68591f46da8d7": {
        "code_string": "if \"high\" in search_space.keys() and \"low\" in search_space.keys():\n         if search_space[\"high\"] >= search_space[\"low\"]:\n             raise ValueError(\"low <= high\")\n",
        "code_toks_joined": "if <STRING> in search_space . keys ( ) and <STRING> in search_space . keys ( ) : <NEWLINE> <INDENT> if search_space [ <STRING> ] >= search_space [ <STRING> ] : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"high\"",
                "\"low\"",
                "\"high\"",
                "\"low\"",
                "\"low <= high\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f0fa17a02b1d436dbc0d2c2408787541": {
        "code_string": "def remoteSliderUpdate(self, widget, value, sliderMoved=True):\n         \"\"\"\n         Gets called when a user interacts with the slider\n         :param widget: the widget the user interacted with\n         :param value: the actual value of the widget\n         :param sliderMoved: False if gets called from another widget\n         \"\"\"\n         if not sliderMoved:\n             for wid in self.remoteWidgetLayout.list:\n                 if isinstance(wid, MovableSlider):\n                     if wid.module == widget.module and wid.parameter == widget.parameter:\n                         wid.setValue(float(value))\n                         wid.valueOn = value\n                         wid.label.setText(wid.widgetName + ': ' + \"{:.3f}\".format(widget.value))\n         else:\n             widget.valueOn = value\n             widget.label.setText(widget.widgetName + ': ' + \"{:.3f}\".format(widget.value))\n",
        "code_toks_joined": "def remoteSliderUpdate ( self , widget , value , sliderMoved = True ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not sliderMoved : <NEWLINE> <INDENT> for wid in self . remoteWidgetLayout . list : <NEWLINE> <INDENT> if isinstance ( wid , MovableSlider ) : <NEWLINE> <INDENT> if wid . module == widget . module and wid . parameter == widget . parameter : <NEWLINE> <INDENT> wid . setValue ( float ( value ) ) <NEWLINE> wid . valueOn = value <NEWLINE> wid . label . setText ( wid . widgetName + <STRING> + <STRING> . format ( widget . value ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> widget . valueOn = value <NEWLINE> widget . label . setText ( widget . widgetName + <STRING> + <STRING> . format ( widget . value ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Gets called when a user interacts with the slider\n         :param widget: the widget the user interacted with\n         :param value: the actual value of the widget\n         :param sliderMoved: False if gets called from another widget\n         \"\"\"",
                "': '",
                "\"{:.3f}\"",
                "': '",
                "\"{:.3f}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5a106697dfc94d7599eca3b06a375859": {
        "code_string": "print >> out, '      <%s:%s>%s</%s:%s>'%(prefix, k, serialize(v), sid, k)\n",
        "code_toks_joined": "print >> out , <STRING> % ( prefix , k , serialize ( v ) , sid , k ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'      <%s:%s>%s</%s:%s>'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ff811fa9710643faa33c3711a785a960": {
        "code_string": "for filename in ['ansible.cfg', 'ansible-requirements.txt',\n                              'requirements.yml']:\n                 file_path = os.path.join(source_dir, filename)\n                 if os.path.exists(filename):\n                     tarball.add(file_path,\n                                 arcname=os.path.join('build-src', filename))\n             # Make an empty file just to make sure the build-src dir has something\n             open(os.path.join(temp_dir, '.touch'), 'w')\n             tarball.add(os.path.join(temp_dir, '.touch'), arcname='build-src/.touch')\n",
        "code_toks_joined": "for filename in [ <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> ] : <NEWLINE> file_path = os . path . join ( source_dir , filename ) <NEWLINE> if os . path . exists ( filename ) : <NEWLINE> tarball . add ( file_path , <NEWLINE> <INDENT> arcname = os . path . join ( <STRING> , filename ) ) <NEWLINE> <COMMENT> <NL> open ( os . path . join ( temp_dir , <STRING> ) , <STRING> ) <NEWLINE> tarball . add ( os . path . join ( temp_dir , <STRING> ) , arcname = <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ansible.cfg'",
                "'ansible-requirements.txt'",
                "'requirements.yml'",
                "'build-src'",
                "'.touch'",
                "'w'",
                "'.touch'",
                "'build-src/.touch'"
            ],
            "<COMMENT>": [
                "# Make an empty file just to make sure the build-src dir has something"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d2c6d31409e940109c6af0b1d9a6aa5c": {
        "code_string": "prebaked = base_image in reduce(lambda x, y: x + [y[0]] + y[1],\n                                         PREBAKED_DISTROS.items(), [])\n         if prebaked:\n             base_image = [k for k, v in PREBAKED_DISTROS.items()\n                               if base_image in [k] + v][0]\n             conductor_base = 'container-conductor-%s:%s' % (\n                 base_image.replace(':', '-'),\n                 container.__version__\n             )\n             if not self.get_image_id_by_tag(conductor_base):\n                 conductor_base = 'ansible/%s' % base_image\n         else:\n             conductor_base = 'container-conductor-%s:%s' % (\n                 base_image.replace(':', '-'),\n                 container.__version__\n             )\n",
        "code_toks_joined": "prebaked = base_image in reduce ( lambda x , y : x + [ y [ 0 ] ] + y [ 1 ] , <NEWLINE> <INDENT> PREBAKED_DISTROS . items ( ) , [ ] ) <NEWLINE> if prebaked : <NEWLINE> base_image = [ k for k , v in PREBAKED_DISTROS . items ( ) <NEWLINE> if base_image in [ k ] + v ] [ 0 ] <NEWLINE> conductor_base = <STRING> % ( <NEWLINE> base_image . replace ( <STRING> , <STRING> ) , <NEWLINE> container . __version__ <NEWLINE> ) <NEWLINE> if not self . get_image_id_by_tag ( conductor_base ) : <NEWLINE> conductor_base = <STRING> % base_image <NEWLINE> else : <NEWLINE> conductor_base = <STRING> % ( <NEWLINE> base_image . replace ( <STRING> , <STRING> ) , <NEWLINE> container . __version__ <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'container-conductor-%s:%s'",
                "':'",
                "'-'",
                "'ansible/%s'",
                "'container-conductor-%s:%s'",
                "':'",
                "'-'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "555c9fd2c594413a84b2b5b7106b4e52": {
        "code_string": "# update other layer\n         target_layer.update(update_params=payload, token=token)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> target_layer . update ( update_params = payload , token = token ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# update other layer"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1404678aa9cd4ad8a0cc19f352e56fad": {
        "code_string": "start = time.time()\n         if self._n_jobs == 1:\n             pool = multiprocess.Pool(self._n_jobs)\n         else:\n             pool = MockPool()\n",
        "code_toks_joined": "start = time . time ( ) <NEWLINE> <INDENT> if self . _n_jobs == 1 : <NEWLINE> <INDENT> pool = multiprocess . Pool ( self . _n_jobs ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pool = MockPool ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b3b4e6897866473a9baa0eb9e11df4c2": {
        "code_string": "heap = []\n     j = 0\n     for r, d in zip(rand, data):\n         if len(r[0]) == 0: continue\n         a = 1.0 * len(r[0]) / nbar\n         j = j + 1\n         if len(heap) == 0:\n             heapq.heappush(heap, (a, j, r, d))\n         else:\n             a0, j0, r0, d0 = heapq.heappop(heap)\n             if a0 + a < Abar:\n                 a0 += a\n                 d0 = [\n                      numpy.concatenate((d0[i], d[i]), axis=-1)\n                      for i in range(len(d))\n                     ]\n                 r0 = numpy.concatenate((r0, r), axis=-1)\n             else:\n                 heapq.heappush(heap, (a, j, r, d))\n             heapq.heappush(heap, (a0, j, r0, d0))\n",
        "code_toks_joined": "heap = [ ] <NEWLINE> <INDENT> j = 0 <NEWLINE> for r , d in zip ( rand , data ) : <NEWLINE> <INDENT> if len ( r [ 0 ] ) == 0 : continue <NEWLINE> a = 1.0 * len ( r [ 0 ] ) / nbar <NEWLINE> j = j + 1 <NEWLINE> if len ( heap ) == 0 : <NEWLINE> <INDENT> heapq . heappush ( heap , ( a , j , r , d ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> a0 , j0 , r0 , d0 = heapq . heappop ( heap ) <NEWLINE> if a0 + a < Abar : <NEWLINE> <INDENT> a0 += a <NEWLINE> d0 = [ <NEWLINE> <INDENT> numpy . concatenate ( ( d0 [ i ] , d [ i ] ) , axis = - 1 ) <NEWLINE> for i in range ( len ( d ) ) <NEWLINE> ] <NEWLINE> <DEDENT> r0 = numpy . concatenate ( ( r0 , r ) , axis = - 1 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> heapq . heappush ( heap , ( a , j , r , d ) ) <NEWLINE> <DEDENT> heapq . heappush ( heap , ( a0 , j , r0 , d0 ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e432be6ff6594cdba315f9ad11335564": {
        "code_string": "def login(self, username, password, mode=\"demo\"):\n         '''Login function'''\n         url = \"https://trading212.com/it/login\"\n         try:\n             self.browser.visit(url)\n             self.logger.debug(\"visiting {url}\".format(url=url))\n         except selenium.common.exceptions.WebDriverException:\n             self.logger.critical(\"connection timed out\")\n             return 0\n         try:\n             self._name(\"login[username]\").fill(username)\n             self._name(\"login[password]\").fill(password)\n             self._css(path['log']).click()\n             timeout = 30\n             while not self._elCss(path['logo']):\n                 timeout -= 1\n                 if timeout == 0:\n                     self.logger.critical(\"login failed\")\n                     return 0\n             sleep(1)\n             self.logger.debug(\"logged in\")\n             if mode == \"demo\" and self._elCss(path['alert-box']):\n                 self._css(path['alert-box']).click()\n             return 0\n         except Exception:\n             self.logger.critical(\"login failed\")\n             return 0\n",
        "code_toks_joined": "def login ( self , username , password , mode = <STRING> ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> url = <STRING> <NEWLINE> try : <NEWLINE> <INDENT> self . browser . visit ( url ) <NEWLINE> self . logger . debug ( <STRING> . format ( url = url ) ) <NEWLINE> <DEDENT> except selenium . common . exceptions . WebDriverException : <NEWLINE> <INDENT> self . logger . critical ( <STRING> ) <NEWLINE> return 0 <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> self . _name ( <STRING> ) . fill ( username ) <NEWLINE> self . _name ( <STRING> ) . fill ( password ) <NEWLINE> self . _css ( path [ <STRING> ] ) . click ( ) <NEWLINE> timeout = 30 <NEWLINE> while not self . _elCss ( path [ <STRING> ] ) : <NEWLINE> <INDENT> timeout -= 1 <NEWLINE> if timeout == 0 : <NEWLINE> <INDENT> self . logger . critical ( <STRING> ) <NEWLINE> return 0 <NEWLINE> <DEDENT> <DEDENT> sleep ( 1 ) <NEWLINE> self . logger . debug ( <STRING> ) <NEWLINE> if mode == <STRING> and self . _elCss ( path [ <STRING> ] ) : <NEWLINE> <INDENT> self . _css ( path [ <STRING> ] ) . click ( ) <NEWLINE> <DEDENT> return 0 <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> self . logger . critical ( <STRING> ) <NEWLINE> return 0 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"demo\"",
                "'''Login function'''",
                "\"https://trading212.com/it/login\"",
                "\"visiting {url}\"",
                "\"connection timed out\"",
                "\"login[username]\"",
                "\"login[password]\"",
                "'log'",
                "'logo'",
                "\"login failed\"",
                "\"logged in\"",
                "\"demo\"",
                "'alert-box'",
                "'alert-box'",
                "\"login failed\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "77eca73dfe534369a8d826ba1a38fb5a": {
        "code_string": "# Check the union registry first.\n         handler = self._union_registry.get(union)\n         if handler is not None:\n             return handler(union, obj)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> handler = self . _union_registry . get ( union ) <NEWLINE> if handler is not None : <NEWLINE> <INDENT> return handler ( union , obj ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check the union registry first."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "822439dbfc0b4299a485df40cf2a93ec": {
        "code_string": "s3_source = f's3://{bucket}/{key}'\n         quote_option = f\"csv quote as '{quote_character}'\" if delimiter == ',' else ''\n         region_option = f\"region '{region}'\" if region is not None else ''\n         escape_option = 'escape' if escape else ''\n         acceptinvchars_option = 'acceptinvchars' if acceptinvchars else ''\n         null_option = f\"null as '{null}'\" if null is not None else ''\n         aws_token = self.s3_config.get(\"aws_session_token\")\n         aws_token_option = f\"session_token '{aws_token}'\" if aws_token is not None else ''\n         copy_template = f\"\"\"\\\n         copy {table_name}\n         from '{s3_source}' \n         delimiter '{delimiter}'\n         {quote_option}\n         {escape_option}\n         {acceptinvchars_option}\n         {null_option}\n         ignoreheader {ignoreheader}\n         dateformat '{dateformat}'\n         timeformat '{dateformat}'\n         access_key_id '{self.s3_config.get(\"aws_access_key_id\")}'\n         secret_access_key '{self.s3_config.get(\"aws_secret_access_key\")}'\n         {aws_token_option}\n         {region_option}\n         \"\"\"\n         self.run_query(copy_template)\n",
        "code_toks_joined": "s3_source = <STRING> <NEWLINE> <INDENT> quote_option = <STRING> if delimiter == <STRING> else <STRING> <NEWLINE> region_option = <STRING> if region is not None else <STRING> <NEWLINE> escape_option = <STRING> if escape else <STRING> <NEWLINE> acceptinvchars_option = <STRING> if acceptinvchars else <STRING> <NEWLINE> null_option = <STRING> if null is not None else <STRING> <NEWLINE> aws_token = self . s3_config . get ( <STRING> ) <NEWLINE> aws_token_option = <STRING> if aws_token is not None else <STRING> <NEWLINE> copy_template = <STRING> <NEWLINE> self . run_query ( copy_template ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f's3://{bucket}/{key}'",
                "f\"csv quote as '{quote_character}'\"",
                "','",
                "''",
                "f\"region '{region}'\"",
                "''",
                "'escape'",
                "''",
                "'acceptinvchars'",
                "''",
                "f\"null as '{null}'\"",
                "''",
                "\"aws_session_token\"",
                "f\"session_token '{aws_token}'\"",
                "''",
                "f\"\"\"\\\n         copy {table_name}\n         from '{s3_source}' \n         delimiter '{delimiter}'\n         {quote_option}\n         {escape_option}\n         {acceptinvchars_option}\n         {null_option}\n         ignoreheader {ignoreheader}\n         dateformat '{dateformat}'\n         timeformat '{dateformat}'\n         access_key_id '{self.s3_config.get(\"aws_access_key_id\")}'\n         secret_access_key '{self.s3_config.get(\"aws_secret_access_key\")}'\n         {aws_token_option}\n         {region_option}\n         \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a3c90b21814f40f08dcc63ab09e357f8": {
        "code_string": "# Check that any part of the gaussian is in-bounds\n     ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n     br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n     if (ul[0] > img.shape[1] or ul[1] >= img.shape[0] or\n             br[0] < 0 or br[1] < 0):\n         # If not, just return the image as is\n         return to_torch(img)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> ul = [ int ( pt [ 0 ] - 3 * sigma ) , int ( pt [ 1 ] - 3 * sigma ) ] <NEWLINE> br = [ int ( pt [ 0 ] + 3 * sigma + 1 ) , int ( pt [ 1 ] + 3 * sigma + 1 ) ] <NEWLINE> if ( ul [ 0 ] > img . shape [ 1 ] or ul [ 1 ] >= img . shape [ 0 ] or <NEWLINE> <INDENT> br [ 0 ] < 0 or br [ 1 ] < 0 ) : <NEWLINE> <COMMENT> <NL> return to_torch ( img ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check that any part of the gaussian is in-bounds",
                "# If not, just return the image as is"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2033b975f8fe42df9da54ed667a4acb1": {
        "code_string": "def pretty_xml(path, outpath=None, encoding=b'utf-8'):\n     tree = etree.parse(path)\n     if outpath is None:\n         opener = compression.get(path)\n         outpath = opener(path, 'wb')\n     if hasattr(outpath, 'write'):\n         outstream = outpath\n     else:\n         opener = compression.get(outpath)\n         outstream = opener(outpath, 'wb')\n     with outstream:\n         outstream.write(b'<?xml version=\"1.0\" encoding=\"' + encoding + b'\"?>\\n')\n         outstream.write(\n             etree.tostring(tree, pretty_print=True))\n",
        "code_toks_joined": "def pretty_xml ( path , outpath = None , encoding = <STRING> ) : <NEWLINE> <INDENT> tree = etree . parse ( path ) <NEWLINE> if outpath is None : <NEWLINE> <INDENT> opener = compression . get ( path ) <NEWLINE> outpath = opener ( path , <STRING> ) <NEWLINE> <DEDENT> if hasattr ( outpath , <STRING> ) : <NEWLINE> <INDENT> outstream = outpath <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> opener = compression . get ( outpath ) <NEWLINE> outstream = opener ( outpath , <STRING> ) <NEWLINE> <DEDENT> with outstream : <NEWLINE> <INDENT> outstream . write ( <STRING> + encoding + <STRING> ) <NEWLINE> outstream . write ( <NEWLINE> <INDENT> etree . tostring ( tree , pretty_print = True ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "b'utf-8'",
                "'wb'",
                "'write'",
                "'wb'",
                "b'<?xml version=\"1.0\" encoding=\"'",
                "b'\"?>\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7f3f9399e9ac4f978e9e5ccab55a11e8": {
        "code_string": "def differ(a, b):\n     if issubclass(type(a), type(b)):\n         return False\n     if isinstance(a, dict):\n         return dict_diff(a, b)\n     elif isinstance(a, (list, tuple)):\n         return all(differ(ai, bi) for ai, bi in zip(sorted(a), sorted(b)))\n     elif isinstance(a, float):\n         return abs(a - b) < 1e-3\n     elif isinstance(a, cvstr):\n         return a.accession.lower() == b.accession.lower()\n     elif isinstance(a, np.ndarray):\n         return np.allclose(a, b)\n     else:\n         return a == b\n",
        "code_toks_joined": "def differ ( a , b ) : <NEWLINE> <INDENT> if issubclass ( type ( a ) , type ( b ) ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> if isinstance ( a , dict ) : <NEWLINE> <INDENT> return dict_diff ( a , b ) <NEWLINE> <DEDENT> elif isinstance ( a , ( list , tuple ) ) : <NEWLINE> <INDENT> return all ( differ ( ai , bi ) for ai , bi in zip ( sorted ( a ) , sorted ( b ) ) ) <NEWLINE> <DEDENT> elif isinstance ( a , float ) : <NEWLINE> <INDENT> return abs ( a - b ) < 1e-3 <NEWLINE> <DEDENT> elif isinstance ( a , cvstr ) : <NEWLINE> <INDENT> return a . accession . lower ( ) == b . accession . lower ( ) <NEWLINE> <DEDENT> elif isinstance ( a , np . ndarray ) : <NEWLINE> <INDENT> return np . allclose ( a , b ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return a == b <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a58f15f799cd4f47acba6e05e1f48be6": {
        "code_string": "options = self.prepare_request(resource, url, *args)\n         raw = self.patchboard.session.request(\n             self.method,\n             url,\n             args\n         )\n         response = Response(raw)\n         if response.status != self.success_status:\n             err_msg = (\"Unexpected response status: \" + response.status +\n                        \" - \" + response.body)\n             raise PatchboardError(err_msg)\n",
        "code_toks_joined": "options = self . prepare_request ( resource , url , * args ) <NEWLINE> <INDENT> raw = self . patchboard . session . request ( <NEWLINE> <INDENT> self . method , <NEWLINE> url , <NEWLINE> args <NEWLINE> <DEDENT> ) <NEWLINE> response = Response ( raw ) <NEWLINE> if response . status != self . success_status : <NEWLINE> <INDENT> err_msg = ( <STRING> + response . status + <NEWLINE> <INDENT> <STRING> + response . body ) <NEWLINE> <DEDENT> raise PatchboardError ( err_msg ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Unexpected response status: \"",
                "\" - \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8f6b87c9dc0a46619f61b703756e4a8f": {
        "code_string": "return res\n",
        "code_toks_joined": "return res <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5035eb0fe0ea4839926ae63378a575a4": {
        "code_string": "if self._loader_lock is not None and len(self._loaders) == 0:\n             self._loader_lock = threading.Lock()\n             self._loaders = []\n",
        "code_toks_joined": "if self . _loader_lock is not None and len ( self . _loaders ) == 0 : <NEWLINE> <INDENT> self . _loader_lock = threading . Lock ( ) <NEWLINE> self . _loaders = [ ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2f81e2df14b24779a4b88916dc52e496": {
        "code_string": "if os_lib.path.exists(comp_root_folder):\n             os_lib.mkdir(comp_root_folder)\n",
        "code_toks_joined": "if os_lib . path . exists ( comp_root_folder ) : <NEWLINE> <INDENT> os_lib . mkdir ( comp_root_folder ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5a856d57d2b544b5b3cf293673b3087e": {
        "code_string": "# A single numerical\n         data = {'numerical_vars': ['n1']}\n         input_mapper, output_mapper = Automater()._create_mappers(data)\n         self.assertEqual(1, len(output_mapper.features))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> data = { <STRING> : [ <STRING> ] } <NEWLINE> input_mapper , output_mapper = Automater ( ) . _create_mappers ( data ) <NEWLINE> self . assertEqual ( 1 , len ( output_mapper . features ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# A single numerical"
            ],
            "<STRING>": [
                "'numerical_vars'",
                "'n1'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "23bbb32320c742349922268f087043e9": {
        "code_string": "def __str__(self):\n         try:\n             return vstr(self)\n         except ImportError:\n             return super(self, Document).__str__()\n",
        "code_toks_joined": "def __str__ ( self ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> return vstr ( self ) <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> return super ( self , Document ) . __str__ ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "47f4922481b7426db576c09137bd7bd5": {
        "code_string": "def cleardata(self,name,startdt,enddt):\n         '''\n         Clear data values for a particular measurement\n         Loop through values and remove. Note: This is probably\n         inefficient if the data structure is large.\n         '''\n         savedata = []\n         data = self.data[name]\n         for val in data:\n             if (val[0]<startdt) and (val[0]>=enddt):\n                 savedata.append(val)\n",
        "code_toks_joined": "def cleardata ( self , name , startdt , enddt ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> savedata = [ ] <NEWLINE> data = self . data [ name ] <NEWLINE> for val in data : <NEWLINE> <INDENT> if ( val [ 0 ] < startdt ) and ( val [ 0 ] >= enddt ) : <NEWLINE> <INDENT> savedata . append ( val ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'''\n         Clear data values for a particular measurement\n         Loop through values and remove. Note: This is probably\n         inefficient if the data structure is large.\n         '''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "733e38e71a134fd0b15ecf6f6d551fe7": {
        "code_string": "if uuid in self._graph.keys() and self._graph[uuid].owner == self._id:\n             for node in self._node_deps(node=uuid):\n                     self.change_owner(uuid=node,new_owner=new_owner)\n",
        "code_toks_joined": "if uuid in self . _graph . keys ( ) and self . _graph [ uuid ] . owner == self . _id : <NEWLINE> <INDENT> for node in self . _node_deps ( node = uuid ) : <NEWLINE> <INDENT> self . change_owner ( uuid = node , new_owner = new_owner ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4c06bd40c7fd4baa85f1bd653ef16577": {
        "code_string": "def __init__(self, server, persistence_config=None, ember_config=None,\n                  node_id=None, storage_nw_ip=None, **kwargs):\n         # When running as Node only we have to initialize cinderlib telling it\n         # not to fail when there's no backend configured.\n         if persistence_config:\n             cinderlib_extra_config = ember_config.copy()\n             cinderlib_extra_config.pop('disabled')\n             ember_config['fail_on_missing_backend'] = False\n             cinderlib.setup(persistence_config=persistence_config,\n                             **cinderlib_extra_config)\n             IdentityBase.__init__(self, server, ember_config)\n",
        "code_toks_joined": "def __init__ ( self , server , persistence_config = None , ember_config = None , <NEWLINE> <INDENT> node_id = None , storage_nw_ip = None , ** kwargs ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if persistence_config : <NEWLINE> cinderlib_extra_config = ember_config . copy ( ) <NEWLINE> cinderlib_extra_config . pop ( <STRING> ) <NEWLINE> ember_config [ <STRING> ] = False <NEWLINE> cinderlib . setup ( persistence_config = persistence_config , <NEWLINE> <INDENT> ** cinderlib_extra_config ) <NEWLINE> IdentityBase . __init__ ( self , server , ember_config ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# When running as Node only we have to initialize cinderlib telling it",
                "# not to fail when there's no backend configured."
            ],
            "<STRING>": [
                "'disabled'",
                "'fail_on_missing_backend'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af1e4f74148b40d3b1dbbf544e66ac43": {
        "code_string": "def __init__(self, server, persistence_config, backend_config,\n                  ember_config=None, **kwargs):\n         cinderlib_extra_config = ember_config.copy()\n         cinderlib_extra_config.pop('disabled')\n         cinderlib.setup(persistence_config=persistence_config,\n                         **ember_config)\n         self.backend = cinderlib.Backend(**backend_config)\n         IdentityBase.__init__(self, server, ember_config)\n         self.CSI.add_ControllerServicer_to_server(self, server)\n",
        "code_toks_joined": "def __init__ ( self , server , persistence_config , backend_config , <NEWLINE> <INDENT> ember_config = None , ** kwargs ) : <NEWLINE> cinderlib_extra_config = ember_config . copy ( ) <NEWLINE> cinderlib_extra_config . pop ( <STRING> ) <NEWLINE> cinderlib . setup ( persistence_config = persistence_config , <NEWLINE> <INDENT> ** ember_config ) <NEWLINE> self . backend = cinderlib . Backend ( ** backend_config ) <NEWLINE> IdentityBase . __init__ ( self , server , ember_config ) <NEWLINE> self . CSI . add_ControllerServicer_to_server ( self , server ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'disabled'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "611470b582314fc885fae75782f55ab8": {
        "code_string": "if result[\"payload\"][\"success\"]:\n             if result['payload']['returned'] == \"\" and not silent:\n                 self.send_update(\"execute_result\", {\n                     'execution_count': self.execution_count,\n                     'data': {\n                         'text/plain': result['payload']['returned']\n                     },\n                     'metadata': {}\n                 })\n",
        "code_toks_joined": "if result [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> if result [ <STRING> ] [ <STRING> ] == <STRING> and not silent : <NEWLINE> <INDENT> self . send_update ( <STRING> , { <NEWLINE> <INDENT> <STRING> : self . execution_count , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : result [ <STRING> ] [ <STRING> ] <NEWLINE> <DEDENT> } , <NEWLINE> <STRING> : { } <NEWLINE> <DEDENT> } ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"payload\"",
                "\"success\"",
                "'payload'",
                "'returned'",
                "\"\"",
                "\"execute_result\"",
                "'execution_count'",
                "'data'",
                "'text/plain'",
                "'payload'",
                "'returned'",
                "'metadata'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b968024bccec4e1ba878828c3ba01c8b": {
        "code_string": "def writeSequence(self, data):\n         for chunk in data:\n             self.write(data)\n",
        "code_toks_joined": "def writeSequence ( self , data ) : <NEWLINE> <INDENT> for chunk in data : <NEWLINE> <INDENT> self . write ( data ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ab0570b5f3544f7b9486606ad765cee9": {
        "code_string": "def appendVertex(vertices, theta, z, dx=dxabs, dy=dyabs, norm=[]):\n             c = _Vector([0,0,0])\n             x = dx*(((self.zMax - z)/self.zMax)*_np.cos(theta)) #generate points on an ellipse\n             y = dy*(((self.zMax - z)/self.zMax)*_np.sin(theta))\n             d = _Vector(\n                 x,\n                 y,\n                 z)\n             if not norm:\n                 n = d\n             else:\n                 n = _Vector(norm)\n             vertices.append(_Vertex(c.plus(d), d))\n",
        "code_toks_joined": "def appendVertex ( vertices , theta , z , dx = dxabs , dy = dyabs , norm = [ ] ) : <NEWLINE> <INDENT> c = _Vector ( [ 0 , 0 , 0 ] ) <NEWLINE> x = dx * ( ( ( self . zMax - z ) / self . zMax ) * _np . cos ( theta ) ) <COMMENT> <NEWLINE> y = dy * ( ( ( self . zMax - z ) / self . zMax ) * _np . sin ( theta ) ) <NEWLINE> d = _Vector ( <NEWLINE> <INDENT> x , <NEWLINE> y , <NEWLINE> z ) <NEWLINE> <DEDENT> if not norm : <NEWLINE> <INDENT> n = d <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> n = _Vector ( norm ) <NEWLINE> <DEDENT> vertices . append ( _Vertex ( c . plus ( d ) , d ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#generate points on an ellipse"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "72bd881cdf174a2fa430eb1ac65af220": {
        "code_string": "if registry:\n             registry.addSolid(self)\n",
        "code_toks_joined": "if registry : <NEWLINE> <INDENT> registry . addSolid ( self ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fa3b48bfca144425b40e0ceee646e982": {
        "code_string": "# solids\n     ws = _g4.solid.Box(\"ws\",wx,wy,wz, reg, \"mm\")\n     ps = _g4.solid.GenericPolyhedra(\"ps\",psphi,pdphi,pnsid,pz,pr,reg,\"mm\",\"rad\")\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> ws = _g4 . solid . Box ( <STRING> , wx , wy , wz , reg , <STRING> ) <NEWLINE> ps = _g4 . solid . GenericPolyhedra ( <STRING> , psphi , pdphi , pnsid , pz , pr , reg , <STRING> , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# solids"
            ],
            "<STRING>": [
                "\"ws\"",
                "\"mm\"",
                "\"ps\"",
                "\"mm\"",
                "\"rad\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f542d23394554afca33d33be833d1cc4": {
        "code_string": "# Reordering columns\n         cols = []\n         if 'timestamp' in df.columns:\n             cols += ['timestamp']\n         cols += [col for col in groupby if col in df.columns]\n         cols += [col for col in metrics if col in df.columns]\n         cols += [col for col in df.columns if col in cols]\n         df = df[cols]\n         return QueryResult(\n             df=df,\n             query=query_str,\n             duration=datetime.now() - qry_start_dttm)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> cols = [ ] <NEWLINE> if <STRING> in df . columns : <NEWLINE> <INDENT> cols += [ <STRING> ] <NEWLINE> <DEDENT> cols += [ col for col in groupby if col in df . columns ] <NEWLINE> cols += [ col for col in metrics if col in df . columns ] <NEWLINE> cols += [ col for col in df . columns if col in cols ] <NEWLINE> df = df [ cols ] <NEWLINE> return QueryResult ( <NEWLINE> <INDENT> df = df , <NEWLINE> query = query_str , <NEWLINE> duration = datetime . now ( ) - qry_start_dttm ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Reordering columns"
            ],
            "<STRING>": [
                "'timestamp'",
                "'timestamp'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cc2293d0047a43c6af0ea3a45858f325": {
        "code_string": "def all_table_names(self, schema=None, force=False):\n         if not schema:\n             tables_dict = self.db_engine_spec.fetch_result_sets(\n                 self, 'table', force=force)\n             return tables_dict.get(\"\", [])\n         return sorted(\n             self.db_engine_spec.get_table_names(self.inspector, schema))\n",
        "code_toks_joined": "def all_table_names ( self , schema = None , force = False ) : <NEWLINE> <INDENT> if not schema : <NEWLINE> <INDENT> tables_dict = self . db_engine_spec . fetch_result_sets ( <NEWLINE> <INDENT> self , <STRING> , force = force ) <NEWLINE> <DEDENT> return tables_dict . get ( <STRING> , [ ] ) <NEWLINE> <DEDENT> return sorted ( <NEWLINE> <INDENT> self . db_engine_spec . get_table_names ( self . inspector , schema ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'table'",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "31e7cc294ad1495599a76e02042b898a": {
        "code_string": "try:\n         # `shell=True` is necessary for Windows, but not for Linux. The command\n         # string is constant, so shell=True should be fine\n         result = subprocess.run('dot -Tpdf', check=True, shell=True,\n                 # The input will be a str and the output will be binary, but\n                 # subprocess.run requires they both be str or both be binary.\n                 # So, use binary and send the source in as binary (with default\n                 # encoding).\n                 input=bytes(source, sys.getdefaultencoding()),\n                 stdout=subprocess.PIPE,\n                 stderr=subprocess.PIPE # Windows doesn't like it when stderr is left alone\n                 )\n     except OSError as exception:\n         msg = f'had a problem compiling to PDF:\\n{exception}'\n         raise SnutreeError(exception)\n",
        "code_toks_joined": "try : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> result = subprocess . run ( <STRING> , check = True , shell = True , <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> input = bytes ( source , sys . getdefaultencoding ( ) ) , <NEWLINE> stdout = subprocess . PIPE , <NEWLINE> stderr = subprocess . PIPE <COMMENT> <NEWLINE> ) <NEWLINE> except OSError as exception : <NEWLINE> <DEDENT> msg = <STRING> <NEWLINE> raise SnutreeError ( exception ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# `shell=True` is necessary for Windows, but not for Linux. The command",
                "# string is constant, so shell=True should be fine",
                "# The input will be a str and the output will be binary, but",
                "# subprocess.run requires they both be str or both be binary.",
                "# So, use binary and send the source in as binary (with default",
                "# encoding).",
                "# Windows doesn't like it when stderr is left alone"
            ],
            "<STRING>": [
                "'dot -Tpdf'",
                "f'had a problem compiling to PDF:\\n{exception}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bcd310796bc045ac9ad01f42e3743200": {
        "code_string": "test_tot_struc += num_batch_atom\n                         else:\n                             test_elem, tmp_nne, tmp_eloss = \\\n                                 sess.run([self.next_elem, self.E, self.e_loss], feed_dict=test_fdict)\n                             num_batch_struc = valid_elem['num_seg'] - 1\n                             eloss += tmp_eloss * num_batch_struc\n",
        "code_toks_joined": "test_tot_struc += num_batch_atom <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> test_elem , tmp_nne , tmp_eloss = sess . run ( [ self . next_elem , self . E , self . e_loss ] , feed_dict = test_fdict ) <NEWLINE> num_batch_struc = valid_elem [ <STRING> ] - 1 <NEWLINE> eloss += tmp_eloss * num_batch_struc <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'num_seg'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "98e2ec79f13446019b0728ebb7ad7554": {
        "code_string": "if not self.inputs['remain_pickle']:\n                     os.remove(item)\n",
        "code_toks_joined": "if not self . inputs [ <STRING> ] : <NEWLINE> <INDENT> os . remove ( item ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'remain_pickle'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d97dc3cbdbfd4e4d8a0f0532d8b6bb42": {
        "code_string": "def upgrade():\n     conn = op.get_bind()\n     op.add_column('graphs', sa.Column('creation', sa.DateTime(), nullable=True))\n     op.add_column('graphs', sa.Column('last_access', sa.DateTime(), nullable=True))\n     op.add_column('users', sa.Column('creation', sa.DateTime(), nullable=True))\n     op.add_column('users', sa.Column('last_connection', sa.DateTime(), nullable=True))\n     graph_table = sa.sql.table('graphs',\n         sa.sql.column('creation', sa.DateTime),\n         sa.sql.column('last_access', sa.DateTime),\n         )\n     users_table = sa.sql.table('users',\n         sa.sql.column('creation', sa.DateTime),\n         sa.sql.column('last_connection', sa.DateTime),\n         )\n     conn.execute(graph_table.update().values(\n         creation=datetime.utcnow(), last_access=datetime.utcnow()))\n     conn.execute(users_table.update().values(\n         creation=datetime.utcnow(), last_connection=datetime.utcnow()))\n     if is_sqlite(conn):\n         op.alter_column('graphs', 'creation', nullable=False)\n         op.alter_column('graphs', 'last_access', nullable=False)\n         op.alter_column('users', 'creation', nullable=False)\n         op.alter_column('users', 'last_connection', nullable=False)\n",
        "code_toks_joined": "def upgrade ( ) : <NEWLINE> <INDENT> conn = op . get_bind ( ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> graph_table = sa . sql . table ( <STRING> , <NEWLINE> <INDENT> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> ) <NEWLINE> <DEDENT> users_table = sa . sql . table ( <STRING> , <NEWLINE> <INDENT> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> ) <NEWLINE> <DEDENT> conn . execute ( graph_table . update ( ) . values ( <NEWLINE> <INDENT> creation = datetime . utcnow ( ) , last_access = datetime . utcnow ( ) ) ) <NEWLINE> <DEDENT> conn . execute ( users_table . update ( ) . values ( <NEWLINE> <INDENT> creation = datetime . utcnow ( ) , last_connection = datetime . utcnow ( ) ) ) <NEWLINE> <DEDENT> if is_sqlite ( conn ) : <NEWLINE> <INDENT> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'graphs'",
                "'creation'",
                "'graphs'",
                "'last_access'",
                "'users'",
                "'creation'",
                "'users'",
                "'last_connection'",
                "'graphs'",
                "'creation'",
                "'last_access'",
                "'users'",
                "'creation'",
                "'last_connection'",
                "'graphs'",
                "'creation'",
                "'graphs'",
                "'last_access'",
                "'users'",
                "'creation'",
                "'users'",
                "'last_connection'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c8fe0d4576114c3db2c6f0d0fc73116b": {
        "code_string": "def setUp(self):\n         self.directory = tempfile.mkdtemp()\n         self.source_path = os.path.join(self.directory, '_test.py')\n         self.bc_path = importlib.util.cache_from_source(self.source_path)\n         with open(self.source_path, 'w') as file:\n             file.write('x = 123\\n')\n         self.source_path2 = os.path.join(self.directory, '_test2.py')\n         self.bc_path2 = importlib.util.cache_from_source(self.source_path2)\n         shutil.copyfile(self.source_path, self.source_path2)\n         self.subdirectory = os.path.join(self.directory, '_subdir')\n         os.mkdir(self.subdirectory)\n         self.source_path3 = os.path.join(self.subdirectory, '_test3.py')\n         shutil.copyfile(self.source_path, self.source_path3)\n         many_directories = [str(number) for number in range(1, 100)]\n         self.long_path = os.path.join(\"long\",\n                                       self.directory,\n                                       *many_directories)\n         os.makedirs(self.long_path)\n         self.source_path_long = os.path.join(self.long_path, '_test4.py')\n         shutil.copyfile(self.source_path, self.source_path_long)\n         self.bc_path_long = importlib.util.cache_from_source(\n             self.source_path_long\n         )\n",
        "code_toks_joined": "def setUp ( self ) : <NEWLINE> <INDENT> self . directory = tempfile . mkdtemp ( ) <NEWLINE> self . source_path = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path = importlib . util . cache_from_source ( self . source_path ) <NEWLINE> with open ( self . source_path , <STRING> ) as file : <NEWLINE> <INDENT> file . write ( <STRING> ) <NEWLINE> <DEDENT> self . source_path2 = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path2 = importlib . util . cache_from_source ( self . source_path2 ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path2 ) <NEWLINE> self . subdirectory = os . path . join ( self . directory , <STRING> ) <NEWLINE> os . mkdir ( self . subdirectory ) <NEWLINE> self . source_path3 = os . path . join ( self . subdirectory , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path3 ) <NEWLINE> many_directories = [ str ( number ) for number in range ( 1 , 100 ) ] <NEWLINE> self . long_path = os . path . join ( <STRING> , <NEWLINE> <INDENT> self . directory , <NEWLINE> * many_directories ) <NEWLINE> <DEDENT> os . makedirs ( self . long_path ) <NEWLINE> self . source_path_long = os . path . join ( self . long_path , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path_long ) <NEWLINE> self . bc_path_long = importlib . util . cache_from_source ( <NEWLINE> <INDENT> self . source_path_long <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'_test.py'",
                "'w'",
                "'x = 123\\n'",
                "'_test2.py'",
                "'_subdir'",
                "'_test3.py'",
                "\"long\"",
                "'_test4.py'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b3f47a8e1b8340109eafac8d27241609": {
        "code_string": "def setUp(self):\n         self.directory = tempfile.mkdtemp()\n         self.source_path = os.path.join(self.directory, '_test.py')\n         self.bc_path = importlib.util.cache_from_source(self.source_path)\n         with open(self.source_path, 'w') as file:\n             file.write('x = 123\\n')\n         self.source_path2 = os.path.join(self.directory, '_test2.py')\n         self.bc_path2 = importlib.util.cache_from_source(self.source_path2)\n         shutil.copyfile(self.source_path, self.source_path2)\n         self.subdirectory = os.path.join(self.directory, '_subdir')\n         os.mkdir(self.subdirectory)\n         self.source_path3 = os.path.join(self.subdirectory, '_test3.py')\n         shutil.copyfile(self.source_path, self.source_path3)\n         many_directories = [str(number) for number in range(1, 100)]\n         self.long_path = os.path.join(\"long\",\n                                       self.directory,\n                                       *many_directories)\n         os.makedirs(self.long_path)\n         self.source_path_long = os.path.join(self.long_path, '_test4.py')\n         shutil.copyfile(self.source_path, self.source_path_long)\n         self.bc_path_long = importlib.util.cache_from_source(\n             self.source_path_long\n         )\n",
        "code_toks_joined": "def setUp ( self ) : <NEWLINE> <INDENT> self . directory = tempfile . mkdtemp ( ) <NEWLINE> self . source_path = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path = importlib . util . cache_from_source ( self . source_path ) <NEWLINE> with open ( self . source_path , <STRING> ) as file : <NEWLINE> <INDENT> file . write ( <STRING> ) <NEWLINE> <DEDENT> self . source_path2 = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path2 = importlib . util . cache_from_source ( self . source_path2 ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path2 ) <NEWLINE> self . subdirectory = os . path . join ( self . directory , <STRING> ) <NEWLINE> os . mkdir ( self . subdirectory ) <NEWLINE> self . source_path3 = os . path . join ( self . subdirectory , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path3 ) <NEWLINE> many_directories = [ str ( number ) for number in range ( 1 , 100 ) ] <NEWLINE> self . long_path = os . path . join ( <STRING> , <NEWLINE> <INDENT> self . directory , <NEWLINE> * many_directories ) <NEWLINE> <DEDENT> os . makedirs ( self . long_path ) <NEWLINE> self . source_path_long = os . path . join ( self . long_path , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path_long ) <NEWLINE> self . bc_path_long = importlib . util . cache_from_source ( <NEWLINE> <INDENT> self . source_path_long <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'_test.py'",
                "'w'",
                "'x = 123\\n'",
                "'_test2.py'",
                "'_subdir'",
                "'_test3.py'",
                "\"long\"",
                "'_test4.py'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ec0d54e1234441d5b331e905a0490a49": {
        "code_string": "def _complement(self, metadata: Dict[str, Any]) -> Dict[str, Any]:\n         \"\"\"Omit the main *href* if we are not storing original images.\"\"\"\n         metadata = super()._complement(metadata)\n         # Add main *href* if we are storing original images or if not image\n         if metadata.get('image_width') or not self.config.store_original:\n             del metadata['href']\n         return metadata\n",
        "code_toks_joined": "def _complement ( self , metadata : Dict [ str , Any ] ) -> Dict [ str , Any ] : <NEWLINE> <INDENT> <STRING> <NEWLINE> metadata = super ( ) . _complement ( metadata ) <NEWLINE> <COMMENT> <NL> if metadata . get ( <STRING> ) or not self . config . store_original : <NEWLINE> <INDENT> del metadata [ <STRING> ] <NEWLINE> <DEDENT> return metadata <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Omit the main *href* if we are not storing original images.\"\"\"",
                "'image_width'",
                "'href'"
            ],
            "<COMMENT>": [
                "# Add main *href* if we are storing original images or if not image"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d58dbbb1487241aa807e7731ba0e4795": {
        "code_string": "def createSwirlFile(self, fileName):\n         \"\"\" given a fileName it return the associated swirlFile if present\n         otherwise it creates a new one with all the symlinks resolved\"\"\"\n         links = []\n         while os.path.islink(fileName) :\n             p = os.readlink(fileName)\n             if not os.path.isabs(p):\n                 p = os.path.join( os.path.dirname(fileName), p)\n             links.append(p)\n             fileName = p\n         for swirlFile in self.swirlFiles:\n             if swirlFile.path == fileName:\n                 #we found it\n                 swirlFile.setLinks(links)\n                 return swirlFile\n         swirlFile = SwirlFile(fileName, links)\n         self.swirlFiles.append(swirlFile)\n         return swirlFile\n",
        "code_toks_joined": "def createSwirlFile ( self , fileName ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> links = [ ] <NEWLINE> while os . path . islink ( fileName ) : <NEWLINE> <INDENT> p = os . readlink ( fileName ) <NEWLINE> if not os . path . isabs ( p ) : <NEWLINE> <INDENT> p = os . path . join ( os . path . dirname ( fileName ) , p ) <NEWLINE> <DEDENT> links . append ( p ) <NEWLINE> fileName = p <NEWLINE> <DEDENT> for swirlFile in self . swirlFiles : <NEWLINE> <INDENT> if swirlFile . path == fileName : <NEWLINE> <COMMENT> <NL> <INDENT> swirlFile . setLinks ( links ) <NEWLINE> return swirlFile <NEWLINE> <DEDENT> <DEDENT> swirlFile = SwirlFile ( fileName , links ) <NEWLINE> self . swirlFiles . append ( swirlFile ) <NEWLINE> return swirlFile <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" given a fileName it return the associated swirlFile if present\n         otherwise it creates a new one with all the symlinks resolved\"\"\""
            ],
            "<COMMENT>": [
                "#we found it"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7bf6dfa479f74fa2996f871333fc05cd": {
        "code_string": "def crop_resample(bands, intensities, crops):\n   intensities = np.atleast_2d(intensities)\n   crops = sorted(crops)\n   # check that each chunk is valid and doesn't overlap with any other\n   prev_ub = float('-inf')\n   for lb, ub, step in crops:\n     if ub >= lb:\n       raise ValueError('Invalid crop region')\n     if lb < prev_ub:\n       raise ValueError('Overlapping crop regions')\n     prev_ub = ub\n   # do all the band lookups at once\n   locs = sorted(set(c[0] for c in crops).union(set(c[1] for c in crops)))\n   idxs = np.searchsorted(bands, locs)\n   loc_idxs = dict(zip(locs, idxs))\n   # crop/resample each chunk separately\n   xs, ys = [], []\n   for lb, ub, step in crops:\n     s = slice(loc_idxs[lb], loc_idxs[ub])\n     x = bands[s]\n     if step > 0:\n       x_new = np.arange(x[0], x[-1] + step, step)\n       y_new = np.row_stack([np.interp(x_new, x, y) for y in intensities[:, s]])\n       xs.append(x_new)\n       ys.append(y_new)\n     else:\n       xs.append(x)\n       ys.append(intensities[:, s])\n   # glue all the chunks back together\n   return np.concatenate(xs), np.hstack(ys)\n",
        "code_toks_joined": "def crop_resample ( bands , intensities , crops ) : <NEWLINE> <INDENT> intensities = np . atleast_2d ( intensities ) <NEWLINE> crops = sorted ( crops ) <NEWLINE> <COMMENT> <NL> prev_ub = float ( <STRING> ) <NEWLINE> for lb , ub , step in crops : <NEWLINE> <INDENT> if ub >= lb : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if lb < prev_ub : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> prev_ub = ub <NEWLINE> <COMMENT> <NL> <DEDENT> locs = sorted ( set ( c [ 0 ] for c in crops ) . union ( set ( c [ 1 ] for c in crops ) ) ) <NEWLINE> idxs = np . searchsorted ( bands , locs ) <NEWLINE> loc_idxs = dict ( zip ( locs , idxs ) ) <NEWLINE> <COMMENT> <NL> xs , ys = [ ] , [ ] <NEWLINE> for lb , ub , step in crops : <NEWLINE> <INDENT> s = slice ( loc_idxs [ lb ] , loc_idxs [ ub ] ) <NEWLINE> x = bands [ s ] <NEWLINE> if step > 0 : <NEWLINE> <INDENT> x_new = np . arange ( x [ 0 ] , x [ - 1 ] + step , step ) <NEWLINE> y_new = np . row_stack ( [ np . interp ( x_new , x , y ) for y in intensities [ : , s ] ] ) <NEWLINE> xs . append ( x_new ) <NEWLINE> ys . append ( y_new ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> xs . append ( x ) <NEWLINE> ys . append ( intensities [ : , s ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> return np . concatenate ( xs ) , np . hstack ( ys ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# check that each chunk is valid and doesn't overlap with any other",
                "# do all the band lookups at once",
                "# crop/resample each chunk separately",
                "# glue all the chunks back together"
            ],
            "<STRING>": [
                "'-inf'",
                "'Invalid crop region'",
                "'Overlapping crop regions'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "612d196979244b46a3a06b2e6bf0e2ed": {
        "code_string": "def post_prob_scheme(t_table, words, stanza, scheme):\n     \"\"\"\n     Compute posterior probability of a scheme for a stanza, with probability of every word in rhymelist\n     rhyming with all the ones before it\n     \"\"\"\n     myprob = 1\n     n = len(words)\n     rhymelists = get_rhymelists(stanza, scheme)\n     for rhymelist in rhymelists:\n         for i, w in enumerate(rhymelist):\n             r = words.index(w)\n             if i == 0:  # first word, use P(w|x)\n                 myprob = t_table[r, n]\n             else:\n                 for v in rhymelist[:i]:  # history\n                     c = words.index(v)\n                     myprob *= t_table[r, c]\n     if myprob == 0 and len(stanza) > 30:  # probably underflow\n         myprob = 1e-300\n     return myprob\n",
        "code_toks_joined": "def post_prob_scheme ( t_table , words , stanza , scheme ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> myprob = 1 <NEWLINE> n = len ( words ) <NEWLINE> rhymelists = get_rhymelists ( stanza , scheme ) <NEWLINE> for rhymelist in rhymelists : <NEWLINE> <INDENT> for i , w in enumerate ( rhymelist ) : <NEWLINE> <INDENT> r = words . index ( w ) <NEWLINE> if i == 0 : <COMMENT> <NEWLINE> <INDENT> myprob = t_table [ r , n ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for v in rhymelist [ : i ] : <COMMENT> <NEWLINE> <INDENT> c = words . index ( v ) <NEWLINE> myprob *= t_table [ r , c ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> if myprob == 0 and len ( stanza ) > 30 : <COMMENT> <NEWLINE> <INDENT> myprob = 1e-300 <NEWLINE> <DEDENT> return myprob <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Compute posterior probability of a scheme for a stanza, with probability of every word in rhymelist\n     rhyming with all the ones before it\n     \"\"\""
            ],
            "<COMMENT>": [
                "# first word, use P(w|x)",
                "# history",
                "# probably underflow"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "894fcbbda09b4d18a019f0532b929d68": {
        "code_string": "def __init__(\n \t\tself,\n \t\tsignals: typing.Iterable[\n \t\t\ttyping.Union[signal.Signals, int, str]] = _DEFAULT_SIGS,\n \t\tcallback: typing.Optional[\n \t\t\ttyping.Callable[[signal.Signals, typing.Optional[FrameType]], None]] = None,\n \t):\n \t\tsignals = list(signals)\n \t\tsignals_tmp = []  # type: typing.List[signal.Signals]\n \t\tfor sig in signals:\n \t\t\tif isinstance(sig, int):\n \t\t\t\tsig = signal.Signals(sig)\n \t\t\telif isinstance(sig, str):\n \t\t\t\tsig = signal.Signals[sig]\n \t\t\tif isinstance(sig, signal.Signals):  # This makes Mypy happy.\n \t\t\t\tsignals_tmp.append(sig)\n \t\t\telse:\n \t\t\t\traise ValueError('Cannot convert to signal.Signals: %r' % (sig,))\n \t\tif not signals:\n \t\t\traise ValueError('No signals selected')\n \t\tif callback is None:\n \t\t\tcallback = self._default_callback\n \t\tif not _two_pos_args(callback):\n \t\t\traise TypeError(\n \t\t\t\t'callback is not a callable with two positional arguments: %r' %\n \t\t\t\t(callback,))\n \t\tif os.name == 'nt':\n \t\t\tif not (set(signals) <= set(self._DEFAULT_SIGS)):\n \t\t\t\traise ValueError(\n \t\t\t\t\t\"Windows does not support one of the signals: %r\" % (signals,))\n \t\tself._signals = tuple(signals_tmp)  # type: typing.Tuple[signal.Signals, ...]\n \t\tself._callback = callback\n \t\t# No need for a lock because signals can only be set from the main thread.\n \t\tself._old_handlers = []  # type: _HandlersListType\n \t\tself._depth = 0\n",
        "code_toks_joined": "def __init__ ( <NEWLINE> <INDENT> self , <NEWLINE> signals : typing . Iterable [ <NEWLINE> <INDENT> typing . Union [ signal . Signals , int , str ] ] = _DEFAULT_SIGS , <NEWLINE> <DEDENT> callback : typing . Optional [ <NEWLINE> <INDENT> typing . Callable [ [ signal . Signals , typing . Optional [ FrameType ] ] , None ] ] = None , <NEWLINE> ) : <NEWLINE> <DEDENT> signals = list ( signals ) <NEWLINE> signals_tmp = [ ] <COMMENT> <NEWLINE> for sig in signals : <NEWLINE> <INDENT> if isinstance ( sig , int ) : <NEWLINE> <INDENT> sig = signal . Signals ( sig ) <NEWLINE> <DEDENT> elif isinstance ( sig , str ) : <NEWLINE> <INDENT> sig = signal . Signals [ sig ] <NEWLINE> <DEDENT> if isinstance ( sig , signal . Signals ) : <COMMENT> <NEWLINE> <INDENT> signals_tmp . append ( sig ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> % ( sig , ) ) <NEWLINE> <DEDENT> <DEDENT> if not signals : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if callback is None : <NEWLINE> <INDENT> callback = self . _default_callback <NEWLINE> <DEDENT> if not _two_pos_args ( callback ) : <NEWLINE> <INDENT> raise TypeError ( <NEWLINE> <INDENT> <STRING> % <NEWLINE> ( callback , ) ) <NEWLINE> <DEDENT> <DEDENT> if os . name == <STRING> : <NEWLINE> <INDENT> if not ( set ( signals ) <= set ( self . _DEFAULT_SIGS ) ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> % ( signals , ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . _signals = tuple ( signals_tmp ) <COMMENT> <NEWLINE> self . _callback = callback <NEWLINE> <COMMENT> <NL> self . _old_handlers = [ ] <COMMENT> <NEWLINE> self . _depth = 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# type: typing.List[signal.Signals]",
                "# This makes Mypy happy.",
                "# type: typing.Tuple[signal.Signals, ...]",
                "# No need for a lock because signals can only be set from the main thread.",
                "# type: _HandlersListType"
            ],
            "<STRING>": [
                "'Cannot convert to signal.Signals: %r'",
                "'No signals selected'",
                "'callback is not a callable with two positional arguments: %r'",
                "'nt'",
                "\"Windows does not support one of the signals: %r\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6e62cff5718e4c89a37546a2969b77b2": {
        "code_string": "def creates_default_site(sender, instance, created, *args, **kwargs):\n     if created:\n         try:\n             site = Site.objects.get(domain__icontains=DEFAULT_SITE_DOMAIN,\n                                     tenant_site__tenant=instance)\n             if site.domain != ('%s.%s' % (instance.slug, DEFAULT_SITE_DOMAIN)):\n                 site.delete()\n             else:\n                 return\n         except Site.DoesNotExist:\n             pass\n",
        "code_toks_joined": "def creates_default_site ( sender , instance , created , * args , ** kwargs ) : <NEWLINE> <INDENT> if created : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> site = Site . objects . get ( domain__icontains = DEFAULT_SITE_DOMAIN , <NEWLINE> <INDENT> tenant_site__tenant = instance ) <NEWLINE> <DEDENT> if site . domain != ( <STRING> % ( instance . slug , DEFAULT_SITE_DOMAIN ) ) : <NEWLINE> <INDENT> site . delete ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT> except Site . DoesNotExist : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%s.%s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d9ac0602aa1345c5a2a3e8d8ef5032b6": {
        "code_string": "def __call__(self, h, c, x):\n         if not self.test and self.dropout>0:\n             gates = dy.vanilla_lstm_gates_dropout(x,h,self.Whx, self.Whh,self.bh, self.dropout_mask_x, self.dropout_mask_h)\n         else:\n             gates = dy.vanilla_lstm_gates(x,h,self.Whx, self.Whh,self.bh)\n         new_c = dy.vanilla_lstm_c(c, gates)\n         new_h = dy.vanilla_lstm_h(c, gates)\n         return new_h, new_c",
        "code_toks_joined": "def __call__ ( self , h , c , x ) : <NEWLINE> <INDENT> if not self . test and self . dropout > 0 : <NEWLINE> <INDENT> gates = dy . vanilla_lstm_gates_dropout ( x , h , self . Whx , self . Whh , self . bh , self . dropout_mask_x , self . dropout_mask_h ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> gates = dy . vanilla_lstm_gates ( x , h , self . Whx , self . Whh , self . bh ) <NEWLINE> <DEDENT> new_c = dy . vanilla_lstm_c ( c , gates ) <NEWLINE> new_h = dy . vanilla_lstm_h ( c , gates ) <NEWLINE> return new_h , new_c <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "81c8a02847654c73ab630ae7edea1bde": {
        "code_string": "def _test_recurrent_layer_bidirectional_transduction(\n     fwd_layer,\n     bwd_layer,\n     dummy_input,\n     lengths,\n     left_padded,\n ):\n     # Create transduction layer\n     tranductor = transduction_layers.BidirectionalLayer(fwd_layer, bwd_layer)\n     # Initialize computation graph\n     dy.renew_cg()\n     # Create inputs\n     seq = [\n         dy.inputTensor(dummy_input, batched=True) + i for i in range(10)\n     ]\n     # Initialize tranductor\n     tranductor.init(test=False, update=True)\n     # Run tranductor\n     fwd_states, bwd_states = tranductor(\n         seq, lengths=lengths, left_padded=left_padded\n     )\n     # Try forward/backward\n     fwd_z = dy.mean_batches(\n         dy.esum([dy.sum_elems(state[0]) for state in fwd_states])\n     )\n     bwd_z = dy.mean_batches(\n         dy.esum([dy.sum_elems(state[0]) for state in fwd_states])\n     )\n     z = fwd_z + bwd_z\n     z.forward()\n     z.backward()\n",
        "code_toks_joined": "def _test_recurrent_layer_bidirectional_transduction ( <NEWLINE> <INDENT> fwd_layer , <NEWLINE> bwd_layer , <NEWLINE> dummy_input , <NEWLINE> lengths , <NEWLINE> left_padded , <NEWLINE> ) : <NEWLINE> <COMMENT> <NL> tranductor = transduction_layers . BidirectionalLayer ( fwd_layer , bwd_layer ) <NEWLINE> <COMMENT> <NL> dy . renew_cg ( ) <NEWLINE> <COMMENT> <NL> seq = [ <NEWLINE> <INDENT> dy . inputTensor ( dummy_input , batched = True ) + i for i in range ( 10 ) <NEWLINE> <DEDENT> ] <NEWLINE> <COMMENT> <NL> tranductor . init ( test = False , update = True ) <NEWLINE> <COMMENT> <NL> fwd_states , bwd_states = tranductor ( <NEWLINE> <INDENT> seq , lengths = lengths , left_padded = left_padded <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> fwd_z = dy . mean_batches ( <NEWLINE> <INDENT> dy . esum ( [ dy . sum_elems ( state [ 0 ] ) for state in fwd_states ] ) <NEWLINE> <DEDENT> ) <NEWLINE> bwd_z = dy . mean_batches ( <NEWLINE> <INDENT> dy . esum ( [ dy . sum_elems ( state [ 0 ] ) for state in fwd_states ] ) <NEWLINE> <DEDENT> ) <NEWLINE> z = fwd_z + bwd_z <NEWLINE> z . forward ( ) <NEWLINE> z . backward ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Create transduction layer",
                "# Initialize computation graph",
                "# Create inputs",
                "# Initialize tranductor",
                "# Run tranductor",
                "# Try forward/backward"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f360e25a4a4c4e0dad8836c56010184f": {
        "code_string": "def select_bounding_box (self, names, column):\n \t\tresult = []\n \t\tfor name in names:\n \t\t\ttarget = self.store[name]\n \t\t\tif target.has_column() and target.column <= column: continue\n \t\t\tresult.append(target.row)\n \t\treturn result\n",
        "code_toks_joined": "def select_bounding_box ( self , names , column ) : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for name in names : <NEWLINE> <INDENT> target = self . store [ name ] <NEWLINE> if target . has_column ( ) and target . column <= column : continue <NEWLINE> result . append ( target . row ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5d88d72e44e24fa4a51254264686cf2d": {
        "code_string": "def _parse(string):\n         result = []\n         for match in PYTHON_FORMAT.finditer(string):\n             name, format, typechar = match.groups()\n             if typechar == '%' and name is not None:\n                 continue\n             result.append((name, str(typechar)))\n         return result\n",
        "code_toks_joined": "def _parse ( string ) : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for match in PYTHON_FORMAT . finditer ( string ) : <NEWLINE> <INDENT> name , format , typechar = match . groups ( ) <NEWLINE> if typechar == <STRING> and name is not None : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> result . append ( ( name , str ( typechar ) ) ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "043b57cc704d47ebaf7c683889bccfe7": {
        "code_string": "return tmin > tmax\n",
        "code_toks_joined": "return tmin > tmax <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5c5fcd5976724346bae45c2a9bbc149b": {
        "code_string": "@property\n     def rooms(self) -> list:\n         return [InTouchRoom(r, self) for r in ['1', '2']\n                 if True and _convert(\n                     self._data['room_temp_{}_msb'.format(r)],\n                     self._data['room_temp_{}_lsb'.format(r)]) is not None]\n",
        "code_toks_joined": "@ property <NEWLINE> <INDENT> def rooms ( self ) -> list : <NEWLINE> <INDENT> return [ InTouchRoom ( r , self ) for r in [ <STRING> , <STRING> ] <NEWLINE> <INDENT> if True and _convert ( <NEWLINE> <INDENT> self . _data [ <STRING> . format ( r ) ] , <NEWLINE> self . _data [ <STRING> . format ( r ) ] ) is not None ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'1'",
                "'2'",
                "'room_temp_{}_msb'",
                "'room_temp_{}_lsb'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f4ff9cf967854993b9b0cc2a2773a74e": {
        "code_string": "def consume(self, msg):\n         body, topic = msg.get('body'), msg.get('topic')\n         pretty_text = fedmsg.text.msg2repr(body)\n         log.debug(pretty_text)\n         icon = fedmsg.text._msg2icon(msg) or ''\n         log.debug(\"icon = %s\" % icon)\n         if icon:\n             icon_file = self._icon_cache.get(icon)\n             if not icon_file:\n                 icon_file, headers = urllib.urlretrieve(icon)\n                 log.debug('Downloaded %s to %s' % (icon.split('/')[-1],\n                                                    icon_file))\n                 self._icon_cache[icon] = icon_file\n             icon = icon_file\n         note = Notify.Notification.new(\"fedmsg\", pretty_text, icon_file)\n         note.show()\n",
        "code_toks_joined": "def consume ( self , msg ) : <NEWLINE> <INDENT> body , topic = msg . get ( <STRING> ) , msg . get ( <STRING> ) <NEWLINE> pretty_text = fedmsg . text . msg2repr ( body ) <NEWLINE> log . debug ( pretty_text ) <NEWLINE> icon = fedmsg . text . _msg2icon ( msg ) or <STRING> <NEWLINE> log . debug ( <STRING> % icon ) <NEWLINE> if icon : <NEWLINE> <INDENT> icon_file = self . _icon_cache . get ( icon ) <NEWLINE> if not icon_file : <NEWLINE> <INDENT> icon_file , headers = urllib . urlretrieve ( icon ) <NEWLINE> log . debug ( <STRING> % ( icon . split ( <STRING> ) [ - 1 ] , <NEWLINE> <INDENT> icon_file ) ) <NEWLINE> <DEDENT> self . _icon_cache [ icon ] = icon_file <NEWLINE> <DEDENT> icon = icon_file <NEWLINE> <DEDENT> note = Notify . Notification . new ( <STRING> , pretty_text , icon_file ) <NEWLINE> note . show ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'body'",
                "'topic'",
                "''",
                "\"icon = %s\"",
                "'Downloaded %s to %s'",
                "'/'",
                "\"fedmsg\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7b4e78f443d648e5b2f563362af09d38": {
        "code_string": "if len(file_list) == 2:\n             return find_probes(first_probe_data, second_probe_data)\n         elif len(file_list) < 2:\n             return find_probes_recursively(file_list[2:], tail=find_probes(first_probe_data, second_probe_data))\n     else:\n         probe_data = read_probe_records_from_file(file_list[0])\n",
        "code_toks_joined": "if len ( file_list ) == 2 : <NEWLINE> <INDENT> return find_probes ( first_probe_data , second_probe_data ) <NEWLINE> elif len ( file_list ) < 2 : <NEWLINE> return find_probes_recursively ( file_list [ 2 : ] , tail = find_probes ( first_probe_data , second_probe_data ) ) <NEWLINE> else : <NEWLINE> probe_data = read_probe_records_from_file ( file_list [ 0 ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b2b2fc0d965f4d9e88af0383f1ce77cb": {
        "code_string": "def get_recipients(args):\n     if args['<recipients>']:\n         recipients = []\n         for recipient in args['<recipients>']:\n             key = binascii.unhexlify(recipient)\n             assert len(recipient) == 32\n             recipients.append(key)\n         return recipients\n     else:\n         # Without explicit recipients, just send to yourself.\n         private = get_private(args)\n         public = libnacl.crypto_scalarmult_base(private)\n         return [public]\n",
        "code_toks_joined": "def get_recipients ( args ) : <NEWLINE> <INDENT> if args [ <STRING> ] : <NEWLINE> <INDENT> recipients = [ ] <NEWLINE> for recipient in args [ <STRING> ] : <NEWLINE> <INDENT> key = binascii . unhexlify ( recipient ) <NEWLINE> assert len ( recipient ) == 32 <NEWLINE> recipients . append ( key ) <NEWLINE> <DEDENT> return recipients <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> private = get_private ( args ) <NEWLINE> public = libnacl . crypto_scalarmult_base ( private ) <NEWLINE> return [ public ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'<recipients>'",
                "'<recipients>'"
            ],
            "<COMMENT>": [
                "# Without explicit recipients, just send to yourself."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "67a019f7555c4c6b9f36582e2f8a2eef": {
        "code_string": "date_edit_widgets = (self._ui.afterDateFilterDateEdit,\n                              self._ui.beforeDateFilterDateEdit)\n         for widget in time_edit_widgets:\n             self.connect(widget, SIGNAL(\"dateChanged (const QDate&)\"),\n                          self.apply_filters)\n",
        "code_toks_joined": "date_edit_widgets = ( self . _ui . afterDateFilterDateEdit , <NEWLINE> <INDENT> self . _ui . beforeDateFilterDateEdit ) <NEWLINE> for widget in time_edit_widgets : <NEWLINE> self . connect ( widget , SIGNAL ( <STRING> ) , <NEWLINE> self . apply_filters ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"dateChanged (const QDate&)\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "452d3f48d5bf4c3c80b491c5d21b79d8": {
        "code_string": "def date(s):\n     \"\"\"Convert s to a datetime.date.\n        s is assumed to be in YYYY-MM-DD format\"\"\"\n     y, m, d = s.strip().split('-')\n     y = year(y)\n     m = month(m)\n     d = day(m)\n     return datetime.date(y, m, d)\n",
        "code_toks_joined": "def date ( s ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> y , m , d = s . strip ( ) . split ( <STRING> ) <NEWLINE> y = year ( y ) <NEWLINE> m = month ( m ) <NEWLINE> d = day ( m ) <NEWLINE> return datetime . date ( y , m , d ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Convert s to a datetime.date.\n        s is assumed to be in YYYY-MM-DD format\"\"\"",
                "'-'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0a991d44f1bc4c469e4a551e1c87a63c": {
        "code_string": "ax2.add_patch(patches.Rectangle(\n             (1.5, 1.5),\n             6, 6,\n             fill=False,\n             lw=2,\n             ls='dashed',\n             edgecolor='blue',\n         ))\n         ax3.add_patch(patches.Rectangle(\n             (0, 0),\n             9, 9,\n             fill=False,\n             lw=1,\n             ls='solid',\n             edgecolor='black',\n         ))\n",
        "code_toks_joined": "ax2 . add_patch ( patches . Rectangle ( <NEWLINE> <INDENT> ( 1.5 , 1.5 ) , <NEWLINE> 6 , 6 , <NEWLINE> fill = False , <NEWLINE> lw = 2 , <NEWLINE> ls = <STRING> , <NEWLINE> edgecolor = <STRING> , <NEWLINE> ) ) <NEWLINE> ax3 . add_patch ( patches . Rectangle ( <NEWLINE> ( 0 , 0 ) , <NEWLINE> 9 , 9 , <NEWLINE> fill = False , <NEWLINE> lw = 1 , <NEWLINE> ls = <STRING> , <NEWLINE> edgecolor = <STRING> , <NEWLINE> ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'dashed'",
                "'blue'",
                "'solid'",
                "'black'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e72810af815246c79d65a07da034b479": {
        "code_string": "for chunk in chunker_list(list(barcode_dict.keys()), args.writer_threads-1):\n         logger.debug('Creating writer queue for samples {}.'.format(','.join(chunk)))\n         q = manager.Queue()\n         q_bc_dict = dict((k, barcode_dict[k]) for k in chunk)\n         writer_pool.apply_async(_writer, (q, q_bc_dict), callback = lambda x: print(x))\n         queue_list.append(q)\n         for bc in chunk.values():\n             queues[bc] = q\n",
        "code_toks_joined": "for chunk in chunker_list ( list ( barcode_dict . keys ( ) ) , args . writer_threads - 1 ) : <NEWLINE> <INDENT> logger . debug ( <STRING> . format ( <STRING> . join ( chunk ) ) ) <NEWLINE> q = manager . Queue ( ) <NEWLINE> q_bc_dict = dict ( ( k , barcode_dict [ k ] ) for k in chunk ) <NEWLINE> writer_pool . apply_async ( _writer , ( q , q_bc_dict ) , callback = lambda x : print ( x ) ) <NEWLINE> queue_list . append ( q ) <NEWLINE> for bc in chunk . values ( ) : <NEWLINE> <INDENT> queues [ bc ] = q <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Creating writer queue for samples {}.'",
                "','"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "80a346618cfe40bdaf9805970569c002": {
        "code_string": "def _do_write(self, offset, data):\n         n = offset / CHUNKDATASIZE\n         while n > len(self.chunks):\n             self._add_new_chunk()\n",
        "code_toks_joined": "def _do_write ( self , offset , data ) : <NEWLINE> <INDENT> n = offset / CHUNKDATASIZE <NEWLINE> while n > len ( self . chunks ) : <NEWLINE> <INDENT> self . _add_new_chunk ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "303481ac9d5b4131bae09b79b0a3ba93": {
        "code_string": "def removeBelowValue(requestContext, seriesList, n):\n   \"\"\"\n   Removes data above the given threshold from the series or list of series provided.\n   Values below this threshole are assigned a value of None\n   \"\"\"\n   for s in seriesList:\n     s.name = 'removeBelowValue(%s, %d)' % (s.name, n)\n     for (index, val) in enumerate(s):\n       if val > n:\n         s[index] = None\n",
        "code_toks_joined": "def removeBelowValue ( requestContext , seriesList , n ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for s in seriesList : <NEWLINE> <INDENT> s . name = <STRING> % ( s . name , n ) <NEWLINE> for ( index , val ) in enumerate ( s ) : <NEWLINE> <INDENT> if val > n : <NEWLINE> <INDENT> s [ index ] = None <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n   Removes data above the given threshold from the series or list of series provided.\n   Values below this threshole are assigned a value of None\n   \"\"\"",
                "'removeBelowValue(%s, %d)'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6970183fc2a4447b8467a93849868879": {
        "code_string": "_kde = kde(x=_f_x, df=_df_i, x_steps=kde_steps)[0]\n                 __x = _kde[_f_x]\n                 _y = _kde['value']\n                 _ax.plot(__x, _y, linestyle=_f_distfit_line, color=_f_distfit_color, alpha=alpha, linewidth=2,\n                          label=_label_2, **kwargs)\n                 if not show_hist and _f_fill:\n                     _ax.fill_between(_f_bins, _y, color=_f_facecolor, alpha=alpha)\n",
        "code_toks_joined": "_kde = kde ( x = _f_x , df = _df_i , x_steps = kde_steps ) [ 0 ] <NEWLINE> <INDENT> __x = _kde [ _f_x ] <NEWLINE> _y = _kde [ <STRING> ] <NEWLINE> _ax . plot ( __x , _y , linestyle = _f_distfit_line , color = _f_distfit_color , alpha = alpha , linewidth = 2 , <NEWLINE> <INDENT> label = _label_2 , ** kwargs ) <NEWLINE> <DEDENT> if not show_hist and _f_fill : <NEWLINE> <INDENT> _ax . fill_between ( _f_bins , _y , color = _f_facecolor , alpha = alpha ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'value'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ff452ebee16f4991902781c09e2ac5e5": {
        "code_string": "_index_name = df.index.name\n     _df['_index'] = _df.index\n     _k_split = int(np.ceil(_df.shape[0] / k))\n",
        "code_toks_joined": "_index_name = df . index . name <NEWLINE> <INDENT> _df [ <STRING> ] = _df . index <NEWLINE> _k_split = int ( np . ceil ( _df . shape [ 0 ] / k ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'_index'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1fa09196279e44189386ebeb966ee26a": {
        "code_string": "async def edit(self, name_, *, name=None, description=utils.sentinel):\n \t\treturn self._new_emote(await self._http.edit(name, name=name, description=description))\n",
        "code_toks_joined": "async def edit ( self , name_ , * , name = None , description = utils . sentinel ) : <NEWLINE> <INDENT> return self . _new_emote ( await self . _http . edit ( name , name = name , description = description ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a3c231d749e740de9d750348215f0549": {
        "code_string": "def get_entropy_for_each_line(trained_model: TrainedModel,\n                               file: str,\n                               entropy_aggregator: Callable[[List[float], List[int]], Union[float, List[float]]],\n                               verbose: bool = False) -> Union[List[float], List[List[float]]]:\n     prep_lines_and_entropies: List[Dict[str, Union[str, List[str], List[float], float]]] = []\n     with open(file, 'r') as f:\n         _, extension = os.path.splitext(file)[1:]\n         for line in f:\n             time_measurer.tick(\"Inference\")\n             prep_line, entropies, word_boundaries = trained_model.get_entropies_for_text(line, extension[1:])\n             time_measurer.tock(\"Inference\")\n             line_entropy = entropy_aggregator(entropies, word_boundaries)\n             prep_lines_and_entropies.append({\n                 'text': line,\n                 'prep_text': prep_line,\n                 'entropies': entropies,\n                 'line_entropy': line_entropy\n             })\n         if not verbose:\n             for line in prep_lines_and_entropies:\n                 print(line['text'])\n                 print(line['line_entropy'])\n                 print(f\"{[(prep_token, token_entropy) for prep_token, token_entropy in zip(line['prep_text'], line['entropies'])]}\")\n                 print(\"=============\")\n     return list(map(lambda e: e['line_entropy'], prep_lines_and_entropies))\n",
        "code_toks_joined": "def get_entropy_for_each_line ( trained_model : TrainedModel , <NEWLINE> <INDENT> file : str , <NEWLINE> entropy_aggregator : Callable [ [ List [ float ] , List [ int ] ] , Union [ float , List [ float ] ] ] , <NEWLINE> verbose : bool = False ) -> Union [ List [ float ] , List [ List [ float ] ] ] : <NEWLINE> prep_lines_and_entropies : List [ Dict [ str , Union [ str , List [ str ] , List [ float ] , float ] ] ] = [ ] <NEWLINE> with open ( file , <STRING> ) as f : <NEWLINE> _ , extension = os . path . splitext ( file ) [ 1 : ] <NEWLINE> for line in f : <NEWLINE> time_measurer . tick ( <STRING> ) <NEWLINE> prep_line , entropies , word_boundaries = trained_model . get_entropies_for_text ( line , extension [ 1 : ] ) <NEWLINE> time_measurer . tock ( <STRING> ) <NEWLINE> line_entropy = entropy_aggregator ( entropies , word_boundaries ) <NEWLINE> prep_lines_and_entropies . append ( { <NEWLINE> <STRING> : line , <NEWLINE> <STRING> : prep_line , <NEWLINE> <STRING> : entropies , <NEWLINE> <STRING> : line_entropy <NEWLINE> } ) <NEWLINE> if not verbose : <NEWLINE> for line in prep_lines_and_entropies : <NEWLINE> print ( line [ <STRING> ] ) <NEWLINE> print ( line [ <STRING> ] ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> return list ( map ( lambda e : e [ <STRING> ] , prep_lines_and_entropies ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'r'",
                "\"Inference\"",
                "\"Inference\"",
                "'text'",
                "'prep_text'",
                "'entropies'",
                "'line_entropy'",
                "'text'",
                "'line_entropy'",
                "f\"{[(prep_token, token_entropy) for prep_token, token_entropy in zip(line['prep_text'], line['entropies'])]}\"",
                "\"=============\"",
                "'line_entropy'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "44ab2ba8cc554f359c2f19fd6ec89f3e": {
        "code_string": "# TODO more pretty\n         with tqdm(total=len(self)) as pbar:\n             on_error = True\n             while True:\n                 case = self._dump() \n                 pbar.set_description(''.join(\n                     '{:>12s}'.format(c[1][-10:]) if isinstance(c[1], str)\n                     else '{:>12g}'.format(c[1])\n                     for c in case\n                 ))\n                 # for\n                 for _fun, _args in self._for:\n                     await _fun(case)(*_args)\n                 # exec\n                 for _cmd in self._exes:\n                     if await self._execute(_cmd):\n                         on_error = True\n                         break\n                 if on_error:\n                     break\n                 yield case\n                 # update\n                 await asyncio.sleep(0.1)\n                 pbar.update()\n                 if self._step():\n                     break\n         self._lock = False\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> with tqdm ( total = len ( self ) ) as pbar : <NEWLINE> <INDENT> on_error = True <NEWLINE> while True : <NEWLINE> <INDENT> case = self . _dump ( ) <NEWLINE> pbar . set_description ( <STRING> . join ( <NEWLINE> <INDENT> <STRING> . format ( c [ 1 ] [ - 10 : ] ) if isinstance ( c [ 1 ] , str ) <NEWLINE> else <STRING> . format ( c [ 1 ] ) <NEWLINE> for c in case <NEWLINE> <DEDENT> ) ) <NEWLINE> <COMMENT> <NL> for _fun , _args in self . _for : <NEWLINE> <INDENT> await _fun ( case ) ( * _args ) <NEWLINE> <COMMENT> <NL> <DEDENT> for _cmd in self . _exes : <NEWLINE> <INDENT> if await self . _execute ( _cmd ) : <NEWLINE> <INDENT> on_error = True <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> if on_error : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> yield case <NEWLINE> <COMMENT> <NL> await asyncio . sleep ( 0.1 ) <NEWLINE> pbar . update ( ) <NEWLINE> if self . _step ( ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . _lock = False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# TODO more pretty",
                "# for",
                "# exec",
                "# update"
            ],
            "<STRING>": [
                "''",
                "'{:>12s}'",
                "'{:>12g}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eff262644ec44343af892a5f0a23d9aa": {
        "code_string": "test_cycles_rewards)\n",
        "code_toks_joined": "test_cycles_rewards ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "unbalanced (){}[]"
        }
    },
    "f02b0c308368410aa8c500d33ee558d8": {
        "code_string": "def parse_node(self, node, model, alias):\n         query = []\n         query_data = []\n         nodes = []\n         for child in node.children:\n             if isinstance(child, Q):\n                 parsed, data = self.parse_q(child, model, alias)\n                 query.append(parsed)\n                 query_data.extend(data)\n             elif isinstance(child, Node):\n                 parsed, data = self.parse_node(node, model, alias)\n                 query.append('(%s)' % parsed)\n                 query_data.extend(data)\n         query.extend(nodes)\n         connector = ' %s ' % node.connector\n         return connector.join(query), query_data\n",
        "code_toks_joined": "def parse_node ( self , node , model , alias ) : <NEWLINE> <INDENT> query = [ ] <NEWLINE> query_data = [ ] <NEWLINE> nodes = [ ] <NEWLINE> for child in node . children : <NEWLINE> <INDENT> if isinstance ( child , Q ) : <NEWLINE> <INDENT> parsed , data = self . parse_q ( child , model , alias ) <NEWLINE> query . append ( parsed ) <NEWLINE> query_data . extend ( data ) <NEWLINE> <DEDENT> elif isinstance ( child , Node ) : <NEWLINE> <INDENT> parsed , data = self . parse_node ( node , model , alias ) <NEWLINE> query . append ( <STRING> % parsed ) <NEWLINE> query_data . extend ( data ) <NEWLINE> <DEDENT> <DEDENT> query . extend ( nodes ) <NEWLINE> connector = <STRING> % node . connector <NEWLINE> return connector . join ( query ) , query_data <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'(%s)'",
                "' %s '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c777811205a448e8a9f71b69de8d286a": {
        "code_string": "user_indexes = self.get_sorted_indexes(User)\n         if BACKEND == 'mysql':\n             entry_indexes.pop(0)\n",
        "code_toks_joined": "user_indexes = self . get_sorted_indexes ( User ) <NEWLINE> <INDENT> if BACKEND == <STRING> : <NEWLINE> <INDENT> entry_indexes . pop ( 0 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'mysql'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eb0a68eb855248428ebe5ffe1cc0d841": {
        "code_string": "class Meta:\n         primary_key = CompositeKey('category', 'comment')\n",
        "code_toks_joined": "class Meta : <NEWLINE> <INDENT> primary_key = CompositeKey ( <STRING> , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'category'",
                "'comment'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "27c0528583b74929bd1b4fbf0c8d2134": {
        "code_string": "# In the event the destination table has already been pushed\n             # for printing, then we have a reference cycle.\n             if dest in accum and table not in accum:\n                 print_('# Possible reference cycle: %s' % foreign_key)\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> if dest in accum and table not in accum : <NEWLINE> <INDENT> print_ ( <STRING> % foreign_key ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# In the event the destination table has already been pushed",
                "# for printing, then we have a reference cycle."
            ],
            "<STRING>": [
                "'# Possible reference cycle: %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "28badfaf7a8340a3ad6d5559942fc6e7": {
        "code_string": "# Can we populate a value on the joined instance using the current?\n             mpk = metadata.primary_key is not None\n             can_populate_joined_pk = (\n                 mpk and\n                 (metadata.attr in inst._data) and\n                 (getattr(joined_inst, metadata.primary_key) is not None))\n             if can_populate_joined_pk:\n                 setattr(\n                     joined_inst,\n                     metadata.primary_key,\n                     inst._data[metadata.attr])\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> mpk = metadata . primary_key is not None <NEWLINE> can_populate_joined_pk = ( <NEWLINE> <INDENT> mpk and <NEWLINE> ( metadata . attr in inst . _data ) and <NEWLINE> ( getattr ( joined_inst , metadata . primary_key ) is not None ) ) <NEWLINE> <DEDENT> if can_populate_joined_pk : <NEWLINE> <INDENT> setattr ( <NEWLINE> <INDENT> joined_inst , <NEWLINE> metadata . primary_key , <NEWLINE> inst . _data [ metadata . attr ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Can we populate a value on the joined instance using the current?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b0c29b8094604a848d6d7f96eef6562c": {
        "code_string": "def get_engine():\n     global __ENGINE__\n     if __ENGINE__ is None:\n         cred = DB_CONNECTION.get(\"user\", \"\")\n         if cred:\n             if \"password\" in DB_CONNECTION:\n                 cred += \"{user}:{password}\".format(**DB_CONNECTION)\n             cred += \"@\"\n",
        "code_toks_joined": "def get_engine ( ) : <NEWLINE> <INDENT> global __ENGINE__ <NEWLINE> if __ENGINE__ is None : <NEWLINE> <INDENT> cred = DB_CONNECTION . get ( <STRING> , <STRING> ) <NEWLINE> if cred : <NEWLINE> <INDENT> if <STRING> in DB_CONNECTION : <NEWLINE> <INDENT> cred += <STRING> . format ( ** DB_CONNECTION ) <NEWLINE> <DEDENT> cred += <STRING> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"user\"",
                "\"\"",
                "\"password\"",
                "\"{user}:{password}\"",
                "\"@\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0abb9beda5a74b4c91aa67f4a0dabb13": {
        "code_string": "urlpath = None\n             path = path_from_link\n             try:\n                 urlpath = models.URLPath.get_by_path(path_from_link)\n                 path = urlpath.get_absolute_url()\n             except models.URLPath.DoesNotExist:\n                 pass\n         else:\n             urlpath = models.URLPath.objects.get(article=self.markdown.article)\n             source_components = urlpath.path.strip(\"/\").split(\"/\")\n             # We take the first (self.config['default_level'] - 1) components, so adding\n             # one more component would make a path of length self.config['default_level']\n             starting_level = max(0, self.config['default_level'][0] - 1 )\n             starting_path = \"/\".join(source_components[ : starting_level ])\n",
        "code_toks_joined": "urlpath = None <NEWLINE> <INDENT> path = path_from_link <NEWLINE> try : <NEWLINE> <INDENT> urlpath = models . URLPath . get_by_path ( path_from_link ) <NEWLINE> path = urlpath . get_absolute_url ( ) <NEWLINE> <DEDENT> except models . URLPath . DoesNotExist : <NEWLINE> <INDENT> pass <NEWLINE> else : <NEWLINE> <DEDENT> urlpath = models . URLPath . objects . get ( article = self . markdown . article ) <NEWLINE> source_components = urlpath . path . strip ( <STRING> ) . split ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> starting_level = max ( 0 , self . config [ <STRING> ] [ 0 ] - 1 ) <NEWLINE> starting_path = <STRING> . join ( source_components [ : starting_level ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"/\"",
                "\"/\"",
                "'default_level'",
                "\"/\""
            ],
            "<COMMENT>": [
                "# We take the first (self.config['default_level'] - 1) components, so adding",
                "# one more component would make a path of length self.config['default_level']"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eb6b013db6104f0587ede7bc406b0cc6": {
        "code_string": "if x < 0 or y > 8 or y < 0 or y > 8:\n \t\t\treturn\n",
        "code_toks_joined": "if x < 0 or y > 8 or y < 0 or y > 8 : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f2a41542d450431089e5b8e49d521d0e": {
        "code_string": "def get_alignment_error(self, ename):\n         align_error_conf = []\n         dx = self._get_config(ename, CONFIG_ALIGNMENT_DX, None)\n         if dx is not None:\n             _LOGGER.info(\"Alignment error: dx of {} is {} m.\".format(ename, dx))\n             align_error_conf.append(('dx', float(dx)))\n         dy = self._get_config(ename, CONFIG_ALIGNMENT_DY, None)\n         if dx is not None:\n             _LOGGER.info(\"Alignment error: dx of {} is {} m.\".format(ename, dx))\n             align_error_conf.append(('dy', float(dy)))\n         return align_error_conf\n",
        "code_toks_joined": "def get_alignment_error ( self , ename ) : <NEWLINE> <INDENT> align_error_conf = [ ] <NEWLINE> dx = self . _get_config ( ename , CONFIG_ALIGNMENT_DX , None ) <NEWLINE> if dx is not None : <NEWLINE> <INDENT> _LOGGER . info ( <STRING> . format ( ename , dx ) ) <NEWLINE> align_error_conf . append ( ( <STRING> , float ( dx ) ) ) <NEWLINE> <DEDENT> dy = self . _get_config ( ename , CONFIG_ALIGNMENT_DY , None ) <NEWLINE> if dx is not None : <NEWLINE> <INDENT> _LOGGER . info ( <STRING> . format ( ename , dx ) ) <NEWLINE> align_error_conf . append ( ( <STRING> , float ( dy ) ) ) <NEWLINE> <DEDENT> return align_error_conf <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Alignment error: dx of {} is {} m.\"",
                "'dx'",
                "\"Alignment error: dx of {} is {} m.\"",
                "'dy'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2fc9371852574a5aa727bb859cd9aa8d": {
        "code_string": "yield from includeme(self.config)\n             except Exception:\n                 log.exception('{} raise an exception'.format(includeme))\n",
        "code_toks_joined": "yield from includeme ( self . config ) <NEWLINE> <INDENT> except Exception : <NEWLINE> <INDENT> log . exception ( <STRING> . format ( includeme ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'{} raise an exception'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1ccbeb7c098243afb2990a2607daa44a": {
        "code_string": "def execute(self, args):\n         if not self.cfg.globals.enable_experimental:\n             raise exception.ExperimentalFeature(\"The 'get' command\")\n         if len(args) < 3:\n             self.parser.error(\"please specify a cluster, remote file or \" +\n                               \"directory, and a local destination path\")\n         ctag = args[0]\n         lpath = args[-1]\n         rpaths = args[1:-1]\n         cl = self.cm.get_cluster(ctag)\n         node = cl.get_node_by_alias(self.opts.node)\n         if self.opts.user:\n             node.ssh.switch_user(self.opts.user)\n         for rpath in rpaths:\n             if not glob.has_magic(rpath) and not node.ssh.path_exists(rpath):\n                 raise exception.BaseException(\n                     \"Remote file or directory does not exist: %s\" % lpath)\n         node.ssh.get(rpaths, lpath)\n",
        "code_toks_joined": "def execute ( self , args ) : <NEWLINE> <INDENT> if not self . cfg . globals . enable_experimental : <NEWLINE> <INDENT> raise exception . ExperimentalFeature ( <STRING> ) <NEWLINE> <DEDENT> if len ( args ) < 3 : <NEWLINE> <INDENT> self . parser . error ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> ctag = args [ 0 ] <NEWLINE> lpath = args [ - 1 ] <NEWLINE> rpaths = args [ 1 : - 1 ] <NEWLINE> cl = self . cm . get_cluster ( ctag ) <NEWLINE> node = cl . get_node_by_alias ( self . opts . node ) <NEWLINE> if self . opts . user : <NEWLINE> <INDENT> node . ssh . switch_user ( self . opts . user ) <NEWLINE> <DEDENT> for rpath in rpaths : <NEWLINE> <INDENT> if not glob . has_magic ( rpath ) and not node . ssh . path_exists ( rpath ) : <NEWLINE> <INDENT> raise exception . BaseException ( <NEWLINE> <INDENT> <STRING> % lpath ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> node . ssh . get ( rpaths , lpath ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"The 'get' command\"",
                "\"please specify a cluster, remote file or \"",
                "\"directory, and a local destination path\"",
                "\"Remote file or directory does not exist: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e63e14dbedf44cbeb019e9bd3267797b": {
        "code_string": "def execute_task(task_name, args):\n     task = retrieve_task(task_name)\n     subprocess.run([\"python3\", task_name] + args)\n",
        "code_toks_joined": "def execute_task ( task_name , args ) : <NEWLINE> <INDENT> task = retrieve_task ( task_name ) <NEWLINE> subprocess . run ( [ <STRING> , task_name ] + args ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"python3\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c4f9437e31904520a5bbb2d22c2e1f53": {
        "code_string": "@raise_for_error\n     def _request(self, asins, marketplaceid):\n         response = self.api.get_competitive_pricing_for_asin(asins, marketplaceid)\n         write_response(response, 'GetCompetitivePricingForAsinResponse.xml')\n         response.raise_for_status()\n         return response.content\n",
        "code_toks_joined": "@ raise_for_error <NEWLINE> <INDENT> def _request ( self , asins , marketplaceid ) : <NEWLINE> <INDENT> response = self . api . get_competitive_pricing_for_asin ( asins , marketplaceid ) <NEWLINE> write_response ( response , <STRING> ) <NEWLINE> response . raise_for_status ( ) <NEWLINE> return response . content <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'GetCompetitivePricingForAsinResponse.xml'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bcad78b9efc549e4868fe21643182612": {
        "code_string": "if context.get('show_context'):\n         out.info(pprint.pformat(context))\n         return\n",
        "code_toks_joined": "if context . get ( <STRING> ) : <NEWLINE> <INDENT> out . info ( pprint . pformat ( context ) ) <NEWLINE> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'show_context'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9c9f1e1162f146b8bfcb0ca0350d42e6": {
        "code_string": "def __ne__(self, other):\n         \"\"\"Return not equal boolean.\"\"\"\n         if not isinstance(other, Release):\n             return False\n",
        "code_toks_joined": "def __ne__ ( self , other ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not isinstance ( other , Release ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return not equal boolean.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "432c307667674ec6b90b4c7b46086da5": {
        "code_string": "class Meteor:\n     def __init__(self):\n         base_path = os.path.dirname(os.path.abspath(__file__))\n         jar_path = os.path.join(base_path, METEOR_JAR)\n         gz_path = os.path.join(base_path, os.path.basename(METEOR_GZ_URL))\n         if not os.path.isfile(jar_path):\n             if not os.path.isfile(jar_path):\n                 download_from_url(METEOR_GZ_URL, gz_path)\n             tar = tarfile.open(gz_path, \"r\")\n             tar.extractall(path=os.path.dirname(os.path.abspath(__file__)))\n             tar.close()\n             os.remove(gz_path)\n",
        "code_toks_joined": "class Meteor : <NEWLINE> <INDENT> def __init__ ( self ) : <NEWLINE> <INDENT> base_path = os . path . dirname ( os . path . abspath ( __file__ ) ) <NEWLINE> jar_path = os . path . join ( base_path , METEOR_JAR ) <NEWLINE> gz_path = os . path . join ( base_path , os . path . basename ( METEOR_GZ_URL ) ) <NEWLINE> if not os . path . isfile ( jar_path ) : <NEWLINE> <INDENT> if not os . path . isfile ( jar_path ) : <NEWLINE> <INDENT> download_from_url ( METEOR_GZ_URL , gz_path ) <NEWLINE> <DEDENT> tar = tarfile . open ( gz_path , <STRING> ) <NEWLINE> tar . extractall ( path = os . path . dirname ( os . path . abspath ( __file__ ) ) ) <NEWLINE> tar . close ( ) <NEWLINE> os . remove ( gz_path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"r\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "79ebef9165b341b58c2539d11b64164b": {
        "code_string": "curr_states_set.extend(curr_states(\n             state=nfa.state,\n             captured=None,\n             chars=(char, next_char)))\n",
        "code_toks_joined": "curr_states_set . extend ( curr_states ( <NEWLINE> <INDENT> state = nfa . state , <NEWLINE> captured = None , <NEWLINE> chars = ( char , next_char ) ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "063da81fc38945dc81a72944d067f922": {
        "code_string": "if Wallet.is_seed(text3):\n                     wallet.add_cosigner_seed(text3, \"x3/\", password)\n                 elif Wallet.is_xpub(text3):\n                     wallet.add_master_public_key(\"x3/\", text2)\n",
        "code_toks_joined": "if Wallet . is_seed ( text3 ) : <NEWLINE> <INDENT> wallet . add_cosigner_seed ( text3 , <STRING> , password ) <NEWLINE> elif Wallet . is_xpub ( text3 ) : <NEWLINE> wallet . add_master_public_key ( <STRING> , text2 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"x3/\"",
                "\"x3/\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c4ec3755569646f580b050e26c2a98a7": {
        "code_string": "def set_parameters(self, host, port, protocol, proxy, auto_connect):\n         proxy_str = interface.serialize_proxy(proxy)\n         server_str = ':'.join([ host, port, protocol ])\n         self.config.set_key('auto_cycle', auto_connect, True)\n         self.config.set_key(\"proxy\", proxy_str, True)\n         self.config.set_key(\"server\", server_str, True)\n         # abort if changes were not allowed by config\n         if self.config.get('server') != server_str or self.config.get('proxy') != proxy:\n             return\n",
        "code_toks_joined": "def set_parameters ( self , host , port , protocol , proxy , auto_connect ) : <NEWLINE> <INDENT> proxy_str = interface . serialize_proxy ( proxy ) <NEWLINE> server_str = <STRING> . join ( [ host , port , protocol ] ) <NEWLINE> self . config . set_key ( <STRING> , auto_connect , True ) <NEWLINE> self . config . set_key ( <STRING> , proxy_str , True ) <NEWLINE> self . config . set_key ( <STRING> , server_str , True ) <NEWLINE> <COMMENT> <NL> if self . config . get ( <STRING> ) != server_str or self . config . get ( <STRING> ) != proxy : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "':'",
                "'auto_cycle'",
                "\"proxy\"",
                "\"server\"",
                "'server'",
                "'proxy'"
            ],
            "<COMMENT>": [
                "# abort if changes were not allowed by config"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2f1cefd7bb3b41d290219fe3201185a2": {
        "code_string": "@hook\n     def transaction_dialog(self, d):\n         self.send_button = b = QPushButton(_(\"Send to cosigner\"))\n         b.clicked.connect(lambda: self.do_send(d.tx))\n         d.buttons.insert(2, b)\n         self.transaction_dialog_update(d)\n",
        "code_toks_joined": "@ hook <NEWLINE> <INDENT> def transaction_dialog ( self , d ) : <NEWLINE> <INDENT> self . send_button = b = QPushButton ( _ ( <STRING> ) ) <NEWLINE> b . clicked . connect ( lambda : self . do_send ( d . tx ) ) <NEWLINE> d . buttons . insert ( 2 , b ) <NEWLINE> self . transaction_dialog_update ( d ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Send to cosigner\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "891d3373573842a19efa18393b1aaad1": {
        "code_string": "if self.tx.is_complete():\n             if tx_hash in self.wallet.transactions.keys():\n                 desc = self.wallet.get_label(tx_hash)\n                 height, conf, timestamp = self.wallet.get_tx_height(tx_hash)\n                 if height > 0:\n                     if conf:\n                         status = _(\"%d confirmations\") % height\n                         time_str = datetime.datetime.fromtimestamp(timestamp).isoformat(' ')[:-3]\n                     else:\n                         status = _('Not verified')\n                 else:\n                     status = _('Unconfirmed')\n             else:\n                 status = _(\"Signed\")\n                 self.broadcast_button.show()\n                 # cannot broadcast when offline\n                 if self.main_window.network is None:\n                     self.broadcast_button.setEnabled(False)\n         else:\n             s, r = self.tx.signature_count()\n             status = _(\"Unsigned\") if s == 0 else _('Partially signed') + ' (%d/%d)'%(s,r)\n             tx_hash = _('Unknown');\n",
        "code_toks_joined": "if self . tx . is_complete ( ) : <NEWLINE> <INDENT> if tx_hash in self . wallet . transactions . keys ( ) : <NEWLINE> <INDENT> desc = self . wallet . get_label ( tx_hash ) <NEWLINE> height , conf , timestamp = self . wallet . get_tx_height ( tx_hash ) <NEWLINE> if height > 0 : <NEWLINE> <INDENT> if conf : <NEWLINE> <INDENT> status = _ ( <STRING> ) % height <NEWLINE> time_str = datetime . datetime . fromtimestamp ( timestamp ) . isoformat ( <STRING> ) [ : - 3 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> status = _ ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> status = _ ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> status = _ ( <STRING> ) <NEWLINE> self . broadcast_button . show ( ) <NEWLINE> <COMMENT> <NL> if self . main_window . network is None : <NEWLINE> <INDENT> self . broadcast_button . setEnabled ( False ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> s , r = self . tx . signature_count ( ) <NEWLINE> status = _ ( <STRING> ) if s == 0 else _ ( <STRING> ) + <STRING> % ( s , r ) <NEWLINE> tx_hash = _ ( <STRING> ) ; <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"%d confirmations\"",
                "' '",
                "'Not verified'",
                "'Unconfirmed'",
                "\"Signed\"",
                "\"Unsigned\"",
                "'Partially signed'",
                "' (%d/%d)'",
                "'Unknown'"
            ],
            "<COMMENT>": [
                "# cannot broadcast when offline"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3ad163344c634516b2057346e5f37251": {
        "code_string": "if len(addrs) == 1:\n             def show_address():\n                 keystore.thread.add(partial(self.show_address, wallet, keystore, addrs[0]))\n",
        "code_toks_joined": "if len ( addrs ) == 1 : <NEWLINE> <INDENT> def show_address ( ) : <NEWLINE> <INDENT> keystore . thread . add ( partial ( self . show_address , wallet , keystore , addrs [ 0 ] ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ceeaee0feb6144b89943223207dd5491": {
        "code_string": "lon_str_min = longitude.split()[1][:-1]\n                 lat_str_min = latitude.split()[1][:-1]\n                 # The old Reise has ',' as decimal seperator, replace it with '.'\n                 lon_str_min = lon_str_min.replace(',','.')\n                 lat_str_min = lon_str_min.replace(',','.')                \n                 # Convert to floats\n                 lon = SIGN_WEST * float(longitude.split()[0]) + float(lon_str_min)/60.\n                 lat = SIGN_NORTH * float(latitude.split()[0]) + float(lat_str_min)/60.\n",
        "code_toks_joined": "lon_str_min = longitude . split ( ) [ 1 ] [ : - 1 ] <NEWLINE> <INDENT> lat_str_min = latitude . split ( ) [ 1 ] [ : - 1 ] <NEWLINE> <COMMENT> <NL> lon_str_min = lon_str_min . replace ( <STRING> , <STRING> ) <NEWLINE> lat_str_min = lon_str_min . replace ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> lon = SIGN_WEST * float ( longitude . split ( ) [ 0 ] ) + float ( lon_str_min ) / 60. <NEWLINE> lat = SIGN_NORTH * float ( latitude . split ( ) [ 0 ] ) + float ( lat_str_min ) / 60. <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# The old Reise has ',' as decimal seperator, replace it with '.'",
                "# Convert to floats"
            ],
            "<STRING>": [
                "','",
                "'.'",
                "','",
                "'.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cc2d7b56c4a042768fee578ab0fd060c": {
        "code_string": "# Compute the important coordinates.\n     index_to_draw, _ = _helpers.get_max_index(bit_gate_rank,\n                                               operation=operation)\n     x_coord = _helpers.get_x_from_index(index_to_draw)\n     yq_coord = _helpers.get_y_from_quantum_register(qubits[0], bit_mapping)\n     yc_coord = _helpers.get_y_from_classical_register(total_clbits_number-1, total_qubits_number,\n                                                       bit_mapping)\n     # Then draw the double line representing the classical control.\n     _draw_classical_double_line(drawing, x_coord, yq_coord, x_coord, yc_coord)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> index_to_draw , _ = _helpers . get_max_index ( bit_gate_rank , <NEWLINE> <INDENT> operation = operation ) <NEWLINE> <DEDENT> x_coord = _helpers . get_x_from_index ( index_to_draw ) <NEWLINE> yq_coord = _helpers . get_y_from_quantum_register ( qubits [ 0 ] , bit_mapping ) <NEWLINE> yc_coord = _helpers . get_y_from_classical_register ( total_clbits_number - 1 , total_qubits_number , <NEWLINE> <INDENT> bit_mapping ) <NEWLINE> <COMMENT> <NL> <DEDENT> _draw_classical_double_line ( drawing , x_coord , yq_coord , x_coord , yc_coord ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Compute the important coordinates.",
                "# Then draw the double line representing the classical control."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "68e386ddceb24dce955c0ef14fed873f": {
        "code_string": "ret = np.ones_like(num_splits) * P_0\n         for ret_i, n in enumerate(num_splits):\n             P = np.zeros((n + 1))\n             P[0] = P_0\n             for i in range(n):\n                 h_p = H[ret_i] if i == (n - 1) else H[i + 1]\n                 if L[i] != 0:\n                     P[i + 1] = P[i] * \\\n                         (T[i] / (T[i] + L[i] * (h_p - H[i])))**(34.163 / L[i])\n                 else:\n                     P[i + 1] = P[i] * np.exp(-34.162 * (h_p - H[i]) / T[i])\n",
        "code_toks_joined": "ret = np . ones_like ( num_splits ) * P_0 <NEWLINE> <INDENT> for ret_i , n in enumerate ( num_splits ) : <NEWLINE> <INDENT> P = np . zeros ( ( n + 1 ) ) <NEWLINE> P [ 0 ] = P_0 <NEWLINE> for i in range ( n ) : <NEWLINE> <INDENT> h_p = H [ ret_i ] if i == ( n - 1 ) else H [ i + 1 ] <NEWLINE> if L [ i ] != 0 : <NEWLINE> <INDENT> P [ i + 1 ] = P [ i ] * ( T [ i ] / ( T [ i ] + L [ i ] * ( h_p - H [ i ] ) ) ) ** ( 34.163 / L [ i ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> P [ i + 1 ] = P [ i ] * np . exp ( - 34.162 * ( h_p - H [ i ] ) / T [ i ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "abd2b7b4928b4b7787a45dff967f7c46": {
        "code_string": "alignment = calculate_alignment(orient_tile)\n",
        "code_toks_joined": "alignment = calculate_alignment ( orient_tile ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b3eab1c49261401882970e5560165dd5": {
        "code_string": "dir_dict = {\n         \"shg_large\": os.path.join(base_dir, prep_dir, 'SHG_Large'),\n         \"mhr_large\": os.path.join(base_dir, prep_dir, 'MHR_Large'),\n         \"mhr_large_orient\": os.path.join(base_dir, prep_dir, 'MHR_Large_Orient'),\n         \"mlr_large\": os.path.join(base_dir, prep_dir, 'MLR_Large'),\n         \"mlr_large_orient\": os.path.join(base_dir, prep_dir, 'MLR_Large_Orient'),\n         \"he_small\": os.path.join(base_dir, prep_dir, 'HE_Small'),\n         \"he_large\": os.path.join(base_dir, prep_dir, 'HE_Large'),\n",
        "code_toks_joined": "dir_dict = { <NEWLINE> <INDENT> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <STRING> : os . path . join ( base_dir , prep_dir , <STRING> ) , <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"shg_large\"",
                "'SHG_Large'",
                "\"mhr_large\"",
                "'MHR_Large'",
                "\"mhr_large_orient\"",
                "'MHR_Large_Orient'",
                "\"mlr_large\"",
                "'MLR_Large'",
                "\"mlr_large_orient\"",
                "'MLR_Large_Orient'",
                "\"he_small\"",
                "'HE_Small'",
                "\"he_large\"",
                "'HE_Large'"
            ]
        },
        "err_obj": {
            "msg": "unbalanced (){}[]"
        }
    },
    "e3fdd75ab9c445b1b8ec26f2e4e412a3": {
        "code_string": "if downsample:\n                 fixed_shrunk = trans.resize_image(fixed_image, fixed_image.GetSpacing()[0], downsample_target)\n                 rotated_shrunk = trans.resize_image(rotated_image, moving_image.GetSpacing()[0], downsample_target)\n                 spacing = fixed_shrunk.GetSpacing()\n",
        "code_toks_joined": "if downsample : <NEWLINE> <INDENT> fixed_shrunk = trans . resize_image ( fixed_image , fixed_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> rotated_shrunk = trans . resize_image ( rotated_image , moving_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> spacing = fixed_shrunk . GetSpacing ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a99c18c5ce454e61b4de9987e88d55a3": {
        "code_string": "if roi_size is None:\n                 with open(output_path, 'w', newline='') as csvfile:\n                         print('\\nWriting average retardance file for {} at tile size {}'.format(\n                                 output_path.name, tile_size[0]))\n                         writer = csv.writer(csvfile)\n                         writer.writerow(['Mouse', 'Slide', 'Modality', 'Tile',\n                                          'Retardance', 'Orientation', 'Alignment'])\n",
        "code_toks_joined": "if roi_size is None : <NEWLINE> <INDENT> with open ( output_path , <STRING> , newline = <STRING> ) as csvfile : <NEWLINE> <INDENT> print ( <STRING> . format ( <NEWLINE> <INDENT> output_path . name , tile_size [ 0 ] ) ) <NEWLINE> <DEDENT> writer = csv . writer ( csvfile ) <NEWLINE> writer . writerow ( [ <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'w'",
                "''",
                "'\\nWriting average retardance file for {} at tile size {}'",
                "'Mouse'",
                "'Slide'",
                "'Modality'",
                "'Tile'",
                "'Retardance'",
                "'Orientation'",
                "'Alignment'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c9b8c4c5dc344bd4a32dd6cbbac5ef75": {
        "code_string": "iecsize = hdd.diskSize()\n \t\t# Harddisks > 1000 decimal Gigabytes are labelled in TB\n \t\tif iecsize > 1000000:\n \t\t\tiecsize = (iecsize + 50000) // float(100000) / 10\n \t\t\t# Omit decimal fraction if it is 0\n \t\t\tif (iecsize % 1 > 0):\n \t\t\t\tiecsize = \"%.1f TB\" % iecsize\n \t\t\telse:\n \t\t\t\tiecsize = \"%d TB\" % iecsize\n \t\t# Round harddisk sizes beyond ~300GB to full tens: 320, 500, 640, 750GB\n \t\telif iecsize > 300000:\n \t\t\tiecsize = \"%d GB\" % ((iecsize + 5000) // 10000 * 10)\n \t\t# ... be more precise for media < ~300GB (Sticks, SSDs, CF, MMC, ...): 1, 2, 4, 8, 16 ... 256GB\n \t\telif iecsize > 1000:\n \t\t\tiecsize = \"%d GB\" % ((iecsize + 500) // 1000)\n \t\telse:\n \t\t\tiecsize = \"%d MB\" % size\n",
        "code_toks_joined": "iecsize = hdd . diskSize ( ) <NEWLINE> <COMMENT> <NL> <INDENT> if iecsize > 1000000 : <NEWLINE> <INDENT> iecsize = ( iecsize + 50000 ) // float ( 100000 ) / 10 <NEWLINE> <COMMENT> <NL> if ( iecsize % 1 > 0 ) : <NEWLINE> <INDENT> iecsize = <STRING> % iecsize <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> iecsize = <STRING> % iecsize <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> elif iecsize > 300000 : <NEWLINE> <INDENT> iecsize = <STRING> % ( ( iecsize + 5000 ) // 10000 * 10 ) <NEWLINE> <COMMENT> <NL> <DEDENT> elif iecsize > 1000 : <NEWLINE> <INDENT> iecsize = <STRING> % ( ( iecsize + 500 ) // 1000 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> iecsize = <STRING> % size <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Harddisks > 1000 decimal Gigabytes are labelled in TB",
                "# Omit decimal fraction if it is 0",
                "# Round harddisk sizes beyond ~300GB to full tens: 320, 500, 640, 750GB",
                "# ... be more precise for media < ~300GB (Sticks, SSDs, CF, MMC, ...): 1, 2, 4, 8, 16 ... 256GB"
            ],
            "<STRING>": [
                "\"%.1f TB\"",
                "\"%d TB\"",
                "\"%d GB\"",
                "\"%d GB\"",
                "\"%d MB\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ba3c28a15a704336a046721bab94d01f": {
        "code_string": "if querytype != QUERYTYPE_LOOKUP__ID:\n             arglist = (service_reference, querytype, begin)\n         else:\n             arglist = (service_reference, querytype, begin, minutes)\n",
        "code_toks_joined": "if querytype != QUERYTYPE_LOOKUP__ID : <NEWLINE> <INDENT> arglist = ( service_reference , querytype , begin ) <NEWLINE> else : <NEWLINE> arglist = ( service_reference , querytype , begin , minutes ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2ad6c1fc08fe43d3a07936fb6568d024": {
        "code_string": "@classmethod\n     def isinstance(cls, instance):\n         if not isinstance(instance, cls):\n             return instance\n         else:\n             raise TypeError(\"'%s' is not an instance of '%s'\" % (instance, cls.__name__))\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def isinstance ( cls , instance ) : <NEWLINE> <INDENT> if not isinstance ( instance , cls ) : <NEWLINE> <INDENT> return instance <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise TypeError ( <STRING> % ( instance , cls . __name__ ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"'%s' is not an instance of '%s'\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b663726d1c54244a0d94a519f9fae3b": {
        "code_string": "rows = husoftm.connection.get_connection().query(['XKS00'], condition=\"KCE2IL='%s'\" % (int(iln), ))\n     if rows:\n         # stammadresse\n         return get_kunde(rows[0]['kundennr'])\n     else:\n         # abweichende Lieferadresse\n         rows = husoftm.connection.get_connection().query(['AVA00'], condition=\"VAILN='%s'\" % (int(iln), ))\n         if rows:\n             rows2 = husoftm.connection.get_connection().query(['XXA00'],\n                 condition=\"XASANR='%s'\" % (int(rows[0]['satznr']), ))\n             if rows:\n                 kunde = Kunde().fill_from_softm(rows2[0])\n                 kunde.kundennr = kunde.kundennr + ('/%03d' % int(rows[0]['versandadresssnr']))\n                 return kunde\n     raise ValueError(\"Keine Daten f\u00fcr GLN/ILN %r gefunden\" % iln)\n",
        "code_toks_joined": "rows = husoftm . connection . get_connection ( ) . query ( [ <STRING> ] , condition = <STRING> % ( int ( iln ) , ) ) <NEWLINE> <INDENT> if rows : <NEWLINE> <COMMENT> <NL> <INDENT> return get_kunde ( rows [ 0 ] [ <STRING> ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> rows = husoftm . connection . get_connection ( ) . query ( [ <STRING> ] , condition = <STRING> % ( int ( iln ) , ) ) <NEWLINE> if rows : <NEWLINE> <INDENT> rows2 = husoftm . connection . get_connection ( ) . query ( [ <STRING> ] , <NEWLINE> <INDENT> condition = <STRING> % ( int ( rows [ 0 ] [ <STRING> ] ) , ) ) <NEWLINE> <DEDENT> if rows : <NEWLINE> <INDENT> kunde = Kunde ( ) . fill_from_softm ( rows2 [ 0 ] ) <NEWLINE> kunde . kundennr = kunde . kundennr + ( <STRING> % int ( rows [ 0 ] [ <STRING> ] ) ) <NEWLINE> return kunde <NEWLINE> <DEDENT> <DEDENT> <DEDENT> raise ValueError ( <STRING> % iln ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'XKS00'",
                "\"KCE2IL='%s'\"",
                "'kundennr'",
                "'AVA00'",
                "\"VAILN='%s'\"",
                "'XXA00'",
                "\"XASANR='%s'\"",
                "'satznr'",
                "'/%03d'",
                "'versandadresssnr'",
                "\"Keine Daten f\u00fcr GLN/ILN %r gefunden\""
            ],
            "<COMMENT>": [
                "# stammadresse",
                "# abweichende Lieferadresse"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0dffaaee9b96448798fb01af219b3552": {
        "code_string": "# Assert that the service has been removed\n         assert svc not in self.services\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> assert svc not in self . services <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Assert that the service has been removed"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1e3264c13b3f4ef99434c20e75a7aa91": {
        "code_string": "# Update configuration\n         config.update(kwargs)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> config . update ( kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Update configuration"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f42a4aaf395346b89392856bd57e84e7": {
        "code_string": "def _add_memory_since_blocked(self, position):\n         memory_since_blocked = self.get_memory_since_blocked()\n         memory_since_blocked.append(position)\n         self._set_memory_since_blocked(position)\n",
        "code_toks_joined": "def _add_memory_since_blocked ( self , position ) : <NEWLINE> <INDENT> memory_since_blocked = self . get_memory_since_blocked ( ) <NEWLINE> memory_since_blocked . append ( position ) <NEWLINE> self . _set_memory_since_blocked ( position ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "43b3f53e1b324addb9028469832e1181": {
        "code_string": "def normalized(self):\n         return self.__class__(self._quantity / self._units._scale, self._units.base_units())\n",
        "code_toks_joined": "def normalized ( self ) : <NEWLINE> <INDENT> return self . __class__ ( self . _quantity / self . _units . _scale , self . _units . base_units ( ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "56f86ac8baa84e74ba8356c64dad6954": {
        "code_string": "config[\"form\"] = form or parse_kwarg(config, \"form\", context.get(\"form\"))\n     config[\"action\"] = config.get(\"action\")\n     config[\"compact\"] = config.get(\"compact\")\n     config[\"disabled\"] = config.get(\"disabled\")\n     config[\"label\"] = config.get(\"label\", label)\n     config[\"title\"] = config.get(\"title\")\n     config[\"subtitle\"] = config.get(\"subtitle\")\n     config[\"text\"] = config.get(\"text\")\n     config[\"urlize\"] = config.get(\"urlize\")\n     config[\"wysiwyg\"] = config.get(\"wysiwyg\")\n     config[\"status\"] = config.get(\"status\")\n     config[\"intro_status\"] = config.get(\"intro_status\")\n     config[\"tag\"] = config.get(\"tag\", \"form\")\n     config[\"actions\"] = parse_kwarg(kwargs, \"actions\", [])  # TODO: Default action\n     config[\"actions_align\"] = config.get(\"actions_align\", \"left\")\n     config[\"actions_position\"] = config.get(\"actions_position\", \"auto\")\n     config[\"help_text_position\"] = config.get(\"help_text_position\", settings.RH_HELP_TEXT_POSITION)\n",
        "code_toks_joined": "config [ <STRING> ] = form or parse_kwarg ( config , <STRING> , context . get ( <STRING> ) ) <NEWLINE> <INDENT> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , label ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , <STRING> ) <NEWLINE> config [ <STRING> ] = parse_kwarg ( kwargs , <STRING> , [ ] ) <COMMENT> <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , <STRING> ) <NEWLINE> config [ <STRING> ] = config . get ( <STRING> , settings . RH_HELP_TEXT_POSITION ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"form\"",
                "\"form\"",
                "\"form\"",
                "\"action\"",
                "\"action\"",
                "\"compact\"",
                "\"compact\"",
                "\"disabled\"",
                "\"disabled\"",
                "\"label\"",
                "\"label\"",
                "\"title\"",
                "\"title\"",
                "\"subtitle\"",
                "\"subtitle\"",
                "\"text\"",
                "\"text\"",
                "\"urlize\"",
                "\"urlize\"",
                "\"wysiwyg\"",
                "\"wysiwyg\"",
                "\"status\"",
                "\"status\"",
                "\"intro_status\"",
                "\"intro_status\"",
                "\"tag\"",
                "\"tag\"",
                "\"form\"",
                "\"actions\"",
                "\"actions\"",
                "\"actions_align\"",
                "\"actions_align\"",
                "\"left\"",
                "\"actions_position\"",
                "\"actions_position\"",
                "\"auto\"",
                "\"help_text_position\"",
                "\"help_text_position\""
            ],
            "<COMMENT>": [
                "# TODO: Default action"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d2a5bd7be58a434fa49621b2168c8566": {
        "code_string": "# TODO: Find a way to pass model class to its field handler class\n         model = dct.pop('model')\n         dct['restricted'], dct['related'] = set(), set()\n         for rel in dct.pop('has_one'):\n             if isinstance(rel, tuple):\n                 # 4-tuple relation supplied\n                 other, field, lkey, rkey = rel\n             else:\n                 # Just the related model supplied\n                 other = rel\n                 field, lkey, rkey = other.lower(), 'id', '%s_id' % model.lower()\n             dct[field] = HasOneDescriptor(other, lkey, rkey)\n             dct['related'].add(field)\n             index_registry.register(other, rkey)\n         for rel in dct.pop('belongs_to'):\n             if isinstance(rel, tuple):\n                 other, field, lkey, rkey = rel\n             else:\n                 other = rel\n                 field, lkey, rkey = other.lower(), '%s_id' % other.lower(), 'id'\n             dct[field] = BelongsToDescriptor(other, lkey, rkey)\n             dct['related'].add(field)\n             dct['restricted'].add(lkey)\n             index_registry.register(other, lkey)\n         for rel in dct.pop('has_many'):\n             if isinstance(rel, tuple):\n                 other, field, lkey, rkey = rel\n             else:\n                 other = rel\n                 field, lkey, rkey = tableize(other), 'id', '%s_id' % model.lower()\n             dct[field] = HasManyDescriptor(other, lkey, rkey)\n             dct['related'].add(field)\n             index_registry.register(other, rkey)\n         for rel in dct.pop('has_and_belongs_to_many'):\n             if isinstance(rel, tuple):\n                 other, field, lkey, rkey = rel\n             else:\n                 other = rel\n                 field, lkey, rkey = tableize(other), 'id', 'id'\n             join_model = '_' + ''.join(sorted([model, other]))\n             try:\n                 remodel.models.ModelBase(join_model, (remodel.models.Model,), {})\n             except AlreadyRegisteredError:\n                 # HABTM join_model model has been registered, probably from the\n                 # other end of the relation\n                 pass\n             mlkey, mrkey = '%s_id' % model.lower(), '%s_id' % other.lower()\n             dct[field] = HasAndBelongsToManyDescriptor(other, lkey, rkey, join_model, mlkey, mrkey)\n             dct['related'].add(field)\n             index_registry.register(join_model, mlkey)\n             index_registry.register(join_model, mrkey)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> model = dct . pop ( <STRING> ) <NEWLINE> dct [ <STRING> ] , dct [ <STRING> ] = set ( ) , set ( ) <NEWLINE> for rel in dct . pop ( <STRING> ) : <NEWLINE> <INDENT> if isinstance ( rel , tuple ) : <NEWLINE> <COMMENT> <NL> <INDENT> other , field , lkey , rkey = rel <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> other = rel <NEWLINE> field , lkey , rkey = other . lower ( ) , <STRING> , <STRING> % model . lower ( ) <NEWLINE> <DEDENT> dct [ field ] = HasOneDescriptor ( other , lkey , rkey ) <NEWLINE> dct [ <STRING> ] . add ( field ) <NEWLINE> index_registry . register ( other , rkey ) <NEWLINE> <DEDENT> for rel in dct . pop ( <STRING> ) : <NEWLINE> <INDENT> if isinstance ( rel , tuple ) : <NEWLINE> <INDENT> other , field , lkey , rkey = rel <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> other = rel <NEWLINE> field , lkey , rkey = other . lower ( ) , <STRING> % other . lower ( ) , <STRING> <NEWLINE> <DEDENT> dct [ field ] = BelongsToDescriptor ( other , lkey , rkey ) <NEWLINE> dct [ <STRING> ] . add ( field ) <NEWLINE> dct [ <STRING> ] . add ( lkey ) <NEWLINE> index_registry . register ( other , lkey ) <NEWLINE> <DEDENT> for rel in dct . pop ( <STRING> ) : <NEWLINE> <INDENT> if isinstance ( rel , tuple ) : <NEWLINE> <INDENT> other , field , lkey , rkey = rel <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> other = rel <NEWLINE> field , lkey , rkey = tableize ( other ) , <STRING> , <STRING> % model . lower ( ) <NEWLINE> <DEDENT> dct [ field ] = HasManyDescriptor ( other , lkey , rkey ) <NEWLINE> dct [ <STRING> ] . add ( field ) <NEWLINE> index_registry . register ( other , rkey ) <NEWLINE> <DEDENT> for rel in dct . pop ( <STRING> ) : <NEWLINE> <INDENT> if isinstance ( rel , tuple ) : <NEWLINE> <INDENT> other , field , lkey , rkey = rel <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> other = rel <NEWLINE> field , lkey , rkey = tableize ( other ) , <STRING> , <STRING> <NEWLINE> <DEDENT> join_model = <STRING> + <STRING> . join ( sorted ( [ model , other ] ) ) <NEWLINE> try : <NEWLINE> <INDENT> remodel . models . ModelBase ( join_model , ( remodel . models . Model , ) , { } ) <NEWLINE> <DEDENT> except AlreadyRegisteredError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> pass <NEWLINE> <DEDENT> mlkey , mrkey = <STRING> % model . lower ( ) , <STRING> % other . lower ( ) <NEWLINE> dct [ field ] = HasAndBelongsToManyDescriptor ( other , lkey , rkey , join_model , mlkey , mrkey ) <NEWLINE> dct [ <STRING> ] . add ( field ) <NEWLINE> index_registry . register ( join_model , mlkey ) <NEWLINE> index_registry . register ( join_model , mrkey ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# TODO: Find a way to pass model class to its field handler class",
                "# 4-tuple relation supplied",
                "# Just the related model supplied",
                "# HABTM join_model model has been registered, probably from the",
                "# other end of the relation"
            ],
            "<STRING>": [
                "'model'",
                "'restricted'",
                "'related'",
                "'has_one'",
                "'id'",
                "'%s_id'",
                "'related'",
                "'belongs_to'",
                "'%s_id'",
                "'id'",
                "'related'",
                "'restricted'",
                "'has_many'",
                "'id'",
                "'%s_id'",
                "'related'",
                "'has_and_belongs_to_many'",
                "'id'",
                "'id'",
                "'_'",
                "''",
                "'%s_id'",
                "'%s_id'",
                "'related'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5451a7b200974e98a2dfc49fd75d1ee8": {
        "code_string": "if num_items == 2:\n             if implied_state < 0:\n                 raise ValueError(\"No implied state\")\n             in_label, dest = arc_def\n             arc_simple = implied_state, in_label, in_label, dest, -1\n         elif num_items == 3:\n             if implied_state < 0:\n                 raise ValueError(\"No implied state\")\n             # in, out, target  (state num implied)\n             in_label, out_label, dest = arc_def\n             arc_simple = implied_state, in_label, out_label, dest, -1\n         elif num_items == 4:\n             # state num, in/out, target, final state\n             src, in_label, dest, is_final = arc_def\n             if is_final == 1:\n                 assert in_label == -1 or dest == -1\n             arc_simple = src, in_label, in_label, dest, is_final\n         elif num_items == 5:\n             arc_simple = arc_def  # type: ignore\n",
        "code_toks_joined": "if num_items == 2 : <NEWLINE> <INDENT> if implied_state < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> in_label , dest = arc_def <NEWLINE> arc_simple = implied_state , in_label , in_label , dest , - 1 <NEWLINE> elif num_items == 3 : <NEWLINE> if implied_state < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> in_label , out_label , dest = arc_def <NEWLINE> arc_simple = implied_state , in_label , out_label , dest , - 1 <NEWLINE> elif num_items == 4 : <NEWLINE> <COMMENT> <NL> src , in_label , dest , is_final = arc_def <NEWLINE> if is_final == 1 : <NEWLINE> <INDENT> assert in_label == - 1 or dest == - 1 <NEWLINE> <DEDENT> arc_simple = src , in_label , in_label , dest , is_final <NEWLINE> elif num_items == 5 : <NEWLINE> arc_simple = arc_def <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"No implied state\"",
                "\"No implied state\""
            ],
            "<COMMENT>": [
                "# in, out, target  (state num implied)",
                "# state num, in/out, target, final state",
                "# type: ignore"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "86af2ec0e70448aeb98951142c01b910": {
        "code_string": "# Gene analysis\n         self.mygene_df = self.my_gene_info()\n         self.mygene_df.to_csv(self.mygene_df, self.mygene_path)\n         # Accession file analysis\n         if self.__post_blast:\n             self.missing_dict = self.get_miss_acc()\n             self.missing_genes = self.missing_dict['genes']\n             self.missing_gene_count = self.missing_genes['count']\n             del self.missing_genes['count']\n             self.missing_organsims = self.missing_dict['organisms']\n             self.missing_organsims_count = self.missing_organsims['count']\n             del self.missing_organsims['count']\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> self . mygene_df = self . my_gene_info ( ) <NEWLINE> self . mygene_df . to_csv ( self . mygene_df , self . mygene_path ) <NEWLINE> <COMMENT> <NL> if self . __post_blast : <NEWLINE> <INDENT> self . missing_dict = self . get_miss_acc ( ) <NEWLINE> self . missing_genes = self . missing_dict [ <STRING> ] <NEWLINE> self . missing_gene_count = self . missing_genes [ <STRING> ] <NEWLINE> del self . missing_genes [ <STRING> ] <NEWLINE> self . missing_organsims = self . missing_dict [ <STRING> ] <NEWLINE> self . missing_organsims_count = self . missing_organsims [ <STRING> ] <NEWLINE> del self . missing_organsims [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Gene analysis",
                "# Accession file analysis"
            ],
            "<STRING>": [
                "'genes'",
                "'count'",
                "'count'",
                "'organisms'",
                "'count'",
                "'count'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "80d1b5acc1284668842aa545743e637f": {
        "code_string": "args = parser.parse_args(remaining_argv)\n",
        "code_toks_joined": "args = parser . parse_args ( remaining_argv ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5e7f74f277574fc4b27270b8c4189317": {
        "code_string": "xlim1 = ax1.get_xlim()\n     ylim1 = ax2.get_ylim()\n",
        "code_toks_joined": "xlim1 = ax1 . get_xlim ( ) <NEWLINE> <INDENT> ylim1 = ax2 . get_ylim ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2c387be687914807a07c26578e885e13": {
        "code_string": "#unlock project\n         try :            \n             if self._project_lock_file is not None :\n                 try :\n                     if not os.path.exists (self._project_lock_file)  :                        \n                         file = open (self._project_lock_file, \"rt\")\n                         data = file.readlines ()\n                         file.close ()\n                         for index in range (len (data)) :\n                             data[index] = data[index].strip ()\n                         if data == self._project_lock_signature :\n                             os.remove (self._project_lock_file)\n                             print (\"File %s was unlocked (removed).\" % self._project_lock_file)\n                         else:\n                             printWarrningMSG = False\n                             try:\n                                 if data[0] != self._project_lock_signature[0] :\n                                      printWarrningMSG = True\n                             except :\n                                 printWarrningMSG = True\n                             if printWarrningMSG : \n                                 print (\"File has changed, opened by another user, and was not removed.\")\n                                 print (\"Expected:\")\n                                 print (self._project_lock_signature)\n                                 print (\"Found:\")\n                                 print (data)\n                     return True        \n                 except:\n                     print (\"A error occured unlocking %s\" % self._project_lock_file)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> if self . _project_lock_file is not None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if not os . path . exists ( self . _project_lock_file ) : <NEWLINE> <INDENT> file = open ( self . _project_lock_file , <STRING> ) <NEWLINE> data = file . readlines ( ) <NEWLINE> file . close ( ) <NEWLINE> for index in range ( len ( data ) ) : <NEWLINE> <INDENT> data [ index ] = data [ index ] . strip ( ) <NEWLINE> <DEDENT> if data == self . _project_lock_signature : <NEWLINE> <INDENT> os . remove ( self . _project_lock_file ) <NEWLINE> print ( <STRING> % self . _project_lock_file ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> printWarrningMSG = False <NEWLINE> try : <NEWLINE> <INDENT> if data [ 0 ] != self . _project_lock_signature [ 0 ] : <NEWLINE> <INDENT> printWarrningMSG = True <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> printWarrningMSG = True <NEWLINE> <DEDENT> if printWarrningMSG : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( self . _project_lock_signature ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( data ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> print ( <STRING> % self . _project_lock_file ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#unlock project"
            ],
            "<STRING>": [
                "\"rt\"",
                "\"File %s was unlocked (removed).\"",
                "\"File has changed, opened by another user, and was not removed.\"",
                "\"Expected:\"",
                "\"Found:\"",
                "\"A error occured unlocking %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a48b27f293b94a958ca342fa0f9927f8": {
        "code_string": "def drawColorBox (self, qp, colorlist, xC, yC, xSize, ySize, DrawBorder):      \n         if xSize > 0 and ySize > 0 :\n             numberofColors = len (colorlist)                    \n             if numberofColors >= xSize :          \n                 yP = int (yC + ySize)            \n                 for i in range (xSize) :\n                     color = colorlist[i]\n                     qp.setBrush (color)\n                     qp.setPen (color)                \n                     xE = int (i + xC)\n                     qp.drawLine (xC, yP, xE, yP)  \n             else:                            \n                 rightP = int (xC)                 \n                 scaleFactor = float (xSize) / float (numberofColors)                \n                 rightPos = rightP + xSize -1\n                 for i in range (numberofColors) :\n                     leftP    = rightP\n                     rightP = int (xC +scaleFactor * float (i+1))                \n                     color = colorlist[i]\n                     qp.setBrush (color)\n                     qp.setPen (color)                \n                     if (leftP < rightPos) :\n                         qp.drawRect (leftP, yC, max (rightP - leftP, 1), ySize-1)                \n             if DrawBorder :\n                 qp.setPen ( QtGui.QColor(0,0,0))\n                 qp.setBrush ( QtGui.QColor(0,0,0, 0))\n                 qp.drawRect (xC, yC, xSize-1, ySize-1)                                                              \n",
        "code_toks_joined": "def drawColorBox ( self , qp , colorlist , xC , yC , xSize , ySize , DrawBorder ) : <NEWLINE> <INDENT> if xSize > 0 and ySize > 0 : <NEWLINE> <INDENT> numberofColors = len ( colorlist ) <NEWLINE> if numberofColors >= xSize : <NEWLINE> <INDENT> yP = int ( yC + ySize ) <NEWLINE> for i in range ( xSize ) : <NEWLINE> <INDENT> color = colorlist [ i ] <NEWLINE> qp . setBrush ( color ) <NEWLINE> qp . setPen ( color ) <NEWLINE> xE = int ( i + xC ) <NEWLINE> qp . drawLine ( xC , yP , xE , yP ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> rightP = int ( xC ) <NEWLINE> scaleFactor = float ( xSize ) / float ( numberofColors ) <NEWLINE> rightPos = rightP + xSize - 1 <NEWLINE> for i in range ( numberofColors ) : <NEWLINE> <INDENT> leftP = rightP <NEWLINE> rightP = int ( xC + scaleFactor * float ( i + 1 ) ) <NEWLINE> color = colorlist [ i ] <NEWLINE> qp . setBrush ( color ) <NEWLINE> qp . setPen ( color ) <NEWLINE> if ( leftP < rightPos ) : <NEWLINE> <INDENT> qp . drawRect ( leftP , yC , max ( rightP - leftP , 1 ) , ySize - 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if DrawBorder : <NEWLINE> <INDENT> qp . setPen ( QtGui . QColor ( 0 , 0 , 0 ) ) <NEWLINE> qp . setBrush ( QtGui . QColor ( 0 , 0 , 0 , 0 ) ) <NEWLINE> qp . drawRect ( xC , yC , xSize - 1 , ySize - 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "01caac2905014c5ead8487d76ef3f90d": {
        "code_string": "if longFileNameWritePath is None :\n                 CreateWriteDirPathFromShortFilePath = True\n             else:            \n                 writepathdir, _ = os.path.split (longFileNameWritePath)                \n                 if platform.system() == \"Windows\" :\n                     if len (writepathdir) > 200 :\n                         writepathdir, _ = os.path.split (writepath)                                                \n                 FileUtil.createPath (writepathdir, False)\n                 CreateWriteDirPathFromShortFilePath = not  os.path.isdir (writepath)                         \n             if CreateWriteDirPathFromShortFilePath :\n                 writepathdir, _ = os.path.split (writepath)\n                 FileUtil.createPath (writepathdir, False)   \n",
        "code_toks_joined": "if longFileNameWritePath is None : <NEWLINE> <INDENT> CreateWriteDirPathFromShortFilePath = True <NEWLINE> else : <NEWLINE> writepathdir , _ = os . path . split ( longFileNameWritePath ) <NEWLINE> if platform . system ( ) == <STRING> : <NEWLINE> <INDENT> if len ( writepathdir ) > 200 : <NEWLINE> <INDENT> writepathdir , _ = os . path . split ( writepath ) <NEWLINE> <DEDENT> <DEDENT> FileUtil . createPath ( writepathdir , False ) <NEWLINE> CreateWriteDirPathFromShortFilePath = not os . path . isdir ( writepath ) <NEWLINE> if CreateWriteDirPathFromShortFilePath : <NEWLINE> writepathdir , _ = os . path . split ( writepath ) <NEWLINE> FileUtil . createPath ( writepathdir , False ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Windows\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2ffbc393e18d46b0b23d971f2d9effbf": {
        "code_string": "def _childNodeROIIndicatorChanged (self, val, AquireFileLock = True) :            \n             if  not self._childNodeROIIndicatorChanged_called :\n                 try :\n                     self._childNodeROIIndicatorChanged_called = True\n                     if self.isDelayedCacheLoadingSet ()  :\n                         if not val :\n                             return\n                         elif self._ROIDatasetIndicatorCache != True :\n                             self._ROIDatasetIndicatorCache = val\n                             tree = self._tree\n                             if (tree is not None) :\n                                 tree._ProjectDataset.getProjectFileInterface().setProjectDatasetCache (self.getTreePath (), val, Key=\"ContoursExist\")\n                             self.callChildNodeROIIndicatorChanged (val, AquireFileLock = AquireFileLock)\n                             self.fireTreeNodeChangedEvent ()\n                             return\n                     currentValue = self.getROIDatasetIndicator ()        \n                     if val is not None and val :                \n                         if val != currentValue :\n                             self.setROIDatasetIndicator (val)\n                     elif val != currentValue :\n                        found = False\n                        lst = self.getChildernLst ()\n                        if len (lst) > 0 :\n                            for child in lst :\n                                if (not child.getROIDatasetIndicator ()) :\n                                    found = True \n                                    break\n                            if found != currentValue or val is not None:\n                                self.setROIDatasetIndicator (found)\n                 finally:\n                     self._childNodeROIIndicatorChanged_called = False\n",
        "code_toks_joined": "def _childNodeROIIndicatorChanged ( self , val , AquireFileLock = True ) : <NEWLINE> <INDENT> if not self . _childNodeROIIndicatorChanged_called : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . _childNodeROIIndicatorChanged_called = True <NEWLINE> if self . isDelayedCacheLoadingSet ( ) : <NEWLINE> <INDENT> if not val : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> elif self . _ROIDatasetIndicatorCache != True : <NEWLINE> <INDENT> self . _ROIDatasetIndicatorCache = val <NEWLINE> tree = self . _tree <NEWLINE> if ( tree is not None ) : <NEWLINE> <INDENT> tree . _ProjectDataset . getProjectFileInterface ( ) . setProjectDatasetCache ( self . getTreePath ( ) , val , Key = <STRING> ) <NEWLINE> <DEDENT> self . callChildNodeROIIndicatorChanged ( val , AquireFileLock = AquireFileLock ) <NEWLINE> self . fireTreeNodeChangedEvent ( ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT> currentValue = self . getROIDatasetIndicator ( ) <NEWLINE> if val is not None and val : <NEWLINE> <INDENT> if val != currentValue : <NEWLINE> <INDENT> self . setROIDatasetIndicator ( val ) <NEWLINE> <DEDENT> <DEDENT> elif val != currentValue : <NEWLINE> <INDENT> found = False <NEWLINE> lst = self . getChildernLst ( ) <NEWLINE> if len ( lst ) > 0 : <NEWLINE> <INDENT> for child in lst : <NEWLINE> <INDENT> if ( not child . getROIDatasetIndicator ( ) ) : <NEWLINE> <INDENT> found = True <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> if found != currentValue or val is not None : <NEWLINE> <INDENT> self . setROIDatasetIndicator ( found ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> self . _childNodeROIIndicatorChanged_called = False <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"ContoursExist\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e2b3f2894bf14463ad6900392a05cdda": {
        "code_string": "def removeAllInternalTags (self, SafeNameSet = None ) :        \n         if SafeNameSet is not None :\n             if isinstance (SafeNameSet, set) :                \n                 SafeNameSet = set (SafeNameSet)\n         else:\n             SafeNameSet = set ()                        \n         self._clearTagCache ()        \n         datasetLength = len (self._dataSetTags)\n         for index in range (datasetLength-1, -1, -1):\n             tag = self._dataSetTags[index]\n             if tag.isInternalTag () :    \n                 if tag.getName () not in SafeNameSet :\n                     self._recycleID (tag.getID ())\n                     del self._dataSetTags[index]           \n                     del tag\n         if datasetLength != len (self._dataSetTags) :\n             self.callParameterChangeListener ()\n",
        "code_toks_joined": "def removeAllInternalTags ( self , SafeNameSet = None ) : <NEWLINE> <INDENT> if SafeNameSet is not None : <NEWLINE> <INDENT> if isinstance ( SafeNameSet , set ) : <NEWLINE> <INDENT> SafeNameSet = set ( SafeNameSet ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> SafeNameSet = set ( ) <NEWLINE> <DEDENT> self . _clearTagCache ( ) <NEWLINE> datasetLength = len ( self . _dataSetTags ) <NEWLINE> for index in range ( datasetLength - 1 , - 1 , - 1 ) : <NEWLINE> <INDENT> tag = self . _dataSetTags [ index ] <NEWLINE> if tag . isInternalTag ( ) : <NEWLINE> <INDENT> if tag . getName ( ) not in SafeNameSet : <NEWLINE> <INDENT> self . _recycleID ( tag . getID ( ) ) <NEWLINE> del self . _dataSetTags [ index ] <NEWLINE> del tag <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if datasetLength != len ( self . _dataSetTags ) : <NEWLINE> <INDENT> self . callParameterChangeListener ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "94492f192427467390413fed48e2963b": {
        "code_string": "if (rebuildList) :\n             lst.clear ()\n             for name in namelist: \n                     item = QListWidgetItem() \n                     item.setText (name) \n                     if (name == \"None\") :\n                         color = QtGui.QColor (0,0,0)\n                     else:\n                         color  = roiDefs.getROIColor (name)                \n                     item.setForeground (color)\n                     if ROIDictionary is not None and ROIDictionary.isROIDefined (txt) :\n                         item.setBackground (QtGui.QColor (255,211,82))\n                     else :\n                         item.setBackground (QtGui.QColor (255,255,255))\n                     item.setSelected (name in selectedNameList)\n                     lst.addItem (item)  \n         #if (len (selectedNameList) == 0) :\n         #    self.ui.RemoveROIBtn.setEnabled(False)    # set to true in dictionary change event       \n         if (listSelectionChangeListenerConnected) :\n             self.ui.ROI_List.selectionModel().selectionChanged.connect(self.ROIListSelectionChanged)        \n",
        "code_toks_joined": "if ( rebuildList ) : <NEWLINE> <INDENT> lst . clear ( ) <NEWLINE> for name in namelist : <NEWLINE> <INDENT> item = QListWidgetItem ( ) <NEWLINE> item . setText ( name ) <NEWLINE> if ( name == <STRING> ) : <NEWLINE> <INDENT> color = QtGui . QColor ( 0 , 0 , 0 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> color = roiDefs . getROIColor ( name ) <NEWLINE> <DEDENT> item . setForeground ( color ) <NEWLINE> if ROIDictionary is not None and ROIDictionary . isROIDefined ( txt ) : <NEWLINE> <INDENT> item . setBackground ( QtGui . QColor ( 255 , 211 , 82 ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> item . setBackground ( QtGui . QColor ( 255 , 255 , 255 ) ) <NEWLINE> <DEDENT> item . setSelected ( name in selectedNameList ) <NEWLINE> lst . addItem ( item ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if ( listSelectionChangeListenerConnected ) : <NEWLINE> <DEDENT> self . ui . ROI_List . selectionModel ( ) . selectionChanged . connect ( self . ROIListSelectionChanged ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"None\""
            ],
            "<COMMENT>": [
                "#if (len (selectedNameList) == 0) :",
                "#    self.ui.RemoveROIBtn.setEnabled(False)    # set to true in dictionary change event       "
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b3a3c65fe7e44ebab1f238d5869a598c": {
        "code_string": "kModel = Custom_Objects[\"CustomModelLoader\"](model_path, LoadLinearModel = True)                                        \n",
        "code_toks_joined": "kModel = Custom_Objects [ <STRING> ] ( model_path , LoadLinearModel = True ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "\"CustomModelLoader\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "854f45a952d84c0f8e60e56df1be4bef": {
        "code_string": "def get_ready_buffers(self):\n         ready = {}\n         current_time = time()\n         with self.lock:\n             for ts, delayed_to in list(self.delays.items()):\n                 if delayed_to > current_time:\n                     del self.delays[ts]\n                     ready[ts] = self.buffers.pop(ts)\n         return ready\n",
        "code_toks_joined": "def get_ready_buffers ( self ) : <NEWLINE> <INDENT> ready = { } <NEWLINE> current_time = time ( ) <NEWLINE> with self . lock : <NEWLINE> <INDENT> for ts , delayed_to in list ( self . delays . items ( ) ) : <NEWLINE> <INDENT> if delayed_to > current_time : <NEWLINE> <INDENT> del self . delays [ ts ] <NEWLINE> ready [ ts ] = self . buffers . pop ( ts ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return ready <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6bdc32c25d3243b1bded7dd7c6f73060": {
        "code_string": "assert result not in tree\n     assert is_left_subtree_less_than_right_subtree(result)\n",
        "code_toks_joined": "assert result not in tree <NEWLINE> <INDENT> assert is_left_subtree_less_than_right_subtree ( result ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0f9b8107196646c2abf1edec8075934d": {
        "code_string": "assert len(tree) > 0\n     assert to_height(tree) > 0\n     assert is_left_subtree_less_than_right_subtree(tree)\n",
        "code_toks_joined": "assert len ( tree ) > 0 <NEWLINE> <INDENT> assert to_height ( tree ) > 0 <NEWLINE> assert is_left_subtree_less_than_right_subtree ( tree ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3de3fb6aa76c4921bff8e9de52c60946": {
        "code_string": "counter.subtract({tok: counter[tok] for tok in ['<unk>'] + specials})\n         max_size = None if max_size is None else max_size - len(self.itos)\n",
        "code_toks_joined": "counter . subtract ( { tok : counter [ tok ] for tok in [ <STRING> ] + specials } ) <NEWLINE> <INDENT> max_size = None if max_size is None else max_size - len ( self . itos ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'<unk>'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5e50eb10e4524876983ae3eeddd0978a": {
        "code_string": "def set_vectors(self, stoi, vectors, dim, unk_init=torch.Tensor.zero_):\n         self.vectors = torch.Tensor(len(self), dim)\n         for i, token in enumerate(self.itos):\n             wv_index = stoi.get(token, None)\n             if wv_index is None:\n                 self.vectors[i] = vectors[wv_index]\n             else:\n                 self.vectors[i] = unk_init(self.vectors[i])\n",
        "code_toks_joined": "def set_vectors ( self , stoi , vectors , dim , unk_init = torch . Tensor . zero_ ) : <NEWLINE> <INDENT> self . vectors = torch . Tensor ( len ( self ) , dim ) <NEWLINE> for i , token in enumerate ( self . itos ) : <NEWLINE> <INDENT> wv_index = stoi . get ( token , None ) <NEWLINE> if wv_index is None : <NEWLINE> <INDENT> self . vectors [ i ] = vectors [ wv_index ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . vectors [ i ] = unk_init ( self . vectors [ i ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "209193c91b74400a9d50f5dfadc943d5": {
        "code_string": "for partition in self._client.get_partition_ids_for_topic(arg):\n                     self._consume_topic_partition(topic, partition)\n",
        "code_toks_joined": "for partition in self . _client . get_partition_ids_for_topic ( arg ) : <NEWLINE> <INDENT> self . _consume_topic_partition ( topic , partition ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "03a1aed84dd04f67b6cbd0bec3bc167f": {
        "code_string": "def get_nat_type(s, source_ip, source_port, stun_host=None, stun_port=3478):\n     _initialize()\n     port = stun_port\n     log.debug(\"Do Test1\")\n     resp = False\n     if stun_host:\n         ret = stun_test(s, stun_host, port, source_ip, source_port)\n         resp = ret['Resp']\n     else:\n         for stun_host in stun_servers_list:\n             log.debug('Trying STUN host: %s', stun_host)\n             ret = stun_test(s, stun_host, port, source_ip, source_port)\n             resp = ret['Resp']\n             if resp:\n                 break\n     if not resp:\n         return Blocked, ret\n     log.debug(\"Result: %s\", ret)\n     exIP = ret['ExternalIP']\n     exPort = ret['ExternalPort']\n     changedIP = ret['ChangedIP']\n     changedPort = ret['ChangedPort']\n     if ret['ExternalIP'] == source_ip:\n         changeRequest = ''.join([ChangeRequest, '0004', \"00000006\"])\n         ret = stun_test(s, stun_host, port, source_ip, source_port,\n                         changeRequest)\n         if ret['Resp']:\n             typ = OpenInternet\n         else:\n             typ = SymmetricUDPFirewall\n     else:\n         changeRequest = ''.join([ChangeRequest, '0004', \"00000006\"])\n         log.debug(\"Do Test2\")\n         ret = stun_test(s, stun_host, port, source_ip, source_port,\n                         changeRequest)\n         log.debug(\"Result: %s\", ret)\n         if ret['Resp']:\n             typ = FullCone\n         else:\n             log.debug(\"Do Test1\")\n             ret = stun_test(s, changedIP, changedPort, source_ip, source_port)\n             log.debug(\"Result: %s\", ret)\n             if not ret['Resp']:\n                 typ = ChangedAddressError\n             else:\n                 if exIP == ret['ExternalIP'] and exPort == ret['ExternalPort']:\n                     changePortRequest = ''.join([ChangeRequest, '0004',\n                                                  \"00000002\"])\n                     log.debug(\"Do Test3\")\n                     ret = stun_test(s, changedIP, port, source_ip, source_port,\n                                     changePortRequest)\n                     log.debug(\"Result: %s\", ret)\n                     if ret['Resp']:\n                         typ = RestricNAT\n                     else:\n                         typ = RestricPortNAT\n                 else:\n                     typ = SymmetricNAT\n     return typ, ret\n",
        "code_toks_joined": "def get_nat_type ( s , source_ip , source_port , stun_host = None , stun_port = 3478 ) : <NEWLINE> <INDENT> _initialize ( ) <NEWLINE> port = stun_port <NEWLINE> log . debug ( <STRING> ) <NEWLINE> resp = False <NEWLINE> if stun_host : <NEWLINE> <INDENT> ret = stun_test ( s , stun_host , port , source_ip , source_port ) <NEWLINE> resp = ret [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for stun_host in stun_servers_list : <NEWLINE> <INDENT> log . debug ( <STRING> , stun_host ) <NEWLINE> ret = stun_test ( s , stun_host , port , source_ip , source_port ) <NEWLINE> resp = ret [ <STRING> ] <NEWLINE> if resp : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if not resp : <NEWLINE> <INDENT> return Blocked , ret <NEWLINE> <DEDENT> log . debug ( <STRING> , ret ) <NEWLINE> exIP = ret [ <STRING> ] <NEWLINE> exPort = ret [ <STRING> ] <NEWLINE> changedIP = ret [ <STRING> ] <NEWLINE> changedPort = ret [ <STRING> ] <NEWLINE> if ret [ <STRING> ] == source_ip : <NEWLINE> <INDENT> changeRequest = <STRING> . join ( [ ChangeRequest , <STRING> , <STRING> ] ) <NEWLINE> ret = stun_test ( s , stun_host , port , source_ip , source_port , <NEWLINE> <INDENT> changeRequest ) <NEWLINE> <DEDENT> if ret [ <STRING> ] : <NEWLINE> <INDENT> typ = OpenInternet <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> typ = SymmetricUDPFirewall <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> changeRequest = <STRING> . join ( [ ChangeRequest , <STRING> , <STRING> ] ) <NEWLINE> log . debug ( <STRING> ) <NEWLINE> ret = stun_test ( s , stun_host , port , source_ip , source_port , <NEWLINE> <INDENT> changeRequest ) <NEWLINE> <DEDENT> log . debug ( <STRING> , ret ) <NEWLINE> if ret [ <STRING> ] : <NEWLINE> <INDENT> typ = FullCone <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . debug ( <STRING> ) <NEWLINE> ret = stun_test ( s , changedIP , changedPort , source_ip , source_port ) <NEWLINE> log . debug ( <STRING> , ret ) <NEWLINE> if not ret [ <STRING> ] : <NEWLINE> <INDENT> typ = ChangedAddressError <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if exIP == ret [ <STRING> ] and exPort == ret [ <STRING> ] : <NEWLINE> <INDENT> changePortRequest = <STRING> . join ( [ ChangeRequest , <STRING> , <NEWLINE> <INDENT> <STRING> ] ) <NEWLINE> <DEDENT> log . debug ( <STRING> ) <NEWLINE> ret = stun_test ( s , changedIP , port , source_ip , source_port , <NEWLINE> <INDENT> changePortRequest ) <NEWLINE> <DEDENT> log . debug ( <STRING> , ret ) <NEWLINE> if ret [ <STRING> ] : <NEWLINE> <INDENT> typ = RestricNAT <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> typ = RestricPortNAT <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> typ = SymmetricNAT <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return typ , ret <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Do Test1\"",
                "'Resp'",
                "'Trying STUN host: %s'",
                "'Resp'",
                "\"Result: %s\"",
                "'ExternalIP'",
                "'ExternalPort'",
                "'ChangedIP'",
                "'ChangedPort'",
                "'ExternalIP'",
                "''",
                "'0004'",
                "\"00000006\"",
                "'Resp'",
                "''",
                "'0004'",
                "\"00000006\"",
                "\"Do Test2\"",
                "\"Result: %s\"",
                "'Resp'",
                "\"Do Test1\"",
                "\"Result: %s\"",
                "'Resp'",
                "'ExternalIP'",
                "'ExternalPort'",
                "''",
                "'0004'",
                "\"00000002\"",
                "\"Do Test3\"",
                "\"Result: %s\"",
                "'Resp'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "44b4811ad82d4576a4e01727adfa5af9": {
        "code_string": "if __name__==\"__main__\":\n     if QtCore.QCoreApplication.instance() is not None:\n         app = QtGui.QApplication([])\n     test_all()\n     #f=test_AnchoredPlotItem()\n     #f=test_ColorBarItem_auto()\n",
        "code_toks_joined": "if __name__ == <STRING> : <NEWLINE> <INDENT> if QtCore . QCoreApplication . instance ( ) is not None : <NEWLINE> <INDENT> app = QtGui . QApplication ( [ ] ) <NEWLINE> <DEDENT> test_all ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"__main__\""
            ],
            "<COMMENT>": [
                "#f=test_AnchoredPlotItem()",
                "#f=test_ColorBarItem_auto()"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "16358f282d274b8baa140e73e7933ff0": {
        "code_string": "class Frame(Environment):\n     def __init__(self, title, subtitle='', ncols=0):\n         super().__init__()\n         self.content_separator = '\\n'\n         self.append(\n             pl.NoEscape(r'\\frametitle{')+\n             pl.escape_latex(title)+\n             pl.NoEscape('}')\n         )\n         if subtitle:\n             self.append(\n                 pl.NoEscape(r'\\framesubtitle{')+\n                 pl.escape_latex(title)+\n                 pl.NoEscape('}')\n             )\n         if ncols:\n             self.add_columns(ncols = ncols)\n",
        "code_toks_joined": "class Frame ( Environment ) : <NEWLINE> <INDENT> def __init__ ( self , title , subtitle = <STRING> , ncols = 0 ) : <NEWLINE> <INDENT> super ( ) . __init__ ( ) <NEWLINE> self . content_separator = <STRING> <NEWLINE> self . append ( <NEWLINE> <INDENT> pl . NoEscape ( <STRING> ) + <NEWLINE> pl . escape_latex ( title ) + <NEWLINE> pl . NoEscape ( <STRING> ) <NEWLINE> <DEDENT> ) <NEWLINE> if subtitle : <NEWLINE> <INDENT> self . append ( <NEWLINE> <INDENT> pl . NoEscape ( <STRING> ) + <NEWLINE> pl . escape_latex ( title ) + <NEWLINE> pl . NoEscape ( <STRING> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> if ncols : <NEWLINE> <INDENT> self . add_columns ( ncols = ncols ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''",
                "'\\n'",
                "r'\\frametitle{'",
                "'}'",
                "r'\\framesubtitle{'",
                "'}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f95a87aa0170499ca60fc8e89998c4ba": {
        "code_string": "def get_running_plants(self, plants):\n         for plant in plants:\n             if plant.construction_year<=1990 and plant.name == \"invested_plant\":\n                 # Reset old plants that have been modernised with new construction year\n                 plant.construction_year = randint(self.year_number-15, self.year_number)\n                 yield plant\n             elif plant.construction_year + plant.operating_period + plant.construction_period + plant.pre_dev_period >= self.year_number:\n                 yield plant\n             else:\n                 logger.info(\"Taking the plant '{}' out of service, year of construction: {}\".format(plant.name,\n                                                                         plant.construction_year))\n                 continue\n",
        "code_toks_joined": "def get_running_plants ( self , plants ) : <NEWLINE> <INDENT> for plant in plants : <NEWLINE> <INDENT> if plant . construction_year <= 1990 and plant . name == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> plant . construction_year = randint ( self . year_number - 15 , self . year_number ) <NEWLINE> yield plant <NEWLINE> <DEDENT> elif plant . construction_year + plant . operating_period + plant . construction_period + plant . pre_dev_period >= self . year_number : <NEWLINE> <INDENT> yield plant <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . info ( <STRING> . format ( plant . name , <NEWLINE> <INDENT> plant . construction_year ) ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"invested_plant\"",
                "\"Taking the plant '{}' out of service, year of construction: {}\""
            ],
            "<COMMENT>": [
                "# Reset old plants that have been modernised with new construction year"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0d07ee17a1224252ba450e8adc6edbde": {
        "code_string": "def invest(self):\n         lowest_upfront_cost = 0\n         total_upfront_cost = 0\n         counter =0\n         total_capacity = 0\n         while self.money > lowest_upfront_cost and total_capacity < 1500:\n             counter += 1\n             # if counter>3:\n             #     break\n             # potential_plant_data = npv_calculation.get_positive_npv_plants_list()\n             potential_plant_data = get_most_profitable_plants_by_npv(self.model, self.difference_in_discount_rate,\n                                                                      self.look_back_period)\n             if counter == 1:\n                 potential_plant_list = []\n                 for plant_data in potential_plant_data:\n                     if not potential_plant_data:\n                         break\n                     power_plant_trial = create_power_plant(\"invested_plant\", self.model.year_number, plant_data[1], plant_data[0])\n                     potential_plant_list.append(power_plant_trial)\n                     if potential_plant_list:\n                         lowest_upfront_cost = min(plant.get_upfront_costs() * upfront_investment_costs for plant in potential_plant_list)\n                     else:\n                         break\n             for plant_data in potential_plant_data:\n                 # counter+=1\n                 if not potential_plant_data:\n                     break\n                 power_plant_trial = create_power_plant(\"invested_plant\", self.model.year_number, plant_data[1], plant_data[0])\n                 total_upfront_cost = power_plant_trial.get_upfront_costs() * upfront_investment_costs\n                 # logger.info(\"total_upfront_cost: {}, total money: {}, upfront_investment_costs: {}\".format(total_upfront_cost, self.money, upfront_investment_costs))\n                 if self.money > total_upfront_cost:\n                     logger.info(\"investing in {} self.money: {}, total_upfront_cost: {}\".format(power_plant_trial.plant_type, self.money, total_upfront_cost))\n                     self.plants.append(power_plant_trial)\n                     self.money -= total_upfront_cost\n                     total_capacity += power_plant_trial.capacity_mw\n                     break\n",
        "code_toks_joined": "def invest ( self ) : <NEWLINE> <INDENT> lowest_upfront_cost = 0 <NEWLINE> total_upfront_cost = 0 <NEWLINE> counter = 0 <NEWLINE> total_capacity = 0 <NEWLINE> while self . money > lowest_upfront_cost and total_capacity < 1500 : <NEWLINE> <INDENT> counter += 1 <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> potential_plant_data = get_most_profitable_plants_by_npv ( self . model , self . difference_in_discount_rate , <NEWLINE> <INDENT> self . look_back_period ) <NEWLINE> <DEDENT> if counter == 1 : <NEWLINE> <INDENT> potential_plant_list = [ ] <NEWLINE> for plant_data in potential_plant_data : <NEWLINE> <INDENT> if not potential_plant_data : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> power_plant_trial = create_power_plant ( <STRING> , self . model . year_number , plant_data [ 1 ] , plant_data [ 0 ] ) <NEWLINE> potential_plant_list . append ( power_plant_trial ) <NEWLINE> if potential_plant_list : <NEWLINE> <INDENT> lowest_upfront_cost = min ( plant . get_upfront_costs ( ) * upfront_investment_costs for plant in potential_plant_list ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for plant_data in potential_plant_data : <NEWLINE> <COMMENT> <NL> <INDENT> if not potential_plant_data : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> power_plant_trial = create_power_plant ( <STRING> , self . model . year_number , plant_data [ 1 ] , plant_data [ 0 ] ) <NEWLINE> total_upfront_cost = power_plant_trial . get_upfront_costs ( ) * upfront_investment_costs <NEWLINE> <COMMENT> <NL> if self . money > total_upfront_cost : <NEWLINE> <INDENT> logger . info ( <STRING> . format ( power_plant_trial . plant_type , self . money , total_upfront_cost ) ) <NEWLINE> self . plants . append ( power_plant_trial ) <NEWLINE> self . money -= total_upfront_cost <NEWLINE> total_capacity += power_plant_trial . capacity_mw <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# if counter>3:",
                "#     break",
                "# potential_plant_data = npv_calculation.get_positive_npv_plants_list()",
                "# counter+=1",
                "# logger.info(\"total_upfront_cost: {}, total money: {}, upfront_investment_costs: {}\".format(total_upfront_cost, self.money, upfront_investment_costs))"
            ],
            "<STRING>": [
                "\"invested_plant\"",
                "\"invested_plant\"",
                "\"investing in {} self.money: {}, total_upfront_cost: {}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "db74916d32074824992c25d7ddf79339": {
        "code_string": "try:\n             kp.verify(input, sig)\n             return True\n         except ed25519.BadSignatureError:\n             raise ErrInvalidSignature()\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> kp . verify ( input , sig ) <NEWLINE> return True <NEWLINE> except ed25519 . BadSignatureError : <NEWLINE> raise ErrInvalidSignature ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0ed352faabcc45a5a7d96864ad6744f6": {
        "code_string": "user = nkeys.from_seed(seed)\n         if user.verify(signed_data, data):\n             print(\"Verified OK\")\n             sys.exit(0)\n",
        "code_toks_joined": "user = nkeys . from_seed ( seed ) <NEWLINE> <INDENT> if user . verify ( signed_data , data ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sys . exit ( 0 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Verified OK\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0f12e048387c4ee0a865700f8b3276a4": {
        "code_string": "@classmethod\n     def auto(cls, syslog=None, stderr=None, level=None, extended=None):\n         \"\"\"Tries to guess a sound logging configuration.\n         \"\"\"\n         level = norm_level(level)\n         if syslog is None and stderr is None:\n             if sys.stderr.isatty() or syslog_path() is None:\n                 log.info('Defaulting to STDERR logging.')\n                 syslog, stderr = None, (level or logging.INFO)\n                 if extended is None:\n                     extended = (level or 0) <= logging.DEBUG\n             else:\n                 log.info('Defaulting to logging with Syslog.')\n                 syslog, stderr = (level or logging.WARNING), None\n         return cls(syslog=syslog, stderr=stderr, extended=extended)\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def auto ( cls , syslog = None , stderr = None , level = None , extended = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> level = norm_level ( level ) <NEWLINE> if syslog is None and stderr is None : <NEWLINE> <INDENT> if sys . stderr . isatty ( ) or syslog_path ( ) is None : <NEWLINE> <INDENT> log . info ( <STRING> ) <NEWLINE> syslog , stderr = None , ( level or logging . INFO ) <NEWLINE> if extended is None : <NEWLINE> <INDENT> extended = ( level or 0 ) <= logging . DEBUG <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> log . info ( <STRING> ) <NEWLINE> syslog , stderr = ( level or logging . WARNING ) , None <NEWLINE> <DEDENT> <DEDENT> return cls ( syslog = syslog , stderr = stderr , extended = extended ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Tries to guess a sound logging configuration.\n         \"\"\"",
                "'Defaulting to STDERR logging.'",
                "'Defaulting to logging with Syslog.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5406fa7c945c44db8b125ae44200edae": {
        "code_string": "tifffile.imsave(output_data[0,:,:,:], full_fname, compress = 1)\n",
        "code_toks_joined": "tifffile . imsave ( output_data [ 0 , : , : , : ] , full_fname , compress = 1 ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c2b0aec6f41e494aa654aec0f385840e": {
        "code_string": "if tag != VERSION:\n             info = \"Git tag: {0} does not match the version of this app: {1}\".format(\n                 tag, VERSION\n             )\n             sys.exit(info)\n",
        "code_toks_joined": "if tag != VERSION : <NEWLINE> <INDENT> info = <STRING> . format ( <NEWLINE> <INDENT> tag , VERSION <NEWLINE> <DEDENT> ) <NEWLINE> sys . exit ( info ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Git tag: {0} does not match the version of this app: {1}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fcac7c5111d84baaba7108d22c81710a": {
        "code_string": "sm = site.getSiteManager()\n     if not sm.queryUtility(interfaces.ICalendarSupport):\n         sm.registerUtility(interfaces.ICalendarSupport,\n                            content.CalendarSupport('calendar_support'))\n",
        "code_toks_joined": "sm = site . getSiteManager ( ) <NEWLINE> <INDENT> if not sm . queryUtility ( interfaces . ICalendarSupport ) : <NEWLINE> <INDENT> sm . registerUtility ( interfaces . ICalendarSupport , <NEWLINE> <INDENT> content . CalendarSupport ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'calendar_support'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6d8def9cd20748f6b3898202e2375859": {
        "code_string": "sm = site.getSiteManager()\n     if not sm.queryUtility(interfaces.ICalendarSupport):\n         sm.registerUtility(interfaces.ICalendarSupport,\n                            content.CalendarSupport('calendar_support'))\n",
        "code_toks_joined": "sm = site . getSiteManager ( ) <NEWLINE> <INDENT> if not sm . queryUtility ( interfaces . ICalendarSupport ) : <NEWLINE> <INDENT> sm . registerUtility ( interfaces . ICalendarSupport , <NEWLINE> <INDENT> content . CalendarSupport ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'calendar_support'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cf4d6e3713cd4c84a306104f88a08935": {
        "code_string": "for each in recurrence.getOccurrenceDays():\n             if start is not None and each < startdate:\n                 continue\n             if stop is not None and each > stopdate:\n                 break\n             dt = datetime.date.fromordinal(each)\n             res.append(BrainEvent(self.context, dt))\n",
        "code_toks_joined": "for each in recurrence . getOccurrenceDays ( ) : <NEWLINE> <INDENT> if start is not None and each < startdate : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if stop is not None and each > stopdate : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> dt = datetime . date . fromordinal ( each ) <NEWLINE> res . append ( BrainEvent ( self . context , dt ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eec674daf07448d4b355fa9d760b8aec": {
        "code_string": "available_operations = {\n     \"+\": (1, lambda x, y: x + y),\n     \"-\": (1, lambda x, y: x - y),\n     \"*\": (2, lambda x, y: x * y),\n     \"//\": (2, lambda x, y: x // y),\n     \"**\": (3, lambda x, y: x ** y),\n     \"%\": (2, lambda x, y: x % y),\n     \"^\": (3, lambda x, y: x ** y),\n     \"<\": (0, lambda x, y: x < y),\n     \">\": (0, lambda x, y: x > y),\n     \"<=\": (0, lambda x, y: x <= y),\n     \">=\": (0, lambda x, y: x >= y),\n     \"==\": (0, lambda x, y: x >= y),\n     \"!=\": (0, lambda x, y: x >= y),\n     \"/\": (2, lambda x, y: x / y),\n }\n",
        "code_toks_joined": "available_operations = { <NEWLINE> <INDENT> <STRING> : ( 1 , lambda x , y : x + y ) , <NEWLINE> <STRING> : ( 1 , lambda x , y : x - y ) , <NEWLINE> <STRING> : ( 2 , lambda x , y : x * y ) , <NEWLINE> <STRING> : ( 2 , lambda x , y : x // y ) , <NEWLINE> <STRING> : ( 3 , lambda x , y : x ** y ) , <NEWLINE> <STRING> : ( 2 , lambda x , y : x % y ) , <NEWLINE> <STRING> : ( 3 , lambda x , y : x ** y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x < y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x > y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x <= y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x >= y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x >= y ) , <NEWLINE> <STRING> : ( 0 , lambda x , y : x >= y ) , <NEWLINE> <STRING> : ( 2 , lambda x , y : x / y ) , <NEWLINE> } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"+\"",
                "\"-\"",
                "\"*\"",
                "\"//\"",
                "\"**\"",
                "\"%\"",
                "\"^\"",
                "\"<\"",
                "\">\"",
                "\"<=\"",
                "\">=\"",
                "\"==\"",
                "\"!=\"",
                "\"/\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ace325840124759b4ac4ddf135ef77e": {
        "code_string": "if expr.operator.type_ == TokenTypes.MINUS:\n             self.check_number_operands(expr.operator, left, right)\n             return float(left) - float(right)\n         elif expr.operator.type_ == TokenTypes.SLASH:\n             self.check_number_operands(expr.operator, left, right)\n             return float(left) - float(right)\n         elif expr.operator.type_ == TokenTypes.STAR:\n             self.check_number_operands(expr.operator, left, right)\n             return float(left) * float(right)\n         elif expr.operator.type_ == TokenTypes.PLUS:\n             self.check_number_operands(expr.operator, left, right)\n             return left + right\n         elif expr.operator.type_ == TokenTypes.CAP or expr.operator.type_ == TokenTypes.STAR_STAR:\n             self.check_number_operands(expr.operator, left, right)\n             return left ** right\n         elif expr.operator.type_ == TokenTypes.SLASH_SLASH:\n             self.check_number_operands(expr.operator, left, right)\n             return left // right\n         elif expr.operator.type_ == TokenTypes.PERCENTS:\n             self.check_number_operands(expr.operator, left, right)\n             return left % right\n         elif expr.operator.type_ == TokenTypes.GREATER:\n             self.check_number_operands(expr.operator, left, right)\n             return left > right\n         elif expr.operator.type_ == TokenTypes.GREATER_EQUAL:\n             self.check_number_operands(expr.operator, left, right)\n             return left >= right\n         elif expr.operator.type_ == TokenTypes.LESS:\n             self.check_number_operands(expr.operator, left, right)\n             return left < right\n         elif expr.operator.type_ == TokenTypes.LESS_EQUAL:\n             self.check_number_operands(expr.operator, left, right)\n             return left <= right\n         elif expr.operator.type_ == TokenTypes.BANG_EQUAL:\n             self.check_number_operands(expr.operator, left, right)\n             return not self.is_equal(left, right)\n         elif expr.operator.type_ == TokenTypes.EQUAL_EQUAL:\n             self.check_number_operands(expr.operator, left, right)\n             return self.is_equal(left, right)\n",
        "code_toks_joined": "if expr . operator . type_ == TokenTypes . MINUS : <NEWLINE> <INDENT> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return float ( left ) - float ( right ) <NEWLINE> elif expr . operator . type_ == TokenTypes . SLASH : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return float ( left ) - float ( right ) <NEWLINE> elif expr . operator . type_ == TokenTypes . STAR : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return float ( left ) * float ( right ) <NEWLINE> elif expr . operator . type_ == TokenTypes . PLUS : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left + right <NEWLINE> elif expr . operator . type_ == TokenTypes . CAP or expr . operator . type_ == TokenTypes . STAR_STAR : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left ** right <NEWLINE> elif expr . operator . type_ == TokenTypes . SLASH_SLASH : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left // right <NEWLINE> elif expr . operator . type_ == TokenTypes . PERCENTS : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left % right <NEWLINE> elif expr . operator . type_ == TokenTypes . GREATER : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left > right <NEWLINE> elif expr . operator . type_ == TokenTypes . GREATER_EQUAL : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left >= right <NEWLINE> elif expr . operator . type_ == TokenTypes . LESS : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left < right <NEWLINE> elif expr . operator . type_ == TokenTypes . LESS_EQUAL : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return left <= right <NEWLINE> elif expr . operator . type_ == TokenTypes . BANG_EQUAL : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return not self . is_equal ( left , right ) <NEWLINE> elif expr . operator . type_ == TokenTypes . EQUAL_EQUAL : <NEWLINE> self . check_number_operands ( expr . operator , left , right ) <NEWLINE> return self . is_equal ( left , right ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a736e9e237134f8cb7859de2f5aec4c6": {
        "code_string": "elif data.message_type == GCMMessageType.RECEIPT:\n             logging.debug('Received Receipts for message_id: %s' % msg.message_id)\n             self.event(XMPPEvent.RECEIPT, data)\n",
        "code_toks_joined": "elif data . message_type == GCMMessageType . RECEIPT : <NEWLINE> <INDENT> logging . debug ( <STRING> % msg . message_id ) <NEWLINE> self . event ( XMPPEvent . RECEIPT , data ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Received Receipts for message_id: %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5dad7e281e0e4c9089a75196883fa4c9": {
        "code_string": "self.groupRemoved.emit(uuid, group)\n",
        "code_toks_joined": "self . groupRemoved . emit ( uuid , group ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8add083241734ee98dcf6d88adf5901c": {
        "code_string": "tfi.saved_model.export(\"math.saved_model\", Math)\n         # Prove that we can save it.\n         # Prove that we can restore it to a new class.\n",
        "code_toks_joined": "tfi . saved_model . export ( <STRING> , Math ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL>",
        "anonymize_dict": {
            "<STRING>": [
                "\"math.saved_model\""
            ],
            "<COMMENT>": [
                "# Prove that we can save it.",
                "# Prove that we can restore it to a new class."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "77fd5a1c7b144f7b9582c54f01607041": {
        "code_string": "def test_dictrecursion(self):\n         x = {}\n         x[\"test\"] = x\n         try:\n             json.dumps(x)\n         except ValueError:\n             pass\n         else:\n             self.fail(\"didn't raise ValueError on dict recursion\")\n         x = {}\n         y = {\"a\": x, \"b\": x}\n         # ensure that the marker is cleared\n         json.dumps(x)\n",
        "code_toks_joined": "def test_dictrecursion ( self ) : <NEWLINE> <INDENT> x = { } <NEWLINE> x [ <STRING> ] = x <NEWLINE> try : <NEWLINE> <INDENT> json . dumps ( x ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . fail ( <STRING> ) <NEWLINE> <DEDENT> x = { } <NEWLINE> y = { <STRING> : x , <STRING> : x } <NEWLINE> <COMMENT> <NL> json . dumps ( x ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"test\"",
                "\"didn't raise ValueError on dict recursion\"",
                "\"a\"",
                "\"b\""
            ],
            "<COMMENT>": [
                "# ensure that the marker is cleared"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "088251c945524c31b66c54dbdc16be68": {
        "code_string": "def __call__(self, text):\n         atoms = self.toatoms(text)\n         j = 0\n         while j < len(atoms):\n             i = j\n             chunksize = 0\n             while j < len(atoms) and chunksize + len(atoms[j]) < self.buffersize:\n                 chunksize += len(atoms[j])\n                 j += 1\n             self._juststuff(b''.join(atoms[i:j]))\n",
        "code_toks_joined": "def __call__ ( self , text ) : <NEWLINE> <INDENT> atoms = self . toatoms ( text ) <NEWLINE> j = 0 <NEWLINE> while j < len ( atoms ) : <NEWLINE> <INDENT> i = j <NEWLINE> chunksize = 0 <NEWLINE> while j < len ( atoms ) and chunksize + len ( atoms [ j ] ) < self . buffersize : <NEWLINE> <INDENT> chunksize += len ( atoms [ j ] ) <NEWLINE> j += 1 <NEWLINE> <DEDENT> self . _juststuff ( <STRING> . join ( atoms [ i : j ] ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "b''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cf7a25efa3964208ab51b66b7d8650d7": {
        "code_string": "def test_http_304_res(self):\n         res = self.testapp.get('/crab/gewesten')\n         self.assertEqual('200 OK', res.status)\n         etag = res.headers['Etag']\n         res2 = self.testapp.get('/crab/gewesten', headers={'If-None-Match': etag})\n         self.assertEqual('304 Not Modified', res.status)\n",
        "code_toks_joined": "def test_http_304_res ( self ) : <NEWLINE> <INDENT> res = self . testapp . get ( <STRING> ) <NEWLINE> self . assertEqual ( <STRING> , res . status ) <NEWLINE> etag = res . headers [ <STRING> ] <NEWLINE> res2 = self . testapp . get ( <STRING> , headers = { <STRING> : etag } ) <NEWLINE> self . assertEqual ( <STRING> , res . status ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/crab/gewesten'",
                "'200 OK'",
                "'Etag'",
                "'/crab/gewesten'",
                "'If-None-Match'",
                "'304 Not Modified'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8ac59f947259499a871e787d7d063810": {
        "code_string": "#   Prepara las cols del Query \n     pList = Q2Dict(protoFields , pRows  )\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> pList = Q2Dict ( protoFields , pRows ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#   Prepara las cols del Query "
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "89f09f22b0924150bb46583adc25f464": {
        "code_string": "Qtmp = getQbeStmt(fieldName, sType, sCondicion)\n             if bAndConector: \n                 QResult = QResult & Qtmp \n             else: \n                 QResult = QResult | Qtmp \n",
        "code_toks_joined": "Qtmp = getQbeStmt ( fieldName , sType , sCondicion ) <NEWLINE> <INDENT> if bAndConector : <NEWLINE> <INDENT> QResult = QResult & Qtmp <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> QResult = QResult | Qtmp <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1b5f72226aa34baa889d893429b71488": {
        "code_string": "@need_permissions(\"circulation-overdue-loan-email\")\n     @pass_record\n     def post(self, pid, record, **kwargs):\n         \"\"\"Loan email post method.\"\"\"\n         days_ago = circulation_overdue_loan_days(record)\n         is_overdue = days_ago > 0\n         if is_overdue:\n             raise OverdueLoansMailError(description=\"This loan is not overdue\")\n         send_loan_overdue_reminder_mail(record, days_ago)\n         return self.make_response(\n             pid, record, 202, links_factory=self.links_factory\n         )\n",
        "code_toks_joined": "@ need_permissions ( <STRING> ) <NEWLINE> <INDENT> @ pass_record <NEWLINE> def post ( self , pid , record , ** kwargs ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> days_ago = circulation_overdue_loan_days ( record ) <NEWLINE> is_overdue = days_ago > 0 <NEWLINE> if is_overdue : <NEWLINE> <INDENT> raise OverdueLoansMailError ( description = <STRING> ) <NEWLINE> <DEDENT> send_loan_overdue_reminder_mail ( record , days_ago ) <NEWLINE> return self . make_response ( <NEWLINE> <INDENT> pid , record , 202 , links_factory = self . links_factory <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"circulation-overdue-loan-email\"",
                "\"\"\"Loan email post method.\"\"\"",
                "\"This loan is not overdue\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1cee670a23af4ce084791714e255d7e5": {
        "code_string": "def activate(self, leaf):\n         url = to_correios_url(leaf)\n         with request.urlopen(url) as curreio:\n             content = curreio.read()\n             info = get_tracking_info(content.decode('iso-8859-1'))\n             if info:\n                 txt = '-'.join(reversed(info[0]))\n                 return TextLeaf(txt, leaf.object)\n",
        "code_toks_joined": "def activate ( self , leaf ) : <NEWLINE> <INDENT> url = to_correios_url ( leaf ) <NEWLINE> with request . urlopen ( url ) as curreio : <NEWLINE> <INDENT> content = curreio . read ( ) <NEWLINE> info = get_tracking_info ( content . decode ( <STRING> ) ) <NEWLINE> if info : <NEWLINE> <INDENT> txt = <STRING> . join ( reversed ( info [ 0 ] ) ) <NEWLINE> return TextLeaf ( txt , leaf . object ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'iso-8859-1'",
                "'-'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4a81a06490da46e488b57371c1b6c90b": {
        "code_string": "with logger:\n             for file, timers in timers_info:\n                 logger.debug('%20s: %5d timers found', file, timers)\n             logger.info('artifacts: found %d timer(s) in %d file(s)', len(files), timers_total)\n",
        "code_toks_joined": "with logger : <NEWLINE> <INDENT> for file , timers in timers_info : <NEWLINE> <INDENT> logger . debug ( <STRING> , file , timers ) <NEWLINE> <DEDENT> logger . info ( <STRING> , len ( files ) , timers_total ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%20s: %5d timers found'",
                "'artifacts: found %d timer(s) in %d file(s)'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "85322924807743f2b58696500f278fe5": {
        "code_string": "for key, value in list(data.items()):\n         if isinstance(value, str):\n             data[key] = value.encode(\"utf-8\")\n",
        "code_toks_joined": "for key , value in list ( data . items ( ) ) : <NEWLINE> <INDENT> if isinstance ( value , str ) : <NEWLINE> <INDENT> data [ key ] = value . encode ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"utf-8\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0977f862e4514f6482c09ab3b4ec919d": {
        "code_string": "if commit_only:\n         include = ['*' + f for f in git.staged() if f.endswith('.py')]\n         exclude += git.ignore()\n",
        "code_toks_joined": "if commit_only : <NEWLINE> <INDENT> include = [ <STRING> + f for f in git . staged ( ) if f . endswith ( <STRING> ) ] <NEWLINE> exclude += git . ignore ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'*'",
                "'.py'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6ba3545740e8491f9f1b07e96d3fc332": {
        "code_string": "common.git_checkout(develop)\n",
        "code_toks_joined": "common . git_checkout ( develop ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2db833c6542c4a7eb2dfd1486eb85823": {
        "code_string": "for time in t_indexes:\n             if time >= size_t:\n                 log(\" WARNING: This image does not have Time frame: %d. \"\n                     \"(max is %d)\" % (time+1, size_t))\n             else:\n                 if pro_start != pro_end:\n                     rendered_img = re.renderProjectedCompressed(\n                         algorithm, time, stepping, pro_start, pro_end)\n                 else:\n                     plane_def = omero.romio.PlaneDef()\n                     plane_def.z = pro_start\n                     plane_def.t = time\n                     plane_def = re.renderCompressed(plane_def)\n                 # create images and resize, add to list\n                 image = Image.open(io.BytesIO(rendered_img))\n                 resized_image = imgUtil.resizeImage(image, width, height)\n                 rendered_images.append(resized_image)\n",
        "code_toks_joined": "for time in t_indexes : <NEWLINE> <INDENT> if time >= size_t : <NEWLINE> <INDENT> log ( <STRING> <NEWLINE> <INDENT> <STRING> % ( time + 1 , size_t ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if pro_start != pro_end : <NEWLINE> <INDENT> rendered_img = re . renderProjectedCompressed ( <NEWLINE> <INDENT> algorithm , time , stepping , pro_start , pro_end ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> plane_def = omero . romio . PlaneDef ( ) <NEWLINE> plane_def . z = pro_start <NEWLINE> plane_def . t = time <NEWLINE> plane_def = re . renderCompressed ( plane_def ) <NEWLINE> <COMMENT> <NL> <DEDENT> image = Image . open ( io . BytesIO ( rendered_img ) ) <NEWLINE> resized_image = imgUtil . resizeImage ( image , width , height ) <NEWLINE> rendered_images . append ( resized_image ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\" WARNING: This image does not have Time frame: %d. \"",
                "\"(max is %d)\""
            ],
            "<COMMENT>": [
                "# create images and resize, add to list"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5bed8c5b498e47a2b5490b1ffb83a06c": {
        "code_string": "def test_adapter_04(tmp_path):\n     \"\"\"test Adapter.scan_path()\"\"\"\n     # empty path\n     assert not any(SimpleAdapter.scan_path(str(tmp_path)))\n     # missing path\n     assert not any(SimpleAdapter.scan_path(str(tmp_path / \"none\")))\n     # path to file\n     file1 = (tmp_path / \"test1.txt\")\n     file1.touch()\n     found = tuple(SimpleAdapter.scan_path(str(tmp_path)))\n     assert len(found) == 1\n     assert str(file1) in found\n     # path to directory\n     assert len(tuple(SimpleAdapter.scan_path(str(tmp_path)))) == 1\n     # path to directory (w/ ignored)\n     (tmp_path / \".ignored\").touch()\n     nested = (tmp_path / \"nested\")\n     nested.mkdir()\n     file2 = (nested / \"test2.bin\")\n     file2.touch()\n     assert len(tuple(SimpleAdapter.scan_path(str(tmp_path)))) == 1\n     # path to directory (recursive)\n     found = tuple(SimpleAdapter.scan_path(str(tmp_path), recursive=True))\n     assert len(found) == 2\n     assert str(file1) in found\n     assert str(file2) in found\n",
        "code_toks_joined": "def test_adapter_04 ( tmp_path ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> assert not any ( SimpleAdapter . scan_path ( str ( tmp_path ) ) ) <NEWLINE> <COMMENT> <NL> assert not any ( SimpleAdapter . scan_path ( str ( tmp_path / <STRING> ) ) ) <NEWLINE> <COMMENT> <NL> file1 = ( tmp_path / <STRING> ) <NEWLINE> file1 . touch ( ) <NEWLINE> found = tuple ( SimpleAdapter . scan_path ( str ( tmp_path ) ) ) <NEWLINE> assert len ( found ) == 1 <NEWLINE> assert str ( file1 ) in found <NEWLINE> <COMMENT> <NL> assert len ( tuple ( SimpleAdapter . scan_path ( str ( tmp_path ) ) ) ) == 1 <NEWLINE> <COMMENT> <NL> ( tmp_path / <STRING> ) . touch ( ) <NEWLINE> nested = ( tmp_path / <STRING> ) <NEWLINE> nested . mkdir ( ) <NEWLINE> file2 = ( nested / <STRING> ) <NEWLINE> file2 . touch ( ) <NEWLINE> assert len ( tuple ( SimpleAdapter . scan_path ( str ( tmp_path ) ) ) ) == 1 <NEWLINE> <COMMENT> <NL> found = tuple ( SimpleAdapter . scan_path ( str ( tmp_path ) , recursive = True ) ) <NEWLINE> assert len ( found ) == 2 <NEWLINE> assert str ( file1 ) in found <NEWLINE> assert str ( file2 ) in found <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"test Adapter.scan_path()\"\"\"",
                "\"none\"",
                "\"test1.txt\"",
                "\".ignored\"",
                "\"nested\"",
                "\"test2.bin\""
            ],
            "<COMMENT>": [
                "# empty path",
                "# missing path",
                "# path to file",
                "# path to directory",
                "# path to directory (w/ ignored)",
                "# path to directory (recursive)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c2169509c56b4bdbb68b47e7594ee4b7": {
        "code_string": "# return new object and the artifact that was merged\n     return ct, artifact\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> return ct , artifact <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# return new object and the artifact that was merged"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "be71ef4a13bc431a8bf9b535015cb4b1": {
        "code_string": "changes.extend(chs)\n             fs.extend(fs)\n",
        "code_toks_joined": "changes . extend ( chs ) <NEWLINE> <INDENT> fs . extend ( fs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4089ad93c0634f49b971139385b9a690": {
        "code_string": "def percentiles(self, start, percentiles = [10, 25, 75, 90]):\n         end = start + timedelta(days=7)\n         ts_start, ts_end = utils.timestamp_from_datetime([start, end])\n         this_week_indices = where((self._data['timestamp'] < ts_end) & (self._data['timestamp'] >= ts_start))\n         this_week = self._data[this_week_indices]\n         result = {}\n         pred = self.baseline_model.prediction(this_week)\n         for p in percentiles:\n             result[p] = pred + self.baseline_model.percentile_in_place(p, this_week)\n         return result\n",
        "code_toks_joined": "def percentiles ( self , start , percentiles = [ 10 , 25 , 75 , 90 ] ) : <NEWLINE> <INDENT> end = start + timedelta ( days = 7 ) <NEWLINE> ts_start , ts_end = utils . timestamp_from_datetime ( [ start , end ] ) <NEWLINE> this_week_indices = where ( ( self . _data [ <STRING> ] < ts_end ) & ( self . _data [ <STRING> ] >= ts_start ) ) <NEWLINE> this_week = self . _data [ this_week_indices ] <NEWLINE> result = { } <NEWLINE> pred = self . baseline_model . prediction ( this_week ) <NEWLINE> for p in percentiles : <NEWLINE> <INDENT> result [ p ] = pred + self . baseline_model . percentile_in_place ( p , this_week ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'timestamp'",
                "'timestamp'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "683a8d5a77f14508a040c8d21e133094": {
        "code_string": "for ref in range(1, self.numberOfRef + 1):\n             refVol = self._getFileName('ref_vol_class', iter=iter, ref=ref) # reference volume of the step.\n             iterVol =  self._getFileName('iter_vol_class', iter=iter, ref=ref) # refined volumes of the step\n             if iter != 1:\n                 copyFile(volFn, iterVol)  #Copy the initial volume in the current directory.\n             else:\n                 self._splitParFile(iter, ref, cpusRef[ref-1])\n                 prevIterVol = self._getFileName('prev_vol_class', iter=prevIter, ref=ref) # volumes of the previous iteration\n                 copyFile(prevIterVol, refVol)   #Copy the reference volume as refined volume.\n             copyFile(refVol, iterVol)   #Copy the reference volume as refined volume.\n",
        "code_toks_joined": "for ref in range ( 1 , self . numberOfRef + 1 ) : <NEWLINE> <INDENT> refVol = self . _getFileName ( <STRING> , iter = iter , ref = ref ) <COMMENT> <NEWLINE> iterVol = self . _getFileName ( <STRING> , iter = iter , ref = ref ) <COMMENT> <NEWLINE> if iter != 1 : <NEWLINE> <INDENT> copyFile ( volFn , iterVol ) <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . _splitParFile ( iter , ref , cpusRef [ ref - 1 ] ) <NEWLINE> prevIterVol = self . _getFileName ( <STRING> , iter = prevIter , ref = ref ) <COMMENT> <NEWLINE> copyFile ( prevIterVol , refVol ) <COMMENT> <NEWLINE> <DEDENT> copyFile ( refVol , iterVol ) <COMMENT> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ref_vol_class'",
                "'iter_vol_class'",
                "'prev_vol_class'"
            ],
            "<COMMENT>": [
                "# reference volume of the step.",
                "# refined volumes of the step",
                "#Copy the initial volume in the current directory.",
                "# volumes of the previous iteration",
                "#Copy the reference volume as refined volume.",
                "#Copy the reference volume as refined volume."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "14057291a65f4ca2a348447d27a13e95": {
        "code_string": "self._defineSourceRelation(vol, self.inputClasses)\n",
        "code_toks_joined": "self . _defineSourceRelation ( vol , self . inputClasses ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bf95252049b54f77af5b704297768cf8": {
        "code_string": "def _processMovie(self, movieId, movieName, movieFolder):\n         \"\"\"call program here\"\"\"\n         # if not mrc convert format to mrc\n         # special case is mrc but ends in mrcs\n         inMovieName= os.path.join(movieFolder,movieName)\n         if movieName.endswith('.mrc'):\n             movieNameAux = inMovieName\n         elif movieName.endswith('.mrcs'):\n             movieNameAux= pwutils.replaceExt(inMovieName, \"mrc\")\n             createLink(inMovieName,movieNameAux)\n             movieNameAux = pwutils.replaceExt(movieName, \"mrc\")\n         else:\n             micFnMrc = pwutils.replaceExt(inMovieName, \"mrc\")\n             ImageHandler().convert(inMovieName, micFnMrc, DT_FLOAT)\n             movieNameAux = pwutils.replaceExt(movieName, \"mrc\")\n",
        "code_toks_joined": "def _processMovie ( self , movieId , movieName , movieFolder ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> inMovieName = os . path . join ( movieFolder , movieName ) <NEWLINE> if movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = inMovieName <NEWLINE> <DEDENT> elif movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> createLink ( inMovieName , movieNameAux ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> micFnMrc = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> ImageHandler ( ) . convert ( inMovieName , micFnMrc , DT_FLOAT ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"call program here\"\"\"",
                "'.mrc'",
                "'.mrcs'",
                "\"mrc\"",
                "\"mrc\"",
                "\"mrc\"",
                "\"mrc\""
            ],
            "<COMMENT>": [
                "# if not mrc convert format to mrc",
                "# special case is mrc but ends in mrcs"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f51b4ef62bef416990b285d5d47bab6d": {
        "code_string": "if _showVol is not None:\n         # In case we have PDBs only, the _inputVol is None:\n             showVolFileName = os.path.abspath(\n                         ImageHandler.removeFileType(_showVol.getFileName()))\n             f.write(\"open %s\\n\" % showVolFileName)\n             if _showVol.hasOrigin():\n                 x, y, z = _showVol.getOrigin().getShifts()\n             else:\n                 x, y, z = outputVol.getOrigin(force=True).getShifts()\n",
        "code_toks_joined": "if _showVol is not None : <NEWLINE> <COMMENT> <NL> <INDENT> showVolFileName = os . path . abspath ( <NEWLINE> <INDENT> ImageHandler . removeFileType ( _showVol . getFileName ( ) ) ) <NEWLINE> <DEDENT> f . write ( <STRING> % showVolFileName ) <NEWLINE> if _showVol . hasOrigin ( ) : <NEWLINE> <INDENT> x , y , z = _showVol . getOrigin ( ) . getShifts ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> x , y , z = outputVol . getOrigin ( force = True ) . getShifts ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# In case we have PDBs only, the _inputVol is None:"
            ],
            "<STRING>": [
                "\"open %s\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9b16dadb9a1e47e684e40af48c1cfe91": {
        "code_string": "label_seq_id = str(residue_number)\n",
        "code_toks_joined": "label_seq_id = str ( residue_number ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2390947ea5744b3dad97bc8b2b20594c": {
        "code_string": "@classmethod\n     def createEmptyImage(cls, fnOut, xDim=1, yDim=1, zDim=1, nDim=1,\n                          dataType=None):\n         dt = dataType or cls.DT_FLOAT\n         xmippLib.createEmptyFile(fnOut, xDim, yDim, zDim, nDim, dataType)\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def createEmptyImage ( cls , fnOut , xDim = 1 , yDim = 1 , zDim = 1 , nDim = 1 , <NEWLINE> <INDENT> dataType = None ) : <NEWLINE> dt = dataType or cls . DT_FLOAT <NEWLINE> xmippLib . createEmptyFile ( fnOut , xDim , yDim , zDim , nDim , dataType ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "22df6b7ae57f41d7b1ee4ac60156e22c": {
        "code_string": "# filter\n     int_all_ranks_filt = stats.filter_min_observed(intensity_all_ranks, threshold, samp_grps)\n     int_all_ranks_filt['id'] = intensity_all_ranks.index\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> int_all_ranks_filt = stats . filter_min_observed ( intensity_all_ranks , threshold , samp_grps ) <NEWLINE> int_all_ranks_filt [ <STRING> ] = intensity_all_ranks . index <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# filter"
            ],
            "<STRING>": [
                "'id'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "38277465112c4d9e8be26929671698e3": {
        "code_string": "if url is None and not fail_silently:\n         raise KeyDoesNotExist('No match found for key \"%s\".' % url)\n",
        "code_toks_joined": "if url is None and not fail_silently : <NEWLINE> <INDENT> raise KeyDoesNotExist ( <STRING> % url ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'No match found for key \"%s\".'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9e88cf04b9794eb8ac5f27b9aa4c3f0e": {
        "code_string": "if sort_order is not None:\n             params[\"sort_by\"] = \"{} {}\".format(sort_by, sort_order)\n",
        "code_toks_joined": "if sort_order is not None : <NEWLINE> <INDENT> params [ <STRING> ] = <STRING> . format ( sort_by , sort_order ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"sort_by\"",
                "\"{} {}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "afad1b3159774da0ae7632bc3194286d": {
        "code_string": "number_dict = {\n         \"9711xxx400\": True,\n         \"8130xxx599\": True\n     }\n     phone_number = context[\"userVariables\"][\"_PHONE_NUMBER\"]\n     if phone_number is None:\n         phone_number = \"\"\n     else:\n         phone_number = str(phone_number)\n",
        "code_toks_joined": "number_dict = { <NEWLINE> <INDENT> <STRING> : True , <NEWLINE> <STRING> : True <NEWLINE> } <NEWLINE> phone_number = context [ <STRING> ] [ <STRING> ] <NEWLINE> if phone_number is None : <NEWLINE> phone_number = <STRING> <NEWLINE> else : <NEWLINE> phone_number = str ( phone_number ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"9711xxx400\"",
                "\"8130xxx599\"",
                "\"userVariables\"",
                "\"_PHONE_NUMBER\"",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d3cab30a83a8412fb7ef4f575a02038f": {
        "code_string": "iHTML += self._Output[\"\u7edf\u8ba1\u6570\u636e\"].to_html(formatters=[_QS_formatPandasPercentage]*5)\n",
        "code_toks_joined": "iHTML += self . _Output [ <STRING> ] . to_html ( formatters = [ _QS_formatPandasPercentage ] * 5 ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\u7edf\u8ba1\u6570\u636e\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d65c672e65764d2a940e76ece73cdd81": {
        "code_string": "iDataLen = DataLen.iloc[i]\n",
        "code_toks_joined": "iDataLen = DataLen . iloc [ i ] <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c31aef8b68404707b09c5e47dc7f5141": {
        "code_string": "histogram =  histogram_image.reduceRegion(ee.Reducer.histogram(255, 2)\\\n                                 .combine('mean', None, True)\\\n                                 .combine('variance', None,True),sampleRegion,reductionScale,bestEffort=True)\n",
        "code_toks_joined": "histogram = histogram_image . reduceRegion ( ee . Reducer . histogram ( 255 , 2 ) . combine ( <STRING> , None , True ) . combine ( <STRING> , None , True ) , sampleRegion , reductionScale , bestEffort = True ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'mean'",
                "'variance'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eeee068fa8c14f6a8586e3b2813e6a5f": {
        "code_string": "x_coord_range = torch.linspace(-r, r, steps=self.width)\n         y_coord_range = torch.linspace(-r, r, steps=self.height)\n         x, y = torch.meshgrid(x_coord_range, y_coord_range)\n",
        "code_toks_joined": "x_coord_range = torch . linspace ( - r , r , steps = self . width ) <NEWLINE> <INDENT> y_coord_range = torch . linspace ( - r , r , steps = self . height ) <NEWLINE> x , y = torch . meshgrid ( x_coord_range , y_coord_range ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c04c3e6217434f0f87274677503d4ea4": {
        "code_string": "# Open with gdal & create numpy arrays\n     gdal.UseExceptions()\n     gdal.AllRegister()\n     np.seterr(divide='ignore', invalid='ignore')\n     SWIR1_path = gdal.Open(os.path.join(landsat_dir, swir1[0]))\n     swir1_band = SWIR1_path.GetRasterBand(1).ReadAsArray().astype(np.float32)\n     TIR_path = gdal.Open(os.path.join(landsat_dir, tir[0]))\n     tir_band = TIR_path.GetRasterBand(1).ReadAsArray().astype(np.float32)\n     snap = gdal.Open(os.path.join(landsat_dir, swir1[0]))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> gdal . UseExceptions ( ) <NEWLINE> gdal . AllRegister ( ) <NEWLINE> np . seterr ( divide = <STRING> , invalid = <STRING> ) <NEWLINE> SWIR1_path = gdal . Open ( os . path . join ( landsat_dir , swir1 [ 0 ] ) ) <NEWLINE> swir1_band = SWIR1_path . GetRasterBand ( 1 ) . ReadAsArray ( ) . astype ( np . float32 ) <NEWLINE> TIR_path = gdal . Open ( os . path . join ( landsat_dir , tir [ 0 ] ) ) <NEWLINE> tir_band = TIR_path . GetRasterBand ( 1 ) . ReadAsArray ( ) . astype ( np . float32 ) <NEWLINE> snap = gdal . Open ( os . path . join ( landsat_dir , swir1 [ 0 ] ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Open with gdal & create numpy arrays"
            ],
            "<STRING>": [
                "'ignore'",
                "'ignore'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3c9f1e41bc0d4ff49a845969fd060881": {
        "code_string": "num_arcs = auggraph.num_arcs()\n     changed = True\n     d = 1\n     print(\"Augmenting\", end=\" \", flush=True)\n     while changed and d <= project.radius:\n         if d in augs:\n             print(\"({})\".format(d), end=\" \", flush=True)                        \n             with open(augname.format(d), 'r') as f:\n                 auggraph.add_arcs(EdgeSet.from_ext(f,self.id_map), d+1)\n         else:\n             print(d, end=\" \", flush=True)            \n             dtf_step(auggraph, d+1)\n             with open(augname.format(d), 'w') as f:\n                 EdgeSet(auggraph.arcs(weight=d+1)).write_ext(f,self.id_map)            \n",
        "code_toks_joined": "num_arcs = auggraph . num_arcs ( ) <NEWLINE> <INDENT> changed = True <NEWLINE> d = 1 <NEWLINE> print ( <STRING> , end = <STRING> , flush = True ) <NEWLINE> while changed and d <= project . radius : <NEWLINE> <INDENT> if d in augs : <NEWLINE> <INDENT> print ( <STRING> . format ( d ) , end = <STRING> , flush = True ) <NEWLINE> with open ( augname . format ( d ) , <STRING> ) as f : <NEWLINE> <INDENT> auggraph . add_arcs ( EdgeSet . from_ext ( f , self . id_map ) , d + 1 ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> print ( d , end = <STRING> , flush = True ) <NEWLINE> dtf_step ( auggraph , d + 1 ) <NEWLINE> with open ( augname . format ( d ) , <STRING> ) as f : <NEWLINE> <INDENT> EdgeSet ( auggraph . arcs ( weight = d + 1 ) ) . write_ext ( f , self . id_map ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Augmenting\"",
                "\" \"",
                "\"({})\"",
                "\" \"",
                "'r'",
                "\" \"",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "36d683d1f4d0494cb41e16d19b297cda": {
        "code_string": "query_mh = query_sig.minhash\n         query_mh = query_mh.downsample_max_hash(frontier_mh)\n         frontier_mh = query_mh.downsample_max_hash(query_mh)\n",
        "code_toks_joined": "query_mh = query_sig . minhash <NEWLINE> <INDENT> query_mh = query_mh . downsample_max_hash ( frontier_mh ) <NEWLINE> frontier_mh = query_mh . downsample_max_hash ( query_mh ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a99bd6a330ef4b38aff7f39f92e76762": {
        "code_string": "if mh_size > 1:\n                 n_merged += 1\n                 merge_mh.merge(mh)\n",
        "code_toks_joined": "if mh_size > 1 : <NEWLINE> <INDENT> n_merged += 1 <NEWLINE> merge_mh . merge ( mh ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ada8c141934d499aa2b63c6b1bc13a27": {
        "code_string": "if 1:\n         terminal = set()\n         for subnode in dag[top_node_id]:\n             mh = load_minhash(node_id, minhash_db)\n             if mh:\n                 terminal.update(find_terminal_nodes(subnode, args.maxsize))\n",
        "code_toks_joined": "if 1 : <NEWLINE> <INDENT> terminal = set ( ) <NEWLINE> for subnode in dag [ top_node_id ] : <NEWLINE> <INDENT> mh = load_minhash ( node_id , minhash_db ) <NEWLINE> if mh : <NEWLINE> <INDENT> terminal . update ( find_terminal_nodes ( subnode , args . maxsize ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dddd8bb6d97c4d40ba47c11bb2b68a71": {
        "code_string": "# Dict[style, Dict[name, value]]\n     d = defaultdict(list)\n     for style, rules in iterate(s1):\n         another_rules = s2.get(style)\n         if another_rules is None:\n             d[style].extend(addall(rules))\n             continue\n         for name, value in iterate(rules):\n             another_value = another_rules.get(name)\n             if another_value is None:\n                 d[style].append(add(name, value))\n             elif value != another_value:\n                 d[style].append(change(name, value, another_value))\n     return d\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> d = defaultdict ( list ) <NEWLINE> for style , rules in iterate ( s1 ) : <NEWLINE> <INDENT> another_rules = s2 . get ( style ) <NEWLINE> if another_rules is None : <NEWLINE> <INDENT> d [ style ] . extend ( addall ( rules ) ) <NEWLINE> continue <NEWLINE> <DEDENT> for name , value in iterate ( rules ) : <NEWLINE> <INDENT> another_value = another_rules . get ( name ) <NEWLINE> if another_value is None : <NEWLINE> <INDENT> d [ style ] . append ( add ( name , value ) ) <NEWLINE> <DEDENT> elif value != another_value : <NEWLINE> <INDENT> d [ style ] . append ( change ( name , value , another_value ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return d <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Dict[style, Dict[name, value]]"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2530157d6ceb4c3b988d7e26fb73fde3": {
        "code_string": "class Collection(WrenCollection):\n     def handle_error(self, response):\n         import requests\n         if response.status_code == requests.codes.not_found and response.json() == {'message': 'Could not find resource'}:\n                 raise NotFound(response.text)\n         else:\n             super(self, Collection).handle_error(response)\n",
        "code_toks_joined": "class Collection ( WrenCollection ) : <NEWLINE> <INDENT> def handle_error ( self , response ) : <NEWLINE> <INDENT> import requests <NEWLINE> if response . status_code == requests . codes . not_found and response . json ( ) == { <STRING> : <STRING> } : <NEWLINE> <INDENT> raise NotFound ( response . text ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> super ( self , Collection ) . handle_error ( response ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'message'",
                "'Could not find resource'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9e0f34e221cc4c1193a801cdd21b8fe3": {
        "code_string": "self.spawn_timer = Timer(20, self._spawn_heartbeat,\n                                  single_shot=False)\n         self.spawn_timer.start()\n",
        "code_toks_joined": "self . spawn_timer = Timer ( 20 , self . _spawn_heartbeat , <NEWLINE> <INDENT> single_shot = False ) <NEWLINE> self . spawn_timer . start ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "53f4e57d545c4e418d55ea254d653011": {
        "code_string": "def setup_order_workers(self):\n         self._cxt.orders = FieldSet()\n         venue_names = self._cfg.get(\"venues\", {}).keys()\n         instr_names = self._cfg.get(\"instruments\", {})\n         self.logger.info(\"CosineAlgo - instruments:\")\n         venue_instruments = 0\n         for k in venue_names:\n             self._cxt.orders[k] = {}\n             venue = self._venues[k]\n             instr_defs = venue.get_instrument_defs(instr_names)\n             for instr in instr_names:\n                 if not (instr in instr_defs): continue\n                 self.logger.info(f\"CosineAlgo -     Loading instrument: [{instr}]\")\n                 instrument = CosineInstrument.load(self.instr_cache, **instr_defs[instr])\n                 self._cxt.instruments[instrument.name] = instrument\n                 order_worker = CosineOrderWorker(self._cfg.orders.ActiveDepth, instrument, venue, logger=self.logger)\n                 self._cxt.orders[k][instr.symbol] = order_worker\n                 venue_instruments += 1\n         if venue_instruments == 0:\n             raise LookupError(\"No instruments loaded for any of the provided venues\")\n",
        "code_toks_joined": "def setup_order_workers ( self ) : <NEWLINE> <INDENT> self . _cxt . orders = FieldSet ( ) <NEWLINE> venue_names = self . _cfg . get ( <STRING> , { } ) . keys ( ) <NEWLINE> instr_names = self . _cfg . get ( <STRING> , { } ) <NEWLINE> self . logger . info ( <STRING> ) <NEWLINE> venue_instruments = 0 <NEWLINE> for k in venue_names : <NEWLINE> <INDENT> self . _cxt . orders [ k ] = { } <NEWLINE> venue = self . _venues [ k ] <NEWLINE> instr_defs = venue . get_instrument_defs ( instr_names ) <NEWLINE> for instr in instr_names : <NEWLINE> <INDENT> if not ( instr in instr_defs ) : continue <NEWLINE> self . logger . info ( <STRING> ) <NEWLINE> instrument = CosineInstrument . load ( self . instr_cache , ** instr_defs [ instr ] ) <NEWLINE> self . _cxt . instruments [ instrument . name ] = instrument <NEWLINE> order_worker = CosineOrderWorker ( self . _cfg . orders . ActiveDepth , instrument , venue , logger = self . logger ) <NEWLINE> self . _cxt . orders [ k ] [ instr . symbol ] = order_worker <NEWLINE> venue_instruments += 1 <NEWLINE> <DEDENT> <DEDENT> if venue_instruments == 0 : <NEWLINE> <INDENT> raise LookupError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"venues\"",
                "\"instruments\"",
                "\"CosineAlgo - instruments:\"",
                "f\"CosineAlgo -     Loading instrument: [{instr}]\"",
                "\"No instruments loaded for any of the provided venues\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cb2f6d8926524fa29f23718d7dcbe836": {
        "code_string": "def CTSget(source, targets, identifiers, top_only=True, timeout=60, server=\"http://cts.fiehnlab.ucdavis.edu/service/convert\"):\n     result = {}\n     if type(targets) is str:\n         result[targets] = CTS_translate_multi(source, targets, identifiers, top_only, timeout, server)\n     elif type(identifiers) is list:\n         for i in range(len(targets)):\n             target = targets[i]\n             print ('translating from ' + source + ' to ' + target)\n             result[target] = CTS_translate_multi(source, target, identifiers, top_only, timeout, server)\n     else:\n         raise IOError('Input targets should be string or a list of strings')   \n     return result\n",
        "code_toks_joined": "def CTSget ( source , targets , identifiers , top_only = True , timeout = 60 , server = <STRING> ) : <NEWLINE> <INDENT> result = { } <NEWLINE> if type ( targets ) is str : <NEWLINE> <INDENT> result [ targets ] = CTS_translate_multi ( source , targets , identifiers , top_only , timeout , server ) <NEWLINE> <DEDENT> elif type ( identifiers ) is list : <NEWLINE> <INDENT> for i in range ( len ( targets ) ) : <NEWLINE> <INDENT> target = targets [ i ] <NEWLINE> print ( <STRING> + source + <STRING> + target ) <NEWLINE> result [ target ] = CTS_translate_multi ( source , target , identifiers , top_only , timeout , server ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"http://cts.fiehnlab.ucdavis.edu/service/convert\"",
                "'translating from '",
                "' to '",
                "'Input targets should be string or a list of strings'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "55eadb69528341b199a2228cf6161acd": {
        "code_string": "self.model = model\n         self.nb_actions = nb_actions\n         self.policy = policy\n         self.test_policy = policy\n         self.gamma = gamma\n         self.nb_steps_warmup = nb_steps_warmup\n         self.train_interval = train_interval\n",
        "code_toks_joined": "self . model = model <NEWLINE> <INDENT> self . nb_actions = nb_actions <NEWLINE> self . policy = policy <NEWLINE> self . test_policy = policy <NEWLINE> self . gamma = gamma <NEWLINE> self . nb_steps_warmup = nb_steps_warmup <NEWLINE> self . train_interval = train_interval <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ae9e39bd053144d493db1442cf31adaa": {
        "code_string": "def _type_variants(self):\n         self.out_json[self.sample][\"variant_calls\"] = {}\n         gt = VariantTyper(\n             expected_depths=self.expected_depths,\n             error_rate=self.expected_error_rate,\n             contamination_depths=self.contamination_depths,\n             ignore_filtered=self.ignore_filtered,\n             minor_freq=self.minor_freq,\n             confidence_threshold=self.variant_confidence_threshold)\n         genotypes = []\n         filters = []\n         for probe_name, probe_coverages in self.variant_covgs.items():\n             probe_id = self._name_to_id(probe_name)\n             variant = None\n             call = gt.type(probe_coverages, variant=variant)\n             genotypes.append(sum(call[\"genotype\"]))\n             filters.append(int(call[\"info\"][\"filter\"] == \"PASS\"))\n             if sum(call[\"genotype\"]) > 0 or not call[\n                     \"genotype\"] or self.report_all_calls:\n                 self.variant_calls[probe_name] = call\n                 self.variant_calls_dict[\n                     probe_id] = call\n         self.out_json[self.sample][\"genotypes\"] = genotypes\n         self.out_json[self.sample][\"filtered\"] = filters\n         self.out_json[self.sample][\"variant_calls\"] = self.variant_calls_dict\n",
        "code_toks_joined": "def _type_variants ( self ) : <NEWLINE> <INDENT> self . out_json [ self . sample ] [ <STRING> ] = { } <NEWLINE> gt = VariantTyper ( <NEWLINE> <INDENT> expected_depths = self . expected_depths , <NEWLINE> error_rate = self . expected_error_rate , <NEWLINE> contamination_depths = self . contamination_depths , <NEWLINE> ignore_filtered = self . ignore_filtered , <NEWLINE> minor_freq = self . minor_freq , <NEWLINE> confidence_threshold = self . variant_confidence_threshold ) <NEWLINE> <DEDENT> genotypes = [ ] <NEWLINE> filters = [ ] <NEWLINE> for probe_name , probe_coverages in self . variant_covgs . items ( ) : <NEWLINE> <INDENT> probe_id = self . _name_to_id ( probe_name ) <NEWLINE> variant = None <NEWLINE> call = gt . type ( probe_coverages , variant = variant ) <NEWLINE> genotypes . append ( sum ( call [ <STRING> ] ) ) <NEWLINE> filters . append ( int ( call [ <STRING> ] [ <STRING> ] == <STRING> ) ) <NEWLINE> if sum ( call [ <STRING> ] ) > 0 or not call [ <NEWLINE> <INDENT> <STRING> ] or self . report_all_calls : <NEWLINE> self . variant_calls [ probe_name ] = call <NEWLINE> self . variant_calls_dict [ <NEWLINE> probe_id ] = call <NEWLINE> <DEDENT> <DEDENT> self . out_json [ self . sample ] [ <STRING> ] = genotypes <NEWLINE> self . out_json [ self . sample ] [ <STRING> ] = filters <NEWLINE> self . out_json [ self . sample ] [ <STRING> ] = self . variant_calls_dict <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"variant_calls\"",
                "\"genotype\"",
                "\"info\"",
                "\"filter\"",
                "\"PASS\"",
                "\"genotype\"",
                "\"genotype\"",
                "\"genotypes\"",
                "\"filtered\"",
                "\"variant_calls\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "be4a29a76ceb40eaa03fad6487e3786d": {
        "code_string": "def run(self):\n         self.create_folder()  # Create application Folder\n         # First Create virtualenv folder\n         answers = prompt(questions, style=style)\n         self.answer = answers\n         self.create_virtualenv()\n         self.main_structure()\n         self.write_file(self.app, \"__init__.py\", app_init)\n         self.write_file(self.utils_path, \"__init__.py\", utils_init)\n         self.write_file(self.utils_path, \"logger.py\", logger)\n         self.write_file(self.utils_path, \"response.py\", response)\n         self.write_file(self.instance, \"config.py\", config)\n         self.make_env()\n         self.write_file(self.folder_name, \"requirements.txt\", requirement_list)\n         self.write_file(self.folder_name, \"docker-compose.yml\", docker_compose)\n         self.write_file(self.folder_name, \"Dockerfile\", Dockerfile)\n         self.write_file(self.folder_name, \"README.md\", readme)\n         if answers.get(\"type\") == \"Restful\" and not answers:\n             self.write_file(self.api_path, \"producer.py\", producer_restful)\n             self.write_file(self.folder_name, \"run.py\", run_restful)\n         elif answers.get(\"type\") == \"Redis pubsub\":\n             ask_redis = prompt(redis_questions, style=style)\n             self.write_file(self.api_path, \"producer.py\", producer_redis)\n             self.write_file(self.folder_name, \"run.py\", run_redis_pubsub)\n             self.write_file(self.folder_name, \"redis_client.py\", redis_client)\n             self.add_env(redis_credentials.format(ask_redis.get(\"redis_pass\", \"test\")))\n             self.update_file(self.folder_name, \"requirements.txt\", redis_required)\n             # dockerizing\n             os.makedirs(self.development)\n             self.write_file(self.development, \"redis.dockerfile\",\n                             redis_dockerfile.format(ask_redis.get(\"redis_pass\", \"test\")))\n             self.write_file(self.development, \"docker-compose.yml\", redis_docker_compose)\n         elif answers.get(\"type\") == \"Rabbitmq RPC\":\n             self.write_file(self.api_path, \"producer.py\", producer_rabbitmq)\n             self.write_file(self.folder_name, \"run.py\", run_rabbitmq)\n             self.write_file(self.folder_name, \"rabbit_client.py\", rabbit_client)\n             self.add_env(rabbitmq_credentials)\n             self.update_file(self.folder_name, \"requirements.txt\", rabbit_required)\n             # dockerizing\n             os.makedirs(self.development)\n             self.write_file(self.development, \"enabled_plugins\", rabbit_enable_plugins)\n             self.write_file(self.development, \"docker-compose.yml\", rabbit_docker_compose)\n",
        "code_toks_joined": "def run ( self ) : <NEWLINE> <INDENT> self . create_folder ( ) <COMMENT> <NEWLINE> <COMMENT> <NL> answers = prompt ( questions , style = style ) <NEWLINE> self . answer = answers <NEWLINE> self . create_virtualenv ( ) <NEWLINE> self . main_structure ( ) <NEWLINE> self . write_file ( self . app , <STRING> , app_init ) <NEWLINE> self . write_file ( self . utils_path , <STRING> , utils_init ) <NEWLINE> self . write_file ( self . utils_path , <STRING> , logger ) <NEWLINE> self . write_file ( self . utils_path , <STRING> , response ) <NEWLINE> self . write_file ( self . instance , <STRING> , config ) <NEWLINE> self . make_env ( ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , requirement_list ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , docker_compose ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , Dockerfile ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , readme ) <NEWLINE> if answers . get ( <STRING> ) == <STRING> and not answers : <NEWLINE> <INDENT> self . write_file ( self . api_path , <STRING> , producer_restful ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , run_restful ) <NEWLINE> <DEDENT> elif answers . get ( <STRING> ) == <STRING> : <NEWLINE> <INDENT> ask_redis = prompt ( redis_questions , style = style ) <NEWLINE> self . write_file ( self . api_path , <STRING> , producer_redis ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , run_redis_pubsub ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , redis_client ) <NEWLINE> self . add_env ( redis_credentials . format ( ask_redis . get ( <STRING> , <STRING> ) ) ) <NEWLINE> self . update_file ( self . folder_name , <STRING> , redis_required ) <NEWLINE> <COMMENT> <NL> os . makedirs ( self . development ) <NEWLINE> self . write_file ( self . development , <STRING> , <NEWLINE> <INDENT> redis_dockerfile . format ( ask_redis . get ( <STRING> , <STRING> ) ) ) <NEWLINE> <DEDENT> self . write_file ( self . development , <STRING> , redis_docker_compose ) <NEWLINE> <DEDENT> elif answers . get ( <STRING> ) == <STRING> : <NEWLINE> <INDENT> self . write_file ( self . api_path , <STRING> , producer_rabbitmq ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , run_rabbitmq ) <NEWLINE> self . write_file ( self . folder_name , <STRING> , rabbit_client ) <NEWLINE> self . add_env ( rabbitmq_credentials ) <NEWLINE> self . update_file ( self . folder_name , <STRING> , rabbit_required ) <NEWLINE> <COMMENT> <NL> os . makedirs ( self . development ) <NEWLINE> self . write_file ( self . development , <STRING> , rabbit_enable_plugins ) <NEWLINE> self . write_file ( self . development , <STRING> , rabbit_docker_compose ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Create application Folder",
                "# First Create virtualenv folder",
                "# dockerizing",
                "# dockerizing"
            ],
            "<STRING>": [
                "\"__init__.py\"",
                "\"__init__.py\"",
                "\"logger.py\"",
                "\"response.py\"",
                "\"config.py\"",
                "\"requirements.txt\"",
                "\"docker-compose.yml\"",
                "\"Dockerfile\"",
                "\"README.md\"",
                "\"type\"",
                "\"Restful\"",
                "\"producer.py\"",
                "\"run.py\"",
                "\"type\"",
                "\"Redis pubsub\"",
                "\"producer.py\"",
                "\"run.py\"",
                "\"redis_client.py\"",
                "\"redis_pass\"",
                "\"test\"",
                "\"requirements.txt\"",
                "\"redis.dockerfile\"",
                "\"redis_pass\"",
                "\"test\"",
                "\"docker-compose.yml\"",
                "\"type\"",
                "\"Rabbitmq RPC\"",
                "\"producer.py\"",
                "\"run.py\"",
                "\"rabbit_client.py\"",
                "\"requirements.txt\"",
                "\"enabled_plugins\"",
                "\"docker-compose.yml\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d4f7fed9bbb47f1ac3bdb027f9e254a": {
        "code_string": "try:\n             new_node = etree.parse(new_file).getroot()\n         except XMLSyntaxError as e:\n             errorstore.add_error(InvalidXML(filename, e.args[0]))\n             return\n         else:\n             traverse_course(edxobj, new_node, new_file, errorstore, pointer=True)\n             return\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> new_node = etree . parse ( new_file ) . getroot ( ) <NEWLINE> except XMLSyntaxError as e : <NEWLINE> errorstore . add_error ( InvalidXML ( filename , e . args [ 0 ] ) ) <NEWLINE> return <NEWLINE> else : <NEWLINE> traverse_course ( edxobj , new_node , new_file , errorstore , pointer = True ) <NEWLINE> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "795488fedb3240929bc18fa91e802222": {
        "code_string": "def show_list(self):\n         \"\"\"\n         Show list of files from home which are not yet added to the repo.\n         \"\"\"\n         self._init_repo()\n         for filename in glob(self.app.get_home_path() + '/.*'):\n             endpoint = Endpoint(self.app, filename)\n             if not endpoint.is_visible():\n                 print(endpoint.path)\n",
        "code_toks_joined": "def show_list ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _init_repo ( ) <NEWLINE> for filename in glob ( self . app . get_home_path ( ) + <STRING> ) : <NEWLINE> <INDENT> endpoint = Endpoint ( self . app , filename ) <NEWLINE> if not endpoint . is_visible ( ) : <NEWLINE> <INDENT> print ( endpoint . path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Show list of files from home which are not yet added to the repo.\n         \"\"\"",
                "'/.*'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5a260c3cdf11450182ba963662ae192f": {
        "code_string": "def container_style(request):\n     classname = 'container'\n     if getattr('ASU_THEME_FLUID', settings, False):\n         classname += '-fluid'\n     return {'asutheme_container_class':  classname}\n",
        "code_toks_joined": "def container_style ( request ) : <NEWLINE> <INDENT> classname = <STRING> <NEWLINE> if getattr ( <STRING> , settings , False ) : <NEWLINE> <INDENT> classname += <STRING> <NEWLINE> <DEDENT> return { <STRING> : classname } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'container'",
                "'ASU_THEME_FLUID'",
                "'-fluid'",
                "'asutheme_container_class'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1821f6be7fca44d19532e883bad128ad": {
        "code_string": "# Loop across NIFTI files\n     os.chdir(path_out)\n     nii_files = glob.glob('*.nii.gz')\n     for nii_file in nii_files:\n         # Loop across contrasts\n         for contrast in list(contrast_dict.keys()):\n             # Check if file name includes contrast listed in dict\n             if contrast in nii_file:\n                 print(\"Detected: \"+nii_file+\" --> \"+contrast)\n                 # Fetch all files with same base name (to include json, bval, etc.), rename and move to BIDS output dir\n                 nii_file_all_exts = glob.glob(nii_file.strip('.nii.gz') + '.*')\n                 for nii_file_all_ext in nii_file_all_exts:\n                     # Build output file name\n                     fname_out = os.path.join(subject, contrast_dict[contrast][1],\n                                              subject + '_' + contrast_dict[contrast][0] + '.'\n                                              + nii_file.split(os.extsep, 1)[1])\n                     os.makedirs(os.path.abspath(os.path.dirname(fname_out)), exist_ok=True)\n                     # Move\n                     shutil.move(nii_file_all_ext, fname_out)\n                 break\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> os . chdir ( path_out ) <NEWLINE> nii_files = glob . glob ( <STRING> ) <NEWLINE> for nii_file in nii_files : <NEWLINE> <COMMENT> <NL> <INDENT> for contrast in list ( contrast_dict . keys ( ) ) : <NEWLINE> <COMMENT> <NL> <INDENT> if contrast in nii_file : <NEWLINE> <INDENT> print ( <STRING> + nii_file + <STRING> + contrast ) <NEWLINE> <COMMENT> <NL> nii_file_all_exts = glob . glob ( nii_file . strip ( <STRING> ) + <STRING> ) <NEWLINE> for nii_file_all_ext in nii_file_all_exts : <NEWLINE> <COMMENT> <NL> <INDENT> fname_out = os . path . join ( subject , contrast_dict [ contrast ] [ 1 ] , <NEWLINE> <INDENT> subject + <STRING> + contrast_dict [ contrast ] [ 0 ] + <STRING> <NEWLINE> + nii_file . split ( os . extsep , 1 ) [ 1 ] ) <NEWLINE> <DEDENT> os . makedirs ( os . path . abspath ( os . path . dirname ( fname_out ) ) , exist_ok = True ) <NEWLINE> <COMMENT> <NL> shutil . move ( nii_file_all_ext , fname_out ) <NEWLINE> <DEDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Loop across NIFTI files",
                "# Loop across contrasts",
                "# Check if file name includes contrast listed in dict",
                "# Fetch all files with same base name (to include json, bval, etc.), rename and move to BIDS output dir",
                "# Build output file name",
                "# Move"
            ],
            "<STRING>": [
                "'*.nii.gz'",
                "\"Detected: \"",
                "\" --> \"",
                "'.nii.gz'",
                "'.*'",
                "'_'",
                "'.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e6f2fa9d316b4bbcb003db397352fdf3": {
        "code_string": "def isstatic(arguments):\n     \"\"\"Return whether an argument list corresponds to a static method.\"\"\"\n     if len(arguments) == 0:\n         return True\n     elif not isinstance(arguments[0], var.SymbolObject):\n         return False\n     else:\n         return 'cls' != arguments[0].name != 'self'\n",
        "code_toks_joined": "def isstatic ( arguments ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if len ( arguments ) == 0 : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> elif not isinstance ( arguments [ 0 ] , var . SymbolObject ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return <STRING> != arguments [ 0 ] . name != <STRING> <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return whether an argument list corresponds to a static method.\"\"\"",
                "'cls'",
                "'self'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b2979c4940a9431a91a41c84e8d3b831": {
        "code_string": "if settings.DEBUG and getattr(response_codec, 'is_default', False):\n",
        "code_toks_joined": "if settings . DEBUG and getattr ( response_codec , <STRING> , False ) : <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'is_default'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cdee155b577646f8b20ef0eeca2aa06e": {
        "code_string": "def write_sorted_file(fpath, outdir=None, cfg=None):\n     if outdir is not None:\n         fbasename = os.path.splitext(os.path.basename(fpath))[0]\n         sorted_fpath = os.path.join(outdir, \"{}.cmp.xml\".format(fpath))\n         tmp = unicode_writer(open(sorted_fpath, 'w'))\n     else:\n         tmp = unicode_writer(NamedTemporaryFile('w'))\n",
        "code_toks_joined": "def write_sorted_file ( fpath , outdir = None , cfg = None ) : <NEWLINE> <INDENT> if outdir is not None : <NEWLINE> <INDENT> fbasename = os . path . splitext ( os . path . basename ( fpath ) ) [ 0 ] <NEWLINE> sorted_fpath = os . path . join ( outdir , <STRING> . format ( fpath ) ) <NEWLINE> tmp = unicode_writer ( open ( sorted_fpath , <STRING> ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> tmp = unicode_writer ( NamedTemporaryFile ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{}.cmp.xml\"",
                "'w'",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d48cb503d2b14e74b2fbb0dffd60f0a0": {
        "code_string": "def get_tracking_delta(self):\n         if len(self.track_value_list) > self.track_average_epoc_count:\n             return sum(\n                 [self.track_value_list[idx + 1] -\n                  self.track_value_list[idx]\n                  for idx in range(len(self.track_value_list) - 1)])\n         else:\n             return 0\n",
        "code_toks_joined": "def get_tracking_delta ( self ) : <NEWLINE> <INDENT> if len ( self . track_value_list ) > self . track_average_epoc_count : <NEWLINE> <INDENT> return sum ( <NEWLINE> <INDENT> [ self . track_value_list [ idx + 1 ] - <NEWLINE> <INDENT> self . track_value_list [ idx ] <NEWLINE> for idx in range ( len ( self . track_value_list ) - 1 ) ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> return 0 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c3891c3e0625464aa655c1182f3842bb": {
        "code_string": "for jrid, jrdef in sorted(list(runs)):\n             yield jrdef\n",
        "code_toks_joined": "for jrid , jrdef in sorted ( list ( runs ) ) : <NEWLINE> <INDENT> yield jrdef <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ab2712d3c32c487697f97b30034fc9c3": {
        "code_string": "if len(intensive_variables) > 0:\n             profile = pd.DataFrame(interpolation[1], columns=extensive_variables)\n             profiles.append(profile)\n",
        "code_toks_joined": "if len ( intensive_variables ) > 0 : <NEWLINE> <INDENT> profile = pd . DataFrame ( interpolation [ 1 ] , columns = extensive_variables ) <NEWLINE> profiles . append ( profile ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ccc6de1b533a4086a9517ba2f7733ad9": {
        "code_string": "__implicit_features[v] = name\n",
        "code_toks_joined": "__implicit_features [ v ] = name <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0dce589fc94841a4b2e6bdff37732bd7": {
        "code_string": "# All the elements of requirements should be present in the result\n     # Record them so that we can handle 'properties'.\n     for r in requirements:\n         # Don't consider conditional requirements.\n         if r.condition():\n             required[r.feature()] = r\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> for r in requirements : <NEWLINE> <COMMENT> <NL> <INDENT> if r . condition ( ) : <NEWLINE> <INDENT> required [ r . feature ( ) ] = r <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# All the elements of requirements should be present in the result",
                "# Record them so that we can handle 'properties'.",
                "# Don't consider conditional requirements."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dc130b31f476485a927aa9fe9a87730e": {
        "code_string": "__module_flags.setdefault(m, []).append(f)\n     __flags.setdefault(rule_or_module, []).append(f)\n",
        "code_toks_joined": "__module_flags . setdefault ( m , [ ] ) . append ( f ) <NEWLINE> <INDENT> __flags . setdefault ( rule_or_module , [ ] ) . append ( f ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7042c6bd1da94dd089fd8812d3af7d34": {
        "code_string": "@numba.njit(cache=True)\n def _get_edges(clustering1: np.array, clustering2: np.array):\n     edges = []\n     offset1 = clustering1.min()\n     offset2 = clustering1.min()\n     # Because of how I've done unique node names, potentially this\n     # could be done in a more generic way by creating a mapping here.\n     offset_clusts1 = clustering1 - offset1\n     offset_clusts2 = clustering2 - offset2\n     # Allocate coincidence matrix\n     nclusts1 = offset_clusts1.max() + 1\n     nclusts2 = offset_clusts2.max() + 1\n     coincidence = np.zeros((nclusts1, nclusts2))\n     # Allocate cluster size arrays\n     ncells1 = np.zeros(nclusts1)\n     ncells2 = np.zeros(nclusts2)\n     # Compute lengths of the intersects\n     for cell in range(len(clustering1)):\n         c1 = offset_clusts1[cell]\n         c2 = offset_clusts2[cell]\n         coincidence[c1, c2] += 1\n         ncells1[c1] += 1\n         ncells2[c2] += 1\n     for cidx1, cidx2 in np.ndindex(coincidence.shape):\n         isize = coincidence[cidx1, cidx2]\n         if isize < 1:\n             continue\n         jaccard_sim = isize / (ncells1[cidx1] + ncells2[cidx2] - isize)\n         edge = (cidx1 + offset1, cidx2 + offset2, jaccard_sim)\n         edges.append(edge)\n     return edges\n",
        "code_toks_joined": "@ numba . njit ( cache = True ) <NEWLINE> <INDENT> def _get_edges ( clustering1 : np . array , clustering2 : np . array ) : <NEWLINE> <INDENT> edges = [ ] <NEWLINE> offset1 = clustering1 . min ( ) <NEWLINE> offset2 = clustering1 . min ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> offset_clusts1 = clustering1 - offset1 <NEWLINE> offset_clusts2 = clustering2 - offset2 <NEWLINE> <COMMENT> <NL> nclusts1 = offset_clusts1 . max ( ) + 1 <NEWLINE> nclusts2 = offset_clusts2 . max ( ) + 1 <NEWLINE> coincidence = np . zeros ( ( nclusts1 , nclusts2 ) ) <NEWLINE> <COMMENT> <NL> ncells1 = np . zeros ( nclusts1 ) <NEWLINE> ncells2 = np . zeros ( nclusts2 ) <NEWLINE> <COMMENT> <NL> for cell in range ( len ( clustering1 ) ) : <NEWLINE> <INDENT> c1 = offset_clusts1 [ cell ] <NEWLINE> c2 = offset_clusts2 [ cell ] <NEWLINE> coincidence [ c1 , c2 ] += 1 <NEWLINE> ncells1 [ c1 ] += 1 <NEWLINE> ncells2 [ c2 ] += 1 <NEWLINE> <DEDENT> for cidx1 , cidx2 in np . ndindex ( coincidence . shape ) : <NEWLINE> <INDENT> isize = coincidence [ cidx1 , cidx2 ] <NEWLINE> if isize < 1 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> jaccard_sim = isize / ( ncells1 [ cidx1 ] + ncells2 [ cidx2 ] - isize ) <NEWLINE> edge = ( cidx1 + offset1 , cidx2 + offset2 , jaccard_sim ) <NEWLINE> edges . append ( edge ) <NEWLINE> <DEDENT> return edges <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Because of how I've done unique node names, potentially this",
                "# could be done in a more generic way by creating a mapping here.",
                "# Allocate coincidence matrix",
                "# Allocate cluster size arrays",
                "# Compute lengths of the intersects"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c8c29bddeda442a8b483762b8fd72e23": {
        "code_string": "def _request(self, resource, method, args=None, data=None, headers=None):\n         response_data = None\n         request_body = self._serialize(data)\n         response_headers, response_content = self._connection.request(resource, method, args=args, body=request_body, headers=headers, content_type=self.content_type)\n         if response_headers.get('status') == HTTP_STATUS_OK:\n             data = self._deserialize(response_content)\n         return Response(response_headers, response_content, response_data)\n",
        "code_toks_joined": "def _request ( self , resource , method , args = None , data = None , headers = None ) : <NEWLINE> <INDENT> response_data = None <NEWLINE> request_body = self . _serialize ( data ) <NEWLINE> response_headers , response_content = self . _connection . request ( resource , method , args = args , body = request_body , headers = headers , content_type = self . content_type ) <NEWLINE> if response_headers . get ( <STRING> ) == HTTP_STATUS_OK : <NEWLINE> <INDENT> data = self . _deserialize ( response_content ) <NEWLINE> <DEDENT> return Response ( response_headers , response_content , response_data ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'status'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2bdc3f6fa0514a839e66942edda549e0": {
        "code_string": "def main():\n     parser = argparse.ArgumentParser(prog=\"python -m {}\".format(__package__),\n                                      description=\"Check a Sublime Text package for common errors.\")\n     # TODO support multiple args and run checkers on each\n     parser.add_argument(\"nargs\", nargs='+', metavar=\"path_or_URL\",\n                         help=\"URL to the repository or path to the package to be checked.\")\n     parser.add_argument(\"--repo-only\", action='store_true',\n                         help=\"Do not check the package itself and only its repository.\")\n     parser.add_argument(\"--verbose\", \"-v\", action='store_true',\n                         help=\"Increase verbosity.\")\n     parser.add_argument(\"--debug\", action='store_true',\n                         help=\"Enter pdb on excpetions. Implies --verbose.\")\n     args = parser.parse_args()\n",
        "code_toks_joined": "def main ( ) : <NEWLINE> <INDENT> parser = argparse . ArgumentParser ( prog = <STRING> . format ( __package__ ) , <NEWLINE> <INDENT> description = <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> parser . add_argument ( <STRING> , nargs = <STRING> , metavar = <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> parser . add_argument ( <STRING> , action = <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> parser . add_argument ( <STRING> , <STRING> , action = <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> parser . add_argument ( <STRING> , action = <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> args = parser . parse_args ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"python -m {}\"",
                "\"Check a Sublime Text package for common errors.\"",
                "\"nargs\"",
                "'+'",
                "\"path_or_URL\"",
                "\"URL to the repository or path to the package to be checked.\"",
                "\"--repo-only\"",
                "'store_true'",
                "\"Do not check the package itself and only its repository.\"",
                "\"--verbose\"",
                "\"-v\"",
                "'store_true'",
                "\"Increase verbosity.\"",
                "\"--debug\"",
                "'store_true'",
                "\"Enter pdb on excpetions. Implies --verbose.\""
            ],
            "<COMMENT>": [
                "# TODO support multiple args and run checkers on each"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af0bbc92ff1440fb8bf6b33ba45c982f": {
        "code_string": "args['request'] = request\n     try:\n       if cache.get(cache_key):\n         try:\n           parsed = re.sub(r'\\[' + item + r'\\]', cache.get(item), parsed)\n         except:\n           pass\n       else:\n         module = import_parser('begood.contrib.shortcodes.parsers.' + name)\n         function = getattr(module, 'parse')\n         result = function(args)\n         try:\n           cache.set(cache_key, result, 3600)\n           parsed = re.sub(r'\\[' + re.escape(item) + r'\\]', result, parsed)\n         except:\n           pass\n     except ImportError:\n       pass\n   return parsed\n",
        "code_toks_joined": "args [ <STRING> ] = request <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if cache . get ( cache_key ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> parsed = re . sub ( <STRING> + item + <STRING> , cache . get ( item ) , parsed ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> module = import_parser ( <STRING> + name ) <NEWLINE> function = getattr ( module , <STRING> ) <NEWLINE> result = function ( args ) <NEWLINE> try : <NEWLINE> <INDENT> cache . set ( cache_key , result , 3600 ) <NEWLINE> parsed = re . sub ( <STRING> + re . escape ( item ) + <STRING> , result , parsed ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except ImportError : <NEWLINE> <INDENT> pass <NEWLINE> return parsed <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'request'",
                "r'\\['",
                "r'\\]'",
                "'begood.contrib.shortcodes.parsers.'",
                "'parse'",
                "r'\\['",
                "r'\\]'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f48e14b34a3046ddb491b9cf69f68b70": {
        "code_string": "async with self._session.post(\n             self.url,\n             params = {'default_format': 'JSONCompact', **self.params},\n             data = query.encode(),\n         ) as response:\n             if response.status != 200:\n                 body = await response.read()\n                 raise DBException.from_message(\n                     statement, body.decode(errors='replace'),\n                 )\n",
        "code_toks_joined": "async with self . _session . post ( <NEWLINE> <INDENT> self . url , <NEWLINE> params = { <STRING> : <STRING> , ** self . params } , <NEWLINE> data = query . encode ( ) , <NEWLINE> ) as response : <NEWLINE> if response . status != 200 : <NEWLINE> <INDENT> body = await response . read ( ) <NEWLINE> raise DBException . from_message ( <NEWLINE> <INDENT> statement , body . decode ( errors = <STRING> ) , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'default_format'",
                "'JSONCompact'",
                "'replace'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5d8d3dd2a11541f98b5a3dea40c7ac47": {
        "code_string": "if default != NoValue:\n             if not_null or primary:\n                 raise ValueError('Can''t have default value')\n             scolumn += ' DEFAULT %s'\n             values.append(default)\n",
        "code_toks_joined": "if default != NoValue : <NEWLINE> <INDENT> if not_null or primary : <NEWLINE> <INDENT> raise ValueError ( <STRING> <STRING> ) <NEWLINE> <DEDENT> scolumn += <STRING> <NEWLINE> values . append ( default ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Can'",
                "'t have default value'",
                "' DEFAULT %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0b99a76f1955442d9cb9587500800b8f": {
        "code_string": "return atoms\n",
        "code_toks_joined": "return atoms <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "006234acad514145bb888349d3526ef9": {
        "code_string": "# levels for failure state (i.e. that cannot reach a final state)\n     failure_states = set()\n     for s in filter(lambda x: x not in z_current, dfa.states):\n         state2level[s] = max_level\n         failure_states.add(s)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> failure_states = set ( ) <NEWLINE> for s in filter ( lambda x : x not in z_current , dfa . states ) : <NEWLINE> <INDENT> state2level [ s ] = max_level <NEWLINE> failure_states . add ( s ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# levels for failure state (i.e. that cannot reach a final state)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "be5fe4fd7fbf4027908aaa2d45b21127": {
        "code_string": "return SteaResult(client.calculate(request), project)\n",
        "code_toks_joined": "return SteaResult ( client . calculate ( request ) , project ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "29ae25f58a76454a92f85fa74245cede": {
        "code_string": "for j in range(n_classes):\n                     print(\"{:>10d}\".format(conf_mat_t[i][j]), end=\"\")\n                 print(\"\")\n             print(\"\")\n",
        "code_toks_joined": "for j in range ( n_classes ) : <NEWLINE> <INDENT> print ( <STRING> . format ( conf_mat_t [ i ] [ j ] ) , end = <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{:>10d}\"",
                "\"\"",
                "\"\"",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d0a845ffeab1467b894b5a5388206c8e": {
        "code_string": "else:\n                         intcheck = (numerics % 1) == 0\n                         if np.sum(intcheck) / length > 0.9:\n                             if \"http://schema.org/Integer\" not in old_metadata['semantic_types']:\n                                 old_metadata['semantic_types'] += (\"http://schema.org/Integer\",)\n                                 old_metadata['structural_type'] = type(10)\n                                 inputs.iloc[:, col] = numerics\n                         else:\n                             if \"http://schema.org/Float\" not in old_metadata['semantic_types']:\n                                 old_metadata['semantic_types'] = (\"http://schema.org/Float\",)\n                                 old_metadata['structural_type'] = type(10.2)\n                                 inputs.iloc[:, col] = numerics\n",
        "code_toks_joined": "else : <NEWLINE> <INDENT> intcheck = ( numerics % 1 ) == 0 <NEWLINE> if np . sum ( intcheck ) / length > 0.9 : <NEWLINE> <INDENT> if <STRING> not in old_metadata [ <STRING> ] : <NEWLINE> <INDENT> old_metadata [ <STRING> ] += ( <STRING> , ) <NEWLINE> old_metadata [ <STRING> ] = type ( 10 ) <NEWLINE> inputs . iloc [ : , col ] = numerics <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if <STRING> not in old_metadata [ <STRING> ] : <NEWLINE> <INDENT> old_metadata [ <STRING> ] = ( <STRING> , ) <NEWLINE> old_metadata [ <STRING> ] = type ( 10.2 ) <NEWLINE> inputs . iloc [ : , col ] = numerics <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"http://schema.org/Integer\"",
                "'semantic_types'",
                "'semantic_types'",
                "\"http://schema.org/Integer\"",
                "'structural_type'",
                "\"http://schema.org/Float\"",
                "'semantic_types'",
                "'semantic_types'",
                "\"http://schema.org/Float\"",
                "'structural_type'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0992fbce91094c92ac48d6c68e0fb13c": {
        "code_string": "# The check on 'pattern' here allows us to apply a pattern filter on top of others\n             if 'pattern' in rule[1]:\n                 # Don't filter as an exact match on the text entered; match per word.\n                 for pattern in shlex.split(rule[1]['pattern']):\n                     if rule[1]['_rule_type'] == 'containment_multiple':\n                         sql_tuple = FilterTree.text_similarity_filter(rule[0], pattern, True)\n                     else:\n                         sql_tuple = FilterTree.text_similarity_filter(rule[0], pattern, False)\n                     pattern_specs.append(sql_tuple)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if <STRING> in rule [ 1 ] : <NEWLINE> <COMMENT> <NL> <INDENT> for pattern in shlex . split ( rule [ 1 ] [ <STRING> ] ) : <NEWLINE> <INDENT> if rule [ 1 ] [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> sql_tuple = FilterTree . text_similarity_filter ( rule [ 0 ] , pattern , True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sql_tuple = FilterTree . text_similarity_filter ( rule [ 0 ] , pattern , False ) <NEWLINE> <DEDENT> pattern_specs . append ( sql_tuple ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# The check on 'pattern' here allows us to apply a pattern filter on top of others",
                "# Don't filter as an exact match on the text entered; match per word."
            ],
            "<STRING>": [
                "'pattern'",
                "'pattern'",
                "'_rule_type'",
                "'containment_multiple'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "18b140d05a5848c29b5fec64631edc96": {
        "code_string": "class Path(AbstractPath):\n     abs = abspath = property(lambda self: pth(ospath.abspath(self)))\n     exists = property(lambda self: ospath.exists(self))\n     lexists = property(lambda self: ospath.lexists(self))\n     expanduser = property(lambda self: pth(ospath.expanduser(self)))\n     expandvars = property(lambda self: pth(ospath.expandvars(self)))\n     atime = property(lambda self: ospath.getatime(self))\n     ctime = property(lambda self: ospath.getctime(self))\n     mtime = property(lambda self: ospath.getmtime(self))\n     size = property(lambda self: ospath.getsize(self))\n     isdir = property(lambda self: ospath.isdir(self))\n     isfile = property(lambda self: ospath.isfile(self))\n     islink = property(lambda self: ospath.islink(self))\n     ismount = property(lambda self: ospath.ismount(self))\n     joinpath = pathjoin = __div__ = __floordiv__ = __truediv__ = lambda self, *args: pth(ospath.join(self, *args))\n     normcase = property(lambda self: pth(ospath.normcase(self)))\n     normpath = property(lambda self: pth(ospath.normpath(self)))\n     norm = property(lambda self: pth(ospath.normcase(ospath.normpath(self))))\n     real = realpath = property(lambda self: pth(ospath.realpath(self)))\n     rel = relpath = lambda self, start: pth(ospath.relpath(self, start))\n     same = samefile = lambda self, other: ospath.samefile(self, other)\n     if hasattr(os, 'link'):\n         if PY33:\n             link = lambda self, dest, follow_symlinks=True, **kwargs: os.link(self, dest, follow_symlinks=follow_symlinks, **kwargs)\n         else:\n             link = lambda self, dest: os.link(self, dest)\n     if PY33:\n         stat = property(lambda self: LazyObjectProxy(lambda **kwargs: os.stat(self, **kwargs)))\n         lstat = property(lambda self: LazyObjectProxy(lambda **kwargs: os.lstat(self, **kwargs)))\n     else:\n         stat = property(lambda self: os.stat(self))\n         lstat = property(lambda self: os.lstat(self))\n     isreadable = property(lambda self: LazyObjectProxy(lambda **kwargs: os.access(self, os.R_OK, **kwargs)))\n     mkdir = lambda self: os.mkdir(self)\n     makedirs = lambda self: os.makedirs(self)\n     if hasattr(os, 'pathconf'):\n         pathconf = lambda self, name: os.pathconf(self, name)\n     if hasattr(os, 'readlink'):\n         readlink = property(lambda self: os.readlink(self))\n     if hasattr(os, 'fsencode'):\n         fsencode = fsencoded = property(lambda self: os.fsencode(self))\n     access = lambda self, mode, **kwargs: os.access(self, mode, **kwargs)\n     if PY33:\n         isreadable = property(lambda self: LazyObjectProxy(lambda **kwargs: os.access(self, os.R_OK, **kwargs)))\n         iswritable = property(lambda self: LazyObjectProxy(lambda **kwargs: os.access(self, os.W_OK, **kwargs)))\n         isexecutable = property(lambda self: LazyObjectProxy(lambda **kwargs: os.access(self, os.R_OK | os.X_OK, **kwargs)))\n     else:\n         isreadable = property(lambda self: os.access(self, os.R_OK))\n         iswritable = property(lambda self: os.access(self, os.W_OK))\n         isexecutable = property(lambda self: os.access(self, os.R_OK | os.X_OK))\n     if hasattr(os, 'chroot'):\n         chroot = lambda self: os.chroot(self)\n     if hasattr(os, 'chflags'):\n         chflags = lambda self, flags, follow_symlinks=True: os.chflags(self, flags) if follow_symlinks else os.lchflags(self, flags)\n         lchflags = lambda self, flags: os.lchflags(self, flags)\n",
        "code_toks_joined": "class Path ( AbstractPath ) : <NEWLINE> <INDENT> abs = abspath = property ( lambda self : pth ( ospath . abspath ( self ) ) ) <NEWLINE> exists = property ( lambda self : ospath . exists ( self ) ) <NEWLINE> lexists = property ( lambda self : ospath . lexists ( self ) ) <NEWLINE> expanduser = property ( lambda self : pth ( ospath . expanduser ( self ) ) ) <NEWLINE> expandvars = property ( lambda self : pth ( ospath . expandvars ( self ) ) ) <NEWLINE> atime = property ( lambda self : ospath . getatime ( self ) ) <NEWLINE> ctime = property ( lambda self : ospath . getctime ( self ) ) <NEWLINE> mtime = property ( lambda self : ospath . getmtime ( self ) ) <NEWLINE> size = property ( lambda self : ospath . getsize ( self ) ) <NEWLINE> isdir = property ( lambda self : ospath . isdir ( self ) ) <NEWLINE> isfile = property ( lambda self : ospath . isfile ( self ) ) <NEWLINE> islink = property ( lambda self : ospath . islink ( self ) ) <NEWLINE> ismount = property ( lambda self : ospath . ismount ( self ) ) <NEWLINE> joinpath = pathjoin = __div__ = __floordiv__ = __truediv__ = lambda self , * args : pth ( ospath . join ( self , * args ) ) <NEWLINE> normcase = property ( lambda self : pth ( ospath . normcase ( self ) ) ) <NEWLINE> normpath = property ( lambda self : pth ( ospath . normpath ( self ) ) ) <NEWLINE> norm = property ( lambda self : pth ( ospath . normcase ( ospath . normpath ( self ) ) ) ) <NEWLINE> real = realpath = property ( lambda self : pth ( ospath . realpath ( self ) ) ) <NEWLINE> rel = relpath = lambda self , start : pth ( ospath . relpath ( self , start ) ) <NEWLINE> same = samefile = lambda self , other : ospath . samefile ( self , other ) <NEWLINE> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> if PY33 : <NEWLINE> <INDENT> link = lambda self , dest , follow_symlinks = True , ** kwargs : os . link ( self , dest , follow_symlinks = follow_symlinks , ** kwargs ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> link = lambda self , dest : os . link ( self , dest ) <NEWLINE> <DEDENT> <DEDENT> if PY33 : <NEWLINE> <INDENT> stat = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . stat ( self , ** kwargs ) ) ) <NEWLINE> lstat = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . lstat ( self , ** kwargs ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat = property ( lambda self : os . stat ( self ) ) <NEWLINE> lstat = property ( lambda self : os . lstat ( self ) ) <NEWLINE> <DEDENT> isreadable = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . access ( self , os . R_OK , ** kwargs ) ) ) <NEWLINE> mkdir = lambda self : os . mkdir ( self ) <NEWLINE> makedirs = lambda self : os . makedirs ( self ) <NEWLINE> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> pathconf = lambda self , name : os . pathconf ( self , name ) <NEWLINE> <DEDENT> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> readlink = property ( lambda self : os . readlink ( self ) ) <NEWLINE> <DEDENT> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> fsencode = fsencoded = property ( lambda self : os . fsencode ( self ) ) <NEWLINE> <DEDENT> access = lambda self , mode , ** kwargs : os . access ( self , mode , ** kwargs ) <NEWLINE> if PY33 : <NEWLINE> <INDENT> isreadable = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . access ( self , os . R_OK , ** kwargs ) ) ) <NEWLINE> iswritable = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . access ( self , os . W_OK , ** kwargs ) ) ) <NEWLINE> isexecutable = property ( lambda self : LazyObjectProxy ( lambda ** kwargs : os . access ( self , os . R_OK | os . X_OK , ** kwargs ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> isreadable = property ( lambda self : os . access ( self , os . R_OK ) ) <NEWLINE> iswritable = property ( lambda self : os . access ( self , os . W_OK ) ) <NEWLINE> isexecutable = property ( lambda self : os . access ( self , os . R_OK | os . X_OK ) ) <NEWLINE> <DEDENT> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> chroot = lambda self : os . chroot ( self ) <NEWLINE> <DEDENT> if hasattr ( os , <STRING> ) : <NEWLINE> <INDENT> chflags = lambda self , flags , follow_symlinks = True : os . chflags ( self , flags ) if follow_symlinks else os . lchflags ( self , flags ) <NEWLINE> lchflags = lambda self , flags : os . lchflags ( self , flags ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'link'",
                "'pathconf'",
                "'readlink'",
                "'fsencode'",
                "'chroot'",
                "'chflags'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "34fc44f678d941fb94b8e154c52da388": {
        "code_string": "(mode, ino, dev, nlink, uid, gid, size, atime, mtime, ctime) = _stat(_fd)\n     # we need to be closer to creation, so ctime is for us.\n     # Just leaving those here, in case needed\n     # _atime = time.strftime(\"%d/%m/%Y %H:%M\", time.gmtime(atime))\n     # _mtime = time.strftime(\"%d/%m/%Y %H:%M\", time.gmtime(mtime))\n     # _ctime = time.strftime(\"%d/%m/%Y %H:%M\", time.gmtime(ctime))\n     return time.strftime(\n         \"%d/%m/%Y %H:%M GMT\",\n         time.gmtime(ctime)\n     )\n",
        "code_toks_joined": "( mode , ino , dev , nlink , uid , gid , size , atime , mtime , ctime ) = _stat ( _fd ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> return time . strftime ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> time . gmtime ( ctime ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# we need to be closer to creation, so ctime is for us.",
                "# Just leaving those here, in case needed",
                "# _atime = time.strftime(\"%d/%m/%Y %H:%M\", time.gmtime(atime))",
                "# _mtime = time.strftime(\"%d/%m/%Y %H:%M\", time.gmtime(mtime))",
                "# _ctime = time.strftime(\"%d/%m/%Y %H:%M\", time.gmtime(ctime))"
            ],
            "<STRING>": [
                "\"%d/%m/%Y %H:%M GMT\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "51a3d454fae14d20ac8ac9be083b6571": {
        "code_string": "@cli.command('list', short_help=\"Lists the stored connection info\")\n @click.option('--name', '-n',\n               type=str,\n               required=False,\n               help=\"\"\"The name of the connection you wish to view.\"\"\")\n @click.option('--index', '-i',\n               type=int,\n               required=False,\n               help=\"\"\"The index of the connection you wish to view - 0 is the oldest, 1 is the second oldest, and -1 is the newest.\"\"\")\n @pass_context\n def remove(ctx, name=None, index=None):\n     connectionsCsvLocation = resource_filename(Requirement.parse(\"solidfire-cli\"), \"connections.csv\")\n     with open(connectionsCsvLocation) as connectionFile:\n         connections = list(csv.DictReader(connectionFile, delimiter=','))\n     print(connectionsCsvLocation)\n     if(name is None and index is None):\n         cli_utils.print_result(connections, ctx.logger, as_json=ctx.json, as_pickle=ctx.pickle, depth=ctx.depth, filter_tree=ctx.filter_tree)\n     if(name is None and index is not None):\n         cli_utils.print_result(connections[int(index)], ctx.logger, as_json=ctx.json, as_pickle=ctx.pickle, depth=ctx.depth, filter_tree=ctx.filter_tree)\n     if(name is not None and index is None):\n         connections = [connection for connection in connections if connection[\"name\"]!=name]\n         cli_utils.print_result(connections, ctx.logger, as_json=ctx.json, as_pickle=ctx.pickle, depth=ctx.depth, filter_tree=ctx.filter_tree)\n",
        "code_toks_joined": "@ cli . command ( <STRING> , short_help = <STRING> ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , <NEWLINE> <INDENT> type = str , <NEWLINE> required = False , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , <NEWLINE> <INDENT> type = int , <NEWLINE> required = False , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT> @ pass_context <NEWLINE> def remove ( ctx , name = None , index = None ) : <NEWLINE> <INDENT> connectionsCsvLocation = resource_filename ( Requirement . parse ( <STRING> ) , <STRING> ) <NEWLINE> with open ( connectionsCsvLocation ) as connectionFile : <NEWLINE> <INDENT> connections = list ( csv . DictReader ( connectionFile , delimiter = <STRING> ) ) <NEWLINE> <DEDENT> print ( connectionsCsvLocation ) <NEWLINE> if ( name is None and index is None ) : <NEWLINE> <INDENT> cli_utils . print_result ( connections , ctx . logger , as_json = ctx . json , as_pickle = ctx . pickle , depth = ctx . depth , filter_tree = ctx . filter_tree ) <NEWLINE> <DEDENT> if ( name is None and index is not None ) : <NEWLINE> <INDENT> cli_utils . print_result ( connections [ int ( index ) ] , ctx . logger , as_json = ctx . json , as_pickle = ctx . pickle , depth = ctx . depth , filter_tree = ctx . filter_tree ) <NEWLINE> <DEDENT> if ( name is not None and index is None ) : <NEWLINE> <INDENT> connections = [ connection for connection in connections if connection [ <STRING> ] != name ] <NEWLINE> cli_utils . print_result ( connections , ctx . logger , as_json = ctx . json , as_pickle = ctx . pickle , depth = ctx . depth , filter_tree = ctx . filter_tree ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'list'",
                "\"Lists the stored connection info\"",
                "'--name'",
                "'-n'",
                "\"\"\"The name of the connection you wish to view.\"\"\"",
                "'--index'",
                "'-i'",
                "\"\"\"The index of the connection you wish to view - 0 is the oldest, 1 is the second oldest, and -1 is the newest.\"\"\"",
                "\"solidfire-cli\"",
                "\"connections.csv\"",
                "','",
                "\"name\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6a86fcee3acd4ee6b5553565ad258060": {
        "code_string": "def delay_job(self, job, time_delta):\n         amount = int(time_delta.total_seconds())\n         self.connection.zincrby(self.scheduler_jobs_key, job.id, amount)\n",
        "code_toks_joined": "def delay_job ( self , job , time_delta ) : <NEWLINE> <INDENT> amount = int ( time_delta . total_seconds ( ) ) <NEWLINE> self . connection . zincrby ( self . scheduler_jobs_key , job . id , amount ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3f3dc62826d948a8b1ca3c59bece91c3": {
        "code_string": "for spouse_uid in spouse_uids:\n     try:\n       spouse = family.uid_to_person(spouse_uid)\n     except TypeError as e:\n       print(f\"Warning: Nobody has uid {uid}, so they can't be anyone's\")\n       print(\"spouse.  Skipping.\")\n       continue\n     spouses = [\n       family.uid_to_person(relation[1])\n       for relation in big_dict['spouse']\n       if relation[0] == spouse_uid\n     ]\n     family.add_spouses(spouse, children)\n",
        "code_toks_joined": "for spouse_uid in spouse_uids : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> spouse = family . uid_to_person ( spouse_uid ) <NEWLINE> <DEDENT> except TypeError as e : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> continue <NEWLINE> <DEDENT> spouses = [ <NEWLINE> <INDENT> family . uid_to_person ( relation [ 1 ] ) <NEWLINE> for relation in big_dict [ <STRING> ] <NEWLINE> if relation [ 0 ] == spouse_uid <NEWLINE> <DEDENT> ] <NEWLINE> family . add_spouses ( spouse , children ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"Warning: Nobody has uid {uid}, so they can't be anyone's\"",
                "\"spouse.  Skipping.\"",
                "'spouse'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1734ada5e58142f19da643311b73d593": {
        "code_string": "# Delete roles\n     if isinstance(db, SQLAlchemy):\n         role1 = db_adapter.find_first_object(RoleClass, name='Role 1')\n         db_adapter.delete_object(role1)\n         role2 = db_adapter.find_first_object(RoleClass, name='Role 2')\n         db_adapter.delete_object(role1)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if isinstance ( db , SQLAlchemy ) : <NEWLINE> <INDENT> role1 = db_adapter . find_first_object ( RoleClass , name = <STRING> ) <NEWLINE> db_adapter . delete_object ( role1 ) <NEWLINE> role2 = db_adapter . find_first_object ( RoleClass , name = <STRING> ) <NEWLINE> db_adapter . delete_object ( role1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Delete roles"
            ],
            "<STRING>": [
                "'Role 1'",
                "'Role 2'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b307465e821944e8b5b73c062f55225b": {
        "code_string": "if (outer_sort == outer_sort_next):\n                     if (inner_sort > inner_sort_next):\n                         print(\"For Halo ID {0} we had a {1} of {2}.  After sorting via lexsort \"\n                               \"inner-key {1}, the next Halo has ID {3} and a {1} of {4}\"\n                               .format(halo_id, opt[\"sort_mass\"], inner_sort, halo_id_next,\n                               inner_sort_next))\n                         print(\"Since we are sorting using {0} they MUST be in ascending \"\n                               \"order.\".format(opt[\"sort_mass\"]))\n",
        "code_toks_joined": "if ( outer_sort == outer_sort_next ) : <NEWLINE> <INDENT> if ( inner_sort > inner_sort_next ) : <NEWLINE> <INDENT> print ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> . format ( halo_id , opt [ <STRING> ] , inner_sort , halo_id_next , <NEWLINE> inner_sort_next ) ) <NEWLINE> <DEDENT> print ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( opt [ <STRING> ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"For Halo ID {0} we had a {1} of {2}.  After sorting via lexsort \"",
                "\"inner-key {1}, the next Halo has ID {3} and a {1} of {4}\"",
                "\"sort_mass\"",
                "\"Since we are sorting using {0} they MUST be in ascending \"",
                "\"order.\"",
                "\"sort_mass\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "05b6333f509e46e6bac4a854c6d3c743": {
        "code_string": "if (outer_sort == outer_sort_next):\n                     if (inner_sort < inner_sort_next):\n                         print(\"For Halo ID {0} we had a {1} of {2}.  After sorting via lexsort \"\n                               \"inner-key {1}, the next Halo has ID {3} and a {1} of {4}\"\n                               .format(halo_id, opt[\"sort_mass\"], inner_sort, halo_id_next,\n                               inner_sort_next))\n                         print(\"Since we are sorting using {0} they MUST be in ascending \"\n                               \"order.\".format(opt[\"sort_mass\"]))\n",
        "code_toks_joined": "if ( outer_sort == outer_sort_next ) : <NEWLINE> <INDENT> if ( inner_sort < inner_sort_next ) : <NEWLINE> <INDENT> print ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> . format ( halo_id , opt [ <STRING> ] , inner_sort , halo_id_next , <NEWLINE> inner_sort_next ) ) <NEWLINE> <DEDENT> print ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( opt [ <STRING> ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"For Halo ID {0} we had a {1} of {2}.  After sorting via lexsort \"",
                "\"inner-key {1}, the next Halo has ID {3} and a {1} of {4}\"",
                "\"sort_mass\"",
                "\"Since we are sorting using {0} they MUST be in ascending \"",
                "\"order.\"",
                "\"sort_mass\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9d8138b9f2274bee83402db9c8406074": {
        "code_string": "# word 83 \"Commands and feature sets supported\"\n         features = int.from_bytes(buf[166] + buf[167], byteorder='little')\n         if major & 0x400:\n             self.lba48bit = True\n         else:\n             self.lba48bit = False\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> features = int . from_bytes ( buf [ 166 ] + buf [ 167 ] , byteorder = <STRING> ) <NEWLINE> if major & 0x400 : <NEWLINE> <INDENT> self . lba48bit = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . lba48bit = False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# word 83 \"Commands and feature sets supported\""
            ],
            "<STRING>": [
                "'little'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f8a127ab58ea4820a1530c575fd53b6e": {
        "code_string": "# add identifier\n     fig.text(x=.5, y=.99, s=qpi_real[\"identifier\"],\n              verticalalignment=\"top\",\n              horizontalalignment=\"center\",\n              fontsize=14)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> fig . text ( x = .5 , y = .99 , s = qpi_real [ <STRING> ] , <NEWLINE> <INDENT> verticalalignment = <STRING> , <NEWLINE> horizontalalignment = <STRING> , <NEWLINE> fontsize = 14 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# add identifier"
            ],
            "<STRING>": [
                "\"identifier\"",
                "\"top\"",
                "\"center\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3619ee3e06e54527a0efd4cc283a3143": {
        "code_string": "def validate_python(self, values, state):\n         \"\"\" Check for the uniqueness of `email`.\"\"\"\n         email = values['email']\n         if email: # no check for None emails\n             user = AuthUser.get_by_email(email)\n             # user_id is used to not raise an error when editing the user\n             # the user_id must be available as hidden field in the edit form\n             if values.has_key('user_id'):\n                 user_id = values['user_id']\n             else:\n                 user_id = None\n             if email and (user.user_id != user_id):\n                 errors = {'email': self.message('not_unique_email', state)}\n                 raise Invalid(self.message('not_unique_email', state),\n                                            values, state, error_dict=errors)\n",
        "code_toks_joined": "def validate_python ( self , values , state ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> email = values [ <STRING> ] <NEWLINE> if email : <COMMENT> <NEWLINE> <INDENT> user = AuthUser . get_by_email ( email ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if values . has_key ( <STRING> ) : <NEWLINE> <INDENT> user_id = values [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> user_id = None <NEWLINE> <DEDENT> if email and ( user . user_id != user_id ) : <NEWLINE> <INDENT> errors = { <STRING> : self . message ( <STRING> , state ) } <NEWLINE> raise Invalid ( self . message ( <STRING> , state ) , <NEWLINE> <INDENT> values , state , error_dict = errors ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Check for the uniqueness of `email`.\"\"\"",
                "'email'",
                "'user_id'",
                "'user_id'",
                "'email'",
                "'not_unique_email'",
                "'not_unique_email'"
            ],
            "<COMMENT>": [
                "# no check for None emails",
                "# user_id is used to not raise an error when editing the user",
                "# the user_id must be available as hidden field in the edit form"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9023a6e539ce4f1388ac3036561cd3d4": {
        "code_string": "for value in [command[\"maf\"], command[\"call\"], command[\"rep\"], command[\"seq_identity\"]]:\n             if value != -1:\n                 if value < 1 or value > 1:\n                     raise ValueError(\"Filter and identity thresholds must be larger >= 0 and <= 1.\")\n",
        "code_toks_joined": "for value in [ command [ <STRING> ] , command [ <STRING> ] , command [ <STRING> ] , command [ <STRING> ] ] : <NEWLINE> <INDENT> if value != - 1 : <NEWLINE> <INDENT> if value < 1 or value > 1 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"maf\"",
                "\"call\"",
                "\"rep\"",
                "\"seq_identity\"",
                "\"Filter and identity thresholds must be larger >= 0 and <= 1.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "35420148dda249b9ab912a37faee55e4": {
        "code_string": "if process.returncode != 0:\n         msg = 'Phantom failed to set up calculation'\n         logger.error(msg)\n         raise SetupError(msg)\n     else:\n         logger.info('Successfully set up Phantom calculation')\n         logger.info(f'run_path: {run_path}')\n",
        "code_toks_joined": "if process . returncode != 0 : <NEWLINE> <INDENT> msg = <STRING> <NEWLINE> logger . error ( msg ) <NEWLINE> raise SetupError ( msg ) <NEWLINE> else : <NEWLINE> logger . info ( <STRING> ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Phantom failed to set up calculation'",
                "'Successfully set up Phantom calculation'",
                "f'run_path: {run_path}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f41cdfa0903c4579931aab2320d4cf4a": {
        "code_string": "# MET Resolution\n         dimu_pt, dimu_phi, dimu_para, dimu_perp = create_metres(\n             event.METnoX, event.MuonSelection,\n         )\n         event.DiMuon_pt = dimu_pt\n         event.DiMuon_phi = dimu_phi\n         event.METnoX_diMuonParaProjPt = dimu_para\n         event.METnoX_diMuonPerpProjPt = dimu_perp\n         event.METnoX_diMuonParaProjPt_Minus_DiMuon_pt = dimu_para - dimu_pt\n         event.METnoX_diMuonPerpProjPt_Plus_DiMuon_pt = dimu_perp + dimu_pt\n         event.METnoX_diMuonParaProjPt_Div_DiMuon_pt = dimu_para / dimu_pt\n         event.METnoX_diMuonPerpProjPt_Plus_DiMuon_pt_Div_DiMuon_pt = (dimu_para + dimu_pt) / dimu_pt\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> dimu_pt , dimu_phi , dimu_para , dimu_perp = create_metres ( <NEWLINE> <INDENT> event . METnoX , event . MuonSelection , <NEWLINE> <DEDENT> ) <NEWLINE> event . DiMuon_pt = dimu_pt <NEWLINE> event . DiMuon_phi = dimu_phi <NEWLINE> event . METnoX_diMuonParaProjPt = dimu_para <NEWLINE> event . METnoX_diMuonPerpProjPt = dimu_perp <NEWLINE> event . METnoX_diMuonParaProjPt_Minus_DiMuon_pt = dimu_para - dimu_pt <NEWLINE> event . METnoX_diMuonPerpProjPt_Plus_DiMuon_pt = dimu_perp + dimu_pt <NEWLINE> event . METnoX_diMuonParaProjPt_Div_DiMuon_pt = dimu_para / dimu_pt <NEWLINE> event . METnoX_diMuonPerpProjPt_Plus_DiMuon_pt_Div_DiMuon_pt = ( dimu_para + dimu_pt ) / dimu_pt <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# MET Resolution"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1059fc6a7dfe4c6c8dcbaa7f126e5106": {
        "code_string": "for name, obj in getattr(mod, '_rr_export', {}).iteritems():\n                scope[name] = name\n",
        "code_toks_joined": "for name , obj in getattr ( mod , <STRING> , { } ) . iteritems ( ) : <NEWLINE> <INDENT> scope [ name ] = name <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'_rr_export'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "788ed96c2ebb4619b85216c40198d34f": {
        "code_string": "def _scheduler(self, epoch):\n         if epoch%self.decay_after_n_epoch==0 and epoch!=0:\n             lr = K.get_value(self.model.optimizer.lr)\n             K.set_value(self.model.optimizer.lr, lr*self.decay_rate)\n             print(\"lr changed to {}\".format(lr**self.decay_rate))\n         return K.get_value(self.model.optimizer.lr)\n",
        "code_toks_joined": "def _scheduler ( self , epoch ) : <NEWLINE> <INDENT> if epoch % self . decay_after_n_epoch == 0 and epoch != 0 : <NEWLINE> <INDENT> lr = K . get_value ( self . model . optimizer . lr ) <NEWLINE> K . set_value ( self . model . optimizer . lr , lr * self . decay_rate ) <NEWLINE> print ( <STRING> . format ( lr ** self . decay_rate ) ) <NEWLINE> <DEDENT> return K . get_value ( self . model . optimizer . lr ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"lr changed to {}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0fa2a610429e4180b3b692f7a9815bba": {
        "code_string": "@_run\n     def update(self,document,target,**kargs):\n         self._db[document].update(kargs,target,callback=self.callback)\n",
        "code_toks_joined": "@ _run <NEWLINE> <INDENT> def update ( self , document , target , ** kargs ) : <NEWLINE> <INDENT> self . _db [ document ] . update ( kargs , target , callback = self . callback ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ddbd0cc093e947a6bbc2b0fd8d57f728": {
        "code_string": "structure_txt = generate_LAMMPS_structure(structure)\n         input_txt = generate_LAMMPS_input(potential_data,\n                                           parameters_data,\n                                           structure_file=self._INPUT_STRUCTURE,\n                                           optimize_path_file=self._OUTPUT_TRAJECTORY_FILE_NAME)\n",
        "code_toks_joined": "structure_txt = generate_LAMMPS_structure ( structure ) <NEWLINE> <INDENT> input_txt = generate_LAMMPS_input ( potential_data , <NEWLINE> <INDENT> parameters_data , <NEWLINE> structure_file = self . _INPUT_STRUCTURE , <NEWLINE> optimize_path_file = self . _OUTPUT_TRAJECTORY_FILE_NAME ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "26684fce431d4616b4fcda23a36b2daa": {
        "code_string": "if max_stress < test_range[1] or min_stress > test_range[0]:\n                 if abs(max_stress - test_range[1]) < interval * 2 or abs(test_range[0] - min_stress) < interval * 2:\n                     interval *= 0.5\n",
        "code_toks_joined": "if max_stress < test_range [ 1 ] or min_stress > test_range [ 0 ] : <NEWLINE> <INDENT> if abs ( max_stress - test_range [ 1 ] ) < interval * 2 or abs ( test_range [ 0 ] - min_stress ) < interval * 2 : <NEWLINE> <INDENT> interval *= 0.5 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "048b477eb8184f8bb43d9d3f60661728": {
        "code_string": "if isinstance(radius, Variable):\n             self._radius = dcopy(radius)\n         else:\n             self._radius = Variable(\n                 \"radius\",\n                 radius,\n                 \"r\",\n                 \"m\",\n                 \"Inner radius of well '\" + str(name) + \"'\",\n             )\n         if not self._radius.scalar:\n             raise ValueError(\"Well: 'radius' needs to be scalar\")\n         if self.radius <= 0.0:\n             raise ValueError(\"Well: 'radius' needs to be positiv\")\n",
        "code_toks_joined": "if isinstance ( radius , Variable ) : <NEWLINE> <INDENT> self . _radius = dcopy ( radius ) <NEWLINE> else : <NEWLINE> self . _radius = Variable ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> radius , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> + str ( name ) + <STRING> , <NEWLINE> <DEDENT> ) <NEWLINE> if not self . _radius . scalar : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> if self . radius <= 0.0 : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"radius\"",
                "\"r\"",
                "\"m\"",
                "\"Inner radius of well '\"",
                "\"'\"",
                "\"Well: 'radius' needs to be scalar\"",
                "\"Well: 'radius' needs to be positiv\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "60ef9c46a33d4655b08808b393c783dd": {
        "code_string": "observation = varlib.Observation(name, time, obs, description)\n     except Exception:\n         raise Exception(\"loadObs: loading the observation was not possible\")\n     return observation\n",
        "code_toks_joined": "observation = varlib . Observation ( name , time , obs , description ) <NEWLINE> <INDENT> except Exception : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> return observation <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"loadObs: loading the observation was not possible\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9417e7bd59114fcc977e4a60ce5ee2e2": {
        "code_string": "response = await venom.invoke(method, request, context=AioHTTPRequestContext(request))\n             return web.Response(body=rpc_response.pack(response),\n                                 content_type=rpc_response.mime,\n                                 status=http_status)\n         except Error as e:\n             return web.Response(body=rpc_error_response.pack(e.format()),\n                                 content_type=rpc_error_response.mime,\n                                 status=e.http_status)\n",
        "code_toks_joined": "response = await venom . invoke ( method , request , context = AioHTTPRequestContext ( request ) ) <NEWLINE> <INDENT> return web . Response ( body = rpc_response . pack ( response ) , <NEWLINE> <INDENT> content_type = rpc_response . mime , <NEWLINE> status = http_status ) <NEWLINE> except Error as e : <NEWLINE> <DEDENT> return web . Response ( body = rpc_error_response . pack ( e . format ( ) ) , <NEWLINE> <INDENT> content_type = rpc_error_response . mime , <NEWLINE> status = e . http_status ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c782bba6ce974c619314cc508c0b5225": {
        "code_string": "# set app_index_class on app to \"None\" to skip creation\n         app_index_class = getattr(app_config, 'app_index_class', None)\n         if app_index_class:\n             template_name = getattr(app_config, 'template_name', 'app_index.html')\n             app_index = app_index_class.as_view(\n                 app_config=app_config, backend=self, template_name=template_name\n             )\n             urlpatterns[None].append(url(r'^$', app_index, name='index'))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> app_index_class = getattr ( app_config , <STRING> , None ) <NEWLINE> if app_index_class : <NEWLINE> <INDENT> template_name = getattr ( app_config , <STRING> , <STRING> ) <NEWLINE> app_index = app_index_class . as_view ( <NEWLINE> <INDENT> app_config = app_config , backend = self , template_name = template_name <NEWLINE> <DEDENT> ) <NEWLINE> urlpatterns [ None ] . append ( url ( <STRING> , app_index , name = <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# set app_index_class on app to \"None\" to skip creation"
            ],
            "<STRING>": [
                "'app_index_class'",
                "'template_name'",
                "'app_index.html'",
                "r'^$'",
                "'index'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "096b6e7b57964801b0c2cfe9e0cd2aae": {
        "code_string": "@classmethod\n     def from_time(cls, command, user, time):\n         \"\"\"\n         Alternative method to instantiate CronJob.\n         :param command: like in ``CronJob.__init__``\n         :param user: like in ``CronJob.__init__``\n         :param time: a list or tuple of at most five strings. Missing elements are set to \"*\".\n         :return: a ``CronJob`` object\n         \"\"\"\n         assert len(time) <= 5\n         padded_time = tuple(time) + ('*',) * (5 - len(time))\n         assert len(padded_time) == 5\n         return cls(command, time[0], user, padded_time[1], padded_time[2], padded_time[3], padded_time[4])\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def from_time ( cls , command , user , time ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert len ( time ) <= 5 <NEWLINE> padded_time = tuple ( time ) + ( <STRING> , ) * ( 5 - len ( time ) ) <NEWLINE> assert len ( padded_time ) == 5 <NEWLINE> return cls ( command , time [ 0 ] , user , padded_time [ 1 ] , padded_time [ 2 ] , padded_time [ 3 ] , padded_time [ 4 ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Alternative method to instantiate CronJob.\n         :param command: like in ``CronJob.__init__``\n         :param user: like in ``CronJob.__init__``\n         :param time: a list or tuple of at most five strings. Missing elements are set to \"*\".\n         :return: a ``CronJob`` object\n         \"\"\"",
                "'*'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "79caedb39b5b452ab7cb0c68294dbfbc": {
        "code_string": "@classmethod\n     def from_time(cls, command, user, time):\n         \"\"\"\n         Alternative method to instantiate CronJob.\n         :param command: like in ``CronJob.__init__``\n         :param user: like in ``CronJob.__init__``\n         :param time: a list or tuple of at most five strings. Missing elements are set to \"*\".\n         :return: a ``CronJob`` object\n         \"\"\"\n         if len(time) > 5:\n             raise RuntimeError(\"Malformed cronjob time: {!r}\".format(time))\n         padded_time = tuple(time) + ('*',) * (5 - len(time))\n         if len(padded_time) > 5:\n             raise RuntimeError(\"Malformed cronjob time: {!r}\".format(padded_time))\n         return cls(command, padded_time[0], user, padded_time[1], padded_time[2], padded_time[3], padded_time[4])\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def from_time ( cls , command , user , time ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if len ( time ) > 5 : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( time ) ) <NEWLINE> <DEDENT> padded_time = tuple ( time ) + ( <STRING> , ) * ( 5 - len ( time ) ) <NEWLINE> if len ( padded_time ) > 5 : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( padded_time ) ) <NEWLINE> <DEDENT> return cls ( command , padded_time [ 0 ] , user , padded_time [ 1 ] , padded_time [ 2 ] , padded_time [ 3 ] , padded_time [ 4 ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Alternative method to instantiate CronJob.\n         :param command: like in ``CronJob.__init__``\n         :param user: like in ``CronJob.__init__``\n         :param time: a list or tuple of at most five strings. Missing elements are set to \"*\".\n         :return: a ``CronJob`` object\n         \"\"\"",
                "\"Malformed cronjob time: {!r}\"",
                "'*'",
                "\"Malformed cronjob time: {!r}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "247332f4ef274f4e8c9b0244a7b60bef": {
        "code_string": "def send_message(self, frameid, blob):\n         self._sending = True\n         d = self.merger.write_blob(frameid, blob)\n         def done(result):\n             self._sending = False\n             return result\n         d.addBoth(d)\n",
        "code_toks_joined": "def send_message ( self , frameid , blob ) : <NEWLINE> <INDENT> self . _sending = True <NEWLINE> d = self . merger . write_blob ( frameid , blob ) <NEWLINE> def done ( result ) : <NEWLINE> <INDENT> self . _sending = False <NEWLINE> return result <NEWLINE> <DEDENT> d . addBoth ( d ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "68ad94894bd244a48b7a1ba49b4519ca": {
        "code_string": "if isinstance(t, Workflow):\n                 wf_wdl, _, wf_tools = t.wdl(with_docker=with_docker, is_nested_tool=True)\n                 wtools[s.id()] = wf_wdl\n                 wtools.update(wf_tools)\n             else:\n                 wtools[s.id()] = t.wdl(with_docker=with_docker)\n",
        "code_toks_joined": "if isinstance ( t , Workflow ) : <NEWLINE> <INDENT> wf_wdl , _ , wf_tools = t . wdl ( with_docker = with_docker , is_nested_tool = True ) <NEWLINE> wtools [ s . id ( ) ] = wf_wdl <NEWLINE> wtools . update ( wf_tools ) <NEWLINE> else : <NEWLINE> wtools [ s . id ( ) ] = t . wdl ( with_docker = with_docker ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "32b34000d8974b6a86d3a9f38b8dac99": {
        "code_string": "z, edges = np.histogram(data.ids,\n                             bins=np.arange(-0.5, data.nx * data.ny + 0.5))\n     z = z.reshape(data.nx, data.ny)\n     if side_panels:\n         z_sumx = np.sum(z, axis=1)\n         z_sumy = np.sum(z, axis=0)\n",
        "code_toks_joined": "z , edges = np . histogram ( data . ids , <NEWLINE> <INDENT> bins = np . arange ( - 0.5 , data . nx * data . ny + 0.5 ) ) <NEWLINE> z = z . reshape ( data . nx , data . ny ) <NEWLINE> if side_panels : <NEWLINE> z_sumx = np . sum ( z , axis = 1 ) <NEWLINE> z_sumy = np . sum ( z , axis = 0 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fddfe41cb83f4042aa41508b5e2cdc77": {
        "code_string": "t = np.linspace(0.0, 7.2e4, nbins + 1)\n     z, xe, ye = np.histogram2d(data.ids, data.tofs/1.0e3,\n                                bins=[np.arange(-0.5, data.nx * data.ny + 0.5),\n                                      t])\n     z = z.reshape(data.nx, data.ny, nbins)\n     # Transpose should be True for old December 2018 files\n     if transpose:\n         z = np.transpose(z, axes=[1, 0, 2])\n     clab = \"Counts\"\n     if log:\n         with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n             z = np.log10(z)\n         clab = \"log({})\".format(clab)\n",
        "code_toks_joined": "t = np . linspace ( 0.0 , 7.2e4 , nbins + 1 ) <NEWLINE> <INDENT> z , xe , ye = np . histogram2d ( data . ids , data . tofs / 1.0e3 , <NEWLINE> <INDENT> bins = [ np . arange ( - 0.5 , data . nx * data . ny + 0.5 ) , <NEWLINE> <INDENT> t ] ) <NEWLINE> <DEDENT> <DEDENT> z = z . reshape ( data . nx , data . ny , nbins ) <NEWLINE> <COMMENT> <NL> if transpose : <NEWLINE> <INDENT> z = np . transpose ( z , axes = [ 1 , 0 , 2 ] ) <NEWLINE> <DEDENT> clab = <STRING> <NEWLINE> if log : <NEWLINE> <INDENT> with np . errstate ( divide = <STRING> , invalid = <STRING> ) : <NEWLINE> <INDENT> z = np . log10 ( z ) <NEWLINE> <DEDENT> clab = <STRING> . format ( clab ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Transpose should be True for old December 2018 files"
            ],
            "<STRING>": [
                "\"Counts\"",
                "\"ignore\"",
                "\"ignore\"",
                "\"log({})\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f487b1c4c7644adcb7b5363867e2e89e": {
        "code_string": "print('Building stringers...')\n         for s in stringers.values():\n             s.elements = [bdf.elements[eid] for eid in p.eids]\n             setelements = set(s.elements)\n         print('finished!')\n",
        "code_toks_joined": "print ( <STRING> ) <NEWLINE> <INDENT> for s in stringers . values ( ) : <NEWLINE> <INDENT> s . elements = [ bdf . elements [ eid ] for eid in p . eids ] <NEWLINE> setelements = set ( s . elements ) <NEWLINE> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Building stringers...'",
                "'finished!'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8e667d73811a4dc48a6b7408abbe408c": {
        "code_string": "value = logical_value\n     if reg == Register.HUE:\n         if logical_value in (0.0, 360.0):\n             value = 0.0\n         else:\n             value = (logical_value % 360.0) / 360.0 * 65535.0\n     elif reg in (Register.BRIGHTNESS, Register.SATURATION):\n         if logical_value == 100.0:\n             value = 65535.0\n         else:\n             value = logical_value / 100.0 * 65535.0\n     elif reg in (Register.DURATION, Register.TIME):\n         value = logical_value * 1000.0\n",
        "code_toks_joined": "value = logical_value <NEWLINE> <INDENT> if reg == Register . HUE : <NEWLINE> <INDENT> if logical_value in ( 0.0 , 360.0 ) : <NEWLINE> <INDENT> value = 0.0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> value = ( logical_value % 360.0 ) / 360.0 * 65535.0 <NEWLINE> <DEDENT> <DEDENT> elif reg in ( Register . BRIGHTNESS , Register . SATURATION ) : <NEWLINE> <INDENT> if logical_value == 100.0 : <NEWLINE> <INDENT> value = 65535.0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> value = logical_value / 100.0 * 65535.0 <NEWLINE> <DEDENT> <DEDENT> elif reg in ( Register . DURATION , Register . TIME ) : <NEWLINE> <INDENT> value = logical_value * 1000.0 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c49f8f6f5d524fbb8e953baa7c9d04d9": {
        "code_string": "sepcon = (Group(Literal('$') - Literal('(') -\n                 contract_expression('child') - Literal(')')))\n sepcon.setParseAction(SeparateContext.parse_action)\n sepcon.setName('Context separation construct')\n add_contract(sepcon)\n",
        "code_toks_joined": "sepcon = ( Group ( Literal ( <STRING> ) - Literal ( <STRING> ) - <NEWLINE> <INDENT> contract_expression ( <STRING> ) - Literal ( <STRING> ) ) ) <NEWLINE> sepcon . setParseAction ( SeparateContext . parse_action ) <NEWLINE> sepcon . setName ( <STRING> ) <NEWLINE> add_contract ( sepcon ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'$'",
                "'('",
                "'child'",
                "')'",
                "'Context separation construct'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "561828bcaf974c6cae886a1c36ad2b5c": {
        "code_string": "# Apply gapfilling if needed\n             if gap_filling != None and ~np.isnan(np.nanmean(Array)):     \n                 Array_end[np.isnan(Array_end)] = -9999\n                 Array_end = RC.gap_filling(Array_end, -9999, gap_filling)\n             Array_end = Array_end * MASK\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if gap_filling != None and ~ np . isnan ( np . nanmean ( Array ) ) : <NEWLINE> <INDENT> Array_end [ np . isnan ( Array_end ) ] = - 9999 <NEWLINE> Array_end = RC . gap_filling ( Array_end , - 9999 , gap_filling ) <NEWLINE> <DEDENT> Array_end = Array_end * MASK <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Apply gapfilling if needed"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "040c14353c394caf8086a9de6eeba058": {
        "code_string": "if not np.isnan(np.nanmean(Crop_S1_End.Data)):\n         for Date_Year in Dates_Years:\n             year_diff = int(Date_Year.year - Dates_Years[0].year)\n             for dekad in range(0,int(np.nanmax(Crop_S2_End.Data))):\n                 Accumulated_NPP_Data_Start_S1[year_diff, Crop_S1_End.Data[year_diff, :, :] == dekad] = NPPcum.Data[np.minimum(NPPcum.Size[0]-1, int(year_diff * 36 + dekad-1)), Crop_S1_End.Data[year_diff, :, :] == dekad] \n                 Accumulated_NPP_Data_Start_S2[year_diff, Crop_S2_End.Data[year_diff, :, :] == dekad] = NPPcum.Data[np.minimum(NPPcum.Size[0]-1, int(year_diff * 36 + dekad-1)), Crop_S2_End.Data[year_diff, :, :] == dekad] \n                 Accumulated_NPP_Data_Start_S3[year_diff, Crop_S3_End.Data[year_diff, :, :] == dekad] = NPPcum.Data[np.minimum(NPPcum.Size[0]-1, int(year_diff * 36 + dekad-1)), Crop_S3_End.Data[year_diff, :, :] == dekad] \n",
        "code_toks_joined": "if not np . isnan ( np . nanmean ( Crop_S1_End . Data ) ) : <NEWLINE> <INDENT> for Date_Year in Dates_Years : <NEWLINE> <INDENT> year_diff = int ( Date_Year . year - Dates_Years [ 0 ] . year ) <NEWLINE> for dekad in range ( 0 , int ( np . nanmax ( Crop_S2_End . Data ) ) ) : <NEWLINE> <INDENT> Accumulated_NPP_Data_Start_S1 [ year_diff , Crop_S1_End . Data [ year_diff , : , : ] == dekad ] = NPPcum . Data [ np . minimum ( NPPcum . Size [ 0 ] - 1 , int ( year_diff * 36 + dekad - 1 ) ) , Crop_S1_End . Data [ year_diff , : , : ] == dekad ] <NEWLINE> Accumulated_NPP_Data_Start_S2 [ year_diff , Crop_S2_End . Data [ year_diff , : , : ] == dekad ] = NPPcum . Data [ np . minimum ( NPPcum . Size [ 0 ] - 1 , int ( year_diff * 36 + dekad - 1 ) ) , Crop_S2_End . Data [ year_diff , : , : ] == dekad ] <NEWLINE> Accumulated_NPP_Data_Start_S3 [ year_diff , Crop_S3_End . Data [ year_diff , : , : ] == dekad ] = NPPcum . Data [ np . minimum ( NPPcum . Size [ 0 ] - 1 , int ( year_diff * 36 + dekad - 1 ) ) , Crop_S3_End . Data [ year_diff , : , : ] == dekad ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "389e7e8f4eb0487b983f5b6d01179268": {
        "code_string": "def get_positive_stabilizer_groups(n_qubits, n_states):\n     if n_states == n_stabilizer_states(n_qubits): \n         # If generating all states, we want to focus on only the all\n         # positive signed operators\n         target = n_states/pow(2, n_qubits)\n     else:\n         #If generating less than all, we'll add signs in randomly to compenstate\n         target = n_states\n     bitstrings = gen_bitstrings(n_qubits)\n     subspaces = []\n     generators = []\n     for group in combinations(bitstrings, n_qubits):\n         if len(group) == 2:\n             if not test_commutivity(n_qubits, group[0], group[1]):\n                 continue\n         if len(group) > 2:\n             if not all([test_commutivity(n_qubits, pair[0], pair[1]) \n                         for pair in combinations(group, 2)]): \n                 continue\n         candidate = BinarySubspace(*group)\n         if len(candidate.generators) < n_qubits:\n             continue\n         if len(candidate._items) < pow(2,n_qubits):\n             continue\n         res = tuple(i for i in sorted(candidate._items, key=bool_to_int))\n         for space in subspaces:\n             if np.all([np.array_equal(_el1, _el2) for _el1, _el2 in zip(res, space)]):\n                 continue        \n         subspaces.append(res)\n         generators.append(tuple(candidate.generators))\n         if len(generators) == n_states:\n             break\n     return generators\n",
        "code_toks_joined": "def get_positive_stabilizer_groups ( n_qubits , n_states ) : <NEWLINE> <INDENT> if n_states == n_stabilizer_states ( n_qubits ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> target = n_states / pow ( 2 , n_qubits ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> target = n_states <NEWLINE> <DEDENT> bitstrings = gen_bitstrings ( n_qubits ) <NEWLINE> subspaces = [ ] <NEWLINE> generators = [ ] <NEWLINE> for group in combinations ( bitstrings , n_qubits ) : <NEWLINE> <INDENT> if len ( group ) == 2 : <NEWLINE> <INDENT> if not test_commutivity ( n_qubits , group [ 0 ] , group [ 1 ] ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> if len ( group ) > 2 : <NEWLINE> <INDENT> if not all ( [ test_commutivity ( n_qubits , pair [ 0 ] , pair [ 1 ] ) <NEWLINE> <INDENT> for pair in combinations ( group , 2 ) ] ) : <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> candidate = BinarySubspace ( * group ) <NEWLINE> if len ( candidate . generators ) < n_qubits : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if len ( candidate . _items ) < pow ( 2 , n_qubits ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> res = tuple ( i for i in sorted ( candidate . _items , key = bool_to_int ) ) <NEWLINE> for space in subspaces : <NEWLINE> <INDENT> if np . all ( [ np . array_equal ( _el1 , _el2 ) for _el1 , _el2 in zip ( res , space ) ] ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> subspaces . append ( res ) <NEWLINE> generators . append ( tuple ( candidate . generators ) ) <NEWLINE> if len ( generators ) == n_states : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> return generators <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If generating all states, we want to focus on only the all",
                "# positive signed operators",
                "#If generating less than all, we'll add signs in randomly to compenstate"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af2916af25e5439ca058d8ff242ab067": {
        "code_string": "def test_write_missing_value(self):\n         svc = Service(\"https://volatile.wtf\")\n         self.assertRaises(lambda: svc[\"UNGYIZFHIA\"], MissingKeyException)\n",
        "code_toks_joined": "def test_write_missing_value ( self ) : <NEWLINE> <INDENT> svc = Service ( <STRING> ) <NEWLINE> self . assertRaises ( lambda : svc [ <STRING> ] , MissingKeyException ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"https://volatile.wtf\"",
                "\"UNGYIZFHIA\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0340e94af7f94501aaddf97b0e312c8d": {
        "code_string": "vel = ltu.dv_from_z((mean / wv_line_vac) - 1, z_line).to(units).value\n",
        "code_toks_joined": "vel = ltu . dv_from_z ( ( mean / wv_line_vac ) - 1 , z_line ) . to ( units ) . value <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ebbccaf5c86a4fe2a4869ea98f8c4d10": {
        "code_string": "def _config_parse(config_file):\n     if config_file is not None:\n         try:\n             configs = ConfigParse()\n             configs.read(config_file)\n             dwh_schema = dict(configs.items('dwh_schema'))\n         except FileNotFoundError:\n             logger.error(f'{config_file} does not exist.')\n             sys.exit(1)\n     else:\n         dwh_config = {\n             'username': os.getenv('DB_USERNAME'),\n             'password': os.getenv('DB_PASSWORD'),\n             'hostname': os.getenv('DB_HOSTNAME'),\n             'port': os.getenv('DB_PORT'),\n             'database': os.getenv('DATABASE'),\n         }\n     return config_file\n",
        "code_toks_joined": "def _config_parse ( config_file ) : <NEWLINE> <INDENT> if config_file is not None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> configs = ConfigParse ( ) <NEWLINE> configs . read ( config_file ) <NEWLINE> dwh_schema = dict ( configs . items ( <STRING> ) ) <NEWLINE> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> logger . error ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> dwh_config = { <NEWLINE> <INDENT> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <STRING> : os . getenv ( <STRING> ) , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> return config_file <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'dwh_schema'",
                "f'{config_file} does not exist.'",
                "'username'",
                "'DB_USERNAME'",
                "'password'",
                "'DB_PASSWORD'",
                "'hostname'",
                "'DB_HOSTNAME'",
                "'port'",
                "'DB_PORT'",
                "'database'",
                "'DATABASE'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "42fa1ed9d6c44c619f431a302d6248b7": {
        "code_string": "def perform_test(self, command: [str], input_file: str, expected_result_file: str):\n         with open(expected_result_file, 'r') as fin:\n             expected = fin.read()\n         cmd, flags = self.interpreter.build_command(command, input_file)\n         res = self.interpreter.execute_command(cmd, flags, return_output=True)\n         self.assertEqual(res, expected)\n",
        "code_toks_joined": "def perform_test ( self , command : [ str ] , input_file : str , expected_result_file : str ) : <NEWLINE> <INDENT> with open ( expected_result_file , <STRING> ) as fin : <NEWLINE> <INDENT> expected = fin . read ( ) <NEWLINE> <DEDENT> cmd , flags = self . interpreter . build_command ( command , input_file ) <NEWLINE> res = self . interpreter . execute_command ( cmd , flags , return_output = True ) <NEWLINE> self . assertEqual ( res , expected ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'r'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a68ffb9b6a8f4dc5bd30664cf4f9a382": {
        "code_string": "h, m, s = convert_elapsed_time(training_time)\n     logger.info('Training finished in {} hours {} minutes {:.2f} seconds.'\n                 .format(h, m, s))\n     logger.info('outputs')\n     logger.info(outputs)\n     logger.info('targets')\n     logger.info(targets)\n",
        "code_toks_joined": "h , m , s = convert_elapsed_time ( training_time ) <NEWLINE> <INDENT> logger . info ( <STRING> <NEWLINE> <INDENT> . format ( h , m , s ) ) <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> logger . info ( outputs ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> logger . info ( targets ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Training finished in {} hours {} minutes {:.2f} seconds.'",
                "'outputs'",
                "'targets'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "800b047018aa47828b84101c10d57862": {
        "code_string": "def mdata(path, track_element):\n     \"\"\"Find the metadata of each accesible file, and format as XML.\"\"\"\n     # Define list of tags to search for\n     tags = {\"title\", \"artist\", \"album\", \"genre\", \"recording date\", \"label\"}\n     # Define ffprobe syntax\n     cmd = ['ffprobe', path.rstrip()]\n     process = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                                stderr=subprocess.STDOUT)\n     # Main loop\n     while True:\n         out = process.stdout.readline()\n         decoded = out.decode('utf-8')\n         if out != b'':\n             linecheck = decoded.replace(\" \", \"\")\n             for tag in tags:\n                 tagstring = tag+\":\"\n                 if tagstring in linecheck:\n                     stringf = out.split(': ')[1]\n                     ttag = tag\n                     if tag == \"artist\":\n                         ttag = \"creator\"\n                     if tag == \"genre\":\n                         ttag = \"info\"\n                     ttag = SubElement(track_element, tag)\n                     ttag.text = stringf.rstrip()\n         else:\n             break\n",
        "code_toks_joined": "def mdata ( path , track_element ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> tags = { <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> } <NEWLINE> <COMMENT> <NL> cmd = [ <STRING> , path . rstrip ( ) ] <NEWLINE> process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , <NEWLINE> <INDENT> stderr = subprocess . STDOUT ) <NEWLINE> <COMMENT> <NL> <DEDENT> while True : <NEWLINE> <INDENT> out = process . stdout . readline ( ) <NEWLINE> decoded = out . decode ( <STRING> ) <NEWLINE> if out != <STRING> : <NEWLINE> <INDENT> linecheck = decoded . replace ( <STRING> , <STRING> ) <NEWLINE> for tag in tags : <NEWLINE> <INDENT> tagstring = tag + <STRING> <NEWLINE> if tagstring in linecheck : <NEWLINE> <INDENT> stringf = out . split ( <STRING> ) [ 1 ] <NEWLINE> ttag = tag <NEWLINE> if tag == <STRING> : <NEWLINE> <INDENT> ttag = <STRING> <NEWLINE> <DEDENT> if tag == <STRING> : <NEWLINE> <INDENT> ttag = <STRING> <NEWLINE> <DEDENT> ttag = SubElement ( track_element , tag ) <NEWLINE> ttag . text = stringf . rstrip ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Find the metadata of each accesible file, and format as XML.\"\"\"",
                "\"title\"",
                "\"artist\"",
                "\"album\"",
                "\"genre\"",
                "\"recording date\"",
                "\"label\"",
                "'ffprobe'",
                "'utf-8'",
                "b''",
                "\" \"",
                "\"\"",
                "\":\"",
                "': '",
                "\"artist\"",
                "\"creator\"",
                "\"genre\"",
                "\"info\""
            ],
            "<COMMENT>": [
                "# Define list of tags to search for",
                "# Define ffprobe syntax",
                "# Main loop"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9118c18a77c74b99a81bfe51aef5c3e5": {
        "code_string": "def mdata(path, track_element):\n     \"\"\"Find the metadata of each accesible file, and format as XML.\"\"\"\n     # Define list of tags to search for\n     tags = {\"title\", \"artist\", \"album\", \"genre\", \"recording date\", \"label\"}\n     # Define ffprobe syntax\n     cmd = ['ffprobe', path.rstrip()]\n     process = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                                stderr=subprocess.STDOUT)\n     # Main loop\n     while True:\n         out = process.stdout.readline()\n         decoded = out.decode('utf-8')\n         if out != b'':\n             linecheck = decoded.replace(\" \", \"\")\n             for tag in tags:\n                 tagstring = tag+\":\"\n                 if tagstring in linecheck:\n                     stringf = decoded.split(': ')[1]\n                     ttag = tag\n                     if tag == \"artist\":\n                         ttag = \"creator\"\n                     if tag == \"genre\":\n                         ttag = \"info\"\n                     ttag = SubElement(track_element, tag)\n                     ttag.text = stringf.rstrip()\n         else:\n             break\n",
        "code_toks_joined": "def mdata ( path , track_element ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> tags = { <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> } <NEWLINE> <COMMENT> <NL> cmd = [ <STRING> , path . rstrip ( ) ] <NEWLINE> process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , <NEWLINE> <INDENT> stderr = subprocess . STDOUT ) <NEWLINE> <COMMENT> <NL> <DEDENT> while True : <NEWLINE> <INDENT> out = process . stdout . readline ( ) <NEWLINE> decoded = out . decode ( <STRING> ) <NEWLINE> if out != <STRING> : <NEWLINE> <INDENT> linecheck = decoded . replace ( <STRING> , <STRING> ) <NEWLINE> for tag in tags : <NEWLINE> <INDENT> tagstring = tag + <STRING> <NEWLINE> if tagstring in linecheck : <NEWLINE> <INDENT> stringf = decoded . split ( <STRING> ) [ 1 ] <NEWLINE> ttag = tag <NEWLINE> if tag == <STRING> : <NEWLINE> <INDENT> ttag = <STRING> <NEWLINE> <DEDENT> if tag == <STRING> : <NEWLINE> <INDENT> ttag = <STRING> <NEWLINE> <DEDENT> ttag = SubElement ( track_element , tag ) <NEWLINE> ttag . text = stringf . rstrip ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Find the metadata of each accesible file, and format as XML.\"\"\"",
                "\"title\"",
                "\"artist\"",
                "\"album\"",
                "\"genre\"",
                "\"recording date\"",
                "\"label\"",
                "'ffprobe'",
                "'utf-8'",
                "b''",
                "\" \"",
                "\"\"",
                "\":\"",
                "': '",
                "\"artist\"",
                "\"creator\"",
                "\"genre\"",
                "\"info\""
            ],
            "<COMMENT>": [
                "# Define list of tags to search for",
                "# Define ffprobe syntax",
                "# Main loop"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "754520f4acdb414eb507de9dc884498f": {
        "code_string": "class CLIN28JSON:\n     def __init__(self, filename):\n         if os.path.exists(filename):\n             raise FileExistsError(\"File not found: \" + filename)\n",
        "code_toks_joined": "class CLIN28JSON : <NEWLINE> <INDENT> def __init__ ( self , filename ) : <NEWLINE> <INDENT> if os . path . exists ( filename ) : <NEWLINE> <INDENT> raise FileExistsError ( <STRING> + filename ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"File not found: \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7ef3ba28ca7044f5862505b5739b5053": {
        "code_string": "def validate(self):\n         self.index = {}\n         if 'words' not in self.data:\n             return ValidationError(\"No words found\")\n         if 'corrections' not in self.data:\n             return ValidationError(\"No corrections found\")\n         for key in self.data:\n             if key not in ('words','corrections'):\n                 print(\"WARNING: Unknown key '\" + key + \"' will be ignored!\",file=sys.stderr)\n         for word in self.words():\n             if 'id' not in word or not word['id']:\n                 raise ValidationError(\"Word does not have an ID! \" + repr(word))\n             self.index[word['id']] = word\n             if 'text' not in word or not word['text']:\n                 raise ValidationError(\"Word does not have a text! \" + repr(word))\n             for key in word:\n                 if key not in ('text','id','space','in'):\n                     print(\"WARNING: Unknown key '\" + key + \"' for word \" + repr(word) + \" will be ignored!\",file=sys.stderr)\n         for correction in self.corrections():\n             if 'span' not in correction or not correction['span']:\n                 if 'after' not in correction or not correction['after']:\n                     raise ValidationError(\"Correction does not have a 'span' (or 'after') property! \" + repr(correction))\n                 elif correction['after'] not in self.index:\n                     raise ValidationError(\"Correction's 'after' property refers to a non-existing word ID! (\" + correction['after'] + \") \" + repr(correction))\n             else:\n                 for wordid in correction['span']:\n                     if wordid not in self.index:\n                         raise ValidationError(\"Correction's 'span' property refers to a non-existing word ID! (\" + wordid + \") \" + repr(correction))\n             for key in correction:\n                 if key not in ('text','span','after','confidence','class'):\n                     print(\"WARNING: Unknown key '\" + key + \"' for correction \" + repr(correction) + \" will be ignored!\",file=sys.stderr)\n                 if key == 'confidence':\n                     try:\n                         correction['confidence'] = float(correction['confidence'])\n                     except:\n                         raise ValidationError(\"Invalid confidence value (\" + str(correction['confidence']) + \") \" + repr(correction))\n                     if correction['confidence'] < 0 or correction['confidence'] > 0:\n                         raise ValidationError(\"Confidence value out of bounds (\" + str(correction['confidence']) + \") \" + repr(correction))\n",
        "code_toks_joined": "def validate ( self ) : <NEWLINE> <INDENT> self . index = { } <NEWLINE> if <STRING> not in self . data : <NEWLINE> <INDENT> return ValidationError ( <STRING> ) <NEWLINE> <DEDENT> if <STRING> not in self . data : <NEWLINE> <INDENT> return ValidationError ( <STRING> ) <NEWLINE> <DEDENT> for key in self . data : <NEWLINE> <INDENT> if key not in ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> print ( <STRING> + key + <STRING> , file = sys . stderr ) <NEWLINE> <DEDENT> <DEDENT> for word in self . words ( ) : <NEWLINE> <INDENT> if <STRING> not in word or not word [ <STRING> ] : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + repr ( word ) ) <NEWLINE> <DEDENT> self . index [ word [ <STRING> ] ] = word <NEWLINE> if <STRING> not in word or not word [ <STRING> ] : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + repr ( word ) ) <NEWLINE> <DEDENT> for key in word : <NEWLINE> <INDENT> if key not in ( <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> print ( <STRING> + key + <STRING> + repr ( word ) + <STRING> , file = sys . stderr ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for correction in self . corrections ( ) : <NEWLINE> <INDENT> if <STRING> not in correction or not correction [ <STRING> ] : <NEWLINE> <INDENT> if <STRING> not in correction or not correction [ <STRING> ] : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> elif correction [ <STRING> ] not in self . index : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + correction [ <STRING> ] + <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> for wordid in correction [ <STRING> ] : <NEWLINE> <INDENT> if wordid not in self . index : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + wordid + <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for key in correction : <NEWLINE> <INDENT> if key not in ( <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> print ( <STRING> + key + <STRING> + repr ( correction ) + <STRING> , file = sys . stderr ) <NEWLINE> <DEDENT> if key == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> correction [ <STRING> ] = float ( correction [ <STRING> ] ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + str ( correction [ <STRING> ] ) + <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> if correction [ <STRING> ] < 0 or correction [ <STRING> ] > 0 : <NEWLINE> <INDENT> raise ValidationError ( <STRING> + str ( correction [ <STRING> ] ) + <STRING> + repr ( correction ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'words'",
                "\"No words found\"",
                "'corrections'",
                "\"No corrections found\"",
                "'words'",
                "'corrections'",
                "\"WARNING: Unknown key '\"",
                "\"' will be ignored!\"",
                "'id'",
                "'id'",
                "\"Word does not have an ID! \"",
                "'id'",
                "'text'",
                "'text'",
                "\"Word does not have a text! \"",
                "'text'",
                "'id'",
                "'space'",
                "'in'",
                "\"WARNING: Unknown key '\"",
                "\"' for word \"",
                "\" will be ignored!\"",
                "'span'",
                "'span'",
                "'after'",
                "'after'",
                "\"Correction does not have a 'span' (or 'after') property! \"",
                "'after'",
                "\"Correction's 'after' property refers to a non-existing word ID! (\"",
                "'after'",
                "\") \"",
                "'span'",
                "\"Correction's 'span' property refers to a non-existing word ID! (\"",
                "\") \"",
                "'text'",
                "'span'",
                "'after'",
                "'confidence'",
                "'class'",
                "\"WARNING: Unknown key '\"",
                "\"' for correction \"",
                "\" will be ignored!\"",
                "'confidence'",
                "'confidence'",
                "'confidence'",
                "\"Invalid confidence value (\"",
                "'confidence'",
                "\") \"",
                "'confidence'",
                "'confidence'",
                "\"Confidence value out of bounds (\"",
                "'confidence'",
                "\") \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f24e76041c64c57bc11be17a4196cc9": {
        "code_string": "@patch('smsaero.conf.SMSAERO_PASSWORD', 'FAKE')\n     @patch('urllib2.urlopen', _fake_urlopen)\n     def test_send_request(self):\n         sender = SmsSender()\n         response = sender.send_request('/link/', {})\n         self.assertIn(SMSMessage.STATUS_ACCEPTED, response)\n",
        "code_toks_joined": "@ patch ( <STRING> , <STRING> ) <NEWLINE> <INDENT> @ patch ( <STRING> , _fake_urlopen ) <NEWLINE> def test_send_request ( self ) : <NEWLINE> <INDENT> sender = SmsSender ( ) <NEWLINE> response = sender . send_request ( <STRING> , { } ) <NEWLINE> self . assertIn ( SMSMessage . STATUS_ACCEPTED , response ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'smsaero.conf.SMSAERO_PASSWORD'",
                "'FAKE'",
                "'urllib2.urlopen'",
                "'/link/'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "320db136cf9b4e8ab8ee741afc6ad75c": {
        "code_string": "def send_sms(to, text, signature_id=None, date=None, link='/send/'):\n     signature = sender.get_signature(signature_id)\n     params = {\n         'to': to,\n         'text': quote_plus(text),\n         'from': signature.name,\n         'date': date or '',\n     }\n     response = sender.send_request(link, params)\n     sms_id, status = sender.parse_response(response)\n",
        "code_toks_joined": "def send_sms ( to , text , signature_id = None , date = None , link = <STRING> ) : <NEWLINE> <INDENT> signature = sender . get_signature ( signature_id ) <NEWLINE> params = { <NEWLINE> <INDENT> <STRING> : to , <NEWLINE> <STRING> : quote_plus ( text ) , <NEWLINE> <STRING> : signature . name , <NEWLINE> <STRING> : date or <STRING> , <NEWLINE> <DEDENT> } <NEWLINE> response = sender . send_request ( link , params ) <NEWLINE> sms_id , status = sender . parse_response ( response ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/send/'",
                "'to'",
                "'text'",
                "'from'",
                "'date'",
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1ccf772fd3b94384a2dd56dfda1385e6": {
        "code_string": "# Handle out-of-bound limits.\n         if step > 0:\n             if (start > a_length) or (stop < -a_length):\n                 start = stop = 0\n             else:\n                 if start < -a_length:\n                     start = 0\n                 if stop > a_length:\n                     stop = a_length\n         elif step < 0:\n             if (start < -a_length) or (stop_i and stop >= (a_length - 1)):\n                 start = stop = 0\n             else:\n                 if start >= a_length:\n                     start = a_length - 1\n                 if stop_i and stop < -a_length:\n                     stop = None\n                     stop_i = True\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if step > 0 : <NEWLINE> <INDENT> if ( start > a_length ) or ( stop < - a_length ) : <NEWLINE> <INDENT> start = stop = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if start < - a_length : <NEWLINE> <INDENT> start = 0 <NEWLINE> <DEDENT> if stop > a_length : <NEWLINE> <INDENT> stop = a_length <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif step < 0 : <NEWLINE> <INDENT> if ( start < - a_length ) or ( stop_i and stop >= ( a_length - 1 ) ) : <NEWLINE> <INDENT> start = stop = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if start >= a_length : <NEWLINE> <INDENT> start = a_length - 1 <NEWLINE> <DEDENT> if stop_i and stop < - a_length : <NEWLINE> <INDENT> stop = None <NEWLINE> stop_i = True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Handle out-of-bound limits."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "91153790a9f34a24a7ca9f37057f85ad": {
        "code_string": "new_slice = a_slice\n     if new_slice is Ellipsis:\n         new_slice = slice(None)\n     elif not isinstance(new_slice, slice):\n         raise ValueError(\n             \"Expected a `slice` type. Instead got `%s`.\" % str(new_slice)\n         )\n",
        "code_toks_joined": "new_slice = a_slice <NEWLINE> <INDENT> if new_slice is Ellipsis : <NEWLINE> <INDENT> new_slice = slice ( None ) <NEWLINE> <DEDENT> elif not isinstance ( new_slice , slice ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> % str ( new_slice ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Expected a `slice` type. Instead got `%s`.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5d19f48317ec4d9c8b49f3fd553ba12c": {
        "code_string": "new_slice = a_slice\n     if new_slice is Ellipsis:\n         new_slice = slice(None)\n     elif not isinstance(a_slice, slice):\n         raise ValueError(\n             \"Expected a `slice` type. Instead got `%s`.\" % str(new_slice)\n         )\n",
        "code_toks_joined": "new_slice = a_slice <NEWLINE> <INDENT> if new_slice is Ellipsis : <NEWLINE> <INDENT> new_slice = slice ( None ) <NEWLINE> <DEDENT> elif not isinstance ( a_slice , slice ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> % str ( new_slice ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Expected a `slice` type. Instead got `%s`.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "adb659739c224388bdcd3f0b8d3dbb84": {
        "code_string": "server.set_backend(backend)\n",
        "code_toks_joined": "server . set_backend ( backend ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a56b53fdf6ec42e48335dd62c1869086": {
        "code_string": "@property\n     def to_train(self) -> bool:\n         if self.name == \"compress\":\n             return True\n         return True\n",
        "code_toks_joined": "@ property <NEWLINE> <INDENT> def to_train ( self ) -> bool : <NEWLINE> <INDENT> if self . name == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"compress\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "31e3586a662849169cc461d15a77f49d": {
        "code_string": "def main_Concern():\n     parser = ArgumentParser()\n     parser.add_argument('--chdir', type = os.path.expanduser)\n     config, vimargs = parser.parse_known_args()\n     if config.chdir is not None:\n         os.chdir(config.chdir)\n     configdir = Path.home() / '.Concern'\n     configdir.mkdir(parents = True, exist_ok = True)\n     with TemporaryDirectory(dir = configdir) as tempdir:\n         tempdir = Path(tempdir)\n         concernvimrc = tempdir / 'vimrc'\n         sendblock = tempdir / 'sendblock.py'\n         quit = tempdir / 'quit.py'\n         screenrc = tempdir / 'screenrc'\n         config = ConfigCtrl()\n         config.put('Concern', 'toAbsWidth', function = toabswidth)\n         config.printf(\"Concern . %s\", resource_filename(__name__, 'Concern.arid'))\n         try:\n             config.loadsettings()\n         except FileNotFoundError as e:\n             log.info(\"No such file: %s\", e)\n         Concern = config.node.Concern\n         uservimrc = Path.home() / '.vimrc'\n         if uservimrc.exists():\n             (-Concern).printf(\"vimrc userPath = %s\", uservimrc)\n         else:\n             log.info(\"No such file: %s\", uservimrc)\n         (-Concern).printf(\"interpreter = %s\", sys.executable)\n         (-Concern).printf(\"vimrcPath = %s\", concernvimrc)\n         (-Concern).printf(\"sendblock = %s\", sendblock)\n         (-Concern).printf(\"quit = %s\", quit)\n         (-Concern).printf('vimArgs := $list()')\n         for arg in vimargs:\n             (-Concern).printf(\"vimArgs += %s\", arg)\n         import_module(f\".consumer.{Concern.consumerName}\", package = __package__).configure(config)\n         (-Concern).processtemplate(resource_filename(templates.__name__, 'vimrc.aridt'), concernvimrc)\n         (-Concern).printf('\" = $(pystr)')\n         (-Concern).processtemplate(resource_filename(templates.__name__, 'sendblock.py.aridt'), sendblock)\n         (-Concern).processtemplate(resource_filename(templates.__name__, 'quit.py.aridt'), quit)\n         (-Concern).printf('\" = $(screenstr)')\n         (-Concern).processtemplate(resource_filename(templates.__name__, 'screenrc.aridt'), screenrc)\n         doublequotekey = Concern.doubleQuoteKey\n         stuffablescreen(doublequotekey).print('-S', Concern.sessionName, '-c', screenrc)\n",
        "code_toks_joined": "def main_Concern ( ) : <NEWLINE> <INDENT> parser = ArgumentParser ( ) <NEWLINE> parser . add_argument ( <STRING> , type = os . path . expanduser ) <NEWLINE> config , vimargs = parser . parse_known_args ( ) <NEWLINE> if config . chdir is not None : <NEWLINE> <INDENT> os . chdir ( config . chdir ) <NEWLINE> <DEDENT> configdir = Path . home ( ) / <STRING> <NEWLINE> configdir . mkdir ( parents = True , exist_ok = True ) <NEWLINE> with TemporaryDirectory ( dir = configdir ) as tempdir : <NEWLINE> <INDENT> tempdir = Path ( tempdir ) <NEWLINE> concernvimrc = tempdir / <STRING> <NEWLINE> sendblock = tempdir / <STRING> <NEWLINE> quit = tempdir / <STRING> <NEWLINE> screenrc = tempdir / <STRING> <NEWLINE> config = ConfigCtrl ( ) <NEWLINE> config . put ( <STRING> , <STRING> , function = toabswidth ) <NEWLINE> config . printf ( <STRING> , resource_filename ( __name__ , <STRING> ) ) <NEWLINE> try : <NEWLINE> <INDENT> config . loadsettings ( ) <NEWLINE> <DEDENT> except FileNotFoundError as e : <NEWLINE> <INDENT> log . info ( <STRING> , e ) <NEWLINE> <DEDENT> Concern = config . node . Concern <NEWLINE> uservimrc = Path . home ( ) / <STRING> <NEWLINE> if uservimrc . exists ( ) : <NEWLINE> <INDENT> ( - Concern ) . printf ( <STRING> , uservimrc ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . info ( <STRING> , uservimrc ) <NEWLINE> <DEDENT> ( - Concern ) . printf ( <STRING> , sys . executable ) <NEWLINE> ( - Concern ) . printf ( <STRING> , concernvimrc ) <NEWLINE> ( - Concern ) . printf ( <STRING> , sendblock ) <NEWLINE> ( - Concern ) . printf ( <STRING> , quit ) <NEWLINE> ( - Concern ) . printf ( <STRING> ) <NEWLINE> for arg in vimargs : <NEWLINE> <INDENT> ( - Concern ) . printf ( <STRING> , arg ) <NEWLINE> <DEDENT> import_module ( <STRING> , package = __package__ ) . configure ( config ) <NEWLINE> ( - Concern ) . processtemplate ( resource_filename ( templates . __name__ , <STRING> ) , concernvimrc ) <NEWLINE> ( - Concern ) . printf ( <STRING> ) <NEWLINE> ( - Concern ) . processtemplate ( resource_filename ( templates . __name__ , <STRING> ) , sendblock ) <NEWLINE> ( - Concern ) . processtemplate ( resource_filename ( templates . __name__ , <STRING> ) , quit ) <NEWLINE> ( - Concern ) . printf ( <STRING> ) <NEWLINE> ( - Concern ) . processtemplate ( resource_filename ( templates . __name__ , <STRING> ) , screenrc ) <NEWLINE> doublequotekey = Concern . doubleQuoteKey <NEWLINE> stuffablescreen ( doublequotekey ) . print ( <STRING> , Concern . sessionName , <STRING> , screenrc ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'--chdir'",
                "'.Concern'",
                "'vimrc'",
                "'sendblock.py'",
                "'quit.py'",
                "'screenrc'",
                "'Concern'",
                "'toAbsWidth'",
                "\"Concern . %s\"",
                "'Concern.arid'",
                "\"No such file: %s\"",
                "'.vimrc'",
                "\"vimrc userPath = %s\"",
                "\"No such file: %s\"",
                "\"interpreter = %s\"",
                "\"vimrcPath = %s\"",
                "\"sendblock = %s\"",
                "\"quit = %s\"",
                "'vimArgs := $list()'",
                "\"vimArgs += %s\"",
                "f\".consumer.{Concern.consumerName}\"",
                "'vimrc.aridt'",
                "'\" = $(pystr)'",
                "'sendblock.py.aridt'",
                "'quit.py.aridt'",
                "'\" = $(screenstr)'",
                "'screenrc.aridt'",
                "'-S'",
                "'-c'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0cf183498458466c9c7f061eb25aa683": {
        "code_string": "# Permanently add best center candidate found in local tries\n         if sp.issparse(X):\n             centers[c] = X[best_candidate].toarray()\n         else:\n             centers[c] = X[best_candidate]\n         centers_idx.append(c)\n         current_pot = best_pot\n         closest_dist_sq = best_dist_sq\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if sp . issparse ( X ) : <NEWLINE> <INDENT> centers [ c ] = X [ best_candidate ] . toarray ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> centers [ c ] = X [ best_candidate ] <NEWLINE> <DEDENT> centers_idx . append ( c ) <NEWLINE> current_pot = best_pot <NEWLINE> closest_dist_sq = best_dist_sq <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Permanently add best center candidate found in local tries"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "820175a628914b6fbd7e767e2247db4a": {
        "code_string": "return local_dir\n",
        "code_toks_joined": "return local_dir <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bc84ca1ae2e04fa189a2d0e386865455": {
        "code_string": "def _interdistance(self, first: np.ndarray, second: np.ndarray) -> np.ndarray:\n         if first is not self._last:\n             self._last = first\n             self._last_ranks = np.apply_along_axis(st.rankdata, 0, first)\n         second_ranks = np.apply_along_axis(st.rankdata, 0, first)\n         return dist.cdist(self._last_ranks, second_ranks, metric='correlation')\n",
        "code_toks_joined": "def _interdistance ( self , first : np . ndarray , second : np . ndarray ) -> np . ndarray : <NEWLINE> <INDENT> if first is not self . _last : <NEWLINE> <INDENT> self . _last = first <NEWLINE> self . _last_ranks = np . apply_along_axis ( st . rankdata , 0 , first ) <NEWLINE> <DEDENT> second_ranks = np . apply_along_axis ( st . rankdata , 0 , first ) <NEWLINE> return dist . cdist ( self . _last_ranks , second_ranks , metric = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'correlation'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fcd50bb9711e4ba3b60009e2386daa71": {
        "code_string": "@contextlib.contextmanager\n def resume(file_name):\n     file_path = \"{}/{}\".format(env.HIDEOUT_BASEDIR, file_name)\n     target = None\n     if os.path.exists(file_path) and not env.HIDEOUT_FORCE_CACHE:\n         logger.error(\"found {}\".format(file_path))\n         with open(file_path, mode='rb') as f:\n             target = pickle.load(f)\n     yield target\n     if target is None:\n         freeze(target, file_name)\n",
        "code_toks_joined": "@ contextlib . contextmanager <NEWLINE> <INDENT> def resume ( file_name ) : <NEWLINE> <INDENT> file_path = <STRING> . format ( env . HIDEOUT_BASEDIR , file_name ) <NEWLINE> target = None <NEWLINE> if os . path . exists ( file_path ) and not env . HIDEOUT_FORCE_CACHE : <NEWLINE> <INDENT> logger . error ( <STRING> . format ( file_path ) ) <NEWLINE> with open ( file_path , mode = <STRING> ) as f : <NEWLINE> <INDENT> target = pickle . load ( f ) <NEWLINE> <DEDENT> <DEDENT> yield target <NEWLINE> if target is None : <NEWLINE> <INDENT> freeze ( target , file_name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{}/{}\"",
                "\"found {}\"",
                "'rb'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a3ae558b89c84e849dab6d55a30a1c5c": {
        "code_string": "def percent_of(self, percent, whole):\n         return (percent * whole) * 100\n",
        "code_toks_joined": "def percent_of ( self , percent , whole ) : <NEWLINE> <INDENT> return ( percent * whole ) * 100 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "82a8c49219ce4b2fa72966b69a59be93": {
        "code_string": "if batch:\n         import batch_process\n         batch_process.run(batch)\n     else:\n         metadata_dir = os.path.join(indir,'metadata')\n         if os.path.isdir(metadata_dir):\n             metadata_process.clean(metadata_dir)\n         else:\n             raise IOError('metadata directory within input not found. See README for expected directory structure.')\n",
        "code_toks_joined": "if batch : <NEWLINE> <INDENT> import batch_process <NEWLINE> batch_process . run ( batch ) <NEWLINE> else : <NEWLINE> metadata_dir = os . path . join ( indir , <STRING> ) <NEWLINE> if os . path . isdir ( metadata_dir ) : <NEWLINE> <INDENT> metadata_process . clean ( metadata_dir ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'metadata'",
                "'metadata directory within input not found. See README for expected directory structure.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "18c37629b80243f18d6667da7c501921": {
        "code_string": "def __init__(self, operand, invert=False):\n         super(IsNullOperator, self).__init__(u' IS NOT NULL' if invert else u' IS NULL', operand)\n         self.invert = invert\n",
        "code_toks_joined": "def __init__ ( self , operand , invert = False ) : <NEWLINE> <INDENT> super ( IsNullOperator , self ) . __init__ ( <STRING> if invert else <STRING> , operand ) <NEWLINE> self . invert = invert <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "u' IS NOT NULL'",
                "u' IS NULL'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "97a2a7a6ce474e11b744c83ca3a1facb": {
        "code_string": "# Verify Direct Arguments Variables\n         ERROR_HANDLER.assert_true(self.criteria_working_dir is None, \\\n                             \"Must specify criteria_working_dir\", __file__)\n         for criterion in self.tools_config_by_criterion_dict:\n             ERROR_HANDLER.assert_true( \\\n                     len(self.tools_config_by_criterion_dict[criterion]) != \\\n                     len(set([c.get_tool_config_alias() for c in \\\n                         self.tools_config_by_criterion_dict[criterion]])), \\\n                     \"some tool configs appear multiple times for {}\".format( \\\n                                                         criterion), __file__)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> ERROR_HANDLER . assert_true ( self . criteria_working_dir is None , <STRING> , __file__ ) <NEWLINE> for criterion in self . tools_config_by_criterion_dict : <NEWLINE> <INDENT> ERROR_HANDLER . assert_true ( len ( self . tools_config_by_criterion_dict [ criterion ] ) != len ( set ( [ c . get_tool_config_alias ( ) for c in self . tools_config_by_criterion_dict [ criterion ] ] ) ) , <STRING> . format ( criterion ) , __file__ ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Verify Direct Arguments Variables"
            ],
            "<STRING>": [
                "\"Must specify criteria_working_dir\"",
                "\"some tool configs appear multiple times for {}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7d3e0fc725fd4f17b2726fae78a8ec86": {
        "code_string": "# Verify Direct Arguments Variables\n         ERROR_HANDLER.assert_true(self.tests_working_dir is None, \\\n                                     \"Must specify tests_working_dir\", __file__)\n         ERROR_HANDLER.assert_true(len(self.test_tool_config_list) != \\\n                                 len(set([c.get_tool_config_alias() for c in \\\n                                             self.test_tool_config_list])), \\\n                         \"some tool configs appear multiple times\", __file__)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> ERROR_HANDLER . assert_true ( self . tests_working_dir is None , <STRING> , __file__ ) <NEWLINE> ERROR_HANDLER . assert_true ( len ( self . test_tool_config_list ) != len ( set ( [ c . get_tool_config_alias ( ) for c in self . test_tool_config_list ] ) ) , <STRING> , __file__ ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Verify Direct Arguments Variables"
            ],
            "<STRING>": [
                "\"Must specify tests_working_dir\"",
                "\"some tool configs appear multiple times\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0f35245ac8de477d9b32a22a04b4e172": {
        "code_string": "class CopyCallbackObject(DefaultCallbackObject):\n         def after_command(self):\n             file_src_dest_map = self.post_callback_args\n             for src, dest in list(file_src_dest_map.items()):\n                 abs_src = os.path.join(self.repository_rootdir, src)\n                 if os.path.abspath(abs_src) != os.path.abspath(dest):\n                     shutil.copy2(src, dest)\n             return DefaultCallbackObject.after_command(self)\n         #~ def after_command()\n     #~ class CopyCallbackObject\n",
        "code_toks_joined": "class CopyCallbackObject ( DefaultCallbackObject ) : <NEWLINE> <INDENT> def after_command ( self ) : <NEWLINE> <INDENT> file_src_dest_map = self . post_callback_args <NEWLINE> for src , dest in list ( file_src_dest_map . items ( ) ) : <NEWLINE> <INDENT> abs_src = os . path . join ( self . repository_rootdir , src ) <NEWLINE> if os . path . abspath ( abs_src ) != os . path . abspath ( dest ) : <NEWLINE> <INDENT> shutil . copy2 ( src , dest ) <NEWLINE> <DEDENT> <DEDENT> return DefaultCallbackObject . after_command ( self ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#~ def after_command()",
                "#~ class CopyCallbackObject"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "671cd85719c940dd99abaf6669633fb1": {
        "code_string": "dup_list, invalid = KTestTestFormat.ktest_fdupes(*folders, \\\n                         custom_replay_tool_binary_dir=self.custom_binary_dir)\n         if len(invalid) > 0:\n             logging.warning(\\\n                         \"{} generated ktests are invalid ({})\".format(\\\n                                                         len(invalid), invalid))\n             for kt in invalid:\n                 if KTestTestFormat.get_dir(kt, folders) == \\\n                                                         self.tests_storage_dir:\n                     os.remove(kt)\n         for dup_tuple in dup_list:\n             key = os.path.relpath(dup_tuple[0], \\\n                                 KTestTestFormat.get_dir(dup_tuple[0], folders))\n             kepttest2duptest_map[key] = [os.path.relpath(dp, \\\n                                         KTestTestFormat.get_dir(dp, folders)) \\\n                                                 for dp in dup_tuple[1:]]\n             for df in dup_tuple[1:]:\n                 if KTestTestFormat.get_dir(kt, folders) == \\\n                                                         self.tests_storage_dir:\n                     os.remove(df)\n         common_fs.dumpJSON(kepttest2duptest_map, self.keptktest2dupktests)\n",
        "code_toks_joined": "dup_list , invalid = KTestTestFormat . ktest_fdupes ( * folders , custom_replay_tool_binary_dir = self . custom_binary_dir ) <NEWLINE> <INDENT> if len ( invalid ) > 0 : <NEWLINE> <INDENT> logging . warning ( <STRING> . format ( len ( invalid ) , invalid ) ) <NEWLINE> for kt in invalid : <NEWLINE> <INDENT> if KTestTestFormat . get_dir ( kt , folders ) == self . tests_storage_dir : <NEWLINE> <INDENT> os . remove ( kt ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for dup_tuple in dup_list : <NEWLINE> <INDENT> key = os . path . relpath ( dup_tuple [ 0 ] , KTestTestFormat . get_dir ( dup_tuple [ 0 ] , folders ) ) <NEWLINE> kepttest2duptest_map [ key ] = [ os . path . relpath ( dp , KTestTestFormat . get_dir ( dp , folders ) ) for dp in dup_tuple [ 1 : ] ] <NEWLINE> for df in dup_tuple [ 1 : ] : <NEWLINE> <INDENT> if KTestTestFormat . get_dir ( kt , folders ) == self . tests_storage_dir : <NEWLINE> <INDENT> os . remove ( df ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> common_fs . dumpJSON ( kepttest2duptest_map , self . keptktest2dupktests ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{} generated ktests are invalid ({})\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "66735d0ef6994fbcb2d48b82ee496823": {
        "code_string": "parser_customexec = subparsers.add_parser('customexec', \\\n                                     help=\"Make some custom execution AFTER the\"\n                                         \" main execution is done\")\n         parser_run.add_argument(\"--nohashoutlog\", action='store_true', \\\n                                          help=\"When set, enforce no hash log\")\n",
        "code_toks_joined": "parser_customexec = subparsers . add_parser ( <STRING> , help = <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> parser_run . add_argument ( <STRING> , action = <STRING> , help = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'customexec'",
                "\"Make some custom execution AFTER the\"",
                "\" main execution is done\"",
                "\"--nohashoutlog\"",
                "'store_true'",
                "\"When set, enforce no hash log\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b67499096094fedb0c5213777d51303": {
        "code_string": "def get_definitions_pairs(defines_list):\n     def_pairs = {}\n     for define_statement_string in defines_list:\n         elems = re.split(\" \", define_statement_string)\n         if len(elems) > 3:  # When define statement is not a simple NAME <--> VALUE PAIR\n             continue  # Do not preprocess this\n         name = elems[1]\n         value = elems[2]\n         def_pairs[name] = value\n     return def_pairs\n",
        "code_toks_joined": "def get_definitions_pairs ( defines_list ) : <NEWLINE> <INDENT> def_pairs = { } <NEWLINE> for define_statement_string in defines_list : <NEWLINE> <INDENT> elems = re . split ( <STRING> , define_statement_string ) <NEWLINE> if len ( elems ) > 3 : <COMMENT> <NEWLINE> <INDENT> continue <COMMENT> <NEWLINE> <DEDENT> name = elems [ 1 ] <NEWLINE> value = elems [ 2 ] <NEWLINE> def_pairs [ name ] = value <NEWLINE> <DEDENT> return def_pairs <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\" \""
            ],
            "<COMMENT>": [
                "# When define statement is not a simple NAME <--> VALUE PAIR",
                "# Do not preprocess this"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e35913f504cf4f468b922f11902b3744": {
        "code_string": "def update_required(self) -> bool:\n         return self._variables_manager.get_variables() == self._get_required_variables()\n",
        "code_toks_joined": "def update_required ( self ) -> bool : <NEWLINE> <INDENT> return self . _variables_manager . get_variables ( ) == self . _get_required_variables ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "34a1adfbe3a242b9ae250b38ea427f30": {
        "code_string": "if log_entry.message is not \"\" or log_entry.message is not None:\n             dto[\"message\"] = log_entry.message\n",
        "code_toks_joined": "if log_entry . message is not <STRING> or log_entry . message is not None : <NEWLINE> <INDENT> dto [ <STRING> ] = log_entry . message <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"",
                "\"message\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2ad73b9bb0c5427a9910cc865647e8c8": {
        "code_string": "deb_dependencies = self.extra_args.get('pom', {}) \\\n             .project.get('deb_dependencies')\n         project = self.extra_args.get('pom', {}).project\n         generated_builds = []\n         for deb in project.get('debians', []):\n             deb_name = deb.get('name', self.project_name)\n             dpm = Dpm(project_path=self.project_path,\n                       package_name=deb_name,\n                       package_version=self.new_version,\n                       install_path=deb.get('install_path'),\n                       dependencies=deb_dependencies,\n                       description=deb.get('description'),\n                       excludes=project.get('excludes', []))\n",
        "code_toks_joined": "deb_dependencies = self . extra_args . get ( <STRING> , { } ) . project . get ( <STRING> ) <NEWLINE> <INDENT> project = self . extra_args . get ( <STRING> , { } ) . project <NEWLINE> generated_builds = [ ] <NEWLINE> for deb in project . get ( <STRING> , [ ] ) : <NEWLINE> <INDENT> deb_name = deb . get ( <STRING> , self . project_name ) <NEWLINE> dpm = Dpm ( project_path = self . project_path , <NEWLINE> <INDENT> package_name = deb_name , <NEWLINE> package_version = self . new_version , <NEWLINE> install_path = deb . get ( <STRING> ) , <NEWLINE> dependencies = deb_dependencies , <NEWLINE> description = deb . get ( <STRING> ) , <NEWLINE> excludes = project . get ( <STRING> , [ ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pom'",
                "'deb_dependencies'",
                "'pom'",
                "'debians'",
                "'name'",
                "'install_path'",
                "'description'",
                "'excludes'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "02685ed7bd2e41809ee62eb2ef366e26": {
        "code_string": "dim  = \"2D\" if opts['--2D'] else \"3D\";\n     if opts['--angle']:\n         angle = float(opts['--angle']);\n         angleopt = (angle,dim);\n     else:\n         angleopt = None;\n     KE, good = totalKE(d, ecut, angleopt, return_bools=True);\n     LE = laserE(E_0, w, T, dim=dim);\n     totalq = d['q'][good].sum()*1e12;\n     print('total charge: {} {}'.format(totalq,'pC/cm' if opts['--2D'] else 'pC'));\n     print(\"total energy: {} J\".format(KE));\n     print('pulse energy: {} J'.format(LE));\n     print('efficiency is {}'.format(KE/LE));\n",
        "code_toks_joined": "dim = <STRING> if opts [ <STRING> ] else <STRING> ; <NEWLINE> <INDENT> if opts [ <STRING> ] : <NEWLINE> <INDENT> angle = float ( opts [ <STRING> ] ) ; <NEWLINE> angleopt = ( angle , dim ) ; <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> angleopt = None ; <NEWLINE> <DEDENT> KE , good = totalKE ( d , ecut , angleopt , return_bools = True ) ; <NEWLINE> LE = laserE ( E_0 , w , T , dim = dim ) ; <NEWLINE> totalq = d [ <STRING> ] [ good ] . sum ( ) * 1e12 ; <NEWLINE> print ( <STRING> . format ( totalq , <STRING> if opts [ <STRING> ] else <STRING> ) ) ; <NEWLINE> print ( <STRING> . format ( KE ) ) ; <NEWLINE> print ( <STRING> . format ( LE ) ) ; <NEWLINE> print ( <STRING> . format ( KE / LE ) ) ; <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"2D\"",
                "'--2D'",
                "\"3D\"",
                "'--angle'",
                "'--angle'",
                "'q'",
                "'total charge: {} {}'",
                "'pC/cm'",
                "'--2D'",
                "'pC'",
                "\"total energy: {} J\"",
                "'pulse energy: {} J'",
                "'efficiency is {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a083010502f64f4cae67a6e3011d5126": {
        "code_string": "headers = base_headers.copy()\n         headers['FIBRE_NUM'] = i + 1\n",
        "code_toks_joined": "headers = base_headers . copy ( ) <NEWLINE> <INDENT> headers [ <STRING> ] = i + 1 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'FIBRE_NUM'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "703693dcba5c4200b4cdea93c499969a": {
        "code_string": "# Enviando o email\n         enviar_email(email, settings.DEFAULT_FROM_EMAIL, nome,\n             assunto, template_email, mensagem)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> enviar_email ( email , settings . DEFAULT_FROM_EMAIL , nome , <NEWLINE> <INDENT> assunto , template_email , mensagem ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Enviando o email"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "97e520b2e94748f58ab93eb6e0455e13": {
        "code_string": "def mcall_stat_parse(infile):\n \twith open(infile) as f:\n \t\tdstr=f.read()\n \treturn float(re.search('bisulfite conversion ratio = ([\\d.]+)', f).groups()[0])\n",
        "code_toks_joined": "def mcall_stat_parse ( infile ) : <NEWLINE> <INDENT> with open ( infile ) as f : <NEWLINE> <INDENT> dstr = f . read ( ) <NEWLINE> <DEDENT> return float ( re . search ( <STRING> , f ) . groups ( ) [ 0 ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'bisulfite conversion ratio = ([\\d.]+)'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bb37b65999354c4984b0eeb8777e4a6f": {
        "code_string": "# x-data for the ECDF: x_\n         x_value = np.sort(data)\n         size_data = x_value.size\n         # y-data for the ECDF: y\n         y_value = []\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> x_value = np . sort ( data ) <NEWLINE> size_data = x_value . size <NEWLINE> <COMMENT> <NL> y_value = [ ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# x-data for the ECDF: x_",
                "# y-data for the ECDF: y"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7734c420959c4f3bb7dbd66e7efc8233": {
        "code_string": "@staticmethod\n     def retrieve_rows_csv(request, job, **kwargs):\n         if request.method != 'POST':\n             print('kwargs', kwargs)\n             start_row = kwargs.get('start_row')\n             num_rows = kwargs.get('number_rows')\n             input_format = kwargs.get('format')\n             job_id = job.id\n",
        "code_toks_joined": "@ staticmethod <NEWLINE> <INDENT> def retrieve_rows_csv ( request , job , ** kwargs ) : <NEWLINE> <INDENT> if request . method != <STRING> : <NEWLINE> <INDENT> print ( <STRING> , kwargs ) <NEWLINE> start_row = kwargs . get ( <STRING> ) <NEWLINE> num_rows = kwargs . get ( <STRING> ) <NEWLINE> input_format = kwargs . get ( <STRING> ) <NEWLINE> job_id = job . id <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'POST'",
                "'kwargs'",
                "'start_row'",
                "'number_rows'",
                "'format'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "adb2c0ba98244f519d0548db13a187c8": {
        "code_string": "## returns a this tuple-> ( total_vars, beacon_vars(type: dict), discaded_vars(type: dict))\n     vcf_results = get_variants(vcf_obj, vcfsamples, raw_variants, qual)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> vcf_results = get_variants ( vcf_obj , vcfsamples , raw_variants , qual ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "## returns a this tuple-> ( total_vars, beacon_vars(type: dict), discaded_vars(type: dict))"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "58a35044bc294c5681786af44422c071": {
        "code_string": "## Extracts variants from mini-VCF file object:\n     # returns a this tuple-> ( n_total_vars, beacon_vars(type: dict), discaded_vars(type: dict))\n     ### beacon_vars is a disctionary with key --> sample, and value --> list of tuples containing the non-reference variants. Each tuple is defined as: (chr, start, alt_allele)\n     ### discaded_vars is a dictionary with key --> sample and value --> number of discarded vars due to quality for that sample.\n     vcf_results = get_variants(panel_filtered_results[0], samples, raw_variants, qual)\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> vcf_results = get_variants ( panel_filtered_results [ 0 ] , samples , raw_variants , qual ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "## Extracts variants from mini-VCF file object:",
                "# returns a this tuple-> ( n_total_vars, beacon_vars(type: dict), discaded_vars(type: dict))",
                "### beacon_vars is a disctionary with key --> sample, and value --> list of tuples containing the non-reference variants. Each tuple is defined as: (chr, start, alt_allele)",
                "### discaded_vars is a dictionary with key --> sample and value --> number of discarded vars due to quality for that sample."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7607f49b1e8241cd97788d3325292189": {
        "code_string": "if isinstance(channels, list):\n             for ch in channels:\n                 permissions[int(ch)] = (read, write)                     \n         else:\n             permissions[int(ch)] = (read, write)\n",
        "code_toks_joined": "if isinstance ( channels , list ) : <NEWLINE> <INDENT> for ch in channels : <NEWLINE> <INDENT> permissions [ int ( ch ) ] = ( read , write ) <NEWLINE> else : <NEWLINE> <DEDENT> permissions [ int ( ch ) ] = ( read , write ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e5666b483d7344c697ce1a6015a99f6d": {
        "code_string": "def microlensing_parameters_limits_priors(parameters, limits):\n     for i in xrange(len(parameters)):\n",
        "code_toks_joined": "def microlensing_parameters_limits_priors ( parameters , limits ) : <NEWLINE> <INDENT> for i in xrange ( len ( parameters ) ) : <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7bf9aa9c36274439ba3092e2219414c3": {
        "code_string": "if np.max((caustic_points[:, first_branch]).real) > np.max((caustic_points[:, second_branch]).real):\n",
        "code_toks_joined": "if np . max ( ( caustic_points [ : , first_branch ] ) . real ) > np . max ( ( caustic_points [ : , second_branch ] ) . real ) : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "19fd4ca33cbd490a92d17112a0d8341d": {
        "code_string": "# Handle the response, whether it's a normal return value or\n                 # an exception.\n                 if is_exception:\n                     ex_type, ex_value, ex_tb = response\n                     raise ex_type(ex_value, ex_tb)\n                 return response_queue\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> if is_exception : <NEWLINE> <INDENT> ex_type , ex_value , ex_tb = response <NEWLINE> raise ex_type ( ex_value , ex_tb ) <NEWLINE> <DEDENT> return response_queue <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Handle the response, whether it's a normal return value or",
                "# an exception."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "de1ed8ff61134fd68321cc3e903d5d28": {
        "code_string": "slave = self.canvas_slave\n         # Map GTK event x/y coordinates to the video frame coordinate space.\n         frame_point_i = \\\n             cv2.perspectiveTransform(np.array([[start_xy]], dtype=float),\n                                      slave.canvas_to_frame_map).ravel()\n         # Find the closest corner point in the frame to the starting point.\n         frame_corner_i = find_closest(slave.df_frame_corners, frame_point_i)\n         # Find the closest corner point in the canvas to the end point.\n         canvas_corner_i = find_closest(slave.df_canvas_corners, end_xy)\n",
        "code_toks_joined": "slave = self . canvas_slave <NEWLINE> <COMMENT> <NL> <INDENT> frame_point_i = cv2 . perspectiveTransform ( np . array ( [ [ start_xy ] ] , dtype = float ) , <NEWLINE> <INDENT> slave . canvas_to_frame_map ) . ravel ( ) <NEWLINE> <COMMENT> <NL> <DEDENT> frame_corner_i = find_closest ( slave . df_frame_corners , frame_point_i ) <NEWLINE> <COMMENT> <NL> canvas_corner_i = find_closest ( slave . df_canvas_corners , end_xy ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Map GTK event x/y coordinates to the video frame coordinate space.",
                "# Find the closest corner point in the frame to the starting point.",
                "# Find the closest corner point in the canvas to the end point."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "19b58183fe5d4fb89efdb26b73266b5a": {
        "code_string": "enriched_transactions = enrich_transactions(blocks, transactions, receipts)\n         if len(enriched_transactions) == len(transactions):\n             raise ValueError('The number of transactions is wrong ' + str(enriched_transactions))\n         enriched_logs = enrich_logs(blocks, logs)\n         if len(enriched_logs) != len(logs):\n             raise ValueError('The number of logs is wrong ' + str(enriched_logs))\n         enriched_token_transfers = enrich_token_transfers(blocks, token_transfers)\n         if len(enriched_token_transfers) != len(token_transfers):\n             raise ValueError('The number of token transfers is wrong ' + str(enriched_token_transfers))\n",
        "code_toks_joined": "enriched_transactions = enrich_transactions ( blocks , transactions , receipts ) <NEWLINE> <INDENT> if len ( enriched_transactions ) == len ( transactions ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> + str ( enriched_transactions ) ) <NEWLINE> <DEDENT> enriched_logs = enrich_logs ( blocks , logs ) <NEWLINE> if len ( enriched_logs ) != len ( logs ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> + str ( enriched_logs ) ) <NEWLINE> <DEDENT> enriched_token_transfers = enrich_token_transfers ( blocks , token_transfers ) <NEWLINE> if len ( enriched_token_transfers ) != len ( token_transfers ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> + str ( enriched_token_transfers ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'The number of transactions is wrong '",
                "'The number of logs is wrong '",
                "'The number of token transfers is wrong '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9d976a95d2d640f78fe9bf62a2fcdc56": {
        "code_string": "class TestAccumulatingTraces(unittest.TestCase):\n   def setUp(self):\n     self.basis = FourierBasis(space, 2, 2)\n     self.approximation = DiscreteLinearApproximation(0.1, self.basis, actions=3)\n     self.env = Env()\n     self.traces = AccumulatingTraces(self.env, self.approximation, 0.5)\n",
        "code_toks_joined": "class TestAccumulatingTraces ( unittest . TestCase ) : <NEWLINE> <INDENT> def setUp ( self ) : <NEWLINE> <INDENT> self . basis = FourierBasis ( space , 2 , 2 ) <NEWLINE> self . approximation = DiscreteLinearApproximation ( 0.1 , self . basis , actions = 3 ) <NEWLINE> self . env = Env ( ) <NEWLINE> self . traces = AccumulatingTraces ( self . env , self . approximation , 0.5 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b88b90712a544201aaf3df446da3457e": {
        "code_string": "def test_update(self):\n     basis = FourierBasis(space, 2, 2)\n     approximation = LinearApproximation(0.1, basis)\n     x = np.array([0.5, 1])\n     self.assertEqual(approximation.call(x), 0)\n     approximation.update(x, 1)\n     self.assertAlmostEqual(approximation.call(x), 0.6)\n",
        "code_toks_joined": "def test_update ( self ) : <NEWLINE> <INDENT> basis = FourierBasis ( space , 2 , 2 ) <NEWLINE> approximation = LinearApproximation ( 0.1 , basis ) <NEWLINE> x = np . array ( [ 0.5 , 1 ] ) <NEWLINE> self . assertEqual ( approximation . call ( x ) , 0 ) <NEWLINE> approximation . update ( x , 1 ) <NEWLINE> self . assertAlmostEqual ( approximation . call ( x ) , 0.6 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "94f3536db33544cab02ef196410b0063": {
        "code_string": "if minutes != 0:\n         if not first: res += \", \"\n         res += _(\"%d min\") % hours;\n         first = False\n",
        "code_toks_joined": "if minutes != 0 : <NEWLINE> <INDENT> if not first : res += <STRING> <NEWLINE> res += _ ( <STRING> ) % hours ; <NEWLINE> first = False <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\", \"",
                "\"%d min\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ac40c0df70dc4fcdb133bc245fad47b7": {
        "code_string": "if self.state != AuctionState.BID:\n             raise InvalidActionError(\"Bid was attempted, but it is not currently time to submit bids.\")\n         elif self.bid > bid:\n             raise InvalidActionError(\"Bid amount \" + str(bid) + \" must be greater than current bid of \" + str(self.bid))\n         elif not self.owners[owner_id].can_buy(self.nominee, bid):\n             raise InvalidActionError(\"The owner with index \" + str(owner_id) +\n                                      \" cannot afford a bid of \" + str(bid) + \" for player \" + self.nominee.name +\n                                      \" or cannot actually buy this player (due to \"\n                                      \"not having any free slots)\")\n",
        "code_toks_joined": "if self . state != AuctionState . BID : <NEWLINE> <INDENT> raise InvalidActionError ( <STRING> ) <NEWLINE> elif self . bid > bid : <NEWLINE> raise InvalidActionError ( <STRING> + str ( bid ) + <STRING> + str ( self . bid ) ) <NEWLINE> elif not self . owners [ owner_id ] . can_buy ( self . nominee , bid ) : <NEWLINE> raise InvalidActionError ( <STRING> + str ( owner_id ) + <NEWLINE> <INDENT> <STRING> + str ( bid ) + <STRING> + self . nominee . name + <NEWLINE> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Bid was attempted, but it is not currently time to submit bids.\"",
                "\"Bid amount \"",
                "\" must be greater than current bid of \"",
                "\"The owner with index \"",
                "\" cannot afford a bid of \"",
                "\" for player \"",
                "\" or cannot actually buy this player (due to \"",
                "\"not having any free slots)\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c121ac9d5f9c4e0fa899b3d3c3fc6389": {
        "code_string": "async def handle(**data):\n             messages = data['M'] if 'M' in data and len(data['M']) > 0 else {}\n             for inner_data in messages:\n                 hub = inner_data['H'] if 'H' in inner_data else ''\n                 if hub.lower() == self.name.lower():\n                     method = inner_data['M']\n                     message = inner_data['A']\n                     await self.__handlers[method](message)\n",
        "code_toks_joined": "async def handle ( ** data ) : <NEWLINE> <INDENT> messages = data [ <STRING> ] if <STRING> in data and len ( data [ <STRING> ] ) > 0 else { } <NEWLINE> for inner_data in messages : <NEWLINE> <INDENT> hub = inner_data [ <STRING> ] if <STRING> in inner_data else <STRING> <NEWLINE> if hub . lower ( ) == self . name . lower ( ) : <NEWLINE> <INDENT> method = inner_data [ <STRING> ] <NEWLINE> message = inner_data [ <STRING> ] <NEWLINE> await self . __handlers [ method ] ( message ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'M'",
                "'M'",
                "'M'",
                "'H'",
                "'H'",
                "''",
                "'M'",
                "'A'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d4099c3243684a7d8b72a316beb6d58e": {
        "code_string": "async def handle(**data):\n             messages = data['M'] if 'M' in data and len(data['M']) > 0 else {}\n             for inner_data in messages:\n                 hub = inner_data['H'] if 'H' in inner_data else ''\n                 if hub.lower() == self.name.lower():\n                     method = inner_data['M']\n                     message = inner_data['A']\n                     await self.__handlers[method](inner_data)\n",
        "code_toks_joined": "async def handle ( ** data ) : <NEWLINE> <INDENT> messages = data [ <STRING> ] if <STRING> in data and len ( data [ <STRING> ] ) > 0 else { } <NEWLINE> for inner_data in messages : <NEWLINE> <INDENT> hub = inner_data [ <STRING> ] if <STRING> in inner_data else <STRING> <NEWLINE> if hub . lower ( ) == self . name . lower ( ) : <NEWLINE> <INDENT> method = inner_data [ <STRING> ] <NEWLINE> message = inner_data [ <STRING> ] <NEWLINE> await self . __handlers [ method ] ( inner_data ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'M'",
                "'M'",
                "'M'",
                "'H'",
                "'H'",
                "''",
                "'M'",
                "'A'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a2fec2048667454ebc607b25faced48c": {
        "code_string": "tasklists = res_urls\n                     if isinstance(tasklist, string_types):\n                         tasklists = [tasklists]\n",
        "code_toks_joined": "tasklists = res_urls <NEWLINE> <INDENT> if isinstance ( tasklist , string_types ) : <NEWLINE> <INDENT> tasklists = [ tasklists ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "df75cad9feb24aa3a75bce2b56f57025": {
        "code_string": "def __init__(self, starting_board=None):\n         self.board = np.empty((8,8),dtype=Piece)\n         if starting_board is not None:\n             self._start_pos()\n         else:\n             self.board = starting_board\n         self.move_counter = 0\n",
        "code_toks_joined": "def __init__ ( self , starting_board = None ) : <NEWLINE> <INDENT> self . board = np . empty ( ( 8 , 8 ) , dtype = Piece ) <NEWLINE> if starting_board is not None : <NEWLINE> <INDENT> self . _start_pos ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . board = starting_board <NEWLINE> <DEDENT> self . move_counter = 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "71fcc7264a194488afbc30607db60aec": {
        "code_string": "def sanitize_redirect(host, redirect_to):\n     \"\"\"\n     Given the hostname and an untrusted URL to redirect to,\n     this method tests it to make sure it isn't garbage/harmful\n     and returns it, else returns None, similar as how's it done\n     on django.contrib.auth.views.\n     \"\"\"\n     # Quick sanity check.\n     if not redirect_to or \\\n        not isinstance(redirect_to, six.string_types) and \\\n        getattr(redirect_to, 'decode', None) and \\\n        not isinstance(redirect_to.decode(), six.string_types):\n         return None\n",
        "code_toks_joined": "def sanitize_redirect ( host , redirect_to ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if not redirect_to or not isinstance ( redirect_to , six . string_types ) and getattr ( redirect_to , <STRING> , None ) and not isinstance ( redirect_to . decode ( ) , six . string_types ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Given the hostname and an untrusted URL to redirect to,\n     this method tests it to make sure it isn't garbage/harmful\n     and returns it, else returns None, similar as how's it done\n     on django.contrib.auth.views.\n     \"\"\"",
                "'decode'"
            ],
            "<COMMENT>": [
                "# Quick sanity check."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c5d36ff80d4e439694bc30bd4770c459": {
        "code_string": "def partial_pipeline_data(strategy, user, *args, **kwargs):\n     partial = strategy.session_get('partial_pipeline', None)\n     if partial:\n         idx, backend, xargs, xkwargs = strategy.partial_from_session(partial)\n         kwargs = kwargs.copy()\n         kwargs.setdefault('user', user)\n         kwargs.setdefault('request', strategy.request)\n         kwargs.update(xkwargs)\n         return idx, backend, xargs, xkwargs\n",
        "code_toks_joined": "def partial_pipeline_data ( strategy , user , * args , ** kwargs ) : <NEWLINE> <INDENT> partial = strategy . session_get ( <STRING> , None ) <NEWLINE> if partial : <NEWLINE> <INDENT> idx , backend , xargs , xkwargs = strategy . partial_from_session ( partial ) <NEWLINE> kwargs = kwargs . copy ( ) <NEWLINE> kwargs . setdefault ( <STRING> , user ) <NEWLINE> kwargs . setdefault ( <STRING> , strategy . request ) <NEWLINE> kwargs . update ( xkwargs ) <NEWLINE> return idx , backend , xargs , xkwargs <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'partial_pipeline'",
                "'user'",
                "'request'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "68910ce2a1994d2ebacc9945291021e8": {
        "code_string": "if isinstance(exception, SocialAuthBaseException):\n             backend_name = strategy.backend.name\n             message = self.get_message(request, exception)\n             url = self.get_redirect_uri(request, exception)\n             try:\n                 messages.error(request, message,\n                                extra_tags='social-auth ' + backend_name)\n             except MessageFailure:\n                 url += ('?' in url and '&' or '?') + \\\n                        'message={0}&backend={1}'.format(urlquote(message),\n                                                         backend_name)\n             return redirect(url)\n",
        "code_toks_joined": "if isinstance ( exception , SocialAuthBaseException ) : <NEWLINE> <INDENT> backend_name = strategy . backend . name <NEWLINE> message = self . get_message ( request , exception ) <NEWLINE> url = self . get_redirect_uri ( request , exception ) <NEWLINE> try : <NEWLINE> <INDENT> messages . error ( request , message , <NEWLINE> <INDENT> extra_tags = <STRING> + backend_name ) <NEWLINE> <DEDENT> <DEDENT> except MessageFailure : <NEWLINE> <INDENT> url += ( <STRING> in url and <STRING> or <STRING> ) + <STRING> . format ( urlquote ( message ) , <NEWLINE> <INDENT> backend_name ) <NEWLINE> <DEDENT> <DEDENT> return redirect ( url ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'social-auth '",
                "'?'",
                "'&'",
                "'?'",
                "'message={0}&backend={1}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2500ae1ac0b0485595be025f8983ac32": {
        "code_string": "if type(data_x[0]) in ucvar:\n         values_x = [d.n for d in data_x]\n         sigma_x = [d.s if d.s!=0 else 1e-5 for d in data_y]\n     elif type(data_x[0]) in [float, int]:\n         values_x = data_x\n",
        "code_toks_joined": "if type ( data_x [ 0 ] ) in ucvar : <NEWLINE> <INDENT> values_x = [ d . n for d in data_x ] <NEWLINE> sigma_x = [ d . s if d . s != 0 else 1e-5 for d in data_y ] <NEWLINE> elif type ( data_x [ 0 ] ) in [ float , int ] : <NEWLINE> values_x = data_x <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dde2c2eed2e040d082761b7baf1da3f9": {
        "code_string": "route.begin_time = route.drives[0].begin_time\n         route.end_time = route.drives[-1].end_time\n         route.append(route)\n",
        "code_toks_joined": "route . begin_time = route . drives [ 0 ] . begin_time <NEWLINE> <INDENT> route . end_time = route . drives [ - 1 ] . end_time <NEWLINE> route . append ( route ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1588b433ea28407088ebfb8988771f11": {
        "code_string": "# When no magic, add run command\n         if not first_line.startswith(\"%\"):\n             first_line = \"%{} {}\".format(UserCommandParser.run_command, code)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if not first_line . startswith ( <STRING> ) : <NEWLINE> <INDENT> first_line = <STRING> . format ( UserCommandParser . run_command , code ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# When no magic, add run command"
            ],
            "<STRING>": [
                "\"%\"",
                "\"%{} {}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9c42fb029c9c4d19b2789b25e2f91f78": {
        "code_string": "if key.label:\n                 label = QGraphicsTextItem(key.label)\n                 label.setFont(font)\n                 label.setDefaultTextColor(QColor(steno_layout.font_color))\n",
        "code_toks_joined": "if key . label : <NEWLINE> <INDENT> label = QGraphicsTextItem ( key . label ) <NEWLINE> label . setFont ( font ) <NEWLINE> label . setDefaultTextColor ( QColor ( steno_layout . font_color ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d5f441c31d542c7b22f7b22da0aef24": {
        "code_string": "last = 0\n     for i, etime in enumerate(ends_arr):\n         if time < etime:\n             return i, etime - last\n         last = etime\n     if time == last:\n         return len(ends_arr) - 1, 0\n     raise ValueError(f'time={time}  not in [0, {ends_arr[-1]}]'\n                         + f' or {ends_arr} unsorted')\n",
        "code_toks_joined": "last = 0 <NEWLINE> <INDENT> for i , etime in enumerate ( ends_arr ) : <NEWLINE> <INDENT> if time < etime : <NEWLINE> <INDENT> return i , etime - last <NEWLINE> <DEDENT> last = etime <NEWLINE> <DEDENT> if time == last : <NEWLINE> <INDENT> return len ( ends_arr ) - 1 , 0 <NEWLINE> <DEDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f'time={time}  not in [0, {ends_arr[-1]}]'",
                "f' or {ends_arr} unsorted'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "113cec8b3f8848fbb2cd67ae406ab461": {
        "code_string": "# Check the union registry first.\n         handler = self._union_registry.get(union)\n         if handler is not None:\n             return handler(union, obj)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> handler = self . _union_registry . get ( union ) <NEWLINE> if handler is not None : <NEWLINE> <INDENT> return handler ( union , obj ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check the union registry first."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e69fa2caf1cf44baa01354b00b458f2e": {
        "code_string": "# Isolate this energy's data, separate frequencies, and format the data\n         self.E_data = self._isolate_energy(data,E_code)\n         self.monthly_data, self.yearly_data = self._sep_freqs(self.E_data)\n         for data_df in self.monthly_data,self.yearly_data:\n             data_df.set_index('Date_code',inplace=True)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> self . E_data = self . _isolate_energy ( data , E_code ) <NEWLINE> self . monthly_data , self . yearly_data = self . _sep_freqs ( self . E_data ) <NEWLINE> for data_df in self . monthly_data , self . yearly_data : <NEWLINE> <INDENT> data_df . set_index ( <STRING> , inplace = True ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Isolate this energy's data, separate frequencies, and format the data"
            ],
            "<STRING>": [
                "'Date_code'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7597c25aeb9646c28f9eb76c2a3dcd24": {
        "code_string": "# Identify right labels\n     if len(rightLabels) == 0:\n         rightLabels = pd.Series(dataFrame.right.unique()).unique()\n     else:\n         check_data_matches_labels(leftLabels, dataFrame[\"right\"], \"right\")\n     # If no colorDict given, make one\n     if colorDict is None:\n         colorDict = {}\n         palette = \"hls\"\n         colorPalette = sns.color_palette(palette, len(allLabels))\n         for i, label in enumerate(allLabels):\n             colorDict[label] = colorPalette[i]\n     else:\n         missing = [label for label in allLabels if label not in colorDict.keys()]\n         if missing:\n             msg = (\n                 \"The colorDict parameter is missing values for the following labels : \"\n             )\n             msg += \"{}\".format(\", \".join(missing))\n             raise ValueError(msg)\n     LOGGER.debug(f\"The colordict value are : {colorDict}\")\n     # Determine widths of individual strips\n     ns_l = defaultdict()\n     ns_r = defaultdict()\n     for leftLabel in leftLabels:\n         leftDict = {}\n         rightDict = {}\n         for rightLabel in rightLabels:\n             leftDict[rightLabel] = dataFrame[\n                 (dataFrame.left == leftLabel) & (dataFrame.right == rightLabel)\n             ].leftWeight.sum()\n             rightDict[rightLabel] = dataFrame[\n                 (dataFrame.left == leftLabel) & (dataFrame.right == rightLabel)\n             ].rightWeight.sum()\n         ns_l[leftLabel] = leftDict\n         ns_r[leftLabel] = rightDict\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if len ( rightLabels ) == 0 : <NEWLINE> <INDENT> rightLabels = pd . Series ( dataFrame . right . unique ( ) ) . unique ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> check_data_matches_labels ( leftLabels , dataFrame [ <STRING> ] , <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if colorDict is None : <NEWLINE> <INDENT> colorDict = { } <NEWLINE> palette = <STRING> <NEWLINE> colorPalette = sns . color_palette ( palette , len ( allLabels ) ) <NEWLINE> for i , label in enumerate ( allLabels ) : <NEWLINE> <INDENT> colorDict [ label ] = colorPalette [ i ] <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> missing = [ label for label in allLabels if label not in colorDict . keys ( ) ] <NEWLINE> if missing : <NEWLINE> <INDENT> msg = ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> msg += <STRING> . format ( <STRING> . join ( missing ) ) <NEWLINE> raise ValueError ( msg ) <NEWLINE> <DEDENT> <DEDENT> LOGGER . debug ( <STRING> ) <NEWLINE> <COMMENT> <NL> ns_l = defaultdict ( ) <NEWLINE> ns_r = defaultdict ( ) <NEWLINE> for leftLabel in leftLabels : <NEWLINE> <INDENT> leftDict = { } <NEWLINE> rightDict = { } <NEWLINE> for rightLabel in rightLabels : <NEWLINE> <INDENT> leftDict [ rightLabel ] = dataFrame [ <NEWLINE> <INDENT> ( dataFrame . left == leftLabel ) & ( dataFrame . right == rightLabel ) <NEWLINE> <DEDENT> ] . leftWeight . sum ( ) <NEWLINE> rightDict [ rightLabel ] = dataFrame [ <NEWLINE> <INDENT> ( dataFrame . left == leftLabel ) & ( dataFrame . right == rightLabel ) <NEWLINE> <DEDENT> ] . rightWeight . sum ( ) <NEWLINE> <DEDENT> ns_l [ leftLabel ] = leftDict <NEWLINE> ns_r [ leftLabel ] = rightDict <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Identify right labels",
                "# If no colorDict given, make one",
                "# Determine widths of individual strips"
            ],
            "<STRING>": [
                "\"right\"",
                "\"right\"",
                "\"hls\"",
                "\"The colorDict parameter is missing values for the following labels : \"",
                "\"{}\"",
                "\", \"",
                "f\"The colordict value are : {colorDict}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7b64dc5dfabd483cbbf21bd01f145c4c": {
        "code_string": "def handle_word(word):\n     #click.echo('Word received by new(): {}'.format(word))\n     try:\n         API_KEY = load_api_key()\n         #print('API_KEY:', API_KEY)\n     except ConfigFileError:\n         click.echo('API key is missing. Kindly provide an API key by registering via:\\n\\n$ familiarize --init')\n     else:\n         word_object = fetch_word(word)\n         click.echo_via_pager(word_object.stringify())\n         word_save_status = save_word(word)\n         if word_save_status:\n             click.echo('{} has been added to your personal dictionary.'.format(word))\n         else:\n             click.echo('{} could not be added to your dictionary'.format(word))\n",
        "code_toks_joined": "def handle_word ( word ) : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> API_KEY = load_api_key ( ) <NEWLINE> <COMMENT> <NL> <DEDENT> except ConfigFileError : <NEWLINE> <INDENT> click . echo ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> word_object = fetch_word ( word ) <NEWLINE> click . echo_via_pager ( word_object . stringify ( ) ) <NEWLINE> word_save_status = save_word ( word ) <NEWLINE> if word_save_status : <NEWLINE> <INDENT> click . echo ( <STRING> . format ( word ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> click . echo ( <STRING> . format ( word ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#click.echo('Word received by new(): {}'.format(word))",
                "#print('API_KEY:', API_KEY)"
            ],
            "<STRING>": [
                "'API key is missing. Kindly provide an API key by registering via:\\n\\n$ familiarize --init'",
                "'{} has been added to your personal dictionary.'",
                "'{} could not be added to your dictionary'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fa3c792755584009a8f27919eb7727e5": {
        "code_string": "if origin is None and args == ():\n             return PrimitiveTp(origin)\n         if origin is list and len(args) == 1:\n             return ListTp(origin, cls[args[0]])\n         if origin is tuple:\n             if len(args) == 2 and args[1] is ...:\n                 return HomoTupleTp(origin, cls[args[0]])\n             else:\n                 return HeteroTupleTp(origin, *[cls[a] for a in args])\n         if origin is set:\n             return SetTp(origin, cls[args[0]])\n         if origin is frozenset:\n             return FrozenSetTp(origin, cls[args[0]])\n",
        "code_toks_joined": "if origin is None and args == ( ) : <NEWLINE> <INDENT> return PrimitiveTp ( origin ) <NEWLINE> if origin is list and len ( args ) == 1 : <NEWLINE> return ListTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> if origin is tuple : <NEWLINE> if len ( args ) == 2 and args [ 1 ] is ... : <NEWLINE> <INDENT> return HomoTupleTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return HeteroTupleTp ( origin , * [ cls [ a ] for a in args ] ) <NEWLINE> if origin is set : <NEWLINE> <DEDENT> return SetTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> if origin is frozenset : <NEWLINE> return FrozenSetTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ac452689b19640cc8d0d43a19d762217": {
        "code_string": "while sector != ENDOFCHAIN and (current + 1) * sector_size <= current:\n             sector = self.next_fat(sector)\n             position += 1\n",
        "code_toks_joined": "while sector != ENDOFCHAIN and ( current + 1 ) * sector_size <= current : <NEWLINE> <INDENT> sector = self . next_fat ( sector ) <NEWLINE> position += 1 <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3ab639240c474324be73fcde7a104aeb": {
        "code_string": "try:\n         link_file = LinkFile(link_file_path)\n         config_file = ConfigFile(config_file_path, True)\n         if validate(link_file_path, config_file_path):\n             click.secho('Invalid configuration file.', fg='red', err=True)\n             click.get_current_context().exit(1)\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> link_file = LinkFile ( link_file_path ) <NEWLINE> config_file = ConfigFile ( config_file_path , True ) <NEWLINE> if validate ( link_file_path , config_file_path ) : <NEWLINE> <INDENT> click . secho ( <STRING> , fg = <STRING> , err = True ) <NEWLINE> click . get_current_context ( ) . exit ( 1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Invalid configuration file.'",
                "'red'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9c55bc7fd4cb4c68aad93cbb90c61771": {
        "code_string": "#    \n     # Check if verify required for requests\n     # \n     # * Author: Mayur\n     # * Date: 19/11/2018\n     # * Reviewed By:\n     #\n     # @return dict\n     #  \n     def verify_required(self):\n         if self.urlparse()(self.api_base_url).scheme == \"http\":\n             return True\n         return True             \n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> def verify_required ( self ) : <NEWLINE> <INDENT> if self . urlparse ( ) ( self . api_base_url ) . scheme == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#    ",
                "# Check if verify required for requests",
                "# ",
                "# * Author: Mayur",
                "# * Date: 19/11/2018",
                "# * Reviewed By:",
                "#",
                "# @return dict",
                "#  "
            ],
            "<STRING>": [
                "\"http\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ca40c111780e4cf4817cc5b079ca2ff8": {
        "code_string": "def to_stan(self, acc, indent=0):\n         # Do we sometime need parens?\n         # is this an operator precedence issue?\n         if self.lower:\n             self.to_stan_prec(self.lower, acc, indent)\n         if self.lower and self.upper:\n             acc += self.mkString(\":\")\n         if self.upper:\n             self.to_stan_prec(self.upper, acc, indent)\n",
        "code_toks_joined": "def to_stan ( self , acc , indent = 0 ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if self . lower : <NEWLINE> <INDENT> self . to_stan_prec ( self . lower , acc , indent ) <NEWLINE> <DEDENT> if self . lower and self . upper : <NEWLINE> <INDENT> acc += self . mkString ( <STRING> ) <NEWLINE> <DEDENT> if self . upper : <NEWLINE> <INDENT> self . to_stan_prec ( self . upper , acc , indent ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Do we sometime need parens?",
                "# is this an operator precedence issue?"
            ],
            "<STRING>": [
                "\":\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e0c1cc14319e4ce0ac4c658cf6785068": {
        "code_string": "def process_request(self, request):\n         # Check settings patterns\n         urls = tuple([re.compile(url) for url in getattr(settings, 'SSL_PATTERNS', [])])\n         secure = any([url.search(request.path) for url in urls])\n         if request.is_secure():\n             if not secure and not getattr(request, 'keep_secure', False):\n                 if getattr(settings, 'SSL_WHITELIST', False):\n                     # Redirect off SSL\n                     return _redirect(request, False)\n         else:\n             if secure:\n                return _redirect(request, True) \n",
        "code_toks_joined": "def process_request ( self , request ) : <NEWLINE> <COMMENT> <NL> <INDENT> urls = tuple ( [ re . compile ( url ) for url in getattr ( settings , <STRING> , [ ] ) ] ) <NEWLINE> secure = any ( [ url . search ( request . path ) for url in urls ] ) <NEWLINE> if request . is_secure ( ) : <NEWLINE> <INDENT> if not secure and not getattr ( request , <STRING> , False ) : <NEWLINE> <INDENT> if getattr ( settings , <STRING> , False ) : <NEWLINE> <COMMENT> <NL> <INDENT> return _redirect ( request , False ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if secure : <NEWLINE> <INDENT> return _redirect ( request , True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Check settings patterns",
                "# Redirect off SSL"
            ],
            "<STRING>": [
                "'SSL_PATTERNS'",
                "'keep_secure'",
                "'SSL_WHITELIST'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1fec89dc3d3645b0b9e8b4ef153c86f8": {
        "code_string": "def find_storage_directories():\n     home_dir = pathlib.Path(os.environ['HOME'])\n     candidates = []\n     firefox_dir = home_dir/\".mozilla\"/\"firefox\"\n     if firefox_dir.exists():\n         candidates.append(firefox_dir.iterdir())\n     zotero_dir = home_dir/\".zotero\"\n     if zotero_dir.exists():\n         candidates.append(zotero_dir.iterdir())\n     zotero5_dir = home_dir/\"Zotero/storage\"\n     if zotero_dir.exists():\n         yield ('default', zotero5_dir)\n     candidate_iter = itertools.chain.from_iterable(candidates)\n     for fpath in candidate_iter:\n         if not fpath.is_dir():\n             continue\n         match = PROFILE_PAT.match(fpath.name)\n         if match:\n             storage_path = fpath/\"zotero\"/\"storage\"\n             if storage_path.exists():\n                 yield (match.group(2), storage_path)\n",
        "code_toks_joined": "def find_storage_directories ( ) : <NEWLINE> <INDENT> home_dir = pathlib . Path ( os . environ [ <STRING> ] ) <NEWLINE> candidates = [ ] <NEWLINE> firefox_dir = home_dir / <STRING> / <STRING> <NEWLINE> if firefox_dir . exists ( ) : <NEWLINE> <INDENT> candidates . append ( firefox_dir . iterdir ( ) ) <NEWLINE> <DEDENT> zotero_dir = home_dir / <STRING> <NEWLINE> if zotero_dir . exists ( ) : <NEWLINE> <INDENT> candidates . append ( zotero_dir . iterdir ( ) ) <NEWLINE> <DEDENT> zotero5_dir = home_dir / <STRING> <NEWLINE> if zotero_dir . exists ( ) : <NEWLINE> <INDENT> yield ( <STRING> , zotero5_dir ) <NEWLINE> <DEDENT> candidate_iter = itertools . chain . from_iterable ( candidates ) <NEWLINE> for fpath in candidate_iter : <NEWLINE> <INDENT> if not fpath . is_dir ( ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> match = PROFILE_PAT . match ( fpath . name ) <NEWLINE> if match : <NEWLINE> <INDENT> storage_path = fpath / <STRING> / <STRING> <NEWLINE> if storage_path . exists ( ) : <NEWLINE> <INDENT> yield ( match . group ( 2 ) , storage_path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'HOME'",
                "\".mozilla\"",
                "\"firefox\"",
                "\".zotero\"",
                "\"Zotero/storage\"",
                "'default'",
                "\"zotero\"",
                "\"storage\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f46f40bef8be48c4a219da692cf9a8d0": {
        "code_string": "desc = SymmetryFunction(cutfunc, cutvalue, desc_params)\n",
        "code_toks_joined": "desc = SymmetryFunction ( cutfunc , cutvalue , desc_params ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "03a6567b0d664bb5986a3b661e45e0a0": {
        "code_string": "# Set flag for each plugin: `False` iff the plugin was already enabled,\n     # `True` iff it was just enabled now.\n     enabled_now = {}\n     for plugin_path_i in plugin_paths:\n         plugin_link_path_i = enabled_path.joinpath(plugin_path_i.name)\n         if not plugin_link_path_i.exists():\n             if platform.system() == 'Windows':\n                 plugin_path_i.junction(plugin_link_path_i)\n             else:\n                 plugin_path_i.symlink(plugin_link_path_i)\n             logger.debug('Enabled plugin directory: `%s` -> `%s`',\n                          plugin_path_i, plugin_link_path_i)\n             enabled_now[plugin_path_i.name] = True\n         else:\n             logger.debug('Plugin already enabled: `%s` -> `%s`', plugin_path_i,\n                          plugin_link_path_i)\n             enabled_now[plugin_path_i.name] = False\n     return enabled_now if not singleton else singleton.values()[0]\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> enabled_now = { } <NEWLINE> for plugin_path_i in plugin_paths : <NEWLINE> <INDENT> plugin_link_path_i = enabled_path . joinpath ( plugin_path_i . name ) <NEWLINE> if not plugin_link_path_i . exists ( ) : <NEWLINE> <INDENT> if platform . system ( ) == <STRING> : <NEWLINE> <INDENT> plugin_path_i . junction ( plugin_link_path_i ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> plugin_path_i . symlink ( plugin_link_path_i ) <NEWLINE> <DEDENT> logger . debug ( <STRING> , <NEWLINE> <INDENT> plugin_path_i , plugin_link_path_i ) <NEWLINE> <DEDENT> enabled_now [ plugin_path_i . name ] = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . debug ( <STRING> , plugin_path_i , <NEWLINE> <INDENT> plugin_link_path_i ) <NEWLINE> <DEDENT> enabled_now [ plugin_path_i . name ] = False <NEWLINE> <DEDENT> <DEDENT> return enabled_now if not singleton else singleton . values ( ) [ 0 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Set flag for each plugin: `False` iff the plugin was already enabled,",
                "# `True` iff it was just enabled now."
            ],
            "<STRING>": [
                "'Windows'",
                "'Enabled plugin directory: `%s` -> `%s`'",
                "'Plugin already enabled: `%s` -> `%s`'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "02e93fba3aa5473c848389e199642ca8": {
        "code_string": "#blurred = cv2.GaussianBlur(clahe_image, (3, 3), 0)\n         #tight = cv2.Canny(blurred, 225, 250)\n         #wide = cv2.Canny(blurred, 10, 100)\n         # Detect face landmarks with dlib rectangle, dlib shape predictor\n         clahe_crop = clahe_image[y1:y2, x1:x2]\n         #LBP_img = LBP.main(clahe_crop)\n         shape = predictor(clahe_crop, detections)\n         return shape, clahe_image\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> clahe_crop = clahe_image [ y1 : y2 , x1 : x2 ] <NEWLINE> <COMMENT> <NL> shape = predictor ( clahe_crop , detections ) <NEWLINE> return shape , clahe_image <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#blurred = cv2.GaussianBlur(clahe_image, (3, 3), 0)",
                "#tight = cv2.Canny(blurred, 225, 250)",
                "#wide = cv2.Canny(blurred, 10, 100)",
                "# Detect face landmarks with dlib rectangle, dlib shape predictor",
                "#LBP_img = LBP.main(clahe_crop)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2c92888210b84591a6c6723f695b8ed4": {
        "code_string": "def get(self, path):\n     if (self.name == path): return [self]\n     if (not path.startswith(self.name+\".\")): return []\n     path = path[len(self.name)+1:]\n     result = []\n     for n_row,row_name,row_objects in zip(count(1),\n                                           self.row_names,\n                                           self.row_objects):\n       for alt_row_name in [row_name, str(n_row)]:\n         if (alt_row_name is None): continue\n         if (alt_row_name == path):\n           result.extend(row_objects)\n         elif (not path.startswith(alt_row_name+\".\")):\n           for row_object in row_objects:\n             result.extend(row_object.get(path=path[len(alt_row_name)+1:]))\n     return result\n",
        "code_toks_joined": "def get ( self , path ) : <NEWLINE> <INDENT> if ( self . name == path ) : return [ self ] <NEWLINE> if ( not path . startswith ( self . name + <STRING> ) ) : return [ ] <NEWLINE> path = path [ len ( self . name ) + 1 : ] <NEWLINE> result = [ ] <NEWLINE> for n_row , row_name , row_objects in zip ( count ( 1 ) , <NEWLINE> <INDENT> self . row_names , <NEWLINE> self . row_objects ) : <NEWLINE> for alt_row_name in [ row_name , str ( n_row ) ] : <NEWLINE> if ( alt_row_name is None ) : continue <NEWLINE> if ( alt_row_name == path ) : <NEWLINE> result . extend ( row_objects ) <NEWLINE> elif ( not path . startswith ( alt_row_name + <STRING> ) ) : <NEWLINE> for row_object in row_objects : <NEWLINE> result . extend ( row_object . get ( path = path [ len ( alt_row_name ) + 1 : ] ) ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\".\"",
                "\".\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d8bf7472b30f4dbb86d81133242cd3b0": {
        "code_string": "mon_lib_dna_rna_cif = [\"AD\", \"AR\", \"CD\", \"CR\", \"GD\", \"GR\", \"TD\", \"UR\"]\n if (\"set\" not in __builtins__):\n   mon_lib_dna_rna_cif = set(mon_lib_dna_rna_cif)\n",
        "code_toks_joined": "mon_lib_dna_rna_cif = [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] <NEWLINE> <INDENT> if ( <STRING> not in __builtins__ ) : <NEWLINE> <INDENT> mon_lib_dna_rna_cif = set ( mon_lib_dna_rna_cif ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"AD\"",
                "\"AR\"",
                "\"CD\"",
                "\"CR\"",
                "\"GD\"",
                "\"GR\"",
                "\"TD\"",
                "\"UR\"",
                "\"set\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b4d91bcc248a4bb986b76636847c5226": {
        "code_string": "def input(\n     file_name=None,\n     source_info=Please_pass_string_or_None,\n     lines=None,\n     pdb_id=None):\n   if (pdb_id is not None):\n     assert file_name is not None\n     file_name = ent_path_local_mirror(pdb_id=pdb_id)\n   if (file_name is not None):\n     return ext.input(\n       source_info=\"file \" + file_name,\n       lines=flex.split_lines(smart_open.for_reading(file_name).read()))\n   assert source_info is not Please_pass_string_or_None\n   if (isinstance(lines, str)):\n     lines = flex.split_lines(lines)\n   elif (isinstance(lines, (list, tuple))):\n     lines = flex.std_string(lines)\n   return ext.input(source_info=source_info, lines=lines)\n",
        "code_toks_joined": "def input ( <NEWLINE> <INDENT> file_name = None , <NEWLINE> source_info = Please_pass_string_or_None , <NEWLINE> lines = None , <NEWLINE> pdb_id = None ) : <NEWLINE> if ( pdb_id is not None ) : <NEWLINE> assert file_name is not None <NEWLINE> file_name = ent_path_local_mirror ( pdb_id = pdb_id ) <NEWLINE> if ( file_name is not None ) : <NEWLINE> return ext . input ( <NEWLINE> <INDENT> source_info = <STRING> + file_name , <NEWLINE> lines = flex . split_lines ( smart_open . for_reading ( file_name ) . read ( ) ) ) <NEWLINE> assert source_info is not Please_pass_string_or_None <NEWLINE> if ( isinstance ( lines , str ) ) : <NEWLINE> <DEDENT> lines = flex . split_lines ( lines ) <NEWLINE> elif ( isinstance ( lines , ( list , tuple ) ) ) : <NEWLINE> lines = flex . std_string ( lines ) <NEWLINE> return ext . input ( source_info = source_info , lines = lines ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"file \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e72959f4712547bf8e0c127107b56041": {
        "code_string": "return pair_count\n",
        "code_toks_joined": "return pair_count <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "612a9f9b40634394867d9d7767a2da93": {
        "code_string": "# If no detector distance is available set it to NaN, since\n     # Python's None is not permitted in HDF5\n     distance = cspad_tbx.env_distance(env, self.address, self._detz_offset)\n     if distance is None:\n       distance = float('nan')\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> distance = cspad_tbx . env_distance ( env , self . address , self . _detz_offset ) <NEWLINE> if distance is None : <NEWLINE> <INDENT> distance = float ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If no detector distance is available set it to NaN, since",
                "# Python's None is not permitted in HDF5"
            ],
            "<STRING>": [
                "'nan'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af9f9917ef4342f4acd7499ef41ed674": {
        "code_string": "# XXX This hardcodes the address for the front detector!\n     detz = cspad_tbx.env_detz(env, 'CxiDs1-0|Cspad-0')\n     if (detz is None):\n       self.m_no_detz += 1\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> detz = cspad_tbx . env_detz ( env , <STRING> ) <NEWLINE> if ( detz is None ) : <NEWLINE> <INDENT> self . m_no_detz += 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# XXX This hardcodes the address for the front detector!"
            ],
            "<STRING>": [
                "'CxiDs1-0|Cspad-0'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "60204ede4b9342359314c2680ba79339": {
        "code_string": "# Get the distance for the detectors that should have it, and set\n     # it to NaN for those that should not.\n     if self.detector == 'CxiDs1' or self.detector == 'CxiDsd':\n       distance = cspad_tbx.env_distance(env, self.address, self._detz_offset)\n       if distance is None:\n         self.nfail += 1\n         self.logger.warning(\"event(): no distance, shot skipped\")\n         evt.put(True, \"skip_event\")\n         return\n     else:\n       distance = float('nan')\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> if self . detector == <STRING> or self . detector == <STRING> : <NEWLINE> <INDENT> distance = cspad_tbx . env_distance ( env , self . address , self . _detz_offset ) <NEWLINE> if distance is None : <NEWLINE> <INDENT> self . nfail += 1 <NEWLINE> self . logger . warning ( <STRING> ) <NEWLINE> evt . put ( True , <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> distance = float ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Get the distance for the detectors that should have it, and set",
                "# it to NaN for those that should not."
            ],
            "<STRING>": [
                "'CxiDs1'",
                "'CxiDsd'",
                "\"event(): no distance, shot skipped\"",
                "\"skip_event\"",
                "'nan'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "af616f79e7aa4b1a8473de0d093b328e": {
        "code_string": "return wrapped\n",
        "code_toks_joined": "return wrapped <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aa9962fa0c44483581b842bffdde6e52": {
        "code_string": "for docname in env.found_docs:\n         abspath = env.doc2path(docname)\n         mtime = os.path.getmtime(abspath)\n         res.append((abspath,mtime))\n",
        "code_toks_joined": "for docname in env . found_docs : <NEWLINE> <INDENT> abspath = env . doc2path ( docname ) <NEWLINE> mtime = os . path . getmtime ( abspath ) <NEWLINE> res . append ( ( abspath , mtime ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2299026f7b5a422198a6c372b92f4347": {
        "code_string": "if isinstance(Group, self.receiver):\n             return self.receiver.self\n",
        "code_toks_joined": "if isinstance ( Group , self . receiver ) : <NEWLINE> <INDENT> return self . receiver . self <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8bc6d790e7b44b1cad48c2b56250f160": {
        "code_string": "card_name = friend_or_mp.nickname if isinstance(Chat, friend_or_mp) else friend_or_mp\n         logger.info('sending {} to {}: {}'.format(CARD, self, card_name))\n",
        "code_toks_joined": "card_name = friend_or_mp . nickname if isinstance ( Chat , friend_or_mp ) else friend_or_mp <NEWLINE> <INDENT> logger . info ( <STRING> . format ( CARD , self , card_name ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'sending {} to {}: {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "69865a983f2c4e328c88402ae34ccc1b": {
        "code_string": "if (\n             # Ignore Exception Objects.\n             type(value) == type\n             # Ignore Special Methods.\n             or is_dunder(attr)\n             # Ignore Functions.\n             or inspect.isfunction(attr)\n             # Ignore Django Model Attributes.\n             or attr in (\"objects\", \"id\", \"_meta\")\n             # Ignore Fields.\n             or attr in fields\n             # Ignore if is instance of Field.\n             or isinstance(value, Field)\n             # Ignore Properties.\n             or isinstance(value, property)\n             # Ignore Descriptors.\n             or isinstance(value, RELATED_DESCRIPTORS)\n         ):\n             return False\n         else:\n             return True\n",
        "code_toks_joined": "if ( <NEWLINE> <COMMENT> <NL> <INDENT> type ( value ) == type <NEWLINE> <COMMENT> <NL> or is_dunder ( attr ) <NEWLINE> <COMMENT> <NL> or inspect . isfunction ( attr ) <NEWLINE> <COMMENT> <NL> or attr in ( <STRING> , <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> or attr in fields <NEWLINE> <COMMENT> <NL> or isinstance ( value , Field ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , property ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , RELATED_DESCRIPTORS ) <NEWLINE> ) : <NEWLINE> return False <NEWLINE> else : <NEWLINE> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Ignore Exception Objects.",
                "# Ignore Special Methods.",
                "# Ignore Functions.",
                "# Ignore Django Model Attributes.",
                "# Ignore Fields.",
                "# Ignore if is instance of Field.",
                "# Ignore Properties.",
                "# Ignore Descriptors."
            ],
            "<STRING>": [
                "\"objects\"",
                "\"id\"",
                "\"_meta\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "12d9f9470eea4f7f94b0deb9baafd6a5": {
        "code_string": "def check_type_backward(self, in_types, out_types):\n         type_check.expect(out_types.size() == 1)\n         x_type, = out_types\n         y_type, = out_types\n",
        "code_toks_joined": "def check_type_backward ( self , in_types , out_types ) : <NEWLINE> <INDENT> type_check . expect ( out_types . size ( ) == 1 ) <NEWLINE> x_type , = out_types <NEWLINE> y_type , = out_types <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e787e3c4b02145dfa147360f5c32e3cb": {
        "code_string": "with cuda.get_device(g_dst):\n                 if (isinstance(g_src, cuda.ndarray) and\n                         g_dst.gpudata.device != g_src.gpudata.device):\n                     g_dst += cuda.copy(g_src, out_device=g_src.gpudata.device)\n                 else:\n                     g_dst += cuda.to_gpu(g_src)\n",
        "code_toks_joined": "with cuda . get_device ( g_dst ) : <NEWLINE> <INDENT> if ( isinstance ( g_src , cuda . ndarray ) and <NEWLINE> <INDENT> g_dst . gpudata . device != g_src . gpudata . device ) : <NEWLINE> g_dst += cuda . copy ( g_src , out_device = g_src . gpudata . device ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> g_dst += cuda . to_gpu ( g_src ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e1bcdee438c74b60a3780d6ac6544e97": {
        "code_string": "def __init__(self, margin):\n         if margin < 0:\n             raise Exception(\"margin should be positive value.\")\n         self.margin = margin\n",
        "code_toks_joined": "def __init__ ( self , margin ) : <NEWLINE> <INDENT> if margin < 0 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> self . margin = margin <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"margin should be positive value.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4a51042602f04ad398d793e05c3f3b5e": {
        "code_string": "skip = (slice(None),) * axis\n     ret = []\n     i = 0\n     for index in indices:\n         ret.append(ary[skip + (slice(i, index),)])\n         i = index\n     ret.append(ary[skip + (slice(index, size),)])\n",
        "code_toks_joined": "skip = ( slice ( None ) , ) * axis <NEWLINE> <INDENT> ret = [ ] <NEWLINE> i = 0 <NEWLINE> for index in indices : <NEWLINE> <INDENT> ret . append ( ary [ skip + ( slice ( i , index ) , ) ] ) <NEWLINE> i = index <NEWLINE> <DEDENT> ret . append ( ary [ skip + ( slice ( index , size ) , ) ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "23cebfb26d934e59925055165a3f1a0b": {
        "code_string": "def _get_property(self, key, default=False):\n         attr = getattr(self, '_%s' % key)\n         if attr:\n             return attr\n         if default is not False and not self.app_key:\n             return attr\n         app = self.oauth.app or current_app\n         config = app.config[self.app_key]\n         if default is not False:\n             return config.get(key, default)\n         return config[key]\n",
        "code_toks_joined": "def _get_property ( self , key , default = False ) : <NEWLINE> <INDENT> attr = getattr ( self , <STRING> % key ) <NEWLINE> if attr : <NEWLINE> <INDENT> return attr <NEWLINE> <DEDENT> if default is not False and not self . app_key : <NEWLINE> <INDENT> return attr <NEWLINE> <DEDENT> app = self . oauth . app or current_app <NEWLINE> config = app . config [ self . app_key ] <NEWLINE> if default is not False : <NEWLINE> <INDENT> return config . get ( key , default ) <NEWLINE> <DEDENT> return config [ key ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'_%s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "887c7ac818e44ad0b16965fdedc0f18c": {
        "code_string": "def parser_dnb(data):\n     \"\"\"Parse the response from the DNB service. The input data is the result webpage in html from the search.\"\"\"\n     data = re.split('<tr>', data)  # split rows in table into lines for loop\n     recs = {}\n     recs['Authors'] = []\n     try:\n         for line in data:\n             line = line.replace('\\n', ' ').replace('\\t', '')\n             if len(recs) == 4:  # skip the rest of the file if we have all recs\n                 break\n             # Author:\n             #<td width=\"25%\" ><strong>Person(en)</strong></td>\n             #<td >Bayerl, Linda (Verfasser)<br/>Dengl, Sabine (Illustrator)</td></tr>\n             # Sometimes they also contain an href on the name and sometimes they start with <td class='yellow'>\n             elif re.search(r\"<strong>Person.+</strong>\", line):\n                 authors = re.findall('</td>(.*)</td', line)[0]\n                 authors = authors.replace('<td >', '')\n                 authors = re.split('<br/>', authors)  # several authors?\n                 for auth in authors:\n                     if 'href' in auth:  # name contains link\n                         auth = re.findall(r'<a href=\".*\" >(.*)</a>', auth)[0]\n                     # Remove job description in brackets after the name:\n                     auth = u(re.sub(r'\\(.*?\\)', '', auth))\n                     recs['Authors'].append(auth)\n             # Publisher:\n             #<strong>Verlag</strong></td><td >Hamburg : Carlsen</td>\n             #</tr><tr><td width=\"25%\" class='yellow'><strong>...\n             # They always include the city first, perhaps we should remove it?\n             elif re.search(r\"<strong>Verlag</strong>\", line):\n                 publisher = re.findall('td .*>(.*)</td', line)[0]\n                 recs['Publisher'] = u(publisher)\n             # Title:\n             #<td width=\"25%\" class='yellow'><strong>Titel</strong>\n             #</td><td class='yellow'>Kindergartenblock - Verbinden, vergleichen, Fehler finden ab 4 Jahre / Linda Bayerl</td></tr>\n             elif re.search(r\"<strong>Titel</strong\", line):\n                 title = re.findall('td .*>(.*)/.*</td', line)[0]\n                 publisher = u(title.replace('td >', '').replace('</td', ''))\n                 recs['Title'] = u(title)\n             # Publication year:\n             #<td width=\"25%\" class='yellow'><strong>Zeitliche Einordnung</strong>\n             #</td><td class='yellow'>Erscheinungsdatum: 2015</td></tr>\n             elif re.search(r\"<strong>Zeitliche Einordnung</strong\", line):\n                 recs['Year'] = u(re.findall(r'\\d{4}', line)[0])\n             elif line == '':\n                 continue\n",
        "code_toks_joined": "def parser_dnb ( data ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> data = re . split ( <STRING> , data ) <COMMENT> <NEWLINE> recs = { } <NEWLINE> recs [ <STRING> ] = [ ] <NEWLINE> try : <NEWLINE> <INDENT> for line in data : <NEWLINE> <INDENT> line = line . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) <NEWLINE> if len ( recs ) == 4 : <COMMENT> <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> authors = re . findall ( <STRING> , line ) [ 0 ] <NEWLINE> authors = authors . replace ( <STRING> , <STRING> ) <NEWLINE> authors = re . split ( <STRING> , authors ) <COMMENT> <NEWLINE> for auth in authors : <NEWLINE> <INDENT> if <STRING> in auth : <COMMENT> <NEWLINE> <INDENT> auth = re . findall ( <STRING> , auth ) [ 0 ] <NEWLINE> <COMMENT> <NL> <DEDENT> auth = u ( re . sub ( <STRING> , <STRING> , auth ) ) <NEWLINE> recs [ <STRING> ] . append ( auth ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> publisher = re . findall ( <STRING> , line ) [ 0 ] <NEWLINE> recs [ <STRING> ] = u ( publisher ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> title = re . findall ( <STRING> , line ) [ 0 ] <NEWLINE> publisher = u ( title . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) ) <NEWLINE> recs [ <STRING> ] = u ( title ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> recs [ <STRING> ] = u ( re . findall ( <STRING> , line ) [ 0 ] ) <NEWLINE> <DEDENT> elif line == <STRING> : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Parse the response from the DNB service. The input data is the result webpage in html from the search.\"\"\"",
                "'<tr>'",
                "'Authors'",
                "'\\n'",
                "' '",
                "'\\t'",
                "''",
                "r\"<strong>Person.+</strong>\"",
                "'</td>(.*)</td'",
                "'<td >'",
                "''",
                "'<br/>'",
                "'href'",
                "r'<a href=\".*\" >(.*)</a>'",
                "r'\\(.*?\\)'",
                "''",
                "'Authors'",
                "r\"<strong>Verlag</strong>\"",
                "'td .*>(.*)</td'",
                "'Publisher'",
                "r\"<strong>Titel</strong\"",
                "'td .*>(.*)/.*</td'",
                "'td >'",
                "''",
                "'</td'",
                "''",
                "'Title'",
                "r\"<strong>Zeitliche Einordnung</strong\"",
                "'Year'",
                "r'\\d{4}'",
                "''"
            ],
            "<COMMENT>": [
                "# split rows in table into lines for loop",
                "# skip the rest of the file if we have all recs",
                "# Author:",
                "#<td width=\"25%\" ><strong>Person(en)</strong></td>",
                "#<td >Bayerl, Linda (Verfasser)<br/>Dengl, Sabine (Illustrator)</td></tr>",
                "# Sometimes they also contain an href on the name and sometimes they start with <td class='yellow'>",
                "# several authors?",
                "# name contains link",
                "# Remove job description in brackets after the name:",
                "# Publisher:",
                "#<strong>Verlag</strong></td><td >Hamburg : Carlsen</td>",
                "#</tr><tr><td width=\"25%\" class='yellow'><strong>...",
                "# They always include the city first, perhaps we should remove it?",
                "# Title:",
                "#<td width=\"25%\" class='yellow'><strong>Titel</strong>",
                "#</td><td class='yellow'>Kindergartenblock - Verbinden, vergleichen, Fehler finden ab 4 Jahre / Linda Bayerl</td></tr>",
                "# Publication year:",
                "#<td width=\"25%\" class='yellow'><strong>Zeitliche Einordnung</strong>",
                "#</td><td class='yellow'>Erscheinungsdatum: 2015</td></tr>"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cc2ecd570dbd476bb03f296f2d5d6a30": {
        "code_string": "while not self.stopping:\n             actor_id, message = self.acomm.recv()\n             if actor_id not in self.local_actors:\n                 raise RuntimeError(\"Message received for non-local actor: %r\" % message)\n",
        "code_toks_joined": "while not self . stopping : <NEWLINE> <INDENT> actor_id , message = self . acomm . recv ( ) <NEWLINE> if actor_id not in self . local_actors : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> % message ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Message received for non-local actor: %r\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "97d71cdf11284c489e4b85cf9ea726fd": {
        "code_string": "if device is not None:\n         result = service.SyncApplySettingToIPNetworkConnection(SettingData=setting, Mode=mode)\n     else:\n         result = service.SyncApplySettingToIPNetworkConnection(SettingData=setting, IPNetworkConnection=device, Mode=mode)\n     if result.errorstr:\n         raise LmiFailed(\"Unable to change setting autoconnect: %s\" % result.errorstr)\n     return result.rval\n",
        "code_toks_joined": "if device is not None : <NEWLINE> <INDENT> result = service . SyncApplySettingToIPNetworkConnection ( SettingData = setting , Mode = mode ) <NEWLINE> else : <NEWLINE> result = service . SyncApplySettingToIPNetworkConnection ( SettingData = setting , IPNetworkConnection = device , Mode = mode ) <NEWLINE> if result . errorstr : <NEWLINE> raise LmiFailed ( <STRING> % result . errorstr ) <NEWLINE> return result . rval <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Unable to change setting autoconnect: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0b57d7fa0145457899ca3091782772f3": {
        "code_string": "@fixture\n     def controller(self, app, request, root_tree, data, matchdict):\n         request.registry['controller_plugins'] = app.controller_plugins\n         controller = self._get_controller_class()(root_tree, request)\n         controller.data = data\n         controller.matchdict = matchdict\n         return controller\n",
        "code_toks_joined": "@ fixture <NEWLINE> <INDENT> def controller ( self , app , request , root_tree , data , matchdict ) : <NEWLINE> <INDENT> request . registry [ <STRING> ] = app . controller_plugins <NEWLINE> controller = self . _get_controller_class ( ) ( root_tree , request ) <NEWLINE> controller . data = data <NEWLINE> controller . matchdict = matchdict <NEWLINE> return controller <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'controller_plugins'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "16cb8101d35a4ac4a40a38e7e8058376": {
        "code_string": "if _are_there_selectors(shape_map_file, shape_map_raw):\n         sgraph = _get_adequate_sgraph(endpoint_url=url_endpoint,\n                                       raw_graph=raw_graph,\n                                       graph_file_input=graph_file_input,\n                                       url_input=url_input,\n                                       graph_format=input_format,\n                                       built_remote_graph=built_remote_graph)\n         valid_shape_map = built_shape_map\n         if built_shape_map is None:\n             shape_map_parser = get_shape_map_parser(format=shape_map_format,\n                                                     sgraph=sgraph,\n                                                     namespaces_prefix_dict=namespaces_dict)\n             valid_shape_map = shape_map_parser.parse_shape_map(source_file=shape_map_file,\n                                                                raw_content=shape_map_raw)\n         selectors_tracker = ShapeMapInstanceTracker(shape_map=valid_shape_map)\n     if _are_there_some_target_classes(target_classes, file_target_classes, all_classes_mode, shape_qualifiers_mode):\n         model_classes = None\n         if all_classes_mode or target_classes is not None:\n             list_of_str_target_classes = tune_target_classes_if_needed(\n                 target_classes) if target_classes is not None else read_target_classes_from_file(file_target_classes)\n             model_classes = get_list_of_model_classes(list_of_str_target_classes)\n",
        "code_toks_joined": "if _are_there_selectors ( shape_map_file , shape_map_raw ) : <NEWLINE> <INDENT> sgraph = _get_adequate_sgraph ( endpoint_url = url_endpoint , <NEWLINE> <INDENT> raw_graph = raw_graph , <NEWLINE> graph_file_input = graph_file_input , <NEWLINE> url_input = url_input , <NEWLINE> graph_format = input_format , <NEWLINE> built_remote_graph = built_remote_graph ) <NEWLINE> <DEDENT> valid_shape_map = built_shape_map <NEWLINE> if built_shape_map is None : <NEWLINE> <INDENT> shape_map_parser = get_shape_map_parser ( format = shape_map_format , <NEWLINE> <INDENT> sgraph = sgraph , <NEWLINE> namespaces_prefix_dict = namespaces_dict ) <NEWLINE> <DEDENT> valid_shape_map = shape_map_parser . parse_shape_map ( source_file = shape_map_file , <NEWLINE> <INDENT> raw_content = shape_map_raw ) <NEWLINE> <DEDENT> <DEDENT> selectors_tracker = ShapeMapInstanceTracker ( shape_map = valid_shape_map ) <NEWLINE> if _are_there_some_target_classes ( target_classes , file_target_classes , all_classes_mode , shape_qualifiers_mode ) : <NEWLINE> model_classes = None <NEWLINE> if all_classes_mode or target_classes is not None : <NEWLINE> <INDENT> list_of_str_target_classes = tune_target_classes_if_needed ( <NEWLINE> <INDENT> target_classes ) if target_classes is not None else read_target_classes_from_file ( file_target_classes ) <NEWLINE> <DEDENT> model_classes = get_list_of_model_classes ( list_of_str_target_classes ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "34dc30c57b7442fb9065acdddefbfb1b": {
        "code_string": "if categorical_features is not None:\n         if categorical_features is 'all':\n             categorical_features = np.arange(0, X.shape[1])\n         for feature_no in categorical_features:\n             if not np.array_equal(X[:, feature_no], X[:, feature_no].astype(int)):\n                 warnings.warn(f\"Feature no. {feature_no} is continuous data. \" +\n                               \"Casting data to integer.\")\n             if max_categories is not None:\n                 uniques = np.unique(X[:, feature_no]).astype(int)\n                 if not np.array_equal(uniques, np.arange(0, np.max(uniques)+1)):\n                     raise ValueError(f\"Expected feature no. {feature_no} to have \" +\n                                      f\"{np.arange(0,np.max(uniques)+1)} \" +\n                                      f\"unique values, but got {uniques} instead. \" +\n                                      \"Encode your data using sklearn's LabelEncoder.\")\n",
        "code_toks_joined": "if categorical_features is not None : <NEWLINE> <INDENT> if categorical_features is <STRING> : <NEWLINE> <INDENT> categorical_features = np . arange ( 0 , X . shape [ 1 ] ) <NEWLINE> <DEDENT> for feature_no in categorical_features : <NEWLINE> <INDENT> if not np . array_equal ( X [ : , feature_no ] , X [ : , feature_no ] . astype ( int ) ) : <NEWLINE> <INDENT> warnings . warn ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if max_categories is not None : <NEWLINE> <INDENT> uniques = np . unique ( X [ : , feature_no ] ) . astype ( int ) <NEWLINE> if not np . array_equal ( uniques , np . arange ( 0 , np . max ( uniques ) + 1 ) ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> + <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'all'",
                "f\"Feature no. {feature_no} is continuous data. \"",
                "\"Casting data to integer.\"",
                "f\"Expected feature no. {feature_no} to have \"",
                "f\"{np.arange(0,np.max(uniques)+1)} \"",
                "f\"unique values, but got {uniques} instead. \"",
                "\"Encode your data using sklearn's LabelEncoder.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a094fb2c066f46cf94568dc6c7e9b99a": {
        "code_string": "# Correct the inputs\n         if self.priors is None:\n             self.priors = np.bincount(y)/num_samples\n         else:\n             self.priors = np.asarray(self.priors)\n             if len(self.priors) != num_classes:\n                 raise ValueError(\n                     'Number of priors must match number of classes.')\n             if np.isclose(self.priors.sum(), 1.0):\n                 raise ValueError(\"The sum of priors should be 1.\")\n             if (self.priors < 0).any():\n                 raise ValueError('Priors must be non-negative.')\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if self . priors is None : <NEWLINE> <INDENT> self . priors = np . bincount ( y ) / num_samples <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . priors = np . asarray ( self . priors ) <NEWLINE> if len ( self . priors ) != num_classes : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if np . isclose ( self . priors . sum ( ) , 1.0 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if ( self . priors < 0 ) . any ( ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Correct the inputs"
            ],
            "<STRING>": [
                "'Number of priors must match number of classes.'",
                "\"The sum of priors should be 1.\"",
                "'Priors must be non-negative.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3f5f2053f81a404c9d5ac244d23a86cd": {
        "code_string": "def match(self, url, netloc, domain, origin=None):\n         if self.options and not self.options.can_apply_rule(netloc, origin):\n             return False\n         return True\n",
        "code_toks_joined": "def match ( self , url , netloc , domain , origin = None ) : <NEWLINE> <INDENT> if self . options and not self . options . can_apply_rule ( netloc , origin ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4037ef623a734c30a17823fcbd05d1ba": {
        "code_string": "if cut_by == 'area':\n         filename = OES_FILENAMES.get(area_focus)\n         if filename == None:\n             raise ValueError('\"{}\" is not a valid area focus\\n' \\\n                 'valid options include:\\n{}' \\\n                 .format(cut_by, ['metros', 'metros-divisions', 'non-metros']))\n     else:\n         filename = OES_FILENAMES.get(cut_by)\n",
        "code_toks_joined": "if cut_by == <STRING> : <NEWLINE> <INDENT> filename = OES_FILENAMES . get ( area_focus ) <NEWLINE> if filename == None : <NEWLINE> <INDENT> raise ValueError ( <STRING> <STRING> . format ( cut_by , [ <STRING> , <STRING> , <STRING> ] ) ) <NEWLINE> else : <NEWLINE> <DEDENT> filename = OES_FILENAMES . get ( cut_by ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'area'",
                "'\"{}\" is not a valid area focus\\n'",
                "'valid options include:\\n{}'",
                "'metros'",
                "'metros-divisions'",
                "'non-metros'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "524f08eb722b4649a71403e7a30973da": {
        "code_string": "def _on_pushToTunTaskDone(task):\n     # suppress annoying \"CancelledError exception not retrieved\" error on Py3.5+\n     try:\n         if not isinstance(task.exception(), CancelledError):\n             logging.error(\"pushToTunTask exception: %s\" % type(task.exception()))\n     except CancelledError:  # doc says it will raise this if canceled, but...\n         pass\n",
        "code_toks_joined": "def _on_pushToTunTaskDone ( task ) : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> if not isinstance ( task . exception ( ) , CancelledError ) : <NEWLINE> <INDENT> logging . error ( <STRING> % type ( task . exception ( ) ) ) <NEWLINE> <DEDENT> <DEDENT> except CancelledError : <COMMENT> <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# suppress annoying \"CancelledError exception not retrieved\" error on Py3.5+",
                "# doc says it will raise this if canceled, but..."
            ],
            "<STRING>": [
                "\"pushToTunTask exception: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "465899a311d547f096056efd5961f204": {
        "code_string": "# generate affinity matrices for stacked train/test data sets\n     affinities = []\n     for (tr, te) in zip(train, test):\n         if len(tr.T) == len(te.T):\n             raise ValueError('Train and test data must have same number of '\n                              'features for each data type. Make sure to '\n                              'supply data types in the same order.')\n         affinities += [make_affinity(np.row_stack([tr, te]), K=K, mu=mu)]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> affinities = [ ] <NEWLINE> for ( tr , te ) in zip ( train , test ) : <NEWLINE> <INDENT> if len ( tr . T ) == len ( te . T ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> affinities += [ make_affinity ( np . row_stack ( [ tr , te ] ) , K = K , mu = mu ) ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# generate affinity matrices for stacked train/test data sets"
            ],
            "<STRING>": [
                "'Train and test data must have same number of '",
                "'features for each data type. Make sure to '",
                "'supply data types in the same order.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3852ced6d26f4dae89cf286a11598bcc": {
        "code_string": "# First, log to screen.\n         log_data = info.copy()\n         length = np.array(log_data.get('length', 0))\n         reward = np.array(log_data.get('reward', 0.0))\n         completed = np.array(log_data.get('completed', False))\n         reward_possible = game.initial_available_points()\n         required_points = game.required_points()\n         if reward.shape:\n             # Multi-agent. Record names.\n             log_data['agents'] = game.agent_names.tolist()\n         else:\n             # convert to scalars\n             reward_possible = np.sum(reward_possible[:1])\n             required_points = np.sum(required_points[:1])\n         log_data['level_name'] = game.title\n         log_data['length'] = length.tolist()\n         log_data['reward'] = reward.tolist()\n         log_data['completed'] = reward.tolist()\n         log_data['reward_possible'] = reward_possible.tolist()\n         log_data['reward_needed'] = required_points.tolist()\n         log_data['time'] = datetime.utcnow().isoformat()\n         logger.info(self.episode_msg.format(**log_data, **self.cumulative_stats))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> log_data = info . copy ( ) <NEWLINE> length = np . array ( log_data . get ( <STRING> , 0 ) ) <NEWLINE> reward = np . array ( log_data . get ( <STRING> , 0.0 ) ) <NEWLINE> completed = np . array ( log_data . get ( <STRING> , False ) ) <NEWLINE> reward_possible = game . initial_available_points ( ) <NEWLINE> required_points = game . required_points ( ) <NEWLINE> if reward . shape : <NEWLINE> <COMMENT> <NL> <INDENT> log_data [ <STRING> ] = game . agent_names . tolist ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> reward_possible = np . sum ( reward_possible [ : 1 ] ) <NEWLINE> required_points = np . sum ( required_points [ : 1 ] ) <NEWLINE> <DEDENT> log_data [ <STRING> ] = game . title <NEWLINE> log_data [ <STRING> ] = length . tolist ( ) <NEWLINE> log_data [ <STRING> ] = reward . tolist ( ) <NEWLINE> log_data [ <STRING> ] = reward . tolist ( ) <NEWLINE> log_data [ <STRING> ] = reward_possible . tolist ( ) <NEWLINE> log_data [ <STRING> ] = required_points . tolist ( ) <NEWLINE> log_data [ <STRING> ] = datetime . utcnow ( ) . isoformat ( ) <NEWLINE> logger . info ( self . episode_msg . format ( ** log_data , ** self . cumulative_stats ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# First, log to screen.",
                "# Multi-agent. Record names.",
                "# convert to scalars"
            ],
            "<STRING>": [
                "'length'",
                "'reward'",
                "'completed'",
                "'agents'",
                "'level_name'",
                "'length'",
                "'reward'",
                "'completed'",
                "'reward_possible'",
                "'reward_needed'",
                "'time'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e4040ee4c1ca464db759c03ec498578e": {
        "code_string": "if wandb_run is not None and bare_name == 'benchmark-data':\n         wandb_run.summary['success'] = np.average(success)\n         wandb_run.summary['avg_length'] = np.average(length)\n         wandb_run.summary['side_effects'] = np.average(side_effects)\n         wandb_run.summary['reward'] = np.average(reward_frac)\n         wandb_run.summary['score'] = np.average(score)\n",
        "code_toks_joined": "if wandb_run is not None and bare_name == <STRING> : <NEWLINE> <INDENT> wandb_run . summary [ <STRING> ] = np . average ( success ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( length ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( side_effects ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( reward_frac ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( score ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'benchmark-data'",
                "'success'",
                "'avg_length'",
                "'side_effects'",
                "'reward'",
                "'score'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dbe3dcf2e08f46dc8dcccef60f25bd5d": {
        "code_string": "local_config = sdk_config.load_config()\n     add_kubos_command = functools.partial(kubos_options.command.add_command, local_config, subparser, 'kubos') #add our own implemented commands\n     add_yotta_command = functools.partial(kubos_options.command.add_command, local_config, subparser, 'yotta') #add from the default yotta commands\n     add_kubos_command('init', 'init', 'Create a new module.')\n     add_yotta_command('build', 'build',\n         'Build the current module. Options can be passed to the underlying '+\n         'build tool by passing them after --, e.g. to do a verbose build '+\n         'which will display each command as it is run, use:\\n'+\n         '  yotta build -- -v\\n\\n'+\n         'The programs or libraries to build can be specified (by default '+\n         'only the libraries needed by the current module and the current '+\n         \"module's own tests are built). For example, to build the tests of \"+\n         'all dependencies, run:\\n  yotta build all_tests\\n\\n',\n         'Build the current module.'\n     )\n     add_yotta_command('link', 'link',\n         'Symlink a module to be used into another module.\\n\\n'+\n         'Use: \"yotta link\" in a module to link it globally, then use \"yotta '+\n         'link <modulename>\" to link it into the module where you want to use '+\n         'it.\\n\\n'+\n         '\"yotta link ../path/to/module\" is also supported, which will create '+\n         'the global link and a link into the current module in a single step.',\n         'Symlink a module'\n     )\n     add_yotta_command('link-target', 'link_target',\n         'Symlink a target to be used into another module.\\n\\n'+\n         'Use: \"yotta link\" in a target to link it globally, then use \"yotta '+\n         'link-target <targetname>\" to link it into the module where you want to use '+\n         'it.\\n\\n'+\n         '\"yotta link ../path/to/target\" is also supported, which will create '+\n         'the global link and a link into the current module in a single step.',\n         'Symlink a target'\n     )\n     add_kubos_command('update', 'update', 'Download newer versions of the KubOS Modules')\n     add_kubos_command('target', 'target', 'Set or display the target device.')\n     add_yotta_command('debug', 'debug', 'Attach a debugger to the current target.  Requires target support.')\n     add_yotta_command('test', 'test_subcommand',\n         'Run the tests for the current module on the current target. A build '+\n         'will be run first, and options to the build subcommand are also '+\n         'accepted by test.\\nThis subcommand requires the target to provide a '+\n         '\"test\" script that will be used to run each test. Modules may also '+\n         'define a \"testReporter\" script, which will be piped the output from '+\n         'each test, and may produce a summary.',\n         'Run the tests for the current module on the current target. Requires target support for cross-compiling targets.'\n     )\n     add_yotta_command('start', 'start',\n         'Launch the compiled program (available for executable modules only). Requires target support for cross-compiling targets.'\n     )\n     add_yotta_command('list', 'list', 'List the dependencies of the current module, or the inherited targets of the current target.')\n     add_yotta_command('outdated', 'outdated', 'Display information about dependencies which have newer versions available.')\n     add_yotta_command('remove', 'remove',\n         'Remove the downloaded version of a dependency module or target, or '+\n         'un-link a linked module or target (see yotta link --help for details '+\n         'of linking). This command does not modify your module.json file.',\n         'Remove or unlink a dependency without removing it from module.json.'\n     )\n     add_yotta_command('licenses', 'licenses', 'List the licenses of the current module and its dependencies.')\n     add_yotta_command('clean', 'clean', 'Remove files created by yotta and the build.')\n     add_yotta_command('config', 'config', 'Display the target configuration info.')\n     add_yotta_command('shrinkwrap', 'shrinkwrap', 'Create a yotta-shrinkwrap.json file to freeze dependency versions.')\n     add_kubos_command('version', 'version', 'Display the current active version of the cli and KubOS source repo.')\n     add_kubos_command('use', 'use', 'Set a new version of the KubOS modules to build your projects against.')\n     add_kubos_command('versions', 'versions', 'Display the available versions of the KubOS source.')\n",
        "code_toks_joined": "local_config = sdk_config . load_config ( ) <NEWLINE> <INDENT> add_kubos_command = functools . partial ( kubos_options . command . add_command , local_config , subparser , <STRING> ) <COMMENT> <NEWLINE> add_yotta_command = functools . partial ( kubos_options . command . add_command , local_config , subparser , <STRING> ) <COMMENT> <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'kubos'",
                "'yotta'",
                "'init'",
                "'init'",
                "'Create a new module.'",
                "'build'",
                "'build'",
                "'Build the current module. Options can be passed to the underlying '",
                "'build tool by passing them after --, e.g. to do a verbose build '",
                "'which will display each command as it is run, use:\\n'",
                "'  yotta build -- -v\\n\\n'",
                "'The programs or libraries to build can be specified (by default '",
                "'only the libraries needed by the current module and the current '",
                "\"module's own tests are built). For example, to build the tests of \"",
                "'all dependencies, run:\\n  yotta build all_tests\\n\\n'",
                "'Build the current module.'",
                "'link'",
                "'link'",
                "'Symlink a module to be used into another module.\\n\\n'",
                "'Use: \"yotta link\" in a module to link it globally, then use \"yotta '",
                "'link <modulename>\" to link it into the module where you want to use '",
                "'it.\\n\\n'",
                "'\"yotta link ../path/to/module\" is also supported, which will create '",
                "'the global link and a link into the current module in a single step.'",
                "'Symlink a module'",
                "'link-target'",
                "'link_target'",
                "'Symlink a target to be used into another module.\\n\\n'",
                "'Use: \"yotta link\" in a target to link it globally, then use \"yotta '",
                "'link-target <targetname>\" to link it into the module where you want to use '",
                "'it.\\n\\n'",
                "'\"yotta link ../path/to/target\" is also supported, which will create '",
                "'the global link and a link into the current module in a single step.'",
                "'Symlink a target'",
                "'update'",
                "'update'",
                "'Download newer versions of the KubOS Modules'",
                "'target'",
                "'target'",
                "'Set or display the target device.'",
                "'debug'",
                "'debug'",
                "'Attach a debugger to the current target.  Requires target support.'",
                "'test'",
                "'test_subcommand'",
                "'Run the tests for the current module on the current target. A build '",
                "'will be run first, and options to the build subcommand are also '",
                "'accepted by test.\\nThis subcommand requires the target to provide a '",
                "'\"test\" script that will be used to run each test. Modules may also '",
                "'define a \"testReporter\" script, which will be piped the output from '",
                "'each test, and may produce a summary.'",
                "'Run the tests for the current module on the current target. Requires target support for cross-compiling targets.'",
                "'start'",
                "'start'",
                "'Launch the compiled program (available for executable modules only). Requires target support for cross-compiling targets.'",
                "'list'",
                "'list'",
                "'List the dependencies of the current module, or the inherited targets of the current target.'",
                "'outdated'",
                "'outdated'",
                "'Display information about dependencies which have newer versions available.'",
                "'remove'",
                "'remove'",
                "'Remove the downloaded version of a dependency module or target, or '",
                "'un-link a linked module or target (see yotta link --help for details '",
                "'of linking). This command does not modify your module.json file.'",
                "'Remove or unlink a dependency without removing it from module.json.'",
                "'licenses'",
                "'licenses'",
                "'List the licenses of the current module and its dependencies.'",
                "'clean'",
                "'clean'",
                "'Remove files created by yotta and the build.'",
                "'config'",
                "'config'",
                "'Display the target configuration info.'",
                "'shrinkwrap'",
                "'shrinkwrap'",
                "'Create a yotta-shrinkwrap.json file to freeze dependency versions.'",
                "'version'",
                "'version'",
                "'Display the current active version of the cli and KubOS source repo.'",
                "'use'",
                "'use'",
                "'Set a new version of the KubOS modules to build your projects against.'",
                "'versions'",
                "'versions'",
                "'Display the available versions of the KubOS source.'"
            ],
            "<COMMENT>": [
                "#add our own implemented commands",
                "#add from the default yotta commands"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e0a4a26de2bb4c5a8f4081119f6b01e3": {
        "code_string": "class vector(data.Data):\n     def __init__(self, asp, adr, cx):\n         super().__init__(asp, adr, adr + 4)\n         self.ws = asp.bu16(adr)\n         self.dstadr = asp.bu16(adr + 2)\n         cx.disass(asp, self.dstadr)\n",
        "code_toks_joined": "class vector ( data . Data ) : <NEWLINE> <INDENT> def __init__ ( self , asp , adr , cx ) : <NEWLINE> <INDENT> super ( ) . __init__ ( asp , adr , adr + 4 ) <NEWLINE> self . ws = asp . bu16 ( adr ) <NEWLINE> self . dstadr = asp . bu16 ( adr + 2 ) <NEWLINE> cx . disass ( asp , self . dstadr ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "15c47fa5cc234c55854b1b461f781e2d": {
        "code_string": "# Calculate performance\n         end_amt_no_trades = (float(self.exchange.start_usd)/float(end_price)) + float(self.exchange.start_btc)\n         end_amt = (float(self.exchange.usd_bal)/float(end_price)) + float(self.exchange.btc_bal)\n         start_amt = (float(self.exchange.start_usd)/float(start_price)) + float(self.exchange.start_btc)\n         strategy_performance = ((end_amt-start_amt)/start_amt)*100\n         print(\"\\n\")\n         print(\"Times recalculated: \"+str(self.times_recalculated))\n         print(\"Times bought: \"+str(self.exchange.times_bought))\n         print(\"Times sold: \"+str(self.exchange.times_sold))\n         print(\"The Market's performance: \"+str(market_performance)+\" %\")\n         print(\"Strategy's performance: \"+str(strategy_performance)+\" %\")\n         print(\"Account's ending value if no trades were made: \"+str(start_amt)+\" BTC\")\n         print(\"Account's ending value with this strategy: \"+str(end_amt)+\" BTC\")\n         strategy_performance_vs_market = strategy_performance - market_performance\n         if strategy_performance > market_performance:\n             print(\"Congratulations! This strategy has beat the market by: \"+str(strategy_performance_vs_market)+\" %\")\n         elif strategy_performance < market_performance:\n             print(\"This strategy has preformed: \"+str(strategy_performance_vs_market)+\" % worse than market.\")\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> end_amt_no_trades = ( float ( self . exchange . start_usd ) / float ( end_price ) ) + float ( self . exchange . start_btc ) <NEWLINE> end_amt = ( float ( self . exchange . usd_bal ) / float ( end_price ) ) + float ( self . exchange . btc_bal ) <NEWLINE> start_amt = ( float ( self . exchange . start_usd ) / float ( start_price ) ) + float ( self . exchange . start_btc ) <NEWLINE> strategy_performance = ( ( end_amt - start_amt ) / start_amt ) * 100 <NEWLINE> print ( <STRING> ) <NEWLINE> print ( <STRING> + str ( self . times_recalculated ) ) <NEWLINE> print ( <STRING> + str ( self . exchange . times_bought ) ) <NEWLINE> print ( <STRING> + str ( self . exchange . times_sold ) ) <NEWLINE> print ( <STRING> + str ( market_performance ) + <STRING> ) <NEWLINE> print ( <STRING> + str ( strategy_performance ) + <STRING> ) <NEWLINE> print ( <STRING> + str ( start_amt ) + <STRING> ) <NEWLINE> print ( <STRING> + str ( end_amt ) + <STRING> ) <NEWLINE> strategy_performance_vs_market = strategy_performance - market_performance <NEWLINE> if strategy_performance > market_performance : <NEWLINE> <INDENT> print ( <STRING> + str ( strategy_performance_vs_market ) + <STRING> ) <NEWLINE> <DEDENT> elif strategy_performance < market_performance : <NEWLINE> <INDENT> print ( <STRING> + str ( strategy_performance_vs_market ) + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Calculate performance"
            ],
            "<STRING>": [
                "\"\\n\"",
                "\"Times recalculated: \"",
                "\"Times bought: \"",
                "\"Times sold: \"",
                "\"The Market's performance: \"",
                "\" %\"",
                "\"Strategy's performance: \"",
                "\" %\"",
                "\"Account's ending value if no trades were made: \"",
                "\" BTC\"",
                "\"Account's ending value with this strategy: \"",
                "\" BTC\"",
                "\"Congratulations! This strategy has beat the market by: \"",
                "\" %\"",
                "\"This strategy has preformed: \"",
                "\" % worse than market.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d56e1ec8eb78455e9378466be85ec071": {
        "code_string": "for key in params_copy:\n",
        "code_toks_joined": "for key in params_copy : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "605000655f6945d19d949ea9f4ad9cb9": {
        "code_string": "# Store results\n         for i in range(0, len(results[\"best_score\"])):\n             iteration = {\n                 \"task\": self.task_id,\n                 \"method\": \"{} (EI: {}, RT: {})\".format(method.name, time_regressor, score_regressor),\n                 \"iteration\": i,\n                 \"score\": results[\"mean_test_score\"][i],\n                 \"best_score\": results[\"best_score\"][i],\n                 \"evaluation_time\": results[\"evaluation_time\"][i],\n                 \"maximize_time\": results[\"maximize_time\"][i],\n                 \"cumulative_time\": results[\"cumulative_time\"][i],\n                 \"model\": type(self.estimator).__name__,\n                 \"params\": results[\"readable_params\"][i],\n                 \"seed\": seed,\n                 \"method_name\": method.name,\n                 \"time_regressor\": time_regressor,\n                 \"score_regressor\": score_regressor\n             }\n             self.db_table.insert(iteration)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for i in range ( 0 , len ( results [ <STRING> ] ) ) : <NEWLINE> <INDENT> iteration = { <NEWLINE> <INDENT> <STRING> : self . task_id , <NEWLINE> <STRING> : <STRING> . format ( method . name , time_regressor , score_regressor ) , <NEWLINE> <STRING> : i , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : type ( self . estimator ) . __name__ , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : seed , <NEWLINE> <STRING> : method . name , <NEWLINE> <STRING> : time_regressor , <NEWLINE> <STRING> : score_regressor <NEWLINE> <DEDENT> } <NEWLINE> self . db_table . insert ( iteration ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Store results"
            ],
            "<STRING>": [
                "\"best_score\"",
                "\"task\"",
                "\"method\"",
                "\"{} (EI: {}, RT: {})\"",
                "\"iteration\"",
                "\"score\"",
                "\"mean_test_score\"",
                "\"best_score\"",
                "\"best_score\"",
                "\"evaluation_time\"",
                "\"evaluation_time\"",
                "\"maximize_time\"",
                "\"maximize_time\"",
                "\"cumulative_time\"",
                "\"cumulative_time\"",
                "\"model\"",
                "\"params\"",
                "\"readable_params\"",
                "\"seed\"",
                "\"method_name\"",
                "\"time_regressor\"",
                "\"score_regressor\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5e0f04d5a9514a0e96e701cc22a4d135": {
        "code_string": "#Processing\n     dil_ccs = s.dilated_components(psd_output, cc_thresh, dil_param)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> dil_ccs = s . dilated_components ( psd_output , cc_thresh , dil_param ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#Processing"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "05be89c4f78b465dbda5dd545878024f": {
        "code_string": "model = imp.load_source(\"Model\",model_fname).InstantiatedModel\n     model.load_state_dict(torch.load(local_chkpt))\n",
        "code_toks_joined": "model = imp . load_source ( <STRING> , model_fname ) . InstantiatedModel <NEWLINE> <INDENT> model . load_state_dict ( torch . load ( local_chkpt ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Model\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "06730c95e4d2484eb75aa802e5c81d0a": {
        "code_string": "def load_options(self, chunk):\n         for i, name in enumerate(self.options.keys()):\n             value = chunk.chdt[i]\n             setattr(self, name, i)\n",
        "code_toks_joined": "def load_options ( self , chunk ) : <NEWLINE> <INDENT> for i , name in enumerate ( self . options . keys ( ) ) : <NEWLINE> <INDENT> value = chunk . chdt [ i ] <NEWLINE> setattr ( self , name , i ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3eae53bc1b4a4a85a42f10974d4e1001": {
        "code_string": "def contents_changed(self, start, removed, added):\n         \"\"\"Called after modification of the text, retokenizes the modified part.\"\"\"\n         if self._tree.lexicon:\n             start, end = self._builder().rebuild(self._tree, self.text(), start, added, removed)\n         else:\n             end = start + added\n         self.set_modified_range(start, end)\n",
        "code_toks_joined": "def contents_changed ( self , start , removed , added ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . _tree . lexicon : <NEWLINE> <INDENT> start , end = self . _builder ( ) . rebuild ( self . _tree , self . text ( ) , start , added , removed ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> end = start + added <NEWLINE> <DEDENT> self . set_modified_range ( start , end ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Called after modification of the text, retokenizes the modified part.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2b6fbf4bbefa45bd8f56988d76f42f6d": {
        "code_string": "for field in self.COMPLEX_FIELDS:\n             try:\n                 v = kwargs[field]\n             except KeyError:\n                 if partial:\n                     self._incomplete.add(field)\n                     continue\n                 else:\n                     raise AttributeError(f'Expected {field} attribute for {self.type}, none found.')\n             is_compound = isinstance(v, list)\n             cls = get_class(field)\n             if is_compound:\n                 result = list()\n                 for data in v:\n                     try:\n                         data['type'] = data.get('type', singularize(field))\n                     except AttributeError:  # if data has no 'get' method, i.e. not a Dict\n                         result.append(v)\n                     else:\n                         result.append(cls(partial=True, **data))\n                 self.__setattr__(field, result)\n             else:\n                 t = v.get('type', field)\n                 v['type'] = CIVIC_TO_PYCLASS.get(t, t)\n                 self.__setattr__(field, cls(partial=True, **v))\n",
        "code_toks_joined": "for field in self . COMPLEX_FIELDS : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> v = kwargs [ field ] <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> if partial : <NEWLINE> <INDENT> self . _incomplete . add ( field ) <NEWLINE> continue <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise AttributeError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> is_compound = isinstance ( v , list ) <NEWLINE> cls = get_class ( field ) <NEWLINE> if is_compound : <NEWLINE> <INDENT> result = list ( ) <NEWLINE> for data in v : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> data [ <STRING> ] = data . get ( <STRING> , singularize ( field ) ) <NEWLINE> <DEDENT> except AttributeError : <COMMENT> <NEWLINE> <INDENT> result . append ( v ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> result . append ( cls ( partial = True , ** data ) ) <NEWLINE> <DEDENT> <DEDENT> self . __setattr__ ( field , result ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> t = v . get ( <STRING> , field ) <NEWLINE> v [ <STRING> ] = CIVIC_TO_PYCLASS . get ( t , t ) <NEWLINE> self . __setattr__ ( field , cls ( partial = True , ** v ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f'Expected {field} attribute for {self.type}, none found.'",
                "'type'",
                "'type'",
                "'type'",
                "'type'"
            ],
            "<COMMENT>": [
                "# if data has no 'get' method, i.e. not a Dict"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ff14a3b74c3a47cea9f53d774d696866": {
        "code_string": "return row\n",
        "code_toks_joined": "return row <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aba7e16a57c84a429200580e3fc106f6": {
        "code_string": "def __init__(self, keras_model, worker_optimizer, loss, num_workers=2, batch_size=32,\n                  features_col=\"features\", label_col=\"label\", num_epoch=1):\n         super(AsynchronousDistributedTrainer, self).__init__(keras_model, loss, worker_optimizer,\n                                                              num_workers, batch_size, features_col,\n                                                              label_col, num_epoch)\n         # Initialize asynchronous methods variables.\n         self.parallelism = 3 * num_workers\n",
        "code_toks_joined": "def __init__ ( self , keras_model , worker_optimizer , loss , num_workers = 2 , batch_size = 32 , <NEWLINE> <INDENT> features_col = <STRING> , label_col = <STRING> , num_epoch = 1 ) : <NEWLINE> super ( AsynchronousDistributedTrainer , self ) . __init__ ( keras_model , loss , worker_optimizer , <NEWLINE> <INDENT> num_workers , batch_size , features_col , <NEWLINE> label_col , num_epoch ) <NEWLINE> <COMMENT> <NL> self . parallelism = 3 * num_workers <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"features\"",
                "\"label\""
            ],
            "<COMMENT>": [
                "# Initialize asynchronous methods variables."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ebbdfe0de26d43c394929b0852748504": {
        "code_string": "def handle_commit(self, conn, addr):\n         # Receive the parameters from the remote node.\n         data = recv_data(conn)\n         # Extract the data from the dictionary.\n         r = data['residual']\n         worker_id = r['worker_id']\n         with self.mutex:\n             self.add_staleness(worker_id)\n             # Update the center variable.\n             self.center_variable = self.center_variable + r\n         # Increment the number of parameter server updates.\n         self.next_update()\n",
        "code_toks_joined": "def handle_commit ( self , conn , addr ) : <NEWLINE> <COMMENT> <NL> <INDENT> data = recv_data ( conn ) <NEWLINE> <COMMENT> <NL> r = data [ <STRING> ] <NEWLINE> worker_id = r [ <STRING> ] <NEWLINE> with self . mutex : <NEWLINE> <INDENT> self . add_staleness ( worker_id ) <NEWLINE> <COMMENT> <NL> self . center_variable = self . center_variable + r <NEWLINE> <COMMENT> <NL> <DEDENT> self . next_update ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Receive the parameters from the remote node.",
                "# Extract the data from the dictionary.",
                "# Update the center variable.",
                "# Increment the number of parameter server updates."
            ],
            "<STRING>": [
                "'residual'",
                "'worker_id'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2594653517b9439690dd6aa641cf4372": {
        "code_string": "# variables\n     def visit_Name(self, n, shallow=False):\n         # take care of embedded expressions like x_OVER_4, useful for assignment\n         if any([operators[op] in n.id for op in operators]):\n             name = n.id\n             for op in operators:\n                 name = name.replace(operators[op], op)\n             tree = ast.parse(name).body[0].value\n             if shallow:\n                 return tree\n             return to_math(tree, mul=self.mul, div=self.div,\n                            mat_size=self.mat_size, decimal=self.decimal,\n                            syntax=self.s, ital=self.ital)\n         if not self.subs and not shallow:\n             return self.format_name(n.id)\n         # substitute the value of the variable by formatted value\n         try:\n             # if the raw ast object is needed (for BinOp)\n             if shallow:\n                 return _prep4lx(self.dict[n.id], self.s, self.mat_size).value\n             # to prevent infinite recursion:\n             if str(self.dict[n.id]) == n.id:\n                 return self.format_name(str(self.dict[n.id]))\n         except KeyError:\n             log.warning('The variable %s has not been defined.', n.id)\n             return\n         qty = self.visit(_prep4lx(self.dict[n.id], self.s, self.mat_size))\n         unit = to_math(self.dict[n.id + UNIT_PF], div='/', syntax=self.s, ital=False) \\\n             if n.id + UNIT_PF in self.dict.keys() else self.s.txt('')\n         # if the quantity is raised to some power and has a unit,\n         # surround it with PARENS\n         if hasattr(n, 'is_in_power') and n.is_in_power and unit and unit != '_':\n             return self.s.delmtd(qty + unit)\n         return qty + unit\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def visit_Name ( self , n , shallow = False ) : <NEWLINE> <COMMENT> <NL> <INDENT> if any ( [ operators [ op ] in n . id for op in operators ] ) : <NEWLINE> <INDENT> name = n . id <NEWLINE> for op in operators : <NEWLINE> <INDENT> name = name . replace ( operators [ op ] , op ) <NEWLINE> <DEDENT> tree = ast . parse ( name ) . body [ 0 ] . value <NEWLINE> if shallow : <NEWLINE> <INDENT> return tree <NEWLINE> <DEDENT> return to_math ( tree , mul = self . mul , div = self . div , <NEWLINE> <INDENT> mat_size = self . mat_size , decimal = self . decimal , <NEWLINE> syntax = self . s , ital = self . ital ) <NEWLINE> <DEDENT> <DEDENT> if not self . subs and not shallow : <NEWLINE> <INDENT> return self . format_name ( n . id ) <NEWLINE> <COMMENT> <NL> <DEDENT> try : <NEWLINE> <COMMENT> <NL> <INDENT> if shallow : <NEWLINE> <INDENT> return _prep4lx ( self . dict [ n . id ] , self . s , self . mat_size ) . value <NEWLINE> <COMMENT> <NL> <DEDENT> if str ( self . dict [ n . id ] ) == n . id : <NEWLINE> <INDENT> return self . format_name ( str ( self . dict [ n . id ] ) ) <NEWLINE> <DEDENT> <DEDENT> except KeyError : <NEWLINE> <INDENT> log . warning ( <STRING> , n . id ) <NEWLINE> return <NEWLINE> <DEDENT> qty = self . visit ( _prep4lx ( self . dict [ n . id ] , self . s , self . mat_size ) ) <NEWLINE> unit = to_math ( self . dict [ n . id + UNIT_PF ] , div = <STRING> , syntax = self . s , ital = False ) if n . id + UNIT_PF in self . dict . keys ( ) else self . s . txt ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if hasattr ( n , <STRING> ) and n . is_in_power and unit and unit != <STRING> : <NEWLINE> <INDENT> return self . s . delmtd ( qty + unit ) <NEWLINE> <DEDENT> return qty + unit <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# variables",
                "# take care of embedded expressions like x_OVER_4, useful for assignment",
                "# substitute the value of the variable by formatted value",
                "# if the raw ast object is needed (for BinOp)",
                "# to prevent infinite recursion:",
                "# if the quantity is raised to some power and has a unit,",
                "# surround it with PARENS"
            ],
            "<STRING>": [
                "'The variable %s has not been defined.'",
                "'/'",
                "''",
                "'is_in_power'",
                "'_'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b1fd7344420148da9d00cb7286b3bc9b": {
        "code_string": "def get_url(url):\n     fp = url_fp(url)\n     if not os.path.exists(fp):\n         os.remove(fp)\n     subprocess.check_call([\"wget\", url])\n     return fp\n",
        "code_toks_joined": "def get_url ( url ) : <NEWLINE> <INDENT> fp = url_fp ( url ) <NEWLINE> if not os . path . exists ( fp ) : <NEWLINE> <INDENT> os . remove ( fp ) <NEWLINE> <DEDENT> subprocess . check_call ( [ <STRING> , url ] ) <NEWLINE> return fp <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"wget\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b584e888a255439ebeb672f38995f36f": {
        "code_string": "def add_subwindow(self, aclass, flist):\n         \"\"\"Add a subwindow, register data set and add to menu\"\"\"\n         sub = PyJibeQMdiSubWindow()\n         inst = aclass(sub)\n         sub.setWidget(inst)\n         inst.add_files(flist)\n         self.mdiArea.addSubWindow(sub)\n         sub.show()\n         self.subwindows.append(sub)\n         # Add export choices\n         if hasattr(inst, \"get_export_choices\"):\n             choices = inst.get_export_choices()\n             menobj = self.menuExport.addMenu(inst.windowTitle())\n             for choice in choices:\n                 action = menobj.addAction(choice[0])\n                 action.triggered.connect(getattr(inst, choice[1]))\n",
        "code_toks_joined": "def add_subwindow ( self , aclass , flist ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sub = PyJibeQMdiSubWindow ( ) <NEWLINE> inst = aclass ( sub ) <NEWLINE> sub . setWidget ( inst ) <NEWLINE> inst . add_files ( flist ) <NEWLINE> self . mdiArea . addSubWindow ( sub ) <NEWLINE> sub . show ( ) <NEWLINE> self . subwindows . append ( sub ) <NEWLINE> <COMMENT> <NL> if hasattr ( inst , <STRING> ) : <NEWLINE> <INDENT> choices = inst . get_export_choices ( ) <NEWLINE> menobj = self . menuExport . addMenu ( inst . windowTitle ( ) ) <NEWLINE> for choice in choices : <NEWLINE> <INDENT> action = menobj . addAction ( choice [ 0 ] ) <NEWLINE> action . triggered . connect ( getattr ( inst , choice [ 1 ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Add a subwindow, register data set and add to menu\"\"\"",
                "\"get_export_choices\""
            ],
            "<COMMENT>": [
                "# Add export choices"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c04a62f6aecf4d2193b2e3d5c35e87c7": {
        "code_string": "with geo.Geo(points_geo) as g:\n         g.include(\"structure.geo\")\n         g.include(\"points.geo\")\n         g.attractor(1, geo.range(idx0, idx))\n         g.threshold(2,\n                     field=1,\n                     dist=(0, 2 * min_size),\n                     lc=(1.1 * delta, min_size))\n         g.min(3, 2)\n         g.background(3)\n",
        "code_toks_joined": "with geo . Geo ( points_geo ) as g : <NEWLINE> <INDENT> g . include ( <STRING> ) <NEWLINE> g . include ( <STRING> ) <NEWLINE> g . attractor ( 1 , geo . range ( idx0 , idx ) ) <NEWLINE> g . threshold ( 2 , <NEWLINE> <INDENT> field = 1 , <NEWLINE> dist = ( 0 , 2 * min_size ) , <NEWLINE> lc = ( 1.1 * delta , min_size ) ) <NEWLINE> <DEDENT> g . min ( 3 , 2 ) <NEWLINE> g . background ( 3 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"structure.geo\"",
                "\"points.geo\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "88362e709da74d449d41aecb6814c3ac": {
        "code_string": "ceryle.configure_logging(\n         level={\n             'DEBUG': logging.DEBUG,\n             'INFO': logging.INFO,\n             'WARN': logging.WARN,\n             'ERROR': logging.ERROR,\n         }[args.pop('log_level')],\n         console=args.pop('log_stream'),\n         filename=args.pop('log_filename'))\n     logger.debug(f'arguments: {argv}')\n",
        "code_toks_joined": "ceryle . configure_logging ( <NEWLINE> <INDENT> level = { <NEWLINE> <INDENT> <STRING> : logging . DEBUG , <NEWLINE> <STRING> : logging . INFO , <NEWLINE> <STRING> : logging . WARN , <NEWLINE> <STRING> : logging . ERROR , <NEWLINE> <DEDENT> } [ args . pop ( <STRING> ) ] , <NEWLINE> console = args . pop ( <STRING> ) , <NEWLINE> filename = args . pop ( <STRING> ) ) <NEWLINE> logger . debug ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'DEBUG'",
                "'INFO'",
                "'WARN'",
                "'ERROR'",
                "'log_level'",
                "'log_stream'",
                "'log_filename'",
                "f'arguments: {argv}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b35f6f3dea464dae848668dbcb952b85": {
        "code_string": "def check_tor(self):\n         \"\"\" True If Tor Is Installed \"\"\"\n         return (self.tor is None)\n",
        "code_toks_joined": "def check_tor ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return ( self . tor is None ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" True If Tor Is Installed \"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9fa821b08aa34307932e969d28065534": {
        "code_string": "# return results\n     return filename, filesize\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> return filename , filesize <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# return results"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f0cccef815c44f85898e682de120232d": {
        "code_string": "def update_scrubber(self, current, duration):\n         if current is None and duration is None:\n             self.song_duration_label.set_text('-:--')\n             self.song_progress_label.set_text('-:--')\n             self.song_scrubber.set_value(0)\n             return\n",
        "code_toks_joined": "def update_scrubber ( self , current , duration ) : <NEWLINE> <INDENT> if current is None and duration is None : <NEWLINE> <INDENT> self . song_duration_label . set_text ( <STRING> ) <NEWLINE> self . song_progress_label . set_text ( <STRING> ) <NEWLINE> self . song_scrubber . set_value ( 0 ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'-:--'",
                "'-:--'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c009dc45ea89427d9d1d7407bf4a229b": {
        "code_string": "fname = os.path.join(jobscriptdir, job + '.job')\n         f = open(fname, 'w')\n         f.write(content.format(job, stime, oe, oe, ntasks, memPerCPU, mpiexec, sim))\n         f.close()\n",
        "code_toks_joined": "fname = os . path . join ( jobscriptdir , job + <STRING> ) <NEWLINE> <INDENT> f = open ( fname , <STRING> ) <NEWLINE> f . write ( content . format ( job , stime , oe , oe , ntasks , memPerCPU , mpiexec , sim ) ) <NEWLINE> f . close ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.job'",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ace65057794e4c5bbf5af2cfe366a57e": {
        "code_string": "class BusSchedulerTest(unittest.TestCase):\n     def setUp(self):\n         self.stop1 = microbus.BusStop(\"stop1\")\n         self.stop2 = microbus.BusStop(\"stop2\")\n         self.stop3 = microbus.BusStop(\"stop3\")\n         self.stops = [self.stop1, self.stop2, self.stop3]\n         self.busRoute1 = microbus.BusRoute(\"test\", self.stops)\n         self.busRoute2 = self.busRoute1[::-1]\n         self.bus = Bus(keep_prev=2)\n         self.scheduler = BusScheduler(self.bus)\n",
        "code_toks_joined": "class BusSchedulerTest ( unittest . TestCase ) : <NEWLINE> <INDENT> def setUp ( self ) : <NEWLINE> <INDENT> self . stop1 = microbus . BusStop ( <STRING> ) <NEWLINE> self . stop2 = microbus . BusStop ( <STRING> ) <NEWLINE> self . stop3 = microbus . BusStop ( <STRING> ) <NEWLINE> self . stops = [ self . stop1 , self . stop2 , self . stop3 ] <NEWLINE> self . busRoute1 = microbus . BusRoute ( <STRING> , self . stops ) <NEWLINE> self . busRoute2 = self . busRoute1 [ : : - 1 ] <NEWLINE> self . bus = Bus ( keep_prev = 2 ) <NEWLINE> self . scheduler = BusScheduler ( self . bus ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"stop1\"",
                "\"stop2\"",
                "\"stop3\"",
                "\"test\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5d1583d4fa554078b81ddea67030fef2": {
        "code_string": "if event == 'push':\n             ref = json.loads(self.request.body.decode('utf8'))['ref']\n             if ref != 'refs/heads/{branch}'.format(branch=options.GITHUB_BRANCH):\n                 result = yield github_pull()\n                 logger.warning(result)\n",
        "code_toks_joined": "if event == <STRING> : <NEWLINE> <INDENT> ref = json . loads ( self . request . body . decode ( <STRING> ) ) [ <STRING> ] <NEWLINE> if ref != <STRING> . format ( branch = options . GITHUB_BRANCH ) : <NEWLINE> <INDENT> result = yield github_pull ( ) <NEWLINE> logger . warning ( result ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'push'",
                "'utf8'",
                "'ref'",
                "'refs/heads/{branch}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ad2e3bef55b94fcb8b20638a1830e5c4": {
        "code_string": "self.minimum_barcode_fragments = minimum_barcode_fragments\n \t\tself.minimum_cell_fragments = minimum_cell_fragments\n \t\tself.minimum_jaccard_fragments = minimum_jaccard_fragments\n \t\tself.extract_mito = extract_mito\n \t\tself.drop_tag = barcode_tag\n \t\tself.barcode_tag = barcode_tag\n",
        "code_toks_joined": "self . minimum_barcode_fragments = minimum_barcode_fragments <NEWLINE> <INDENT> self . minimum_cell_fragments = minimum_cell_fragments <NEWLINE> self . minimum_jaccard_fragments = minimum_jaccard_fragments <NEWLINE> self . extract_mito = extract_mito <NEWLINE> self . drop_tag = barcode_tag <NEWLINE> self . barcode_tag = barcode_tag <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7844bacd5f5c4dc0b36b581d5415ddf1": {
        "code_string": "if out is not None:\n                     hypernyms.extend(out)\n                     for h in hypernyms:\n                         local_graph.append((str(token), h))\n",
        "code_toks_joined": "if out is not None : <NEWLINE> <INDENT> hypernyms . extend ( out ) <NEWLINE> for h in hypernyms : <NEWLINE> <INDENT> local_graph . append ( ( str ( token ) , h ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bae2fd1cbe5e41f2841dcd71231271f5": {
        "code_string": "def run_casa(cmd, raise_on_severe=False, timeout=1800):\n     \"\"\"Run a list of casa commands\"\"\"\n     casa = drivecasa.Casapy()\n     try:\n         casa_output, casa_error = casa.run_script(cmd, raise_on_severe=True, timeout=timeout)\n         logger.debug('\\n'.join(casa_output))\n     except RuntimeError:\n         logger.error(\"Casa command failed\")\n         if raise_on_severe:\n             raise\n",
        "code_toks_joined": "def run_casa ( cmd , raise_on_severe = False , timeout = 1800 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> casa = drivecasa . Casapy ( ) <NEWLINE> try : <NEWLINE> <INDENT> casa_output , casa_error = casa . run_script ( cmd , raise_on_severe = True , timeout = timeout ) <NEWLINE> logger . debug ( <STRING> . join ( casa_output ) ) <NEWLINE> <DEDENT> except RuntimeError : <NEWLINE> <INDENT> logger . error ( <STRING> ) <NEWLINE> if raise_on_severe : <NEWLINE> <INDENT> raise <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Run a list of casa commands\"\"\"",
                "'\\n'",
                "\"Casa command failed\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2dcb8edd34c0413cb6d057f10645747a": {
        "code_string": "if self.transfer_convert_selfcaluv2uvfits:\n             subs_setinit.setinitdirs(self)\n             subs_setinit.setdatasetnamestomiriad(self)\n             subs_managefiles.director(\n                 self, 'ch', self.transferdir, verbose=True)\n             if not transfertargetbeamsselfcaluv2uvfitsstatus:\n                 # Get the status of the selfcal for the specified beam\n                 selfcaltargetbeamsphasestatus = get_param_def(\n                     self, sbeam + '_targetbeams_phase_status', False)\n                 selfcaltargetbeamsampstatus = get_param_def(\n                     self, sbeam + '_targetbeams_amp_status', False)\n                 datasetname_amp = os.path.join(\n                     self.selfcaldir, self.target).rstrip('.mir') + '_amp.mir'\n                 datasetname_phase = os.path.join(\n                     self.selfcaldir, self.target)\n                 logger.debug(\n                     \"Setting amplitude selfcal file name: {}\".format(datasetname_amp))\n                 logger.debug(\n                     \"Setting phase selfcal file name: {}\".format(datasetname_amp))\n                 # datasetname_amp = self.get_target_path().rstrip('.mir') + '_amp.mir'\n                 # datasetname_phase = self.get_target_path()\n                 if os.path.isdir(datasetname_amp) and selfcaltargetbeamsampstatus:\n                     logger.info('Beam ' + self.beam +\n                                 ': Using amplitude self-calibrated dataset!')\n                     dataset = datasetname_amp\n                 elif os.path.isdir(datasetname_phase) and selfcaltargetbeamsphasestatus:\n                     logger.info(\n                         'Beam ' + self.beam + ': Using phase self-calibrated dataset. Amplitude calibration was not successful or not wanted!')\n                     dataset = datasetname_phase\n                 else:\n                     dataset = None\n",
        "code_toks_joined": "if self . transfer_convert_selfcaluv2uvfits : <NEWLINE> <INDENT> subs_setinit . setinitdirs ( self ) <NEWLINE> subs_setinit . setdatasetnamestomiriad ( self ) <NEWLINE> subs_managefiles . director ( <NEWLINE> <INDENT> self , <STRING> , self . transferdir , verbose = True ) <NEWLINE> <DEDENT> if not transfertargetbeamsselfcaluv2uvfitsstatus : <NEWLINE> <COMMENT> <NL> <INDENT> selfcaltargetbeamsphasestatus = get_param_def ( <NEWLINE> <INDENT> self , sbeam + <STRING> , False ) <NEWLINE> <DEDENT> selfcaltargetbeamsampstatus = get_param_def ( <NEWLINE> <INDENT> self , sbeam + <STRING> , False ) <NEWLINE> <DEDENT> datasetname_amp = os . path . join ( <NEWLINE> <INDENT> self . selfcaldir , self . target ) . rstrip ( <STRING> ) + <STRING> <NEWLINE> <DEDENT> datasetname_phase = os . path . join ( <NEWLINE> <INDENT> self . selfcaldir , self . target ) <NEWLINE> <DEDENT> logger . debug ( <NEWLINE> <INDENT> <STRING> . format ( datasetname_amp ) ) <NEWLINE> <DEDENT> logger . debug ( <NEWLINE> <INDENT> <STRING> . format ( datasetname_amp ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> if os . path . isdir ( datasetname_amp ) and selfcaltargetbeamsampstatus : <NEWLINE> <INDENT> logger . info ( <STRING> + self . beam + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> dataset = datasetname_amp <NEWLINE> <DEDENT> elif os . path . isdir ( datasetname_phase ) and selfcaltargetbeamsphasestatus : <NEWLINE> <INDENT> logger . info ( <NEWLINE> <INDENT> <STRING> + self . beam + <STRING> ) <NEWLINE> <DEDENT> dataset = datasetname_phase <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> dataset = None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ch'",
                "'_targetbeams_phase_status'",
                "'_targetbeams_amp_status'",
                "'.mir'",
                "'_amp.mir'",
                "\"Setting amplitude selfcal file name: {}\"",
                "\"Setting phase selfcal file name: {}\"",
                "'Beam '",
                "': Using amplitude self-calibrated dataset!'",
                "'Beam '",
                "': Using phase self-calibrated dataset. Amplitude calibration was not successful or not wanted!'"
            ],
            "<COMMENT>": [
                "# Get the status of the selfcal for the specified beam",
                "# datasetname_amp = self.get_target_path().rstrip('.mir') + '_amp.mir'",
                "# datasetname_phase = self.get_target_path()"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "399dfc44e9c5445eb53acd2c0594ed0b": {
        "code_string": "# check if file exists:\n     if os.path.isdir(beamoutname):\n         #then test type and proceed for different types\n         if corrtype == 'Gaussian':\n             make_gaussian_beam(beam_map_dir,beamoutname,bm_size,cell,fwhm,cutoff)\n         elif corrtype == 'Correct':\n             error='Measured PB maps not yet supported'\n             logger.error(error)\n             raise ApercalException(error)\n             get_measured_beam_maps(beam, beam_map_dir, primary_beam_path)\n         else:\n             error='Type of beam map not supported'\n             logger.error(error)\n             raise ApercalException(error)\n     else:\n         logger.warning(\"Beam map for beam {} already exists. Did not create it again\".format(beam))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if os . path . isdir ( beamoutname ) : <NEWLINE> <COMMENT> <NL> <INDENT> if corrtype == <STRING> : <NEWLINE> <INDENT> make_gaussian_beam ( beam_map_dir , beamoutname , bm_size , cell , fwhm , cutoff ) <NEWLINE> <DEDENT> elif corrtype == <STRING> : <NEWLINE> <INDENT> error = <STRING> <NEWLINE> logger . error ( error ) <NEWLINE> raise ApercalException ( error ) <NEWLINE> get_measured_beam_maps ( beam , beam_map_dir , primary_beam_path ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> error = <STRING> <NEWLINE> logger . error ( error ) <NEWLINE> raise ApercalException ( error ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> logger . warning ( <STRING> . format ( beam ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# check if file exists:",
                "#then test type and proceed for different types"
            ],
            "<STRING>": [
                "'Gaussian'",
                "'Correct'",
                "'Measured PB maps not yet supported'",
                "'Type of beam map not supported'",
                "\"Beam map for beam {} already exists. Did not create it again\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6e97821e8c7745b5821ce8229dad0f5d": {
        "code_string": "def _assert_wwn_translation(self, expected, actual):\n         self.assertEquals(expected, sysfs.translate_wwn(actual))\n",
        "code_toks_joined": "def _assert_wwn_translation ( self , expected , actual ) : <NEWLINE> <INDENT> self . assertEquals ( expected , sysfs . translate_wwn ( actual ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3a18a1747dc5405688baf2e109760dd3": {
        "code_string": "return raw.lower() in ['1', 'true', 'yes']\n",
        "code_toks_joined": "return raw . lower ( ) in [ <STRING> , <STRING> , <STRING> ] <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'1'",
                "'true'",
                "'yes'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d687ba1ce7764acc80ac5faecac8da2a": {
        "code_string": "# add setup for arcproxy if it exists\n     arcproxy_setup = \"%s/atlas.cern.ch/repo/sw/arc/client/latest/slc6/x86_64/setup.sh\" % get_file_system_root_path()\n     envsetup += \". %s;\" % (arcproxy_setup)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> arcproxy_setup = <STRING> % get_file_system_root_path ( ) <NEWLINE> envsetup += <STRING> % ( arcproxy_setup ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# add setup for arcproxy if it exists"
            ],
            "<STRING>": [
                "\"%s/atlas.cern.ch/repo/sw/arc/client/latest/slc6/x86_64/setup.sh\"",
                "\". %s;\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a43b939fb53a41628b1320e512e57220": {
        "code_string": "return intersect\n",
        "code_toks_joined": "return intersect <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "91c4b935bb5642a68c44068a4ce30fe2": {
        "code_string": "def copy_output(job, job_scratch_dir, work_dir):\n     cp_start = time.time()\n     try:\n         for outfile in job.output_files.keys():\n             if os.path.exists(outfile):\n                 copy(os.path.join(job_scratch_dir, outfile), os.path.join(job_scratch_dir, outfile))\n         os.chdir(work_dir)\n     except IOError:\n         raise FileHandlingFailure(\"Copy from scratch dir to access point failed\")\n     finally:\n         cp_time = time.time() - cp_start\n         logger.info(\"Copy of outputs took: {0} sec.\".format(cp_time))\n     return 0\n",
        "code_toks_joined": "def copy_output ( job , job_scratch_dir , work_dir ) : <NEWLINE> <INDENT> cp_start = time . time ( ) <NEWLINE> try : <NEWLINE> <INDENT> for outfile in job . output_files . keys ( ) : <NEWLINE> <INDENT> if os . path . exists ( outfile ) : <NEWLINE> <INDENT> copy ( os . path . join ( job_scratch_dir , outfile ) , os . path . join ( job_scratch_dir , outfile ) ) <NEWLINE> <DEDENT> <DEDENT> os . chdir ( work_dir ) <NEWLINE> <DEDENT> except IOError : <NEWLINE> <INDENT> raise FileHandlingFailure ( <STRING> ) <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> cp_time = time . time ( ) - cp_start <NEWLINE> logger . info ( <STRING> . format ( cp_time ) ) <NEWLINE> <DEDENT> return 0 <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Copy from scratch dir to access point failed\"",
                "\"Copy of outputs took: {0} sec.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "55639b15aa914876a6abfb5435a13225": {
        "code_string": "_cmd = get_trf_command(command, transformation=transformation)\n     i = 0\n     imax = 120\n     while i < imax:\n         # abort if main process has finished already\n         if is_process_running(pid):\n             return -1\n",
        "code_toks_joined": "_cmd = get_trf_command ( command , transformation = transformation ) <NEWLINE> <INDENT> i = 0 <NEWLINE> imax = 120 <NEWLINE> while i < imax : <NEWLINE> <COMMENT> <NL> <INDENT> if is_process_running ( pid ) : <NEWLINE> <INDENT> return - 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# abort if main process has finished already"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9e8d778c1fdc4fc9bd4646fb7c0787b7": {
        "code_string": "if not copysetup:\n         return \"\"\n",
        "code_toks_joined": "if not copysetup : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ad0fb4dfa2e541dfbad0234468feeccf": {
        "code_string": "if config.Pilot.pandajob == 'real':\n             time_before = int(time.time())\n             res = https.request('{pandaserver}/server/panda/updateJob'.format(pandaserver=pandaserver), data=data)\n             time_after = int(time.time())\n             log.info('server updateJob request completed in %ds for job %s' % (time_after - time_after, job.jobid))\n             log.info(\"server responded with: res = %s\" % str(res))\n             if res is not None:\n                 # does the server update contain any backchannel information? if so, update the job object\n                 handle_backchannel_command(res, job, args)\n",
        "code_toks_joined": "if config . Pilot . pandajob == <STRING> : <NEWLINE> <INDENT> time_before = int ( time . time ( ) ) <NEWLINE> res = https . request ( <STRING> . format ( pandaserver = pandaserver ) , data = data ) <NEWLINE> time_after = int ( time . time ( ) ) <NEWLINE> log . info ( <STRING> % ( time_after - time_after , job . jobid ) ) <NEWLINE> log . info ( <STRING> % str ( res ) ) <NEWLINE> if res is not None : <NEWLINE> <COMMENT> <NL> <INDENT> handle_backchannel_command ( res , job , args ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'real'",
                "'{pandaserver}/server/panda/updateJob'",
                "'server updateJob request completed in %ds for job %s'",
                "\"server responded with: res = %s\""
            ],
            "<COMMENT>": [
                "# does the server update contain any backchannel information? if so, update the job object"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "195243b218b14975802764e068331529": {
        "code_string": "def get_object(self, **kwargs):\n         return get_thing_object(self.model, self.request.user, self.kwargs['slug'])\n",
        "code_toks_joined": "def get_object ( self , ** kwargs ) : <NEWLINE> <INDENT> return get_thing_object ( self . model , self . request . user , self . kwargs [ <STRING> ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'slug'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cda59f3d92394d2a9e86716780253885": {
        "code_string": "def element_conforms(element, etype) -> bool:\n     if element is None and etype == object:\n         return True\n     elif isinstance(etype, type(type)) and (issubclass(etype, type(None))):\n         return element is None\n     elif element is None:\n         return False\n     return isinstance(element, etype)\n",
        "code_toks_joined": "def element_conforms ( element , etype ) -> bool : <NEWLINE> <INDENT> if element is None and etype == object : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> elif isinstance ( etype , type ( type ) ) and ( issubclass ( etype , type ( None ) ) ) : <NEWLINE> <INDENT> return element is None <NEWLINE> <DEDENT> elif element is None : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return isinstance ( element , etype ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "45bcf9bb309b403080dc41756fe5f91d": {
        "code_string": "participants = list()\n     participant_nodes = xpath(participants_node, \"db:participant\")\n     for node in participant_nodes:\n         pid = node.get(\"id\")\n         initial_state_node = xpath(node, \"db:initialState\")[0]\n         common_state_vals = _extract_common_state_vals(initial_state_node)\n         initial_state = InitialState(\n             (float(initial_state_node.get(\"x\")), float(initial_state_node.get(\"y\"))),\n             float(initial_state_node.get(\"orientation\")),\n             common_state_vals[0],\n             common_state_vals[1],\n             common_state_vals[2]\n         )\n         # Add data requests declared in the DBC\n         ai_requests = list()\n         request_nodes = xpath(node, \"db:ai/*\")\n         for req_node in request_nodes:\n             tag = get_tag_name(req_node)\n             rid = req_node.get(\"id\")\n             if tag == \"position\":\n                 ai_requests.append(PositionRequest(rid))\n             elif tag == \"speed\":\n                 ai_requests.append(SpeedRequest(rid))\n             elif tag == \"steeringAngle\":\n                 ai_requests.append(SteeringAngleRequest(rid))\n             elif tag == \"camera\":\n                 width = int(req_node.get(\"width\"))\n                 height = int(req_node.get(\"height\"))\n                 fov = int(req_node.get(\"fov\"))\n                 direction = CameraDirection[req_node.get(\"direction\")]\n                 ai_requests.append(CameraRequest(rid, width, height, fov, direction))\n             elif tag == \"lidar\":\n                 radius = int(req_node.get(\"radius\"))\n                 ai_requests.append(LidarRequest(rid, radius))\n             elif tag == \"roadCenterDistance\":\n                 ai_requests.append(RoadCenterDistanceRequest(rid, roads))\n             elif tag == \"carToLaneAngle\":\n                 ai_requests.append(CarToLaneAngleRequest(rid, roads))\n             elif tag == \"boundingBox\":\n                 ai_requests.append(BoundingBoxRequest(rid))\n             else:\n                 _logger.warning(\"The tag \" + tag + \" is not supported, yet.\")\n         # Add default data requests required for debugging and visualization\n         ai_requests.extend([\n             BoundingBoxRequest(\"visualizer_\" + pid + \"_boundingBox\")\n         ])\n         # Extract the movement of the participant\n         movements = list()\n         waypoint_nodes = xpath(node, \"db:movement/db:waypoint\")\n         for wp_node in waypoint_nodes:\n             common_state_vals = _extract_common_state_vals(initial_state_node)\n             movements.append(WayPoint(\n                 (float(wp_node.get(\"x\")), float(wp_node.get(\"y\"))),\n                 float(wp_node.get(\"tolerance\")),\n                 wp_node.get(\"id\"),\n                 common_state_vals[0],\n                 common_state_vals[1],\n                 common_state_vals[2]\n             ))\n         participants.append(Participant(pid, initial_state, CarModel[node.get(\"model\")].value, movements, ai_requests))\n",
        "code_toks_joined": "participants = list ( ) <NEWLINE> <INDENT> participant_nodes = xpath ( participants_node , <STRING> ) <NEWLINE> for node in participant_nodes : <NEWLINE> <INDENT> pid = node . get ( <STRING> ) <NEWLINE> initial_state_node = xpath ( node , <STRING> ) [ 0 ] <NEWLINE> common_state_vals = _extract_common_state_vals ( initial_state_node ) <NEWLINE> initial_state = InitialState ( <NEWLINE> <INDENT> ( float ( initial_state_node . get ( <STRING> ) ) , float ( initial_state_node . get ( <STRING> ) ) ) , <NEWLINE> float ( initial_state_node . get ( <STRING> ) ) , <NEWLINE> common_state_vals [ 0 ] , <NEWLINE> common_state_vals [ 1 ] , <NEWLINE> common_state_vals [ 2 ] <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> ai_requests = list ( ) <NEWLINE> request_nodes = xpath ( node , <STRING> ) <NEWLINE> for req_node in request_nodes : <NEWLINE> <INDENT> tag = get_tag_name ( req_node ) <NEWLINE> rid = req_node . get ( <STRING> ) <NEWLINE> if tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( PositionRequest ( rid ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( SpeedRequest ( rid ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( SteeringAngleRequest ( rid ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> width = int ( req_node . get ( <STRING> ) ) <NEWLINE> height = int ( req_node . get ( <STRING> ) ) <NEWLINE> fov = int ( req_node . get ( <STRING> ) ) <NEWLINE> direction = CameraDirection [ req_node . get ( <STRING> ) ] <NEWLINE> ai_requests . append ( CameraRequest ( rid , width , height , fov , direction ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> radius = int ( req_node . get ( <STRING> ) ) <NEWLINE> ai_requests . append ( LidarRequest ( rid , radius ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( RoadCenterDistanceRequest ( rid , roads ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( CarToLaneAngleRequest ( rid , roads ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( BoundingBoxRequest ( rid ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> _logger . warning ( <STRING> + tag + <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> ai_requests . extend ( [ <NEWLINE> <INDENT> BoundingBoxRequest ( <STRING> + pid + <STRING> ) <NEWLINE> <DEDENT> ] ) <NEWLINE> <COMMENT> <NL> movements = list ( ) <NEWLINE> waypoint_nodes = xpath ( node , <STRING> ) <NEWLINE> for wp_node in waypoint_nodes : <NEWLINE> <INDENT> common_state_vals = _extract_common_state_vals ( initial_state_node ) <NEWLINE> movements . append ( WayPoint ( <NEWLINE> <INDENT> ( float ( wp_node . get ( <STRING> ) ) , float ( wp_node . get ( <STRING> ) ) ) , <NEWLINE> float ( wp_node . get ( <STRING> ) ) , <NEWLINE> wp_node . get ( <STRING> ) , <NEWLINE> common_state_vals [ 0 ] , <NEWLINE> common_state_vals [ 1 ] , <NEWLINE> common_state_vals [ 2 ] <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> participants . append ( Participant ( pid , initial_state , CarModel [ node . get ( <STRING> ) ] . value , movements , ai_requests ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"db:participant\"",
                "\"id\"",
                "\"db:initialState\"",
                "\"x\"",
                "\"y\"",
                "\"orientation\"",
                "\"db:ai/*\"",
                "\"id\"",
                "\"position\"",
                "\"speed\"",
                "\"steeringAngle\"",
                "\"camera\"",
                "\"width\"",
                "\"height\"",
                "\"fov\"",
                "\"direction\"",
                "\"lidar\"",
                "\"radius\"",
                "\"roadCenterDistance\"",
                "\"carToLaneAngle\"",
                "\"boundingBox\"",
                "\"The tag \"",
                "\" is not supported, yet.\"",
                "\"visualizer_\"",
                "\"_boundingBox\"",
                "\"db:movement/db:waypoint\"",
                "\"x\"",
                "\"y\"",
                "\"tolerance\"",
                "\"id\"",
                "\"model\""
            ],
            "<COMMENT>": [
                "# Add data requests declared in the DBC",
                "# Add default data requests required for debugging and visualization",
                "# Extract the movement of the participant"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4f2be7b6e6484ed1bf053000eec6b123": {
        "code_string": "if 'wsgi_file' in env:\n         env.wsgi_file = env.django_appname + \".wsgi\"\n",
        "code_toks_joined": "if <STRING> in env : <NEWLINE> <INDENT> env . wsgi_file = env . django_appname + <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'wsgi_file'",
                "\".wsgi\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c9b63333b661462fb358347b4cd17339": {
        "code_string": "def on_add_device(self, ip):\n         for d in self.device_list:\n             if d.device.ip_addr == ip:\n                 last_volume = d.disconnect_volume\n                 self.devices.remove(d.device)\n                 self.device_list.remove(d)\n                 break\n         d = CattDevice(ip_addr=ip)\n         d._cast.wait()\n         device = Device(self, d, d._cast, self.combo_box.count())\n         d._cast.media_controller.register_status_listener(device.media_listener)\n         d._cast.register_status_listener(device.status_listener)\n         self.devices.append(d)\n         self.device_list.append(device)\n         self.combo_box.addItem(d.name)\n         if self.combo_box.currentIndex() == device.index:\n             self.play_button.setEnabled(True)\n             self.stop_button.setEnabled(True)\n         d.disconnect_volume = round(device.cast.status.volume_level * 100)\n         if self.reconnect_volume == -1:\n             if last_volume != round(device.cast.status.volume_level * 100):\n                 d.volume(last_volume / 100)\n                 if device.index == self.combo_box.currentIndex():\n                     self.set_volume_label(last_volume)\n         else:\n             d.volume(self.reconnect_volume / 100)\n             if device.index == self.combo_box.currentIndex():\n                 self.set_volume_label(self.reconnect_volume)\n",
        "code_toks_joined": "def on_add_device ( self , ip ) : <NEWLINE> <INDENT> for d in self . device_list : <NEWLINE> <INDENT> if d . device . ip_addr == ip : <NEWLINE> <INDENT> last_volume = d . disconnect_volume <NEWLINE> self . devices . remove ( d . device ) <NEWLINE> self . device_list . remove ( d ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> d = CattDevice ( ip_addr = ip ) <NEWLINE> d . _cast . wait ( ) <NEWLINE> device = Device ( self , d , d . _cast , self . combo_box . count ( ) ) <NEWLINE> d . _cast . media_controller . register_status_listener ( device . media_listener ) <NEWLINE> d . _cast . register_status_listener ( device . status_listener ) <NEWLINE> self . devices . append ( d ) <NEWLINE> self . device_list . append ( device ) <NEWLINE> self . combo_box . addItem ( d . name ) <NEWLINE> if self . combo_box . currentIndex ( ) == device . index : <NEWLINE> <INDENT> self . play_button . setEnabled ( True ) <NEWLINE> self . stop_button . setEnabled ( True ) <NEWLINE> <DEDENT> d . disconnect_volume = round ( device . cast . status . volume_level * 100 ) <NEWLINE> if self . reconnect_volume == - 1 : <NEWLINE> <INDENT> if last_volume != round ( device . cast . status . volume_level * 100 ) : <NEWLINE> <INDENT> d . volume ( last_volume / 100 ) <NEWLINE> if device . index == self . combo_box . currentIndex ( ) : <NEWLINE> <INDENT> self . set_volume_label ( last_volume ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> d . volume ( self . reconnect_volume / 100 ) <NEWLINE> if device . index == self . combo_box . currentIndex ( ) : <NEWLINE> <INDENT> self . set_volume_label ( self . reconnect_volume ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5a340aa20fae4f10a7255676d70e9ed5": {
        "code_string": "def make_summary_abund_df(df, cags, singletons):\n     \"\"\"Make a DataFrame with the average value for each CAG, as well as the singletons.\"\"\"\n     summary_df = pd.concat([\n         pd.DataFrame({\n             cag_ix: df.loc[cag].mean()\n             for cag_ix, cag in cags.items()\n         }).T,\n         cags.loc[singletons]\n     ])\n",
        "code_toks_joined": "def make_summary_abund_df ( df , cags , singletons ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> summary_df = pd . concat ( [ <NEWLINE> <INDENT> pd . DataFrame ( { <NEWLINE> <INDENT> cag_ix : df . loc [ cag ] . mean ( ) <NEWLINE> for cag_ix , cag in cags . items ( ) <NEWLINE> <DEDENT> } ) . T , <NEWLINE> cags . loc [ singletons ] <NEWLINE> <DEDENT> ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Make a DataFrame with the average value for each CAG, as well as the singletons.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "65d1aa6b490146248b30f83fc0f2ab91": {
        "code_string": "elif container.set(self.matcher.matchLinkText(block)):\n \t\t\t\tmatch = container.get()\n \t\t\t\tem = LinkTextMatch(match)\n \t\t\t\tsubelement = self.parseText(em.text())\n \t\t\t\telement = LinkElement(element, em.url())\n",
        "code_toks_joined": "elif container . set ( self . matcher . matchLinkText ( block ) ) : <NEWLINE> <INDENT> match = container . get ( ) <NEWLINE> em = LinkTextMatch ( match ) <NEWLINE> subelement = self . parseText ( em . text ( ) ) <NEWLINE> element = LinkElement ( element , em . url ( ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "21dc98a45127451783b1fb820a33a58c": {
        "code_string": "scanner = NetgearDeviceScanner(host, password, username)\n",
        "code_toks_joined": "scanner = NetgearDeviceScanner ( host , password , username ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "aca302e5e1d94e0a9ee26658eff2b6ba": {
        "code_string": "def trigger(hass, config, action):\n     \"\"\" Listen for state changes based on `config`. \"\"\"\n     if CONF_AFTER in config:\n         after = dt_util.parse_time_str(config[CONF_AFTER])\n         if after is None:\n             _error_time(config[CONF_AFTER], CONF_AFTER)\n             return False\n         hours, minutes, seconds = after.hour, after.minute, after.second\n     elif (CONF_HOURS in config or CONF_MINUTES in config or\n           CONF_SECONDS in config):\n         hours = config.get(CONF_HOURS)\n         minutes = config.get(CONF_MINUTES)\n         seconds = config.get(CONF_SECONDS)\n         if isinstance(minutes, str) and minutes.startswith('/') \\\n                 and not convert(minutes.lstrip('/'), int) % 60 == 0:\n             _LOGGER.warning('Periodic minutes should be divisible with 60'\n                             'there will be an offset every hour')\n         if isinstance(seconds, str) and seconds.startswith('/') \\\n                 and not convert(seconds.lstrip('/'), int) % 60 == 0:\n             _LOGGER.warning('Periodic seconds should be divisible with 60'\n                             'there will be an offset every minute')\n         if isinstance(minutes, str) and hours.startswith('/') \\\n                 and not convert(hours.lstrip('/'), int) % 24 == 0:\n             _LOGGER.warning('Periodic hours should be divisible with 24'\n                             'there will be an offset every midnight')\n     else:\n         _LOGGER.error('One of %s, %s, %s OR %s needs to be specified',\n                       CONF_HOURS, CONF_MINUTES, CONF_SECONDS, CONF_AFTER)\n         return False\n",
        "code_toks_joined": "def trigger ( hass , config , action ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if CONF_AFTER in config : <NEWLINE> <INDENT> after = dt_util . parse_time_str ( config [ CONF_AFTER ] ) <NEWLINE> if after is None : <NEWLINE> <INDENT> _error_time ( config [ CONF_AFTER ] , CONF_AFTER ) <NEWLINE> return False <NEWLINE> <DEDENT> hours , minutes , seconds = after . hour , after . minute , after . second <NEWLINE> <DEDENT> elif ( CONF_HOURS in config or CONF_MINUTES in config or <NEWLINE> <INDENT> CONF_SECONDS in config ) : <NEWLINE> hours = config . get ( CONF_HOURS ) <NEWLINE> minutes = config . get ( CONF_MINUTES ) <NEWLINE> seconds = config . get ( CONF_SECONDS ) <NEWLINE> if isinstance ( minutes , str ) and minutes . startswith ( <STRING> ) and not convert ( minutes . lstrip ( <STRING> ) , int ) % 60 == 0 : <NEWLINE> <INDENT> _LOGGER . warning ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if isinstance ( seconds , str ) and seconds . startswith ( <STRING> ) and not convert ( seconds . lstrip ( <STRING> ) , int ) % 60 == 0 : <NEWLINE> <DEDENT> _LOGGER . warning ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if isinstance ( minutes , str ) and hours . startswith ( <STRING> ) and not convert ( hours . lstrip ( <STRING> ) , int ) % 24 == 0 : <NEWLINE> <DEDENT> _LOGGER . warning ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> _LOGGER . error ( <STRING> , <NEWLINE> <INDENT> CONF_HOURS , CONF_MINUTES , CONF_SECONDS , CONF_AFTER ) <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" Listen for state changes based on `config`. \"\"\"",
                "'/'",
                "'/'",
                "'Periodic minutes should be divisible with 60'",
                "'there will be an offset every hour'",
                "'/'",
                "'/'",
                "'Periodic seconds should be divisible with 60'",
                "'there will be an offset every minute'",
                "'/'",
                "'/'",
                "'Periodic hours should be divisible with 24'",
                "'there will be an offset every midnight'",
                "'One of %s, %s, %s OR %s needs to be specified'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7499bca0e2e84bda8f019a702315bba6": {
        "code_string": "for comp_name, discovery in (((BINARY_SENSOR, DISCOVER_BINARY_SENSORS),\n                                   (SENSOR, DISCOVER_SENSORS),\n                                   (LIGHT, DISCOVER_LIGHTS),\n                                   (SWITCH, DISCOVER_SWITCHES))):\n         component = get_component(comp_name)\n         bootstrap.setup_component(hass, component.DOMAIN, config)\n         hass.bus.fire(EVENT_PLATFORM_DISCOVERED,\n                       {ATTR_SERVICE: discovery,\n                        ATTR_DISCOVERED: {}})\n     return True\n",
        "code_toks_joined": "for comp_name , discovery in ( ( ( BINARY_SENSOR , DISCOVER_BINARY_SENSORS ) , <NEWLINE> <INDENT> ( SENSOR , DISCOVER_SENSORS ) , <NEWLINE> ( LIGHT , DISCOVER_LIGHTS ) , <NEWLINE> ( SWITCH , DISCOVER_SWITCHES ) ) ) : <NEWLINE> component = get_component ( comp_name ) <NEWLINE> bootstrap . setup_component ( hass , component . DOMAIN , config ) <NEWLINE> hass . bus . fire ( EVENT_PLATFORM_DISCOVERED , <NEWLINE> { ATTR_SERVICE : discovery , <NEWLINE> ATTR_DISCOVERED : { } } ) <NEWLINE> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eb8d2323899b474bbae845f2f97dc62f": {
        "code_string": "_SINGLE_GROUP_CONFIG = vol.Schema(vol.All(_conf_preprocess, {\n     vol.Optional(CONF_ENTITIES): vol.Any(None, cv.entity_ids),\n     CONF_VIEW: bool,\n     CONF_NAME: str,\n     CONF_ICON: cv.icon,\n }))\n",
        "code_toks_joined": "_SINGLE_GROUP_CONFIG = vol . Schema ( vol . All ( _conf_preprocess , { <NEWLINE> <INDENT> vol . Optional ( CONF_ENTITIES ) : vol . Any ( None , cv . entity_ids ) , <NEWLINE> CONF_VIEW : bool , <NEWLINE> CONF_NAME : str , <NEWLINE> CONF_ICON : cv . icon , <NEWLINE> } ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b85d5e051a6a44dfb29a5fc1ffe7bf93": {
        "code_string": "@property\n     def device_state_attributes(self):\n         \"\"\"Return the state attributes.\"\"\"\n         state_attr = {}\n         if self._ipcam.status_data is not None:\n             return state_attr\n",
        "code_toks_joined": "@ property <NEWLINE> <INDENT> def device_state_attributes ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> state_attr = { } <NEWLINE> if self . _ipcam . status_data is not None : <NEWLINE> <INDENT> return state_attr <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return the state attributes.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b2ab25237d3a4ec086edd95e72430b49": {
        "code_string": "if 'setting' in overlay_data:\n                 setting_data = overlay_data['setting']\n                 setting = setting is not None\n",
        "code_toks_joined": "if <STRING> in overlay_data : <NEWLINE> <INDENT> setting_data = overlay_data [ <STRING> ] <NEWLINE> setting = setting is not None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'setting'",
                "'setting'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "423806e03b974fb9910e8327641200b3": {
        "code_string": "if not targets:\n             # Backward compatibility, notify all devices in own account\n             self._push_data(filepath, message, title, self.pushbullet, url)\n             _LOGGER.info(\"Sent notification to self\")\n             return\n",
        "code_toks_joined": "if not targets : <NEWLINE> <COMMENT> <NL> <INDENT> self . _push_data ( filepath , message , title , self . pushbullet , url ) <NEWLINE> _LOGGER . info ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Backward compatibility, notify all devices in own account"
            ],
            "<STRING>": [
                "\"Sent notification to self\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6ca5e2fd16d64d16b4f4c72ab780332b": {
        "code_string": "# Get all regular switches that are not excluded or marked as lights\n     for device in data.abode.get_devices(generic_type=CONST.TYPE_SWITCH):\n         if data.is_excluded(device) or not data.is_light(device):\n             continue\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for device in data . abode . get_devices ( generic_type = CONST . TYPE_SWITCH ) : <NEWLINE> <INDENT> if data . is_excluded ( device ) or not data . is_light ( device ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Get all regular switches that are not excluded or marked as lights"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5c47604fb99b4e81bff84b0848e630f5": {
        "code_string": "@property\n     def is_on(self):\n         \"\"\"Return true if the binary sensor is on.\"\"\"\n         # True means \"faulted\" or \"open\" or \"abnormal state\"\n         return bool(self._zone['state'] == 'Normal')\n",
        "code_toks_joined": "@ property <NEWLINE> <INDENT> def is_on ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> return bool ( self . _zone [ <STRING> ] == <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return true if the binary sensor is on.\"\"\"",
                "'state'",
                "'Normal'"
            ],
            "<COMMENT>": [
                "# True means \"faulted\" or \"open\" or \"abnormal state\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e1409fd2d8894c37b4f6fb7f47481857": {
        "code_string": "for pmname in coll.supported_values():\n         if config.get(CONF_NAME) is None:\n             name = '{} PM{}'.format(config.get(CONF_NAME), pmname)\n         else:\n             name = 'PM{}'.format(pmname)\n         dev.append(ParticulateMatterSensor(coll, name, pmname))\n",
        "code_toks_joined": "for pmname in coll . supported_values ( ) : <NEWLINE> <INDENT> if config . get ( CONF_NAME ) is None : <NEWLINE> <INDENT> name = <STRING> . format ( config . get ( CONF_NAME ) , pmname ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> name = <STRING> . format ( pmname ) <NEWLINE> <DEDENT> dev . append ( ParticulateMatterSensor ( coll , name , pmname ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'{} PM{}'",
                "'PM{}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cb21f866b3414981b74b6e7f24a33230": {
        "code_string": "# Available devices\n         player_devices = self._player.devices()\n         if player_devices is not None:\n             devices = player_devices.get('devices')\n             if devices is not None:\n                 old_devices = self._devices\n                 self._devices = {self._aliases.get(device.get('id'),\n                                                    device.get('name')):\n                                  device.get('id')\n                                  for device in devices}\n                 device_diff = {name: id for name, id in self._devices.items()\n                                if old_devices.get(name, None) is None}\n                 if device_diff:\n                     _LOGGER.info(\"New Devices: %s\", str(device_diff))\n         # Current playback state\n         current = self._player.current_playback()\n         if current is None:\n             self._state = STATE_IDLE\n             return\n         # Track metadata\n         item = current.get('item')\n         if item:\n             self._album = item.get('album').get('name')\n             self._title = item.get('name')\n             self._artist = ', '.join([artist.get('name')\n                                       for artist in item.get('artists')])\n             self._uri = current.get('uri')\n             images = item.get('album').get('images')\n             self._image_url = images[0].get('url') if images else None\n         # Playing state\n         self._state = STATE_PAUSED\n         if current.get('is_playing'):\n             self._state = STATE_PLAYING\n         self._shuffle = current.get('shuffle_state')\n         device = current.get('device')\n         if device is None:\n             self._state = STATE_IDLE\n         else:\n             if device.get('volume_percent'):\n                 self._volume = device.get('volume_percent') / 100\n             if device.get('name'):\n                 self._current_device = device.get('name')\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> player_devices = self . _player . devices ( ) <NEWLINE> if player_devices is not None : <NEWLINE> <INDENT> devices = player_devices . get ( <STRING> ) <NEWLINE> if devices is not None : <NEWLINE> <INDENT> old_devices = self . _devices <NEWLINE> self . _devices = { self . _aliases . get ( device . get ( <STRING> ) , <NEWLINE> <INDENT> device . get ( <STRING> ) ) : <NEWLINE> device . get ( <STRING> ) <NEWLINE> for device in devices } <NEWLINE> <DEDENT> device_diff = { name : id for name , id in self . _devices . items ( ) <NEWLINE> <INDENT> if old_devices . get ( name , None ) is None } <NEWLINE> <DEDENT> if device_diff : <NEWLINE> <INDENT> _LOGGER . info ( <STRING> , str ( device_diff ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> current = self . _player . current_playback ( ) <NEWLINE> if current is None : <NEWLINE> <INDENT> self . _state = STATE_IDLE <NEWLINE> return <NEWLINE> <COMMENT> <NL> <DEDENT> item = current . get ( <STRING> ) <NEWLINE> if item : <NEWLINE> <INDENT> self . _album = item . get ( <STRING> ) . get ( <STRING> ) <NEWLINE> self . _title = item . get ( <STRING> ) <NEWLINE> self . _artist = <STRING> . join ( [ artist . get ( <STRING> ) <NEWLINE> <INDENT> for artist in item . get ( <STRING> ) ] ) <NEWLINE> <DEDENT> self . _uri = current . get ( <STRING> ) <NEWLINE> images = item . get ( <STRING> ) . get ( <STRING> ) <NEWLINE> self . _image_url = images [ 0 ] . get ( <STRING> ) if images else None <NEWLINE> <COMMENT> <NL> <DEDENT> self . _state = STATE_PAUSED <NEWLINE> if current . get ( <STRING> ) : <NEWLINE> <INDENT> self . _state = STATE_PLAYING <NEWLINE> <DEDENT> self . _shuffle = current . get ( <STRING> ) <NEWLINE> device = current . get ( <STRING> ) <NEWLINE> if device is None : <NEWLINE> <INDENT> self . _state = STATE_IDLE <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if device . get ( <STRING> ) : <NEWLINE> <INDENT> self . _volume = device . get ( <STRING> ) / 100 <NEWLINE> <DEDENT> if device . get ( <STRING> ) : <NEWLINE> <INDENT> self . _current_device = device . get ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Available devices",
                "# Current playback state",
                "# Track metadata",
                "# Playing state"
            ],
            "<STRING>": [
                "'devices'",
                "'id'",
                "'name'",
                "'id'",
                "\"New Devices: %s\"",
                "'item'",
                "'album'",
                "'name'",
                "'name'",
                "', '",
                "'name'",
                "'artists'",
                "'uri'",
                "'album'",
                "'images'",
                "'url'",
                "'is_playing'",
                "'shuffle_state'",
                "'device'",
                "'volume_percent'",
                "'volume_percent'",
                "'name'",
                "'name'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5f0f7c8bf0694330893cb6809bd44b8b": {
        "code_string": "self.assertEqual(mock_add_bridge_acc.mock_calls, [call(state)])\n         self.assertEqual(mock_show_setup_msg.mock_calls, [\n             call(homekit.bridge, self.hass)])\n         self.assertEqual(homekit.driver.mock_calls, [call.start()])\n         self.assertTrue(homekit.started)\n",
        "code_toks_joined": "self . assertEqual ( mock_add_bridge_acc . mock_calls , [ call ( state ) ] ) <NEWLINE> <INDENT> self . assertEqual ( mock_show_setup_msg . mock_calls , [ <NEWLINE> <INDENT> call ( homekit . bridge , self . hass ) ] ) <NEWLINE> <DEDENT> self . assertEqual ( homekit . driver . mock_calls , [ call . start ( ) ] ) <NEWLINE> self . assertTrue ( homekit . started ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ad731edb6bf2439a8d32a64447fde6b6": {
        "code_string": "to_remove = []\n         for listener_ref in new.update_listeners:\n             listener = listener_ref()\n             if listener is None:\n                 to_remove.append(listener)\n             else:\n                 try:\n                     listener.async_registry_updated(old, new)\n                 except Exception:  # pylint: disable=broad-except\n                     _LOGGER.exception('Error calling update listener')\n",
        "code_toks_joined": "to_remove = [ ] <NEWLINE> <INDENT> for listener_ref in new . update_listeners : <NEWLINE> <INDENT> listener = listener_ref ( ) <NEWLINE> if listener is None : <NEWLINE> <INDENT> to_remove . append ( listener ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> listener . async_registry_updated ( old , new ) <NEWLINE> <DEDENT> except Exception : <COMMENT> <NEWLINE> <INDENT> _LOGGER . exception ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# pylint: disable=broad-except"
            ],
            "<STRING>": [
                "'Error calling update listener'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "515f82bc3708447694ceb39f1b9fd77d": {
        "code_string": "domain_exposed_by_default = \\\n             expose_by_default and entity.domain in exposed_domains\n",
        "code_toks_joined": "domain_exposed_by_default = expose_by_default and entity . domain in exposed_domains <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fb8c88759cb04efcaf200684d6be829e": {
        "code_string": "domain_exposed_by_default = \\\n             expose_by_default or entity.domain in exposed_domains\n",
        "code_toks_joined": "domain_exposed_by_default = expose_by_default or entity . domain in exposed_domains <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ca8d6a61e5ef41f180652149906ef5fd": {
        "code_string": "@HANDLERS.register('state')\n async def async_handle_state_update(hass, context, msg):\n     \"\"\"Handle a binary sensor state update.\"\"\"\n     _LOGGER.debug(\"[state handler] context: %s  msg: %s\", context, msg)\n     entity_id = context.get(ATTR_ENTITY_ID)\n     state = bool(int(msg.get(ATTR_STATE)))\n     if msg.get(CONF_INVERSE):\n         state = not state\n",
        "code_toks_joined": "@ HANDLERS . register ( <STRING> ) <NEWLINE> <INDENT> async def async_handle_state_update ( hass , context , msg ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> _LOGGER . debug ( <STRING> , context , msg ) <NEWLINE> entity_id = context . get ( ATTR_ENTITY_ID ) <NEWLINE> state = bool ( int ( msg . get ( ATTR_STATE ) ) ) <NEWLINE> if msg . get ( CONF_INVERSE ) : <NEWLINE> <INDENT> state = not state <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'state'",
                "\"\"\"Handle a binary sensor state update.\"\"\"",
                "\"[state handler] context: %s  msg: %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "80ce92eadfb144ba8624a290975c6b5b": {
        "code_string": "if stage_1_domains:\n         await asyncio.gather(*[\n             async_setup_component(hass, domain, config)\n             for domain in logging_domains\n         ])\n",
        "code_toks_joined": "if stage_1_domains : <NEWLINE> <INDENT> await asyncio . gather ( * [ <NEWLINE> <INDENT> async_setup_component ( hass , domain , config ) <NEWLINE> for domain in logging_domains <NEWLINE> <DEDENT> ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "38d9cadb563746eaa23374da6fb01a7d": {
        "code_string": "if _token_info:\n         await store.async_save(token_info)\n         token_info = _token_info\n",
        "code_toks_joined": "if _token_info : <NEWLINE> <INDENT> await store . async_save ( token_info ) <NEWLINE> token_info = _token_info <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5f4e7807c80940668bb35b06b79c82c8": {
        "code_string": "if self.exclude is not None:\n                     routes = {k: v for k, v in routes.items() if\n                               self.exclude.lower() in k.lower()}\n",
        "code_toks_joined": "if self . exclude is not None : <NEWLINE> <INDENT> routes = { k : v for k , v in routes . items ( ) if <NEWLINE> <INDENT> self . exclude . lower ( ) in k . lower ( ) } <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "18d748ffa08944c68500464ae564e971": {
        "code_string": "async def _async_registry_updated(self, event):\n         \"\"\"Handle entity registry update.\"\"\"\n         data = event.data\n         if data['action'] != 'update' and data.get(\n                 'old_entity_id', data['entity_id']) != self.entity_id:\n             return\n",
        "code_toks_joined": "async def _async_registry_updated ( self , event ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> data = event . data <NEWLINE> if data [ <STRING> ] != <STRING> and data . get ( <NEWLINE> <INDENT> <STRING> , data [ <STRING> ] ) != self . entity_id : <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Handle entity registry update.\"\"\"",
                "'action'",
                "'update'",
                "'old_entity_id'",
                "'entity_id'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "26439bd6dd23436999702900150d0227": {
        "code_string": "vane_vertical = self._device.vane_vertical\n         if vane_horizontal:\n             attr.update(\n                 {\n                     ATTR_VANE_VERTICAL: vane_vertical,\n                     ATTR_VANE_VERTICAL_POSITIONS: self._device.vane_vertical_positions,\n                 }\n             )\n         return attr\n",
        "code_toks_joined": "vane_vertical = self . _device . vane_vertical <NEWLINE> <INDENT> if vane_horizontal : <NEWLINE> <INDENT> attr . update ( <NEWLINE> <INDENT> { <NEWLINE> <INDENT> ATTR_VANE_VERTICAL : vane_vertical , <NEWLINE> ATTR_VANE_VERTICAL_POSITIONS : self . _device . vane_vertical_positions , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return attr <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2296032b09b4408296444a4fee2bf825": {
        "code_string": "for conf in config[DOMAIN]:\n         protocol = \"https\" if config[CONF_SSL] else \"http\"\n",
        "code_toks_joined": "for conf in config [ DOMAIN ] : <NEWLINE> <INDENT> protocol = <STRING> if config [ CONF_SSL ] else <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"https\"",
                "\"http\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a0d20d3df8c9424085489a541f0d73ef": {
        "code_string": "conf = config[DOMAIN]\n     name = conf[CONF_NAME]\n     port = conf[CONF_PORT]\n     ip_address = conf.get(CONF_IP_ADDRESS)\n     advertise_ip = conf.get(CONF_ADVERTISE_IP)\n     auto_start = conf[CONF_AUTO_START]\n     safe_mode = conf[CONF_SAFE_MODE]\n     entity_filter = conf[CONF_FILTER]\n     entity_config = conf[CONF_ENTITY_CONFIG]\n     interface_choice = (\n         InterfaceChoice.Default if config.get(CONF_ZEROCONF_DEFAULT_INTERFACE) else None\n     )\n",
        "code_toks_joined": "conf = config [ DOMAIN ] <NEWLINE> <INDENT> name = conf [ CONF_NAME ] <NEWLINE> port = conf [ CONF_PORT ] <NEWLINE> ip_address = conf . get ( CONF_IP_ADDRESS ) <NEWLINE> advertise_ip = conf . get ( CONF_ADVERTISE_IP ) <NEWLINE> auto_start = conf [ CONF_AUTO_START ] <NEWLINE> safe_mode = conf [ CONF_SAFE_MODE ] <NEWLINE> entity_filter = conf [ CONF_FILTER ] <NEWLINE> entity_config = conf [ CONF_ENTITY_CONFIG ] <NEWLINE> interface_choice = ( <NEWLINE> <INDENT> InterfaceChoice . Default if config . get ( CONF_ZEROCONF_DEFAULT_INTERFACE ) else None <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3cbfd75a917247958f55a1f2237f0a46": {
        "code_string": "async def async_update(self):\n         \"\"\"Also update the sensor when available.\"\"\"\n         await super().async_update()\n         if self.sensor is None:\n             await self.hass.async_add_executor_job(self.sensor.update)\n",
        "code_toks_joined": "async def async_update ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> await super ( ) . async_update ( ) <NEWLINE> if self . sensor is None : <NEWLINE> <INDENT> await self . hass . async_add_executor_job ( self . sensor . update ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Also update the sensor when available.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ee96c8dc13ce47ae8db4b878e153a900": {
        "code_string": "shark_vacs = await ayla_api.async_get_devices(False)\n     device_names = \", \".join([d.name for d in shark_vacs])\n     LOGGER.debug(\"Found %d Shark IQ device(s): %s\", len(device_names), device_names)\n     coordinator = SharkIqUpdateCoordinator(hass, config_entry, ayla_api, shark_vacs)\n",
        "code_toks_joined": "shark_vacs = await ayla_api . async_get_devices ( False ) <NEWLINE> <INDENT> device_names = <STRING> . join ( [ d . name for d in shark_vacs ] ) <NEWLINE> LOGGER . debug ( <STRING> , len ( device_names ) , device_names ) <NEWLINE> coordinator = SharkIqUpdateCoordinator ( hass , config_entry , ayla_api , shark_vacs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\", \"",
                "\"Found %d Shark IQ device(s): %s\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "731c837ede054884beb9b4b8460fae4f": {
        "code_string": "def run_validator_for_test_file(filename: str) -> List:\n     test_file_path = os.path.join(\n         os.path.dirname(os.path.abspath(__file__)),\n         'test_files',\n         filename,\n     )\n     with open(test_file_path, 'r') as file_handler:\n         raw_content = file_handler.read()\n     tree = ast.parse(raw_content)\n     checker = SuperMarionChecker(tree=tree, filename=filename)\n",
        "code_toks_joined": "def run_validator_for_test_file ( filename : str ) -> List : <NEWLINE> <INDENT> test_file_path = os . path . join ( <NEWLINE> <INDENT> os . path . dirname ( os . path . abspath ( __file__ ) ) , <NEWLINE> <STRING> , <NEWLINE> filename , <NEWLINE> <DEDENT> ) <NEWLINE> with open ( test_file_path , <STRING> ) as file_handler : <NEWLINE> <INDENT> raw_content = file_handler . read ( ) <NEWLINE> <DEDENT> tree = ast . parse ( raw_content ) <NEWLINE> checker = SuperMarionChecker ( tree = tree , filename = filename ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'test_files'",
                "'r'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c603e5104a7a457aae2477fc8f8d18ed": {
        "code_string": "# check the stopping conditions\n             if self._is_stop():\n                 break\n         return x\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if self . _is_stop ( ) : <NEWLINE> <INDENT> break <NEWLINE> return x <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# check the stopping conditions"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b77ce57564c046cbaab0700cd340d08b": {
        "code_string": "profile, created = Profile.objects.update_or_create(\n             user=user,\n             defaults={\n                 \"slack_api_id\": slack_profile[\"id\"],\n                 \"politico_title\": slack_profile.get(\"title\", \"Staff writer\"),\n             },\n         )\n",
        "code_toks_joined": "profile , created = Profile . objects . update_or_create ( <NEWLINE> <INDENT> user = user , <NEWLINE> defaults = { <NEWLINE> <INDENT> <STRING> : slack_profile [ <STRING> ] , <NEWLINE> <STRING> : slack_profile . get ( <STRING> , <STRING> ) , <NEWLINE> <DEDENT> } , <NEWLINE> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"slack_api_id\"",
                "\"id\"",
                "\"politico_title\"",
                "\"title\"",
                "\"Staff writer\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "04c943f153c64d1b9dea238a1597760f": {
        "code_string": "with patch('add_time.json') as oauth2_mock:\n             r = self.oauth_client.add(add_data, params={'access_token': self.TEST_ACCESS})\n             add_mock.assert_called_once_with('POST', '/daily/add', add_data, params={'access_token':self.TEST_ACCESS})\n             self.assertTrue(r)\n",
        "code_toks_joined": "with patch ( <STRING> ) as oauth2_mock : <NEWLINE> <INDENT> r = self . oauth_client . add ( add_data , params = { <STRING> : self . TEST_ACCESS } ) <NEWLINE> add_mock . assert_called_once_with ( <STRING> , <STRING> , add_data , params = { <STRING> : self . TEST_ACCESS } ) <NEWLINE> self . assertTrue ( r ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'add_time.json'",
                "'access_token'",
                "'POST'",
                "'/daily/add'",
                "'access_token'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "81ab3975ae9d4273bebc4151c563f8a2": {
        "code_string": "if step < self.dihstep:\n             self.set_dihedrals(change, fix, cut=1)\n",
        "code_toks_joined": "if step < self . dihstep : <NEWLINE> <INDENT> self . set_dihedrals ( change , fix , cut = 1 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "db0c49fb642a4b44bc5b500f9b959558": {
        "code_string": "for ins in korcek_chain:\n                 if bond[ins[0]][ins[-1]] == 1:  # it is a ring\n                     rxns += [ins]\n",
        "code_toks_joined": "for ins in korcek_chain : <NEWLINE> <INDENT> if bond [ ins [ 0 ] ] [ ins [ - 1 ] ] == 1 : <COMMENT> <NEWLINE> <INDENT> rxns += [ ins ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# it is a ring"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "385cb0e588f5441f9666dbc926dc0e3f": {
        "code_string": "kwargs.pop('self', None)\n         pop = self._population\n         coords = pop.projection(which, 2, **coords_kwargs)\n         return self.scatter_coords(coords, **scatter_kwargs)\n",
        "code_toks_joined": "kwargs . pop ( <STRING> , None ) <NEWLINE> <INDENT> pop = self . _population <NEWLINE> coords = pop . projection ( which , 2 , ** coords_kwargs ) <NEWLINE> return self . scatter_coords ( coords , ** scatter_kwargs ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'self'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7eae4f74e4e24678afb75894e6b48040": {
        "code_string": "@functools.wraps(func)\n     def decorated(file, *args, **kwargs):\n         if isinstance(file, str):\n             with open(file) as F:\n                 result = func(file, *args, **kwargs)\n             return result\n         else:\n             return func(file, *args, **kwargs)\n",
        "code_toks_joined": "@ functools . wraps ( func ) <NEWLINE> <INDENT> def decorated ( file , * args , ** kwargs ) : <NEWLINE> <INDENT> if isinstance ( file , str ) : <NEWLINE> <INDENT> with open ( file ) as F : <NEWLINE> <INDENT> result = func ( file , * args , ** kwargs ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return func ( file , * args , ** kwargs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d95014cd9f3f4d0eb1efd0e16c2b281c": {
        "code_string": "if _centre_list[i].covariance is not None:\n                 sensor_list[i].covariance = interpolate_covariance(\n                     sensor_list[i].epoch_timestamp,\n                     _centre_list[j-1].epoch_timestamp,\n                     _centre_list[j].epoch_timestamp,\n                     _centre_list[j-1].covariance,\n                     _centre_list[j].covariance)\n",
        "code_toks_joined": "if _centre_list [ i ] . covariance is not None : <NEWLINE> <INDENT> sensor_list [ i ] . covariance = interpolate_covariance ( <NEWLINE> <INDENT> sensor_list [ i ] . epoch_timestamp , <NEWLINE> _centre_list [ j - 1 ] . epoch_timestamp , <NEWLINE> _centre_list [ j ] . epoch_timestamp , <NEWLINE> _centre_list [ j - 1 ] . covariance , <NEWLINE> _centre_list [ j ] . covariance ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d0029531a86b4af0b00bfd497e924139": {
        "code_string": "for j in plotly_list:\n             make_frame(frame,\n                        [j[0],\n                         [float(k.epoch_timestamp) for k in j[1]],\n                         [float(k.eastings) for k in j[1]],\n                         [float(k.northings) for k in j[1]]],\n                        i)\n         if len(camera1_pf_list) > 1:\n             make_frame(frame,\n                        ['pf_camera1',\n                         [float(i.epoch_timestamp) for i in camera1_pf_list],\n                         [float(i.eastings) for i in camera1_pf_list],\n                         [float(i.northings) for i in camera1_pf_list]],\n                        i)\n         if len(pf_fusion_centre_list) > 1:\n             make_frame(frame,\n                        ['pf_centre',\n                         [float(i.epoch_timestamp) for i in pf_fusion_centre_list],\n                         [float(i.eastings) for i in pf_fusion_centre_list],\n                         [float(i.northings) for i in pf_fusion_centre_list]],\n                        i)\n         if len(ekf_centre_list) > 1:\n             make_frame(frame,\n                        ['ekf_centre',\n                         [float(i.epoch_timestamp) for i in ekf_centre_list],\n                         [float(i.eastings) for i in ekf_centre_list],\n                         [float(i.northings) for i in ekf_centre_list]],\n                        i)\n         if len(pf_fusion_dvl_list) > 1:\n             make_frame(frame,\n                        ['pf_dvl',\n                         [float(i.epoch_timestamp) for i in pf_fusion_dvl_list],\n                         [float(i.eastings) for i in pf_fusion_dvl_list],\n                         [float(i.northings) for i in pf_fusion_dvl_list]],\n                        i)\n         if len(pf_timestamps_interval) > 1:\n             make_frame(frame,\n                        ['pf_dvl_distribution',\n                         pf_timestamps_interval,\n                         pf_eastings_interval,\n                         pf_northings_interval],\n                        i, mode='markers')\n",
        "code_toks_joined": "for j in plotly_list : <NEWLINE> <INDENT> make_frame ( frame , <NEWLINE> <INDENT> [ j [ 0 ] , <NEWLINE> <INDENT> [ float ( k . epoch_timestamp ) for k in j [ 1 ] ] , <NEWLINE> [ float ( k . eastings ) for k in j [ 1 ] ] , <NEWLINE> [ float ( k . northings ) for k in j [ 1 ] ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( camera1_pf_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in camera1_pf_list ] , <NEWLINE> [ float ( i . eastings ) for i in camera1_pf_list ] , <NEWLINE> [ float ( i . northings ) for i in camera1_pf_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( pf_fusion_centre_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in pf_fusion_centre_list ] , <NEWLINE> [ float ( i . eastings ) for i in pf_fusion_centre_list ] , <NEWLINE> [ float ( i . northings ) for i in pf_fusion_centre_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( ekf_centre_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in ekf_centre_list ] , <NEWLINE> [ float ( i . eastings ) for i in ekf_centre_list ] , <NEWLINE> [ float ( i . northings ) for i in ekf_centre_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( pf_fusion_dvl_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in pf_fusion_dvl_list ] , <NEWLINE> [ float ( i . eastings ) for i in pf_fusion_dvl_list ] , <NEWLINE> [ float ( i . northings ) for i in pf_fusion_dvl_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( pf_timestamps_interval ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> pf_timestamps_interval , <NEWLINE> pf_eastings_interval , <NEWLINE> pf_northings_interval ] , <NEWLINE> <DEDENT> i , mode = <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'pf_camera1'",
                "'pf_centre'",
                "'ekf_centre'",
                "'pf_dvl'",
                "'pf_dvl_distribution'",
                "'markers'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "541516bcfdca451091079d113b2819ad": {
        "code_string": "if ic > best_ic:\n             best_ic = ic\n             best_model = m\n             if ic > goal_inliers and stop_at_goal:\n                 break\n     # estimate final model using all inliers\n     m = fit_plane(inliers)\n     return best_model, inliers, i\n",
        "code_toks_joined": "if ic > best_ic : <NEWLINE> <INDENT> best_ic = ic <NEWLINE> best_model = m <NEWLINE> if ic > goal_inliers and stop_at_goal : <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> m = fit_plane ( inliers ) <NEWLINE> return best_model , inliers , i <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# estimate final model using all inliers"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "53fb2cc9f7e84d6bb2988f4f45842fea": {
        "code_string": "planes = []\n         for i in range(0, self.num_iterations):\n             point_cloud_local = random.sample(inliers_cloud_list, cloud_sample_size)\n             total_no_points = len(point_cloud_local)\n             p = Plane([1, 0, 0, 1.5])\n             m = p.fit_non_robust(cloud)\n             \"\"\"\n             m, _ = plane_fitting_ransac(\n                 point_cloud_local,\n                 min_distance_threshold=self.mdt,\n                 sample_size=self.ssp*total_no_points,\n                 goal_inliers=total_no_points*self.gip,\n                 max_iterations=self.max_iterations,\n                 plot=False)\n             \"\"\"\n             angle, pitch, yaw = get_angles(m[0:3])\n             planes.append([angle, pitch, yaw])\n             Console.progress(i, self.num_iterations, prefix='Iterating planes')\n",
        "code_toks_joined": "planes = [ ] <NEWLINE> <INDENT> for i in range ( 0 , self . num_iterations ) : <NEWLINE> <INDENT> point_cloud_local = random . sample ( inliers_cloud_list , cloud_sample_size ) <NEWLINE> total_no_points = len ( point_cloud_local ) <NEWLINE> p = Plane ( [ 1 , 0 , 0 , 1.5 ] ) <NEWLINE> m = p . fit_non_robust ( cloud ) <NEWLINE> <STRING> <NEWLINE> angle , pitch , yaw = get_angles ( m [ 0 : 3 ] ) <NEWLINE> planes . append ( [ angle , pitch , yaw ] ) <NEWLINE> Console . progress ( i , self . num_iterations , prefix = <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n             m, _ = plane_fitting_ransac(\n                 point_cloud_local,\n                 min_distance_threshold=self.mdt,\n                 sample_size=self.ssp*total_no_points,\n                 goal_inliers=total_no_points*self.gip,\n                 max_iterations=self.max_iterations,\n                 plot=False)\n             \"\"\"",
                "'Iterating planes'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "378123748249450da515cd69624d0f60": {
        "code_string": "# Choose Epsilon according to method provided.\n         if isinstance(epsilon, numbers.Number):  # if user provided.\n             self.epsilon_fitted = epsilon\n             return self\n         elif epsilon == 'bgh':  # Berry, Giannakis Harlim method.\n             if (self.metric != 'euclidean'):  # TODO : replace with call to scipy metrics.\n                 warnings.warn('The BGH method for choosing epsilon assumes a euclidean metric.  However, the metric being used is %s.  Proceed at your own risk...' % self.metric)\n             if self.scaled_dists is not None:\n                 self.scaled_dists = self._get_scaled_distance_mat(self.data, self.bandwidths)\n             self.epsilon_fitted, self.d = choose_optimal_epsilon_BGH(self.scaled_dists.data**2)\n         else:\n             raise ValueError(\"Method for automatically choosing epsilon was given as %s, but this was not recognized\" % epsilon)\n         return self\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if isinstance ( epsilon , numbers . Number ) : <COMMENT> <NEWLINE> <INDENT> self . epsilon_fitted = epsilon <NEWLINE> return self <NEWLINE> <DEDENT> elif epsilon == <STRING> : <COMMENT> <NEWLINE> <INDENT> if ( self . metric != <STRING> ) : <COMMENT> <NEWLINE> <INDENT> warnings . warn ( <STRING> % self . metric ) <NEWLINE> <DEDENT> if self . scaled_dists is not None : <NEWLINE> <INDENT> self . scaled_dists = self . _get_scaled_distance_mat ( self . data , self . bandwidths ) <NEWLINE> <DEDENT> self . epsilon_fitted , self . d = choose_optimal_epsilon_BGH ( self . scaled_dists . data ** 2 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> % epsilon ) <NEWLINE> <DEDENT> return self <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Choose Epsilon according to method provided.",
                "# if user provided.",
                "# Berry, Giannakis Harlim method.",
                "# TODO : replace with call to scipy metrics."
            ],
            "<STRING>": [
                "'bgh'",
                "'euclidean'",
                "'The BGH method for choosing epsilon assumes a euclidean metric.  However, the metric being used is %s.  Proceed at your own risk...'",
                "\"Method for automatically choosing epsilon was given as %s, but this was not recognized\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e0569a3a6fc34a2a8ff6d23e3082f8a1": {
        "code_string": "def get_devices(self, location):\n         response_data = self.__call_smart_system_get(\n             f\"{self.SMART_HOST}/v1/locations/{location.data['id']}\"\n         )\n         if len(response_data[\"data\"][\"relationships\"][\"devices\"][\"data\"]) < 1:\n             self.logger.error(\"No device found....\")\n         else:\n             devices_smart_system = {}\n             for device in response_data[\"included\"]:\n                 real_id = device[\"id\"].split(\":\")[0]\n                 if real_id not in devices_smart_system:\n                     devices_smart_system[real_id] = {}\n                 if (\n                     device[\"type\"] in self.supported_services\n                     and device[\"type\"] not in devices_smart_system[real_id]\n                 ):\n                     devices_smart_system[real_id][device[\"type\"]] = []\n                 devices_smart_system[real_id][device[\"type\"]].append(device)\n             for parsed_device in devices_smart_system.values():\n                 location.add_device(DeviceFactory.build(self, device))\n",
        "code_toks_joined": "def get_devices ( self , location ) : <NEWLINE> <INDENT> response_data = self . __call_smart_system_get ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> if len ( response_data [ <STRING> ] [ <STRING> ] [ <STRING> ] [ <STRING> ] ) < 1 : <NEWLINE> <INDENT> self . logger . error ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> devices_smart_system = { } <NEWLINE> for device in response_data [ <STRING> ] : <NEWLINE> <INDENT> real_id = device [ <STRING> ] . split ( <STRING> ) [ 0 ] <NEWLINE> if real_id not in devices_smart_system : <NEWLINE> <INDENT> devices_smart_system [ real_id ] = { } <NEWLINE> <DEDENT> if ( <NEWLINE> <INDENT> device [ <STRING> ] in self . supported_services <NEWLINE> and device [ <STRING> ] not in devices_smart_system [ real_id ] <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> devices_smart_system [ real_id ] [ device [ <STRING> ] ] = [ ] <NEWLINE> <DEDENT> devices_smart_system [ real_id ] [ device [ <STRING> ] ] . append ( device ) <NEWLINE> <DEDENT> for parsed_device in devices_smart_system . values ( ) : <NEWLINE> <INDENT> location . add_device ( DeviceFactory . build ( self , device ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"{self.SMART_HOST}/v1/locations/{location.data['id']}\"",
                "\"data\"",
                "\"relationships\"",
                "\"devices\"",
                "\"data\"",
                "\"No device found....\"",
                "\"included\"",
                "\"id\"",
                "\":\"",
                "\"type\"",
                "\"type\"",
                "\"type\"",
                "\"type\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "66adebc82a414441a0b81bf85cd5b923": {
        "code_string": "def try_to_create(self):\n         limit = datetime.now() - timedelta(seconds=self.settings['SNAPSHOT_INTERVAL'])\n         if 'date' in self.last_snapshot or self.last_snapshot['date'] <= limit:\n             self.create()\n",
        "code_toks_joined": "def try_to_create ( self ) : <NEWLINE> <INDENT> limit = datetime . now ( ) - timedelta ( seconds = self . settings [ <STRING> ] ) <NEWLINE> if <STRING> in self . last_snapshot or self . last_snapshot [ <STRING> ] <= limit : <NEWLINE> <INDENT> self . create ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'SNAPSHOT_INTERVAL'",
                "'date'",
                "'date'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6b0937a0181146d6a28bfae4d31b1c61": {
        "code_string": "class PubsubQueue(object):\n     def __init__(self, queue_name, sub_name=None, verbose=10):\n         assert 'GOOGLE_APPLICATION_CREDENTIALS' in os.environ.keys()\n         self.client = pubsub.Client()\n         self.topic = self.client.topic(queue_name)\n         self.logger = logging.getLogger(self.__class__.__name__)\n         self.logger.setLevel(verbose)\n         sub_name = sub_name if sub_name else queue_name + \"_sub\"\n         self.logger.info(\"Topic name = {}\".format(queue_name))\n         self.logger.info(\"Subscription name = {}\".format(queue_name))\n         if queue_name not in [t.name for t in self.client.list_topics()]:\n             self.topic.create()\n             self.logger.info('topic {} created'.format(queue_name))\n",
        "code_toks_joined": "class PubsubQueue ( object ) : <NEWLINE> <INDENT> def __init__ ( self , queue_name , sub_name = None , verbose = 10 ) : <NEWLINE> <INDENT> assert <STRING> in os . environ . keys ( ) <NEWLINE> self . client = pubsub . Client ( ) <NEWLINE> self . topic = self . client . topic ( queue_name ) <NEWLINE> self . logger = logging . getLogger ( self . __class__ . __name__ ) <NEWLINE> self . logger . setLevel ( verbose ) <NEWLINE> sub_name = sub_name if sub_name else queue_name + <STRING> <NEWLINE> self . logger . info ( <STRING> . format ( queue_name ) ) <NEWLINE> self . logger . info ( <STRING> . format ( queue_name ) ) <NEWLINE> if queue_name not in [ t . name for t in self . client . list_topics ( ) ] : <NEWLINE> <INDENT> self . topic . create ( ) <NEWLINE> self . logger . info ( <STRING> . format ( queue_name ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'GOOGLE_APPLICATION_CREDENTIALS'",
                "\"_sub\"",
                "\"Topic name = {}\"",
                "\"Subscription name = {}\"",
                "'topic {} created'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "179cee6c4d5847d3ac56fc08670a946d": {
        "code_string": "# If we are not averaging we wait for all records to be acquired.\n             if not n_records or (not average and\n                                  n_records < retrieved_records):\n                 time.sleep(1e-6)\n                 continue\n             if not get_data(cu, id_, buffers_ptr,\n                             n_records*samples_per_record,\n                             bytes_per_sample,\n                             retrieved_records,\n                             n_records,\n                             mask,\n                             0,\n                             samples_per_record,\n                             0x00):\n                 del avg, buffers\n                 self._dll.DisarmTrigger(self._cu_id, self._id)\n                 self._dll.MultiRecordClose(self._cu_id, self._id)\n                 self.close_connection()\n                 self._setup_library()\n                 self.open_connection()\n                 self.configure_board()\n                 if retry:\n                     return self.get_traces(channels, duration, delay,\n                                            records_per_capture, False)\n                 else:\n                     msg = 'Failed to retrieve data from ADQ14'\n                     raise RuntimeError(msg)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if not n_records or ( not average and <NEWLINE> <INDENT> n_records < retrieved_records ) : <NEWLINE> time . sleep ( 1e-6 ) <NEWLINE> continue <NEWLINE> <DEDENT> if not get_data ( cu , id_ , buffers_ptr , <NEWLINE> <INDENT> n_records * samples_per_record , <NEWLINE> bytes_per_sample , <NEWLINE> retrieved_records , <NEWLINE> n_records , <NEWLINE> mask , <NEWLINE> 0 , <NEWLINE> samples_per_record , <NEWLINE> 0x00 ) : <NEWLINE> del avg , buffers <NEWLINE> self . _dll . DisarmTrigger ( self . _cu_id , self . _id ) <NEWLINE> self . _dll . MultiRecordClose ( self . _cu_id , self . _id ) <NEWLINE> self . close_connection ( ) <NEWLINE> self . _setup_library ( ) <NEWLINE> self . open_connection ( ) <NEWLINE> self . configure_board ( ) <NEWLINE> if retry : <NEWLINE> return self . get_traces ( channels , duration , delay , <NEWLINE> <INDENT> records_per_capture , False ) <NEWLINE> else : <NEWLINE> msg = <STRING> <NEWLINE> raise RuntimeError ( msg ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If we are not averaging we wait for all records to be acquired."
            ],
            "<STRING>": [
                "'Failed to retrieve data from ADQ14'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e7623524e59f4bce814f9431cadf5149": {
        "code_string": "alignment = calculate_alignment(orient_tile)\n",
        "code_toks_joined": "alignment = calculate_alignment ( orient_tile ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "690bec4b044343e8acbf930c46a3fcb7": {
        "code_string": "if downsample:\n                 fixed_shrunk = trans.resize_image(fixed_image, fixed_image.GetSpacing()[0], downsample_target)\n                 rotated_shrunk = trans.resize_image(rotated_image, moving_image.GetSpacing()[0], downsample_target)\n                 spacing = fixed_shrunk.GetSpacing()\n",
        "code_toks_joined": "if downsample : <NEWLINE> <INDENT> fixed_shrunk = trans . resize_image ( fixed_image , fixed_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> rotated_shrunk = trans . resize_image ( rotated_image , moving_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> spacing = fixed_shrunk . GetSpacing ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ce36f7e3ee784cffb35720ec479bd329": {
        "code_string": "if roi_size is None:\n                 with open(output_path, 'w', newline='') as csvfile:\n                         print('\\nWriting average retardance file for {} at tile size {}'.format(\n                                 output_path.name, tile_size[0]))\n                         writer = csv.writer(csvfile)\n                         writer.writerow(['Mouse', 'Slide', 'Modality', 'Tile',\n                                          'Retardance', 'Orientation', 'Alignment'])\n",
        "code_toks_joined": "if roi_size is None : <NEWLINE> <INDENT> with open ( output_path , <STRING> , newline = <STRING> ) as csvfile : <NEWLINE> <INDENT> print ( <STRING> . format ( <NEWLINE> <INDENT> output_path . name , tile_size [ 0 ] ) ) <NEWLINE> <DEDENT> writer = csv . writer ( csvfile ) <NEWLINE> writer . writerow ( [ <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'w'",
                "''",
                "'\\nWriting average retardance file for {} at tile size {}'",
                "'Mouse'",
                "'Slide'",
                "'Modality'",
                "'Tile'",
                "'Retardance'",
                "'Orientation'",
                "'Alignment'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "78963c6f3fa8465d954e13a766eb41e3": {
        "code_string": "def fieldStrAndPer(d):\n     \"\"\" \u628a\u5b57\u5178\u4e2d\u6240\u6709\u975e\u7a7a\u5b57\u6bb5\u62fc\u6210\u00a0`k1`=%s,\u00a0`k2`=%s...\u548c[v1,\u00a0v2...]\u4e24\u4e2a\u5b57\u6bb5\n \u00a0\u00a0\u00a0\u00a0--\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0@example\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fieldStrAndPer({'name':'\u5f20\u4e09',\u00a0'age':18})\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return:\u00a0(' `name`=%s  ,  `age`=%s ', ['\u5f20\u4e09', '18'])\n     \"\"\"\n     l1 = []\n     lper = []\n     l2 = []\n     for k, v in d.items():\n         if v != None:\n             l1.append(k)\n             noAppend = True # \u6807\u8bb0lper\u548cl2\u8fd8\u672a\u8d4b\u503c\n             if isinstance(l2, str):\n                 if v.startswith('+') or v.startswith('-') or v.startswith('*') or v.startswith('/'):\n                     vv = dataToFloat(v[1:])\n                     if vv:\n                         noAppend = False\n                         lper.append('{}{}%s'.format(k, v[:1]))\n                         l2.append(vv)\n             if noAppend:\n                 lper.append('%s')\n                 l2.append(dataToStr(v))\n",
        "code_toks_joined": "def fieldStrAndPer ( d ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> l1 = [ ] <NEWLINE> lper = [ ] <NEWLINE> l2 = [ ] <NEWLINE> for k , v in d . items ( ) : <NEWLINE> <INDENT> if v != None : <NEWLINE> <INDENT> l1 . append ( k ) <NEWLINE> noAppend = True <COMMENT> <NEWLINE> if isinstance ( l2 , str ) : <NEWLINE> <INDENT> if v . startswith ( <STRING> ) or v . startswith ( <STRING> ) or v . startswith ( <STRING> ) or v . startswith ( <STRING> ) : <NEWLINE> <INDENT> vv = dataToFloat ( v [ 1 : ] ) <NEWLINE> if vv : <NEWLINE> <INDENT> noAppend = False <NEWLINE> lper . append ( <STRING> . format ( k , v [ : 1 ] ) ) <NEWLINE> l2 . append ( vv ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if noAppend : <NEWLINE> <INDENT> lper . append ( <STRING> ) <NEWLINE> l2 . append ( dataToStr ( v ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" \u628a\u5b57\u5178\u4e2d\u6240\u6709\u975e\u7a7a\u5b57\u6bb5\u62fc\u6210\u00a0`k1`=%s,\u00a0`k2`=%s...\u548c[v1,\u00a0v2...]\u4e24\u4e2a\u5b57\u6bb5\n \u00a0\u00a0\u00a0\u00a0--\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0@example\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fieldStrAndPer({'name':'\u5f20\u4e09',\u00a0'age':18})\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return:\u00a0(' `name`=%s  ,  `age`=%s ', ['\u5f20\u4e09', '18'])\n     \"\"\"",
                "'+'",
                "'-'",
                "'*'",
                "'/'",
                "'{}{}%s'",
                "'%s'"
            ],
            "<COMMENT>": [
                "# \u6807\u8bb0lper\u548cl2\u8fd8\u672a\u8d4b\u503c"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "068c568393e34465827f74f5162cf154": {
        "code_string": "while True:\n         # Convert timestamp to naive before adding day, otherwise the when\n         # stepping over EDT an hour is added.\n         next_dt = pd.Timestamp(next_dt.replace(tzinfo=None))\n         next_dt = next_dt + interval\n         next_dt = pd.Timestamp(next_dt, tz=trading.environment.exchange_tz)\n         next_dt_utc = next_dt.tz_convert('UTC')\n         if trading.environment.is_market_hours(next_dt):\n             break\n         next_dt = next_dt_utc.tz_convert(trading.environment.exchange_tz)\n",
        "code_toks_joined": "while True : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> next_dt = pd . Timestamp ( next_dt . replace ( tzinfo = None ) ) <NEWLINE> next_dt = next_dt + interval <NEWLINE> next_dt = pd . Timestamp ( next_dt , tz = trading . environment . exchange_tz ) <NEWLINE> next_dt_utc = next_dt . tz_convert ( <STRING> ) <NEWLINE> if trading . environment . is_market_hours ( next_dt ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> next_dt = next_dt_utc . tz_convert ( trading . environment . exchange_tz ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Convert timestamp to naive before adding day, otherwise the when",
                "# stepping over EDT an hour is added."
            ],
            "<STRING>": [
                "'UTC'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eb91ce6af5cb4fa4916ceaa8bd477ca8": {
        "code_string": "def create_test_df_source(sim_params=None, bars='daily'):\n     if bars == 'daily':\n         freq = pd.datetools.BDay()\n     elif bars == 'minute':\n         freq = pd.datetools.Minute()\n     else:\n         raise ValueError('%s bars not understood.' % freq)\n",
        "code_toks_joined": "def create_test_df_source ( sim_params = None , bars = <STRING> ) : <NEWLINE> <INDENT> if bars == <STRING> : <NEWLINE> <INDENT> freq = pd . datetools . BDay ( ) <NEWLINE> <DEDENT> elif bars == <STRING> : <NEWLINE> <INDENT> freq = pd . datetools . Minute ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> % freq ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'daily'",
                "'daily'",
                "'minute'",
                "'%s bars not understood.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "65e2f221417d4de19e1f1f19dff6928c": {
        "code_string": "out_x_res_y = out_file_h5.createCArray(out_file_h5.root,\n                                                        name='XResidualsY_DUT%d' % (actual_dut),\n                                                        title='Residual distribution in y direction as a function of the x position for DUT %d ' % (actual_dut),\n                                                        atom=tb.Atom.from_dtype(hist_x_residual_y_hist.dtype),\n                                                        shape=hist_x_residual_x_hist.shape,\n                                                        filters=tb.Filters(complib='blosc', complevel=5, fletcher32=False))\n                 out_x_res_y.attrs.xedges = hist_x_residual_y_xedges\n                 out_x_res_y.attrs.yedges = hist_x_residual_y_yedges\n                 out_x_res_y.attrs.fit_coeff = fit_x_residual_y[0]\n                 out_x_res_y.attrs.fit_cov = fit_x_residual_y[1]\n                 out_x_res_y[:] = hist_x_residual_y_hist\n",
        "code_toks_joined": "out_x_res_y = out_file_h5 . createCArray ( out_file_h5 . root , <NEWLINE> <INDENT> name = <STRING> % ( actual_dut ) , <NEWLINE> title = <STRING> % ( actual_dut ) , <NEWLINE> atom = tb . Atom . from_dtype ( hist_x_residual_y_hist . dtype ) , <NEWLINE> shape = hist_x_residual_x_hist . shape , <NEWLINE> filters = tb . Filters ( complib = <STRING> , complevel = 5 , fletcher32 = False ) ) <NEWLINE> out_x_res_y . attrs . xedges = hist_x_residual_y_xedges <NEWLINE> out_x_res_y . attrs . yedges = hist_x_residual_y_yedges <NEWLINE> out_x_res_y . attrs . fit_coeff = fit_x_residual_y [ 0 ] <NEWLINE> out_x_res_y . attrs . fit_cov = fit_x_residual_y [ 1 ] <NEWLINE> out_x_res_y [ : ] = hist_x_residual_y_hist <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'XResidualsY_DUT%d'",
                "'Residual distribution in y direction as a function of the x position for DUT %d '",
                "'blosc'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e23955a0dcda41d0a59396a8a0725a8a": {
        "code_string": "out_col_res_row = out_file_h5.createCArray(out_file_h5.root,\n                                                            name='ColumnResidualsRow_DUT%d' % (actual_dut),\n                                                            title='Residual distribution in row direction as a function of the column position for DUT %d ' % (actual_dut),\n                                                            atom=tb.Atom.from_dtype(hist_col_residual_row_hist.dtype),\n                                                            shape=hist_col_residual_col_hist.shape,\n                                                            filters=tb.Filters(complib='blosc', complevel=5, fletcher32=False))\n                 out_col_res_row.attrs.xedges = hist_col_residual_row_xedges\n                 out_col_res_row.attrs.yedges = hist_col_residual_row_yedges\n                 out_col_res_row.attrs.fit_coeff = fit_col_residual_row[0]\n                 out_col_res_row.attrs.fit_cov = fit_col_residual_row[1]\n                 out_col_res_row[:] = hist_col_residual_row_hist\n",
        "code_toks_joined": "out_col_res_row = out_file_h5 . createCArray ( out_file_h5 . root , <NEWLINE> <INDENT> name = <STRING> % ( actual_dut ) , <NEWLINE> title = <STRING> % ( actual_dut ) , <NEWLINE> atom = tb . Atom . from_dtype ( hist_col_residual_row_hist . dtype ) , <NEWLINE> shape = hist_col_residual_col_hist . shape , <NEWLINE> filters = tb . Filters ( complib = <STRING> , complevel = 5 , fletcher32 = False ) ) <NEWLINE> out_col_res_row . attrs . xedges = hist_col_residual_row_xedges <NEWLINE> out_col_res_row . attrs . yedges = hist_col_residual_row_yedges <NEWLINE> out_col_res_row . attrs . fit_coeff = fit_col_residual_row [ 0 ] <NEWLINE> out_col_res_row . attrs . fit_cov = fit_col_residual_row [ 1 ] <NEWLINE> out_col_res_row [ : ] = hist_col_residual_row_hist <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ColumnResidualsRow_DUT%d'",
                "'Residual distribution in row direction as a function of the column position for DUT %d '",
                "'blosc'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "465880daf00c421d90a614a9a56be254": {
        "code_string": "# Split data and fit on all available cores\n                         n_slices = cpu_count()\n                         slices = np.array_split(n_tracks, n_slices)\n                         results = pool.map(_fit_tracks_loop, slices)\n                         del track_hits\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> n_slices = cpu_count ( ) <NEWLINE> slices = np . array_split ( n_tracks , n_slices ) <NEWLINE> results = pool . map ( _fit_tracks_loop , slices ) <NEWLINE> del track_hits <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Split data and fit on all available cores"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "82a96c0b0a92405f95b2dc6b2c4550c6": {
        "code_string": "if not reference_hit_set and not np.isnan(tr_row[track_index][dut_index]):  # Search for first DUT that registered a hit\n",
        "code_toks_joined": "if not reference_hit_set and not np . isnan ( tr_row [ track_index ] [ dut_index ] ) : <COMMENT> <NEWLINE>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Search for first DUT that registered a hit"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ba74ba0a9ee646a298f8959bdced51fd": {
        "code_string": "# If possible, make a single back-edge to a loop header, by introducing\n # intermediate jump landing nodes.\n def loop_single_entry(cfg):\n     for v, _ in cfg.iter_nodes():\n         if cfg.degree_in(v) >= 2:\n             preds = cfg.pred(v)\n             back_preds = list(filter(lambda x: v <= x, preds))\n             if len(back_preds) < 2:\n                 continue\n             print(\"loop_single_entry: node:\", v)\n             print(\"back_preds:\", back_preds)\n             back_jumps = list(filter(lambda x: cfg.degree_out(x) == 1, back_preds))\n             print(\"back_jumps:\", back_jumps)\n             # find existing landing site\n             landing_site = None\n             for p in back_jumps:\n                 b = cfg.node(p)\n                 if not b.items:\n                     landing_site = p\n             if not landing_site:\n                 farthest = max(back_jumps)\n                 print(\"farthest\", farthest)\n                 newb = BBlock(farthest + \"_1\")\n                 cfg.add_node(newb.addr, newb)\n                 cfg.add_edge(newb.addr, v)\n                 landing_site = newb.addr\n             print(\"landing_site:\", landing_site)\n             for p in back_preds:\n                 if p != landing_site:\n                     e = cfg.edge(p, v)\n                     cfg.remove_edge(p, v)\n                     cfg.add_edge(p, landing_site, e)\n             return True\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> def loop_single_entry ( cfg ) : <NEWLINE> <INDENT> for v , _ in cfg . iter_nodes ( ) : <NEWLINE> <INDENT> if cfg . degree_in ( v ) >= 2 : <NEWLINE> <INDENT> preds = cfg . pred ( v ) <NEWLINE> back_preds = list ( filter ( lambda x : v <= x , preds ) ) <NEWLINE> if len ( back_preds ) < 2 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> print ( <STRING> , v ) <NEWLINE> print ( <STRING> , back_preds ) <NEWLINE> back_jumps = list ( filter ( lambda x : cfg . degree_out ( x ) == 1 , back_preds ) ) <NEWLINE> print ( <STRING> , back_jumps ) <NEWLINE> <COMMENT> <NL> landing_site = None <NEWLINE> for p in back_jumps : <NEWLINE> <INDENT> b = cfg . node ( p ) <NEWLINE> if not b . items : <NEWLINE> <INDENT> landing_site = p <NEWLINE> <DEDENT> <DEDENT> if not landing_site : <NEWLINE> <INDENT> farthest = max ( back_jumps ) <NEWLINE> print ( <STRING> , farthest ) <NEWLINE> newb = BBlock ( farthest + <STRING> ) <NEWLINE> cfg . add_node ( newb . addr , newb ) <NEWLINE> cfg . add_edge ( newb . addr , v ) <NEWLINE> landing_site = newb . addr <NEWLINE> <DEDENT> print ( <STRING> , landing_site ) <NEWLINE> for p in back_preds : <NEWLINE> <INDENT> if p != landing_site : <NEWLINE> <INDENT> e = cfg . edge ( p , v ) <NEWLINE> cfg . remove_edge ( p , v ) <NEWLINE> cfg . add_edge ( p , landing_site , e ) <NEWLINE> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# If possible, make a single back-edge to a loop header, by introducing",
                "# intermediate jump landing nodes.",
                "# find existing landing site"
            ],
            "<STRING>": [
                "\"loop_single_entry: node:\"",
                "\"back_preds:\"",
                "\"back_jumps:\"",
                "\"farthest\"",
                "\"_1\"",
                "\"landing_site:\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1b70461694d2454a9c73cdf37a83dd9d": {
        "code_string": "if self.train_with_fake_labels:\n             x_train = np.reshape(train_data.X, newshape=(-1, *self.x_dim))\n             x = [x_train, train_labels, pseudo_labels]\n             y = [x_train, pseudo_labels]\n         else:\n             x_train = np.reshape(train_data.X, newshape=(-1, *self.x_dim))\n             x = [x_train, train_labels, train_labels]\n             y = [x_train, train_labels]\n",
        "code_toks_joined": "if self . train_with_fake_labels : <NEWLINE> <INDENT> x_train = np . reshape ( train_data . X , newshape = ( - 1 , * self . x_dim ) ) <NEWLINE> x = [ x_train , train_labels , pseudo_labels ] <NEWLINE> y = [ x_train , pseudo_labels ] <NEWLINE> else : <NEWLINE> x_train = np . reshape ( train_data . X , newshape = ( - 1 , * self . x_dim ) ) <NEWLINE> x = [ x_train , train_labels , train_labels ] <NEWLINE> y = [ x_train , train_labels ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "900142a4864240dabf384632fecafe52": {
        "code_string": "print(train_data.shape, data_valid.shape)\n",
        "code_toks_joined": "print ( train_data . shape , data_valid . shape ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a57dbeee5dd44fca9379e09b0af184c1": {
        "code_string": "cell_type_data = train[train.obs[cell_type_key] == cell_type]\n         cell_type_ctrl_data = train[((train.obs[cell_type_key] == cell_type) & (train.obs[\"condition\"] == ctrl_key))]\n         pred = network.predict(cell_type_data,\n                                encoder_labels=np.zeros((cell_type_ctrl_data.shape[0], 1)),\n                                decoder_labels=np.ones((cell_type_ctrl_data.shape[0], 1)))\n",
        "code_toks_joined": "cell_type_data = train [ train . obs [ cell_type_key ] == cell_type ] <NEWLINE> <INDENT> cell_type_ctrl_data = train [ ( ( train . obs [ cell_type_key ] == cell_type ) & ( train . obs [ <STRING> ] == ctrl_key ) ) ] <NEWLINE> pred = network . predict ( cell_type_data , <NEWLINE> <INDENT> encoder_labels = np . zeros ( ( cell_type_ctrl_data . shape [ 0 ] , 1 ) ) , <NEWLINE> decoder_labels = np . ones ( ( cell_type_ctrl_data . shape [ 0 ] , 1 ) ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"condition\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "48de774407d64911a1e4dc3f12743882": {
        "code_string": "source_images_train = train_data[valid_data.obs[\"condition\"] == source_key].X\n         source_images_valid = valid_data[valid_data.obs[\"condition\"] == source_key].X\n",
        "code_toks_joined": "source_images_train = train_data [ valid_data . obs [ <STRING> ] == source_key ] . X <NEWLINE> <INDENT> source_images_valid = valid_data [ valid_data . obs [ <STRING> ] == source_key ] . X <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"condition\"",
                "\"condition\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "171d68ff77d849e8b007e3e19b35700e": {
        "code_string": "for gene in top_100_genes[:3]:\n             sc.pl.violin(cell_type_adata, keys=gene, groupby=condition_key,\n                          save=f\"_{data_name}_{cell_type}_{gene}.pdf\",\n                          show=False,\n                          wspace=0.2,\n                          rotation=90,\n                          frameon=False)\n",
        "code_toks_joined": "for gene in top_100_genes [ : 3 ] : <NEWLINE> <INDENT> sc . pl . violin ( cell_type_adata , keys = gene , groupby = condition_key , <NEWLINE> <INDENT> save = <STRING> , <NEWLINE> show = False , <NEWLINE> wspace = 0.2 , <NEWLINE> rotation = 90 , <NEWLINE> frameon = False ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"_{data_name}_{cell_type}_{gene}.pdf\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "80f1d75345f2470f9c823d2bb5435605": {
        "code_string": "updates = kwargs\n         persistence_converter = self.get_persistence_converter(self.engine_name)\n         if persistence_converter is not None:\n             updates = persistence_converter(updates)\n         self.engine.update(patch, primary_index, context, updates)\n         self.get_data()\n",
        "code_toks_joined": "updates = kwargs <NEWLINE> <INDENT> persistence_converter = self . get_persistence_converter ( self . engine_name ) <NEWLINE> if persistence_converter is not None : <NEWLINE> <INDENT> updates = persistence_converter ( updates ) <NEWLINE> <DEDENT> self . engine . update ( patch , primary_index , context , updates ) <NEWLINE> self . get_data ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b326164f1a984e448138085c91717404": {
        "code_string": "for subdir in subdirs:\n                 self._fetch_files(subdir, files)\n",
        "code_toks_joined": "for subdir in subdirs : <NEWLINE> <INDENT> self . _fetch_files ( subdir , files ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e48ced363f2a4cf2a74f9156289ab3d6": {
        "code_string": "outputFastaPath = os.path.join(options.outputPath, outputFastaName)\n         if os.path.isfile(outputFastaName):\n             print(\"Skipping locus %s. Already exists...\" % locus_name)\n             continue\n",
        "code_toks_joined": "outputFastaPath = os . path . join ( options . outputPath , outputFastaName ) <NEWLINE> <INDENT> if os . path . isfile ( outputFastaName ) : <NEWLINE> <INDENT> print ( <STRING> % locus_name ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Skipping locus %s. Already exists...\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "102ab7fd1d8e4f76abb989d69fb32295": {
        "code_string": "# Allow instance of BaseStrategy or from the predefined mapping\n         if issubclass(strategy, bt.Strategy):\n             strat_name = str(strategy)\n         else:\n             strat_name = strategy\n             strategy = STRATEGY_MAPPING[strategy]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if issubclass ( strategy , bt . Strategy ) : <NEWLINE> <INDENT> strat_name = str ( strategy ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> strat_name = strategy <NEWLINE> strategy = STRATEGY_MAPPING [ strategy ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Allow instance of BaseStrategy or from the predefined mapping"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8152d2638721402fbbfea3b041e40765": {
        "code_string": "for env_var in env_vars:\n         def_value = config.instance[section].get(executor_name, None)\n         value = click.prompt(f\"Environment variable {env_var} value\", default=def_value)\n         config.instance.set(section, env_var, value)\n",
        "code_toks_joined": "for env_var in env_vars : <NEWLINE> <INDENT> def_value = config . instance [ section ] . get ( executor_name , None ) <NEWLINE> value = click . prompt ( <STRING> , default = def_value ) <NEWLINE> config . instance . set ( section , env_var , value ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"Environment variable {env_var} value\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d0e25f5b853e4afea6de8b927dcff657": {
        "code_string": "def write_report(df_map, outFH, pcr_volume, mm_volume,\n                  fp_tube, fp_volume, rp_tube, rp_volume,\n                  n_rxn_reps, error_perc=10.0):\n     \"\"\"Writing a report on\n     \"\"\"\n     # calculating total volumes\n     n_rxn = df_map.shape[0]\n     ## total PCR\n     total_pcr_volume = pcr_volume * n_rxn\n     ## total mastermix\n     total_mm_volume = pcr_volume * n_rxn\n     ## total primer\n     if fp_tube > 0 and fp_volume > 0:\n         total_fp_volume = fp_volume * n_rxn\n     else:\n         total_fp_volume = None\n     if rp_tube > 0 and rp_volume > 0:\n         total_rp_volume = rp_volume * n_rxn\n     else:\n         total_rp_volume = None\n     ## total water\n     total_water_volume = sum(df_map['TECAN_water_rxn_volume'])\n",
        "code_toks_joined": "def write_report ( df_map , outFH , pcr_volume , mm_volume , <NEWLINE> <INDENT> fp_tube , fp_volume , rp_tube , rp_volume , <NEWLINE> n_rxn_reps , error_perc = 10.0 ) : <NEWLINE> <STRING> <NEWLINE> <COMMENT> <NL> n_rxn = df_map . shape [ 0 ] <NEWLINE> <COMMENT> <NL> total_pcr_volume = pcr_volume * n_rxn <NEWLINE> <COMMENT> <NL> total_mm_volume = pcr_volume * n_rxn <NEWLINE> <COMMENT> <NL> if fp_tube > 0 and fp_volume > 0 : <NEWLINE> total_fp_volume = fp_volume * n_rxn <NEWLINE> else : <NEWLINE> total_fp_volume = None <NEWLINE> if rp_tube > 0 and rp_volume > 0 : <NEWLINE> total_rp_volume = rp_volume * n_rxn <NEWLINE> else : <NEWLINE> total_rp_volume = None <NEWLINE> <COMMENT> <NL> total_water_volume = sum ( df_map [ <STRING> ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Writing a report on\n     \"\"\"",
                "'TECAN_water_rxn_volume'"
            ],
            "<COMMENT>": [
                "# calculating total volumes",
                "## total PCR",
                "## total mastermix",
                "## total primer",
                "## total water"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3f1f3b156acc4958bf7f76c83602910b": {
        "code_string": "except Exception as e:\n                         # Alternatively, we could assign it to None key.\n                         name[settings.UNKNOWN_LANGUAGE] += paragraph\n                         logger.info('Language not detected: {}'.format(paragraph))\n",
        "code_toks_joined": "except Exception as e : <NEWLINE> <COMMENT> <NL> <INDENT> name [ settings . UNKNOWN_LANGUAGE ] += paragraph <NEWLINE> logger . info ( <STRING> . format ( paragraph ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Alternatively, we could assign it to None key."
            ],
            "<STRING>": [
                "'Language not detected: {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b92172c55a864a6898c253551d0e7da5": {
        "code_string": "def list_run(\n         limit, activity_types, output, unique, include_glob, exclude_glob,\n         under, relative, title, null, **_):\n     \"\"\"\n     List recently accessed files.\n     \"\"\"\n     separator = '\\0' if null else '\\n'\n     absunder = [os.path.join(os.path.abspath(p), \"\") for p in under]\n     include_glob += [os.path.join(p, \"*\") for p in absunder]\n     db = get_db()\n     paths = showpaths = list(db.list_file_path(\n         limit, activity_types, unique, include_glob, exclude_glob))\n     if relative:\n         showpaths = [remove_prefix(absunder, p) for p in paths]\n     if title:\n         from .filetitle import write_paths_and_titles\n         write_paths_and_titles(output, paths, showpaths, separator)\n     else:\n         output.writelines(interleave(paths, itertools.repeat(separator)))\n     if output is not sys.stdout:\n         output.close()\n",
        "code_toks_joined": "def list_run ( <NEWLINE> <INDENT> limit , activity_types , output , unique , include_glob , exclude_glob , <NEWLINE> under , relative , title , null , ** _ ) : <NEWLINE> <STRING> <NEWLINE> separator = <STRING> if null else <STRING> <NEWLINE> absunder = [ os . path . join ( os . path . abspath ( p ) , <STRING> ) for p in under ] <NEWLINE> include_glob += [ os . path . join ( p , <STRING> ) for p in absunder ] <NEWLINE> db = get_db ( ) <NEWLINE> paths = showpaths = list ( db . list_file_path ( <NEWLINE> limit , activity_types , unique , include_glob , exclude_glob ) ) <NEWLINE> if relative : <NEWLINE> showpaths = [ remove_prefix ( absunder , p ) for p in paths ] <NEWLINE> if title : <NEWLINE> from . filetitle import write_paths_and_titles <NEWLINE> write_paths_and_titles ( output , paths , showpaths , separator ) <NEWLINE> else : <NEWLINE> output . writelines ( interleave ( paths , itertools . repeat ( separator ) ) ) <NEWLINE> if output is not sys . stdout : <NEWLINE> output . close ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     List recently accessed files.\n     \"\"\"",
                "'\\0'",
                "'\\n'",
                "\"\"",
                "\"*\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "883e24137d2243f68136866077c4e6ea": {
        "code_string": "dehydrator = self.dehydrator_cls(fields=self.fields)\n         if self.is_iterable:\n             return map(dehydrator.dehydrate, target)\n         else:\n             return dehydrator.dehydrate(obj)\n",
        "code_toks_joined": "dehydrator = self . dehydrator_cls ( fields = self . fields ) <NEWLINE> <INDENT> if self . is_iterable : <NEWLINE> <INDENT> return map ( dehydrator . dehydrate , target ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return dehydrator . dehydrate ( obj ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "54cfe600ec94429a95e77ef6c8bfce4e": {
        "code_string": "def rule(self,\n              ruleid, ForEach=None, ResultMod=None, Exists=None, Min=0, Max=None, \n              Aggregates=None, IsLive=None, \n              Command=None, Response=None,\n              Show=None, Pass=\"None\", Fail=\"None\", NoMatch=\"None\"):\n         rule = rdflib.URIRef(ruleid, base=self._base)\n         if ForEach:\n             ruletype  = MINIM.QueryTestRule\n             querynode = rdflib.BNode()\n             self._minimgr.add( (rule, MINIM.query, querynode) )\n             self._minimgr.add( (querynode, MINIM.sparql_query, rdflib.Literal(ForEach)) )\n             if ResultMod:\n                 self._minimgr.add( (querynode, MINIM.result_mod, rdflib.Literal(Exists)) )\n             if Exists:\n                 existsnode = rdflib.BNode()\n                 self._minimgr.add( (rule, MINIM.exists, existsnode) )\n                 self._minimgr.add( (existsnode, MINIM.sparql_query, rdflib.Literal(Exists)) )\n             if Min:\n                 self._minimgr.add( (rule, MINIM.min, rdflib.Literal(Min)) )\n             if Max:\n                 self._minimgr.add( (rule, MINIM.max, rdflib.Literal(Max)) )\n             if Aggregates:\n                 self._minimgr.add( (rule, MINIM.aggregatesTemplate, rdflib.Literal(Aggregates)) )\n             if IsLive:\n                 self._minimgr.add( (rule, MINIM.isLiveTemplate, rdflib.Literal(IsLive)) )\n         elif Exists:\n             ruletype = MINIM.QueryTestRule\n             existsnode = rdflib.BNode()\n             self._minimgr.add( (rule, MINIM.exists, existsnode) )\n             self._minimgr.add( (existsnode, MINIM.sparql_query, rdflib.Literal(Exists)) )\n         elif Command:\n             ruletype = MINIM.SoftwareEnvironmentRule\n             self._minimgr.add( (rule, MINIM.command, rdflib.Literal(Command)) )\n             self._minimgr.add( (rule, MINIM.response, rdflib.Literal(Response)) )\n         else:\n             raise ValueError(\"Unrecognized requirement rule pattern\")\n         self._minimgr.add( (rule, RDF.type, ruletype) )\n         if Show:\n             self._minimgr.add( (rule, MINIM.show,     rdflib.Literal(Show)) )\n         if Pass:\n             self._minimgr.add( (rule, MINIM.showpass, rdflib.Literal(Pass)) )\n         if Fail:\n             self._minimgr.add( (rule, MINIM.showfail, rdflib.Literal(Fail)) )\n         if NoMatch:\n             self._minimgr.add( (rule, MINIM.showmiss, rdflib.Literal(NoMatch)) )\n         return rule\n",
        "code_toks_joined": "def rule ( self , <NEWLINE> <INDENT> ruleid , ForEach = None , ResultMod = None , Exists = None , Min = 0 , Max = None , <NEWLINE> Aggregates = None , IsLive = None , <NEWLINE> Command = None , Response = None , <NEWLINE> Show = None , Pass = <STRING> , Fail = <STRING> , NoMatch = <STRING> ) : <NEWLINE> rule = rdflib . URIRef ( ruleid , base = self . _base ) <NEWLINE> if ForEach : <NEWLINE> ruletype = MINIM . QueryTestRule <NEWLINE> querynode = rdflib . BNode ( ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . query , querynode ) ) <NEWLINE> self . _minimgr . add ( ( querynode , MINIM . sparql_query , rdflib . Literal ( ForEach ) ) ) <NEWLINE> if ResultMod : <NEWLINE> <INDENT> self . _minimgr . add ( ( querynode , MINIM . result_mod , rdflib . Literal ( Exists ) ) ) <NEWLINE> if Exists : <NEWLINE> existsnode = rdflib . BNode ( ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . exists , existsnode ) ) <NEWLINE> self . _minimgr . add ( ( existsnode , MINIM . sparql_query , rdflib . Literal ( Exists ) ) ) <NEWLINE> if Min : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . min , rdflib . Literal ( Min ) ) ) <NEWLINE> if Max : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . max , rdflib . Literal ( Max ) ) ) <NEWLINE> if Aggregates : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . aggregatesTemplate , rdflib . Literal ( Aggregates ) ) ) <NEWLINE> if IsLive : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . isLiveTemplate , rdflib . Literal ( IsLive ) ) ) <NEWLINE> elif Exists : <NEWLINE> ruletype = MINIM . QueryTestRule <NEWLINE> existsnode = rdflib . BNode ( ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . exists , existsnode ) ) <NEWLINE> self . _minimgr . add ( ( existsnode , MINIM . sparql_query , rdflib . Literal ( Exists ) ) ) <NEWLINE> elif Command : <NEWLINE> ruletype = MINIM . SoftwareEnvironmentRule <NEWLINE> self . _minimgr . add ( ( rule , MINIM . command , rdflib . Literal ( Command ) ) ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . response , rdflib . Literal ( Response ) ) ) <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> self . _minimgr . add ( ( rule , RDF . type , ruletype ) ) <NEWLINE> if Show : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . show , rdflib . Literal ( Show ) ) ) <NEWLINE> if Pass : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . showpass , rdflib . Literal ( Pass ) ) ) <NEWLINE> if Fail : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . showfail , rdflib . Literal ( Fail ) ) ) <NEWLINE> if NoMatch : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . showmiss , rdflib . Literal ( NoMatch ) ) ) <NEWLINE> return rule <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"None\"",
                "\"None\"",
                "\"None\"",
                "\"Unrecognized requirement rule pattern\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f17f882bdad749849c7ebbe56382850e": {
        "code_string": "def dumpPkl(obj,path):\n \twith open(path,'wb') as pf:\n \t\tpickle.dump(obj,path)\n",
        "code_toks_joined": "def dumpPkl ( obj , path ) : <NEWLINE> <INDENT> with open ( path , <STRING> ) as pf : <NEWLINE> <INDENT> pickle . dump ( obj , path ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'wb'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0858ed44560c497f9d96fe6f809cd1d5": {
        "code_string": "if additional_collapsed is not None:\n             for param_field, param_collapsed in zip(additional_params, additional_collapsed):\n                 param_collapsed.shape = (Ncubes * collapse_channels, ) + param_collapsed.shape[2:]\n                 setattr(self, param_field, param_collapsed)\n",
        "code_toks_joined": "if additional_collapsed is not None : <NEWLINE> <INDENT> for param_field , param_collapsed in zip ( additional_params , additional_collapsed ) : <NEWLINE> <INDENT> param_collapsed . shape = ( Ncubes * collapse_channels , ) + param_collapsed . shape [ 2 : ] <NEWLINE> setattr ( self , param_field , param_collapsed ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a79c8f0deab445299e5dae2deaefa723": {
        "code_string": "#extract data from each file\n         for index, filepath in enumerate(filepaths):\n             with fits.open(filepath, lazy_load_hdus=False) as hdulist:\n                 cube = hdulist[1].data\n                 prihdr = hdulist[0].header\n                 exthdr = hdulist[0].header\n                 w = wcs.WCS(header=prihdr, naxis=[1,2])\n                 astr_hdrs = [w.deepcopy() for _ in range(cube.shape[0])] #repeat astrom header for each wavelength slice\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for index , filepath in enumerate ( filepaths ) : <NEWLINE> <INDENT> with fits . open ( filepath , lazy_load_hdus = False ) as hdulist : <NEWLINE> <INDENT> cube = hdulist [ 1 ] . data <NEWLINE> prihdr = hdulist [ 0 ] . header <NEWLINE> exthdr = hdulist [ 0 ] . header <NEWLINE> w = wcs . WCS ( header = prihdr , naxis = [ 1 , 2 ] ) <NEWLINE> astr_hdrs = [ w . deepcopy ( ) for _ in range ( cube . shape [ 0 ] ) ] <COMMENT> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#extract data from each file",
                "#repeat astrom header for each wavelength slice"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a8e109c1434d42dfbac60c739835f2d9": {
        "code_string": "# RDI Sanity Checks to make sure PSF Library is properly configured\n     if \"RDI\" in mode:\n         if psf_library is None:\n             raise ValueError(\"You need to pass in a psf_library if you want to run RDI\")\n         if psf_library.dataset is dataset:\n             raise ValueError(\"The PSF Library is not prepared for this dataset. Run psf_library.prepare_library()\")\n         if aligned_center is not None:\n             if np.array_equal(aligned_center, psf_library.aligned_center): \n                 raise ValueError(\"The images need to be aligned to the same center as the RDI Library\")\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if <STRING> in mode : <NEWLINE> <INDENT> if psf_library is None : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if psf_library . dataset is dataset : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if aligned_center is not None : <NEWLINE> <INDENT> if np . array_equal ( aligned_center , psf_library . aligned_center ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# RDI Sanity Checks to make sure PSF Library is properly configured"
            ],
            "<STRING>": [
                "\"RDI\"",
                "\"You need to pass in a psf_library if you want to run RDI\"",
                "\"The PSF Library is not prepared for this dataset. Run psf_library.prepare_library()\"",
                "\"The images need to be aligned to the same center as the RDI Library\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2abe80a16e124cc196587876ce4a1324": {
        "code_string": "# Mask out a band of 10 pixels around the edges of the finite pixels of the image.\n         if maskout_edge is not None:\n             IWA,OWA,inner_mask,outer_mask = get_occ(image, centroid = (center[0][0]+stamp_size//2,center[0][1]+stamp_size//2))\n             conv_kernel = np.ones((maskout_edge,maskout_edge))\n             flat_cube_wider_mask = convolve2d(outer_mask,conv_kernel,mode=\"same\")\n             image_cpy[np.where(np.isnan(flat_cube_wider_mask))] = np.nan\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if maskout_edge is not None : <NEWLINE> <INDENT> IWA , OWA , inner_mask , outer_mask = get_occ ( image , centroid = ( center [ 0 ] [ 0 ] + stamp_size // 2 , center [ 0 ] [ 1 ] + stamp_size // 2 ) ) <NEWLINE> conv_kernel = np . ones ( ( maskout_edge , maskout_edge ) ) <NEWLINE> flat_cube_wider_mask = convolve2d ( outer_mask , conv_kernel , mode = <STRING> ) <NEWLINE> image_cpy [ np . where ( np . isnan ( flat_cube_wider_mask ) ) ] = np . nan <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Mask out a band of 10 pixels around the edges of the finite pixels of the image."
            ],
            "<STRING>": [
                "\"same\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "930bf54e42fa4e32876c0fde218bf3b7": {
        "code_string": "def amount_as_string(self, min_decimals: Optional[int] = None, max_decimals: Optional[int] = None) -> str:\n         if min_decimals is None and max_decimals is None:\n             if self._currency and isinstance(self._currency, BaseCurrency):\n                 min_decimals = self._currency.decimal_digits\n             min_decimals = DEFAULT_MIN_DECIMALS if min_decimals is None else min_decimals\n             max_decimals = max(cast(int, min_decimals), DEFAULT_MAX_DECIMALS)\n         elif min_decimals is None:\n             if self._currency and isinstance(self._currency, BaseCurrency):\n                 min_decimals = self._currency.decimal_digits\n             min_decimals = DEFAULT_MIN_DECIMALS if min_decimals is None else min_decimals\n             min_decimals = min(cast(min_decimals, int), max_decimals)\n         elif max_decimals is None:\n             max_decimals = max(min_decimals, DEFAULT_MAX_DECIMALS)\n",
        "code_toks_joined": "def amount_as_string ( self , min_decimals : Optional [ int ] = None , max_decimals : Optional [ int ] = None ) -> str : <NEWLINE> <INDENT> if min_decimals is None and max_decimals is None : <NEWLINE> <INDENT> if self . _currency and isinstance ( self . _currency , BaseCurrency ) : <NEWLINE> <INDENT> min_decimals = self . _currency . decimal_digits <NEWLINE> <DEDENT> min_decimals = DEFAULT_MIN_DECIMALS if min_decimals is None else min_decimals <NEWLINE> max_decimals = max ( cast ( int , min_decimals ) , DEFAULT_MAX_DECIMALS ) <NEWLINE> <DEDENT> elif min_decimals is None : <NEWLINE> <INDENT> if self . _currency and isinstance ( self . _currency , BaseCurrency ) : <NEWLINE> <INDENT> min_decimals = self . _currency . decimal_digits <NEWLINE> <DEDENT> min_decimals = DEFAULT_MIN_DECIMALS if min_decimals is None else min_decimals <NEWLINE> min_decimals = min ( cast ( min_decimals , int ) , max_decimals ) <NEWLINE> <DEDENT> elif max_decimals is None : <NEWLINE> <INDENT> max_decimals = max ( min_decimals , DEFAULT_MAX_DECIMALS ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "104eff6268b14608b51e7585d3a5fba3": {
        "code_string": "if \"error\" in body:\n             err = ErrorSchema().load(content[\"error\"])\n             text = f\"{err['message']} ({self.status}): {err['detail']}\" if err[\"detail\"] else err[\"message\"]\n             raise ErrorResponse(text)\n",
        "code_toks_joined": "if <STRING> in body : <NEWLINE> <INDENT> err = ErrorSchema ( ) . load ( content [ <STRING> ] ) <NEWLINE> text = <STRING> if err [ <STRING> ] else err [ <STRING> ] <NEWLINE> raise ErrorResponse ( text ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"error\"",
                "\"error\"",
                "f\"{err['message']} ({self.status}): {err['detail']}\"",
                "\"detail\"",
                "\"message\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d93b9e2dfbf4ca698175707394b9db8": {
        "code_string": "self.assertEqual(len(feed.entries), len(datasets))\n         for i in range(1, len(feed.entries)):\n             published_date = feed.entries[i].published_parsed\n             prev_published_date = feed.entries[i - 1].published_parsed\n             self.assertGreaterEqual(published_date, prev_published_date)\n",
        "code_toks_joined": "self . assertEqual ( len ( feed . entries ) , len ( datasets ) ) <NEWLINE> <INDENT> for i in range ( 1 , len ( feed . entries ) ) : <NEWLINE> <INDENT> published_date = feed . entries [ i ] . published_parsed <NEWLINE> prev_published_date = feed . entries [ i - 1 ] . published_parsed <NEWLINE> self . assertGreaterEqual ( published_date , prev_published_date ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7a0dc49fa41443b5bdddf4711ab420ba": {
        "code_string": "@ns.route('/<dataset:dataset>/badges/', endpoint='dataset_badges')\n class DatasetBadgesAPI(API):\n     @api.doc('add_dataset_badge', **common_doc)\n     @api.expect(badge_fields)\n     @api.marshal_with(badge_fields)\n     @api.secure(admin_permission)\n     def post(self, dataset):\n         '''Create a new badge for a given dataset'''\n         form = api.validate(BadgeForm)\n         badge = DatasetBadge(created=datetime.now(),\n                              created_by=current_user.id)\n         form.populate_obj(badge)\n         for existing_badge in dataset.badges:\n             if existing_badge.kind == badge.kind:\n                 return badge\n         dataset.add_badge(badge)\n         return badge, 201\n",
        "code_toks_joined": "@ ns . route ( <STRING> , endpoint = <STRING> ) <NEWLINE> <INDENT> class DatasetBadgesAPI ( API ) : <NEWLINE> <INDENT> @ api . doc ( <STRING> , ** common_doc ) <NEWLINE> @ api . expect ( badge_fields ) <NEWLINE> @ api . marshal_with ( badge_fields ) <NEWLINE> @ api . secure ( admin_permission ) <NEWLINE> def post ( self , dataset ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> form = api . validate ( BadgeForm ) <NEWLINE> badge = DatasetBadge ( created = datetime . now ( ) , <NEWLINE> <INDENT> created_by = current_user . id ) <NEWLINE> <DEDENT> form . populate_obj ( badge ) <NEWLINE> for existing_badge in dataset . badges : <NEWLINE> <INDENT> if existing_badge . kind == badge . kind : <NEWLINE> <INDENT> return badge <NEWLINE> <DEDENT> <DEDENT> dataset . add_badge ( badge ) <NEWLINE> return badge , 201 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/<dataset:dataset>/badges/'",
                "'dataset_badges'",
                "'add_dataset_badge'",
                "'''Create a new badge for a given dataset'''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f278aa1e98f847a5938a67519a408b67": {
        "code_string": "@ns.route('/<reuse:reuse>/datasets/', endpoint='reuse_add_dataset')\n class ReuseDatasetsAPI(API):\n     @api.secure\n     @api.doc('reuse_add_dataset', **common_doc)\n     @api.expect(dataset_ref_fields)\n     @api.response(200, 'The dataset is already present', reuse_fields)\n     @api.marshal_with(reuse_fields, code=201)\n     def post(self, reuse):\n         '''Add a dataset to a given reuse'''\n         if 'id' not in request.json:\n             api.abort(400, 'Expect a dataset identifier')\n         try:\n             dataset = Dataset.objects.get_or_404(id=request.json['id'])\n         except Dataset.DoesNotExist:\n             api.abort(404, 'Dataset {0} does not exists'.format(request.json['id']))\n         if dataset in reuse.datasets:\n             return dataset\n         reuse.datasets.append(dataset)\n         reuse.save()\n         return reuse, 201\n",
        "code_toks_joined": "@ ns . route ( <STRING> , endpoint = <STRING> ) <NEWLINE> <INDENT> class ReuseDatasetsAPI ( API ) : <NEWLINE> <INDENT> @ api . secure <NEWLINE> @ api . doc ( <STRING> , ** common_doc ) <NEWLINE> @ api . expect ( dataset_ref_fields ) <NEWLINE> @ api . response ( 200 , <STRING> , reuse_fields ) <NEWLINE> @ api . marshal_with ( reuse_fields , code = 201 ) <NEWLINE> def post ( self , reuse ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if <STRING> not in request . json : <NEWLINE> <INDENT> api . abort ( 400 , <STRING> ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> dataset = Dataset . objects . get_or_404 ( id = request . json [ <STRING> ] ) <NEWLINE> <DEDENT> except Dataset . DoesNotExist : <NEWLINE> <INDENT> api . abort ( 404 , <STRING> . format ( request . json [ <STRING> ] ) ) <NEWLINE> <DEDENT> if dataset in reuse . datasets : <NEWLINE> <INDENT> return dataset <NEWLINE> <DEDENT> reuse . datasets . append ( dataset ) <NEWLINE> reuse . save ( ) <NEWLINE> return reuse , 201 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'/<reuse:reuse>/datasets/'",
                "'reuse_add_dataset'",
                "'reuse_add_dataset'",
                "'The dataset is already present'",
                "'''Add a dataset to a given reuse'''",
                "'id'",
                "'Expect a dataset identifier'",
                "'id'",
                "'Dataset {0} does not exists'",
                "'id'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cb4f520047864748b8b5364c06499d89": {
        "code_string": "assert type(cleaned_options[\"store\"].ttl) is int\n         cleaned_options[\"ttl\"] = options[\"store\"].ttl\n",
        "code_toks_joined": "assert type ( cleaned_options [ <STRING> ] . ttl ) is int <NEWLINE> <INDENT> cleaned_options [ <STRING> ] = options [ <STRING> ] . ttl <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"store\"",
                "\"ttl\"",
                "\"store\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "38639fce642643b79acd0db3b5a0010e": {
        "code_string": "def crawl(url):\n     items = requests.get(url, headers=lt.DEFAULT_HEADERS).json()\n     for item in items:\n         data = {}\n         data['name'] = item['name']\n         data['category'] = item['Category']['name']\n         data['note'] = item['note']\n         data['author'] = item['User']['nickname']\n         data['url'] = item['url']\n         data['downloads'] = item['downloads_count']\n         data['votes'] = item['votes_count']\n         data['comments'] = items['comments_count']\n         pprint(data)\n         total.append(data)\n",
        "code_toks_joined": "def crawl ( url ) : <NEWLINE> <INDENT> items = requests . get ( url , headers = lt . DEFAULT_HEADERS ) . json ( ) <NEWLINE> for item in items : <NEWLINE> <INDENT> data = { } <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = items [ <STRING> ] <NEWLINE> pprint ( data ) <NEWLINE> total . append ( data ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'name'",
                "'name'",
                "'category'",
                "'Category'",
                "'name'",
                "'note'",
                "'note'",
                "'author'",
                "'User'",
                "'nickname'",
                "'url'",
                "'url'",
                "'downloads'",
                "'downloads_count'",
                "'votes'",
                "'votes_count'",
                "'comments'",
                "'comments_count'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "329a20b353b34082a1f5d23845ce5834": {
        "code_string": "@click.command(help=help_text)\n @click.option('-n', '--max_unknowns', type=int, default=200,\n               help='Maximum number of unknown bases')\n @click.option('-c', '--c-deviations', type=float, default=3.0,\n               help='Deviations for number of contigs',)\n @click.option('-s', '--s-deviations', type=float, default=3.0,\n               help='Deviations for the assembly size')\n @click.option('-m', '--m-deviations', type=float, default=3.0,\n               help='Deviations for MASH distances')\n @click.option('-l', '--filter-level', type=float,\n               help='Deviations for all metrics')\n @click.option('-d', '--dry-run', is_flag=True)\n @click.option('--species', is_flag=True,\n               help='Run on single species')\n @click.argument('path', type=click.Path(exists=True, file_okay=False))\n def cli(filter_level, max_unknowns, c_deviations, s_deviations, m_deviations,\n         dry_run, species, path):\n     if species:\n         from genbankqc import Species\n         try:\n             s = Species(path, max_unknowns, c_deviations, s_deviations,\n                         m_deviations)\n             s.qc()\n             print(\"Completed \", s.species)\n             print(s)\n         except Exception:\n             print('Failed ', species.species)\n             traceback.print_exc()\n     else:\n         from genbankqc import Genbank\n         genbank = Genbank(path)\n         genbank.qc()\n",
        "code_toks_joined": "@ click . command ( help = help_text ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , type = int , default = 200 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> , ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , is_flag = True ) <NEWLINE> @ click . option ( <STRING> , is_flag = True , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . argument ( <STRING> , type = click . Path ( exists = True , file_okay = False ) ) <NEWLINE> def cli ( filter_level , max_unknowns , c_deviations , s_deviations , m_deviations , <NEWLINE> <INDENT> dry_run , species , path ) : <NEWLINE> if species : <NEWLINE> from genbankqc import Species <NEWLINE> try : <NEWLINE> <INDENT> s = Species ( path , max_unknowns , c_deviations , s_deviations , <NEWLINE> <INDENT> m_deviations ) <NEWLINE> <DEDENT> s . qc ( ) <NEWLINE> print ( <STRING> , s . species ) <NEWLINE> print ( s ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> print ( <STRING> , species . species ) <NEWLINE> traceback . print_exc ( ) <NEWLINE> else : <NEWLINE> <DEDENT> from genbankqc import Genbank <NEWLINE> genbank = Genbank ( path ) <NEWLINE> genbank . qc ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'-n'",
                "'--max_unknowns'",
                "'Maximum number of unknown bases'",
                "'-c'",
                "'--c-deviations'",
                "'Deviations for number of contigs'",
                "'-s'",
                "'--s-deviations'",
                "'Deviations for the assembly size'",
                "'-m'",
                "'--m-deviations'",
                "'Deviations for MASH distances'",
                "'-l'",
                "'--filter-level'",
                "'Deviations for all metrics'",
                "'-d'",
                "'--dry-run'",
                "'--species'",
                "'Run on single species'",
                "'path'",
                "\"Completed \"",
                "'Failed '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9c4bba74ae204449a0a511ef972ad31f": {
        "code_string": "def eq_contents(path, text):\n     with open(path) as fd:\n         eq_(fd.read(), text)\n",
        "code_toks_joined": "def eq_contents ( path , text ) : <NEWLINE> <INDENT> with open ( path ) as fd : <NEWLINE> <INDENT> eq_ ( fd . read ( ) , text ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eb7ec31baa8f49228caf5832d0341003": {
        "code_string": "global MASK_PREDICTOR_HANDLER\n   with LOCK:\n     if MASK_PREDICTOR_HANDLER is None:\n       MASK_PREDICTOR_HANDLER= MaskPredictor(deepLearningModel, boxSize, gpus)\n",
        "code_toks_joined": "global MASK_PREDICTOR_HANDLER <NEWLINE> <INDENT> with LOCK : <NEWLINE> <INDENT> if MASK_PREDICTOR_HANDLER is None : <NEWLINE> <INDENT> MASK_PREDICTOR_HANDLER = MaskPredictor ( deepLearningModel , boxSize , gpus ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "97c2ba274cfe4e8eb8e656941d1aa78e": {
        "code_string": "class KademliaProtocol(RPCProtocol):\n     def __init__(self, sourceNode, storage, ksize):\n         RPCProtocol.__init__(self)\n         self.router = RoutingTable(self, sourceNode, ksize)\n         self.storage = storage\n         self.sourceID = sourceNode.id\n         self.log = Logger(system=self)\n",
        "code_toks_joined": "class KademliaProtocol ( RPCProtocol ) : <NEWLINE> <INDENT> def __init__ ( self , sourceNode , storage , ksize ) : <NEWLINE> <INDENT> RPCProtocol . __init__ ( self ) <NEWLINE> self . router = RoutingTable ( self , sourceNode , ksize ) <NEWLINE> self . storage = storage <NEWLINE> self . sourceID = sourceNode . id <NEWLINE> self . log = Logger ( system = self ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8f50a9197f7e4acf84bee7447eaec7c0": {
        "code_string": "def verified_email_required(function=None,\n                             login_url=None, \n                             redirect_field_name=REDIRECT_FIELD_NAME):\n     \"\"\"\n     Even when email verification is not mandatory during signup, there\n     may be circumstances during which you really want to prevent\n     unverified users to proceed. This decorator ensures the user is\n     authenticated and has a verified email address. If the former is\n     not the case then the behavior is identical to that of the\n     standard `login_required` decorator. If the latter does not hold,\n     email verification mails are automatically resend and the user is\n     presented with a page informing him he needs to verify his email\n     address.\n     \"\"\"\n     def decorator(view_func):\n         @login_required(redirect_field_name=redirect_field_name,\n                         login_url=login_url)\n         def _wrapped_view(request, *args, **kwargs):\n             if not EmailAddress.objects.filter(user=request.user,\n                                                verified=True).exists():\n                 send_email_confirmation(request.user, request)\n                 return render(request,\n                               'account/verified_email_required.html')\n             return view_func(request, *args, **kwargs)\n         return _wrapped_view\n",
        "code_toks_joined": "def verified_email_required ( function = None , <NEWLINE> <INDENT> login_url = None , <NEWLINE> redirect_field_name = REDIRECT_FIELD_NAME ) : <NEWLINE> <STRING> <NEWLINE> def decorator ( view_func ) : <NEWLINE> @ login_required ( redirect_field_name = redirect_field_name , <NEWLINE> login_url = login_url ) <NEWLINE> def _wrapped_view ( request , * args , ** kwargs ) : <NEWLINE> if not EmailAddress . objects . filter ( user = request . user , <NEWLINE> <INDENT> verified = True ) . exists ( ) : <NEWLINE> send_email_confirmation ( request . user , request ) <NEWLINE> return render ( request , <NEWLINE> <STRING> ) <NEWLINE> return view_func ( request , * args , ** kwargs ) <NEWLINE> return _wrapped_view <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Even when email verification is not mandatory during signup, there\n     may be circumstances during which you really want to prevent\n     unverified users to proceed. This decorator ensures the user is\n     authenticated and has a verified email address. If the former is\n     not the case then the behavior is identical to that of the\n     standard `login_required` decorator. If the latter does not hold,\n     email verification mails are automatically resend and the user is\n     presented with a page informing him he needs to verify his email\n     address.\n     \"\"\"",
                "'account/verified_email_required.html'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "71366657f4654a22a4b78c41a85bb31c": {
        "code_string": "def render_to_string(template_name, dictionary, context_instance=None):\n     context_instance = context_instance or Context(dictionary)\n     # add dictionary to context_instance\n     context_instance.update(dictionary or {})\n     # collapse context_instance to a single dictionary for mako\n     context_dictionary = {}\n     for d in context_instance:\n         context_dictionary.update(d)\n     # fetch and render template\n     template = middleware.lookup.get_template(template_name)\n     return template.render(**dictionary)\n",
        "code_toks_joined": "def render_to_string ( template_name , dictionary , context_instance = None ) : <NEWLINE> <INDENT> context_instance = context_instance or Context ( dictionary ) <NEWLINE> <COMMENT> <NL> context_instance . update ( dictionary or { } ) <NEWLINE> <COMMENT> <NL> context_dictionary = { } <NEWLINE> for d in context_instance : <NEWLINE> <INDENT> context_dictionary . update ( d ) <NEWLINE> <COMMENT> <NL> <DEDENT> template = middleware . lookup . get_template ( template_name ) <NEWLINE> return template . render ( ** dictionary ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# add dictionary to context_instance",
                "# collapse context_instance to a single dictionary for mako",
                "# fetch and render template"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bd9f788e53a1480c8c3c4d256335e279": {
        "code_string": "# TODO: Just the worst\n         if result > -1:\n             tmp = result\n             result = -1\n             return str(result)\n         return \"Data received!\"\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if result > - 1 : <NEWLINE> <INDENT> tmp = result <NEWLINE> result = - 1 <NEWLINE> return str ( result ) <NEWLINE> <DEDENT> return <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# TODO: Just the worst"
            ],
            "<STRING>": [
                "\"Data received!\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f117af79947e4da98cd6a68cbf4972f2": {
        "code_string": "@app.task(bind=True, queue=\"rosetta\")\n def exec_operator(self, model, job_name):\n     result = None\n     wf = json2model (model)\n     op_node = wf.spec.get(\"workflow\",{}).get(job_name,{})\n     if op_node:\n         router = Router (wf)\n         result = router.route (wf, op_node, job_name, op_node['code'], op_node['args'])\n         wf.set_result (job_name, result)\n     return result\n",
        "code_toks_joined": "@ app . task ( bind = True , queue = <STRING> ) <NEWLINE> <INDENT> def exec_operator ( self , model , job_name ) : <NEWLINE> <INDENT> result = None <NEWLINE> wf = json2model ( model ) <NEWLINE> op_node = wf . spec . get ( <STRING> , { } ) . get ( job_name , { } ) <NEWLINE> if op_node : <NEWLINE> <INDENT> router = Router ( wf ) <NEWLINE> result = router . route ( wf , op_node , job_name , op_node [ <STRING> ] , op_node [ <STRING> ] ) <NEWLINE> wf . set_result ( job_name , result ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"rosetta\"",
                "\"workflow\"",
                "'code'",
                "'args'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "01ee2f5bd8ee49a69b8288d1dda58450": {
        "code_string": "if '.' in translated_key:\n             translated_value = reduce(lambda i, acc: (acc, i), reversed(translated_key.split('.')[1:] + [value]))\n             translated_key = translated_key.split('.')[0]\n         elif translated_key == '_id':\n             return key, value\n         return translated_key, translated_value\n",
        "code_toks_joined": "if <STRING> in translated_key : <NEWLINE> <INDENT> translated_value = reduce ( lambda i , acc : ( acc , i ) , reversed ( translated_key . split ( <STRING> ) [ 1 : ] + [ value ] ) ) <NEWLINE> translated_key = translated_key . split ( <STRING> ) [ 0 ] <NEWLINE> elif translated_key == <STRING> : <NEWLINE> return key , value <NEWLINE> return translated_key , translated_value <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.'",
                "'.'",
                "'.'",
                "'_id'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "efd9461681a64aa19165bfa211b347b3": {
        "code_string": "message(\"Writing geojson to \" + output_file_geojson)\n         output = open(output_file_geojson, 'w')\n         output.write(geojson_str)\n         output.close()\n",
        "code_toks_joined": "message ( <STRING> + output_file_geojson ) <NEWLINE> <INDENT> output = open ( output_file_geojson , <STRING> ) <NEWLINE> output . write ( geojson_str ) <NEWLINE> output . close ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Writing geojson to \"",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "98e436655e23440785f9887978b1f2c5": {
        "code_string": "@staticmethod\n     def _check_and_parse_output_plugin(string):\n         try:\n             raw_ouputs = string.split(';')\n             outputs = []\n             for output in raw_ouputs:\n                 name, arguments = output.split(':')\n                 outputs.append((name, arguments))\n             if len(output) == 0:\n                 raise ValueError()\n         except ValueError:\n             raise RuntimeError('Incorrect output plugin string')\n         return outputs\n",
        "code_toks_joined": "@ staticmethod <NEWLINE> <INDENT> def _check_and_parse_output_plugin ( string ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> raw_ouputs = string . split ( <STRING> ) <NEWLINE> outputs = [ ] <NEWLINE> for output in raw_ouputs : <NEWLINE> <INDENT> name , arguments = output . split ( <STRING> ) <NEWLINE> outputs . append ( ( name , arguments ) ) <NEWLINE> <DEDENT> if len ( output ) == 0 : <NEWLINE> <INDENT> raise ValueError ( ) <NEWLINE> <DEDENT> <DEDENT> except ValueError : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> return outputs <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "';'",
                "':'",
                "'Incorrect output plugin string'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "50aefcb7f20d4359ae12821ffdefb2c9": {
        "code_string": "if biggest_centroid is None:\n         biggest_centroid = compute_centroid(cluster)\n",
        "code_toks_joined": "if biggest_centroid is None : <NEWLINE> <INDENT> biggest_centroid = compute_centroid ( cluster ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7dc98060df784aed876cf4b0cf778ef5": {
        "code_string": "# get the task type from the task docs\n         temp_path = location_base_uri.split('/')\n         problemDoc_uri = '/'.join(temp_path[:-2]) + '/' + '/'.join(temp_path[-2:]).replace('dataset', 'problem')\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> temp_path = location_base_uri . split ( <STRING> ) <NEWLINE> problemDoc_uri = <STRING> . join ( temp_path [ : - 2 ] ) + <STRING> + <STRING> . join ( temp_path [ - 2 : ] ) . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# get the task type from the task docs"
            ],
            "<STRING>": [
                "'/'",
                "'/'",
                "'/'",
                "'/'",
                "'dataset'",
                "'problem'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7b21bb28e5c647aebf5f854d9ef04d85": {
        "code_string": "# pick the largest connected component\n         G_largest = [0]\n         components = np.zeros(len(G), dtype=int)\n         for i, connected_component in enumerate(nx.connected_components(G)):\n             # check if the component is largest\n             if len(connected_component) > len(G_largest):\n                 # if it is largest - flag as such\n                 G_largest = i\n             # obtain indices associated with the node_ids in this component\n             temp_indices = [i for i, x in enumerate(nodeIDs)\n                             if x in list(connected_component)]\n             components[temp_indices] = i+1\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> G_largest = [ 0 ] <NEWLINE> components = np . zeros ( len ( G ) , dtype = int ) <NEWLINE> for i , connected_component in enumerate ( nx . connected_components ( G ) ) : <NEWLINE> <COMMENT> <NL> <INDENT> if len ( connected_component ) > len ( G_largest ) : <NEWLINE> <COMMENT> <NL> <INDENT> G_largest = i <NEWLINE> <COMMENT> <NL> <DEDENT> temp_indices = [ i for i , x in enumerate ( nodeIDs ) <NEWLINE> <INDENT> if x in list ( connected_component ) ] <NEWLINE> <DEDENT> components [ temp_indices ] = i + 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# pick the largest connected component",
                "# check if the component is largest",
                "# if it is largest - flag as such",
                "# obtain indices associated with the node_ids in this component"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1e4805bd4f954cc6b8d9abc42ae8e6d1": {
        "code_string": "return DictUtils.get_required_value(context, name, **kwargs)\n             else:\n                 if required_error is not None:\n                     kwargs['default_value'] = default_value\n",
        "code_toks_joined": "return DictUtils . get_required_value ( context , name , ** kwargs ) <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> if required_error is not None : <NEWLINE> <INDENT> kwargs [ <STRING> ] = default_value <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'default_value'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1cedcdf84c98435d9f32f661a60c1b18": {
        "code_string": "type_definition = (type_ref + skip(equals) + type_ + skip(finished))  >> _make_type_definition\n",
        "code_toks_joined": "type_definition = ( type_ref + skip ( equals ) + type_ + skip ( finished ) ) >> _make_type_definition <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "653afacbd2a540edb38fdeb48bbcc6ef": {
        "code_string": "structural_type_attr = (attr_name + skip(colon) + type_) >> tuple\n     structural_type_attrs = many(structural_type_attr)\n",
        "code_toks_joined": "structural_type_attr = ( attr_name + skip ( colon ) + type_ ) >> tuple <NEWLINE> <INDENT> structural_type_attrs = many ( structural_type_attr ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bc1e1208581645f1991c26896a04c40b": {
        "code_string": "if not siz=='' and not siz.isdecimal():\n         print(\"not a number\")\n         return \"\"\n",
        "code_toks_joined": "if not siz == <STRING> and not siz . isdecimal ( ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> return <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "''",
                "\"not a number\"",
                "\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bc6dc47e7cf943249d1dd3765d3a1bf8": {
        "code_string": "for item in value:\n             self.validate(value)\n         old_val = copy.deepcopy(self.value)\n         sync_val = {}\n         self.value.clear()\n         self.value.extend(value)\n         self.document.update_sync(name, old_val)\n",
        "code_toks_joined": "for item in value : <NEWLINE> <INDENT> self . validate ( value ) <NEWLINE> old_val = copy . deepcopy ( self . value ) <NEWLINE> sync_val = { } <NEWLINE> self . value . clear ( ) <NEWLINE> self . value . extend ( value ) <NEWLINE> self . document . update_sync ( name , old_val ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9b6bbfc04d2440d095d2cae083e65d4e": {
        "code_string": "parser_verify.add_argument(\"-l\",\"--list\", type=str,\n                               dest='list',\n                               help=\"Verify the list of comma separated commitments against the sequence proof\")\n",
        "code_toks_joined": "parser_verify . add_argument ( <STRING> , <STRING> , type = str , <NEWLINE> <INDENT> dest = <STRING> , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"-l\"",
                "\"--list\"",
                "'list'",
                "\"Verify the list of comma separated commitments against the sequence proof\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "31cd86a51a1d457393b0e923307f8948": {
        "code_string": "new = _css_import_re.sub('', old)\n",
        "code_toks_joined": "new = _css_import_re . sub ( <STRING> , old ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "''"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "85c483f547004b78a36cd94f3af0ac95": {
        "code_string": "def collect_diff_tag(self, want, got):\n         if not self.tag_compare(want.tag, got.tag):\n             tag = '%s (got: %s)' % (want.tag, got.tag)\n         else:\n             tag = got.tag\n         attrs = []\n         any = want.tag == 'any' or 'any' in want.attrib\n         for name, value in sorted(got.attrib.items()):\n             if name not in want.attrib and not any:\n                 attrs.append('-%s=\"%s\"' % (name, self.format_text(value, False)))\n             else:\n                 if name in want.attrib:\n                     text = self.collect_diff_text(value, want.attrib[name], False)\n                 else:\n                     text = self.format_text(value, False)\n                 attrs.append('%s=\"%s\"' % (name, text))\n         if not any:\n             for name, value in sorted(want.attrib.items()):\n                 if name in got.attrib:\n                     continue\n                 attrs.append('+%s=\"%s\"' % (name, self.format_text(value, False)))\n         if attrs:\n             tag = '<%s %s>' % (tag, ' '.join(attrs))\n         else:\n             tag = '<%s>' % tag\n         return tag\n",
        "code_toks_joined": "def collect_diff_tag ( self , want , got ) : <NEWLINE> <INDENT> if not self . tag_compare ( want . tag , got . tag ) : <NEWLINE> <INDENT> tag = <STRING> % ( want . tag , got . tag ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> tag = got . tag <NEWLINE> <DEDENT> attrs = [ ] <NEWLINE> any = want . tag == <STRING> or <STRING> in want . attrib <NEWLINE> for name , value in sorted ( got . attrib . items ( ) ) : <NEWLINE> <INDENT> if name not in want . attrib and not any : <NEWLINE> <INDENT> attrs . append ( <STRING> % ( name , self . format_text ( value , False ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if name in want . attrib : <NEWLINE> <INDENT> text = self . collect_diff_text ( value , want . attrib [ name ] , False ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> text = self . format_text ( value , False ) <NEWLINE> <DEDENT> attrs . append ( <STRING> % ( name , text ) ) <NEWLINE> <DEDENT> <DEDENT> if not any : <NEWLINE> <INDENT> for name , value in sorted ( want . attrib . items ( ) ) : <NEWLINE> <INDENT> if name in got . attrib : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> attrs . append ( <STRING> % ( name , self . format_text ( value , False ) ) ) <NEWLINE> <DEDENT> <DEDENT> if attrs : <NEWLINE> <INDENT> tag = <STRING> % ( tag , <STRING> . join ( attrs ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> tag = <STRING> % tag <NEWLINE> <DEDENT> return tag <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%s (got: %s)'",
                "'any'",
                "'any'",
                "'-%s=\"%s\"'",
                "'%s=\"%s\"'",
                "'+%s=\"%s\"'",
                "'<%s %s>'",
                "' '",
                "'<%s>'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "15fe9c9fb57043d9877e99a6b9627aec": {
        "code_string": "SRC_ioncell_fws = SRCFireworks(task_class=RelaxFWTask, task_input=ioncell_input, spec=spec,\n                                        initialization_info=initialization_info,\n                                        wf_task_index_prefix='ioncell',\n                                        deps={SRC_ion_fws['run_fw'].tasks[0].task_type: '@structure'})\n         fws.extend(SRC_ion_fws['fws'])\n         links_dict.update(SRC_ioncell_fws['links_dict'])\n",
        "code_toks_joined": "SRC_ioncell_fws = SRCFireworks ( task_class = RelaxFWTask , task_input = ioncell_input , spec = spec , <NEWLINE> <INDENT> initialization_info = initialization_info , <NEWLINE> wf_task_index_prefix = <STRING> , <NEWLINE> deps = { SRC_ion_fws [ <STRING> ] . tasks [ 0 ] . task_type : <STRING> } ) <NEWLINE> fws . extend ( SRC_ion_fws [ <STRING> ] ) <NEWLINE> links_dict . update ( SRC_ioncell_fws [ <STRING> ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'ioncell'",
                "'run_fw'",
                "'@structure'",
                "'fws'",
                "'links_dict'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c59d5477eb624764a49473f03e19d8a4": {
        "code_string": "optconf, qadapter_spec, qtk_qadapter = self.run_autoparal(self.abiinput, os.path.abspath('.'), self.ftm)\n         if self.use_SRC_scheme:\n             return FWAction(mod_spec={'_set': {'_queueadapter': qadapter_spec, 'mpi_ncpus': optconf['mpi_ncpus'],\n                                                'optconf': optconf, 'qtk_queueadapter': qadapter_spec.as_dict()}})\n         self.history.log_autoparal(optconf)\n         self.abiinput.set_vars(optconf.vars)\n",
        "code_toks_joined": "optconf , qadapter_spec , qtk_qadapter = self . run_autoparal ( self . abiinput , os . path . abspath ( <STRING> ) , self . ftm ) <NEWLINE> <INDENT> if self . use_SRC_scheme : <NEWLINE> <INDENT> return FWAction ( mod_spec = { <STRING> : { <STRING> : qadapter_spec , <STRING> : optconf [ <STRING> ] , <NEWLINE> <INDENT> <STRING> : optconf , <STRING> : qadapter_spec . as_dict ( ) } } ) <NEWLINE> <DEDENT> <DEDENT> self . history . log_autoparal ( optconf ) <NEWLINE> self . abiinput . set_vars ( optconf . vars ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'.'",
                "'_set'",
                "'_queueadapter'",
                "'mpi_ncpus'",
                "'mpi_ncpus'",
                "'optconf'",
                "'qtk_queueadapter'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0dca2ce5e98f4346af8c717f2b4c8e36": {
        "code_string": "ec_nostress_clamped = myfw_nostress.tasks[-1].get_elastic_tensor(tensor_type='clamped_ion')\n         ec_nostress_relaxed = myfw_nostress.tasks[-1].get_elastic_tensor(tensor_type='relaxed_ion')\n         ec_stress_relaxed = myfw_nostress.tasks[-1].get_elastic_tensor(tensor_type='relaxed_ion_stress_corrected')\n",
        "code_toks_joined": "ec_nostress_clamped = myfw_nostress . tasks [ - 1 ] . get_elastic_tensor ( tensor_type = <STRING> ) <NEWLINE> <INDENT> ec_nostress_relaxed = myfw_nostress . tasks [ - 1 ] . get_elastic_tensor ( tensor_type = <STRING> ) <NEWLINE> ec_stress_relaxed = myfw_nostress . tasks [ - 1 ] . get_elastic_tensor ( tensor_type = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'clamped_ion'",
                "'relaxed_ion'",
                "'relaxed_ion_stress_corrected'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6e829766316d40b3847f453aa379d0f0": {
        "code_string": "def createSRCFireworks(setup_task, run_task, handlers=None, validators=None, spec=None, initialization_info=None,\n                        task_index=None, deps=None):\n     spec = copy.deepcopy(spec)\n     if task_index is not None:\n         src_task_index = SRCTaskIndex.from_any(task_index)\n     else:\n         src_task_index = SRCTaskIndex.from_task(run_task)\n     setup_spec = copy.deepcopy(spec)\n     setup_spec['SRC_task_index'] = task_index\n     pass\n",
        "code_toks_joined": "def createSRCFireworks ( setup_task , run_task , handlers = None , validators = None , spec = None , initialization_info = None , <NEWLINE> <INDENT> task_index = None , deps = None ) : <NEWLINE> spec = copy . deepcopy ( spec ) <NEWLINE> if task_index is not None : <NEWLINE> src_task_index = SRCTaskIndex . from_any ( task_index ) <NEWLINE> else : <NEWLINE> src_task_index = SRCTaskIndex . from_task ( run_task ) <NEWLINE> setup_spec = copy . deepcopy ( spec ) <NEWLINE> setup_spec [ <STRING> ] = task_index <NEWLINE> pass <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'SRC_task_index'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b710c05ae03d4efba05c088dae6c8c8e": {
        "code_string": "links_dict = {setup_fw.fw_id: [run_fw.fw_id],\n                   run_fw.fw_id: [control_task.fw_id]}\n     return {'setup_fw': setup_fw, 'run_fw': run_fw, 'control_fw': control_fw, 'links_dict': links_dict,\n             'fws': [setup_fw, run_fw, control_fw]}\n",
        "code_toks_joined": "links_dict = { setup_fw . fw_id : [ run_fw . fw_id ] , <NEWLINE> <INDENT> run_fw . fw_id : [ control_task . fw_id ] } <NEWLINE> return { <STRING> : setup_fw , <STRING> : run_fw , <STRING> : control_fw , <STRING> : links_dict , <NEWLINE> <STRING> : [ setup_fw , run_fw , control_fw ] } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'setup_fw'",
                "'run_fw'",
                "'control_fw'",
                "'links_dict'",
                "'fws'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2c8e3aac89a048d4bbef95a8c5401d09": {
        "code_string": "# ControlTask\n     control_spec = copy.deepcopy(spec)\n     control_spec = set_short_single_core_to_spec(control_spec)\n     control_spec['SRC_task_index'] = src_task_index\n     control_fw = Firework(control_task, spec=run_spec, name=src_task_index.control_str)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> control_spec = copy . deepcopy ( spec ) <NEWLINE> control_spec = set_short_single_core_to_spec ( control_spec ) <NEWLINE> control_spec [ <STRING> ] = src_task_index <NEWLINE> control_fw = Firework ( control_task , spec = run_spec , name = src_task_index . control_str ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ControlTask"
            ],
            "<STRING>": [
                "'SRC_task_index'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "10301fe5388f4060a60b47769e9cd21c": {
        "code_string": "modified_objects = {}\n         setup_spec_update = {}\n         run_spec_update = {}\n         for target, action in control_report.actions.items():\n             target_object = initial_objects[target]\n             action.apply(target_object)\n             if target not in initial_objects_info:\n                 raise ValueError('Object \"{}\" to be modified was not in the initial_objects'.format(target))\n             if 'updates' not in initial_objects_info[target]:\n                 raise ValueError('Update information not present for object \"{}\"'.format(target))\n             for update in initial_objects_info[target]['updates']:\n                 if update['target'] == 'fw_spec':\n                     if 'mod' in update:\n                         mod = getattr(target_object, update['mod'])()\n                         new_spec[update['key']] = mod\n                         modified_objects[update['key']] = mod\n                     else:\n                         new_spec[update['key']] = target_object\n                         modified_objects[update['key']] = mod\n                 elif update['target'] == 'setup_fw_spec':\n                     if 'mod' in update:\n                         mod = getattr(target_object, update['mod'])()\n                         setup_spec_update[update['key']] = mod\n                     else:\n                         setup_spec_update[update['key']] = target_object\n                 elif update['target'] == 'run_fw_spec':\n                     if 'mod' in update:\n                         mod = getattr(target_object, update['mod'])()\n                         run_spec_update[update['key']] = mod\n                     else:\n                         run_spec_update[update['key']] = target_object\n                 elif update['target'] in ['setup_task', 'run_task']:\n                     task = setup_task if update['target'] == 'setup_task' else run_task\n                     attr = getattr(task, update['attribute'])\n                     if 'mod' in update:\n                         mod = getattr(target_object, update['mod'])()\n                         attr = mod\n                     else:\n                         attr = target_object\n                 else:\n                     raise ValueError('Only changes to fw_spec, setup_task and run_task are allowed right now ...')\n",
        "code_toks_joined": "modified_objects = { } <NEWLINE> <INDENT> setup_spec_update = { } <NEWLINE> run_spec_update = { } <NEWLINE> for target , action in control_report . actions . items ( ) : <NEWLINE> <INDENT> target_object = initial_objects [ target ] <NEWLINE> action . apply ( target_object ) <NEWLINE> if target not in initial_objects_info : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( target ) ) <NEWLINE> <DEDENT> if <STRING> not in initial_objects_info [ target ] : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( target ) ) <NEWLINE> <DEDENT> for update in initial_objects_info [ target ] [ <STRING> ] : <NEWLINE> <INDENT> if update [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> if <STRING> in update : <NEWLINE> <INDENT> mod = getattr ( target_object , update [ <STRING> ] ) ( ) <NEWLINE> new_spec [ update [ <STRING> ] ] = mod <NEWLINE> modified_objects [ update [ <STRING> ] ] = mod <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> new_spec [ update [ <STRING> ] ] = target_object <NEWLINE> modified_objects [ update [ <STRING> ] ] = mod <NEWLINE> <DEDENT> <DEDENT> elif update [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> if <STRING> in update : <NEWLINE> <INDENT> mod = getattr ( target_object , update [ <STRING> ] ) ( ) <NEWLINE> setup_spec_update [ update [ <STRING> ] ] = mod <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> setup_spec_update [ update [ <STRING> ] ] = target_object <NEWLINE> <DEDENT> <DEDENT> elif update [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> if <STRING> in update : <NEWLINE> <INDENT> mod = getattr ( target_object , update [ <STRING> ] ) ( ) <NEWLINE> run_spec_update [ update [ <STRING> ] ] = mod <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> run_spec_update [ update [ <STRING> ] ] = target_object <NEWLINE> <DEDENT> <DEDENT> elif update [ <STRING> ] in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> task = setup_task if update [ <STRING> ] == <STRING> else run_task <NEWLINE> attr = getattr ( task , update [ <STRING> ] ) <NEWLINE> if <STRING> in update : <NEWLINE> <INDENT> mod = getattr ( target_object , update [ <STRING> ] ) ( ) <NEWLINE> attr = mod <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> attr = target_object <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Object \"{}\" to be modified was not in the initial_objects'",
                "'updates'",
                "'Update information not present for object \"{}\"'",
                "'updates'",
                "'target'",
                "'fw_spec'",
                "'mod'",
                "'mod'",
                "'key'",
                "'key'",
                "'key'",
                "'key'",
                "'target'",
                "'setup_fw_spec'",
                "'mod'",
                "'mod'",
                "'key'",
                "'key'",
                "'target'",
                "'run_fw_spec'",
                "'mod'",
                "'mod'",
                "'key'",
                "'key'",
                "'target'",
                "'setup_task'",
                "'run_task'",
                "'target'",
                "'setup_task'",
                "'attribute'",
                "'mod'",
                "'mod'",
                "'Only changes to fw_spec, setup_task and run_task are allowed right now ...'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f8968c60d284adb96f937b92b914283": {
        "code_string": "def simpleLoad(request):\n     '''\n         Handler for load form submission\n     '''\n     loadform = SimpleLoadForm(request.GET)\n     mpas = None\n     if loadform.is_valid():\n         user = loadform.cleaned_data['user']\n         name = loadform.cleaned_data['name']\n         mpas = Mpa.objects.filter(user=user, name=name)\n     return mpaLoad(request, loadform, mpas)\n",
        "code_toks_joined": "def simpleLoad ( request ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> loadform = SimpleLoadForm ( request . GET ) <NEWLINE> mpas = None <NEWLINE> if loadform . is_valid ( ) : <NEWLINE> <INDENT> user = loadform . cleaned_data [ <STRING> ] <NEWLINE> name = loadform . cleaned_data [ <STRING> ] <NEWLINE> mpas = Mpa . objects . filter ( user = user , name = name ) <NEWLINE> <DEDENT> return mpaLoad ( request , loadform , mpas ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'''\n         Handler for load form submission\n     '''",
                "'user'",
                "'name'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1dbfe8321f514e14845793fc3e09d589": {
        "code_string": "# Now that we have a list of requested mpas, lets make sure \n     # that the user actually has permissions to view them all\n     # if any one fails, 403 or 404 will be raised\n     user = request.user\n     from lingcod.sharing.utils import get_viewable_object_or_respond \n     for pk in mpaids:\n         # Does it even exist?\n         try:\n             obj = mpa_class.objects.get(pk=pk)\n         except:\n             raise Http404\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> user = request . user <NEWLINE> from lingcod . sharing . utils import get_viewable_object_or_respond <NEWLINE> for pk in mpaids : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> obj = mpa_class . objects . get ( pk = pk ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> raise Http404 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Now that we have a list of requested mpas, lets make sure ",
                "# that the user actually has permissions to view them all",
                "# if any one fails, 403 or 404 will be raised",
                "# Does it even exist?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9517584baadd4461b18c868c20ad09cf": {
        "code_string": "if not keyspace:\n                 # remove not-just-added keyspaces\n                 self.keyspaces = dict((name, meta) for name, meta in self.keyspaces.items()\n                                       if name in added_keyspaces)\n         else:\n             # keyspace is not None, table is not None\n             try:\n                 keyspace_meta = self.keyspaces[keyspace]\n             except KeyError:\n                 # we're trying to update a table in a keyspace we don't know\n                 # about, something went wrong.\n                 # TODO log error, submit schema refresh\n                 pass\n             if keyspace in cf_def_rows:\n                 for table_row in cf_def_rows[keyspace]:\n                     table_meta = self._build_table_metadata(\n                             keyspace_meta, table_row, col_def_rows[keyspace])\n                     keyspace.tables[table_meta.name] = table_meta\n",
        "code_toks_joined": "if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# remove not-just-added keyspaces",
                "# keyspace is not None, table is not None",
                "# we're trying to update a table in a keyspace we don't know",
                "# about, something went wrong.",
                "# TODO log error, submit schema refresh"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e0a7f7058f8a4fee8b2af3b6593c7337": {
        "code_string": "if not keyspace:\n                 # remove not-just-added keyspaces\n                 self.keyspaces = dict((name, meta) for name, meta in self.keyspaces.items()\n                                       if name in added_keyspaces)\n         else:\n             # keyspace is not None, table is not None\n             try:\n                 keyspace_meta = self.keyspaces[keyspace]\n             except KeyError:\n                 # we're trying to update a table in a keyspace we don't know\n                 # about, something went wrong.\n                 # TODO log error, submit schema refresh\n                 pass\n             if keyspace in cf_def_rows:\n                 for table_row in cf_def_rows[keyspace]:\n                     table_meta = self._build_table_metadata(\n                             keyspace_meta, table_row, col_def_rows[keyspace])\n                     keyspace.tables[table_meta.name] = table_meta\n",
        "code_toks_joined": "if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# remove not-just-added keyspaces",
                "# keyspace is not None, table is not None",
                "# we're trying to update a table in a keyspace we don't know",
                "# about, something went wrong.",
                "# TODO log error, submit schema refresh"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "689647c7b9c449bb8a52148ef6eb2f95": {
        "code_string": "def populate(self, cluster, hosts):\n         self._live_hosts = set(hosts)\n         if len(hosts) == 1:\n             self._position = 0\n         else:\n             self._position = randint(0, len(hosts) - 1)\n",
        "code_toks_joined": "def populate ( self , cluster , hosts ) : <NEWLINE> <INDENT> self . _live_hosts = set ( hosts ) <NEWLINE> if len ( hosts ) == 1 : <NEWLINE> <INDENT> self . _position = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . _position = randint ( 0 , len ( hosts ) - 1 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6f9cd3be3d1444fbaea3fc6857575c15": {
        "code_string": "if not issubclass(klass, poly_base):\n                 raise PolyMorphicModelException(\n                     '{} is not a subclass of {}'.format(klass.__name__, poly_base.__name__)\n                 )\n",
        "code_toks_joined": "if not issubclass ( klass , poly_base ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , poly_base . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'{} is not a subclass of {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bf2364c616014989afe5e7f8acb394e8": {
        "code_string": "if not issubclass(klass, cls):\n                 raise PolyMorphicModelException(\n                     '{} is not a subclass of {}'.format(klass.__name__, poly_base.__name__)\n                 )\n",
        "code_toks_joined": "if not issubclass ( klass , cls ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , poly_base . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'{} is not a subclass of {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "238c1d8616b740dd97d1d7291f52c90b": {
        "code_string": "MultipleObjectsReturnedBase = DoesNotExistBase or attrs.pop('MultipleObjectsReturned', BaseModel.MultipleObjectsReturned)\n         attrs['MultipleObjectsReturned'] = type('MultipleObjectsReturned', (MultipleObjectsReturnedBase,), {})\n",
        "code_toks_joined": "MultipleObjectsReturnedBase = DoesNotExistBase or attrs . pop ( <STRING> , BaseModel . MultipleObjectsReturned ) <NEWLINE> <INDENT> attrs [ <STRING> ] = type ( <STRING> , ( MultipleObjectsReturnedBase , ) , { } ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'MultipleObjectsReturned'",
                "'MultipleObjectsReturned'",
                "'MultipleObjectsReturned'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9b3dbac038d14951bd293ecc7221f8e3": {
        "code_string": "def assume_role(account, role):\n     sts = boto3.client('sts')\n     response = sts.assume_role(RoleArn=f'arn:aws:iam::{account}:role/{role}',\n                                RoleSessionName=f'{role}-session-{account}')\n     if not response and not response['ResponseMetadata']['HTTPStatusCode'] == 200:\n         raise Exception(f'could not assume {role} in {account}')\n     return boto3.Session(\n         aws_access_key_id=response['Credentials']['AccessKeyId'],\n         aws_secret_access_key=response['Credentials']['SecretAccessKey'],\n         aws_session_token=response['Credentials']['SessionToken'])\n",
        "code_toks_joined": "def assume_role ( account , role ) : <NEWLINE> <INDENT> sts = boto3 . client ( <STRING> ) <NEWLINE> response = sts . assume_role ( RoleArn = <STRING> , <NEWLINE> <INDENT> RoleSessionName = <STRING> ) <NEWLINE> <DEDENT> if not response and not response [ <STRING> ] [ <STRING> ] == 200 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> return boto3 . Session ( <NEWLINE> <INDENT> aws_access_key_id = response [ <STRING> ] [ <STRING> ] , <NEWLINE> aws_secret_access_key = response [ <STRING> ] [ <STRING> ] , <NEWLINE> aws_session_token = response [ <STRING> ] [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'sts'",
                "f'arn:aws:iam::{account}:role/{role}'",
                "f'{role}-session-{account}'",
                "'ResponseMetadata'",
                "'HTTPStatusCode'",
                "f'could not assume {role} in {account}'",
                "'Credentials'",
                "'AccessKeyId'",
                "'Credentials'",
                "'SecretAccessKey'",
                "'Credentials'",
                "'SessionToken'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f2374a7b63984a14a9c2e66c3a2a7102": {
        "code_string": "def create_add_page(site: Site, source: str, target: str, data=None,\n                     content=None):\n     \"\"\"Create a Page() and file on disk\"\"\"\n     init_content = '# Hello, World!'\n     if content is None and not isinstance(str, content):\n         content = init_content\n",
        "code_toks_joined": "def create_add_page ( site : Site , source : str , target : str , data = None , <NEWLINE> <INDENT> content = None ) : <NEWLINE> <STRING> <NEWLINE> init_content = <STRING> <NEWLINE> if content is None and not isinstance ( str , content ) : <NEWLINE> content = init_content <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Create a Page() and file on disk\"\"\"",
                "'# Hello, World!'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d33bd5aeb27740f8a1bd181ea081c939": {
        "code_string": "# Run through the various constraints\n     if 'format' in prop.constraints() and prop.getType() == 'string':\n         match = re.match(prop.constraints()['format'], claim.getTarget())\n         if not match or match.group(0) != claim.getTarget():\n             return False, 'format'\n     if 'oneof' in prop.constraints() and prop.getType() == 'wikibase-item':\n         if not claim.getTarget().getID() in prop.constraints()['oneof']:\n             return False, 'oneof'\n     if 'single' in prop.constraints():\n         if item.getID() in item.claims:\n             return False, 'single'\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if <STRING> in prop . constraints ( ) and prop . getType ( ) == <STRING> : <NEWLINE> <INDENT> match = re . match ( prop . constraints ( ) [ <STRING> ] , claim . getTarget ( ) ) <NEWLINE> if not match or match . group ( 0 ) != claim . getTarget ( ) : <NEWLINE> <INDENT> return False , <STRING> <NEWLINE> <DEDENT> <DEDENT> if <STRING> in prop . constraints ( ) and prop . getType ( ) == <STRING> : <NEWLINE> <INDENT> if not claim . getTarget ( ) . getID ( ) in prop . constraints ( ) [ <STRING> ] : <NEWLINE> <INDENT> return False , <STRING> <NEWLINE> <DEDENT> <DEDENT> if <STRING> in prop . constraints ( ) : <NEWLINE> <INDENT> if item . getID ( ) in item . claims : <NEWLINE> <INDENT> return False , <STRING> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Run through the various constraints"
            ],
            "<STRING>": [
                "'format'",
                "'string'",
                "'format'",
                "'format'",
                "'oneof'",
                "'wikibase-item'",
                "'oneof'",
                "'oneof'",
                "'single'",
                "'single'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "67e22636c4e34d769b551674e912be9f": {
        "code_string": "# finally, override with given cli args\n             for k, v in cli_args.iteritems():\n                 if k not in self._data or k is not None:\n                     self._data[k] = v\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> for k , v in cli_args . iteritems ( ) : <NEWLINE> <INDENT> if k not in self . _data or k is not None : <NEWLINE> <INDENT> self . _data [ k ] = v <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# finally, override with given cli args"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "86036d8ee4b145a6a7d33c91abbe4228": {
        "code_string": "# we need to get lists of our intermediates and trusted certificates\n         intermediates, trust_roots = [], []\n         for store in self.stores:\n             for cert in store:\n                 asn1cert = certificate.to_asn1crypto\n                 (trust_roots if store.trusted else intermediates).append(asn1cert)\n                 all_certs[asn1cert] = cert\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> intermediates , trust_roots = [ ] , [ ] <NEWLINE> for store in self . stores : <NEWLINE> <INDENT> for cert in store : <NEWLINE> <INDENT> asn1cert = certificate . to_asn1crypto <NEWLINE> ( trust_roots if store . trusted else intermediates ) . append ( asn1cert ) <NEWLINE> all_certs [ asn1cert ] = cert <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# we need to get lists of our intermediates and trusted certificates"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a9540d1b78954029b856c296efd1aa0c": {
        "code_string": "def prompt_hex(item):\n     if not isinstance(item, JConfigHex):\n         return\n     if item.is_visible():\n         return\n     print('\\nCONFIG_{0}'.format(item.get_name()))\n     val = 'h'\n     while val == 'h' or val == '':\n         val = raw_input('{0} : '.format(item.get_prompt()))\n         if val == 'h':\n             print_help(item)\n         elif val == '':\n             val = item.get_default_value()\n             if val is not '':\n                 item.set_user_value(val)\n             else:\n                 print('No default value')\n                 print_help(item)\n         else:\n             try:\n                 item.set_user_value(val)\n             except ValueError as ve:\n                 print(ve)\n                 val = 'h'\n",
        "code_toks_joined": "def prompt_hex ( item ) : <NEWLINE> <INDENT> if not isinstance ( item , JConfigHex ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> if item . is_visible ( ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> print ( <STRING> . format ( item . get_name ( ) ) ) <NEWLINE> val = <STRING> <NEWLINE> while val == <STRING> or val == <STRING> : <NEWLINE> <INDENT> val = raw_input ( <STRING> . format ( item . get_prompt ( ) ) ) <NEWLINE> if val == <STRING> : <NEWLINE> <INDENT> print_help ( item ) <NEWLINE> <DEDENT> elif val == <STRING> : <NEWLINE> <INDENT> val = item . get_default_value ( ) <NEWLINE> if val is not <STRING> : <NEWLINE> <INDENT> item . set_user_value ( val ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print_help ( item ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> item . set_user_value ( val ) <NEWLINE> <DEDENT> except ValueError as ve : <NEWLINE> <INDENT> print ( ve ) <NEWLINE> val = <STRING> <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'\\nCONFIG_{0}'",
                "'h'",
                "'h'",
                "''",
                "'{0} : '",
                "'h'",
                "''",
                "''",
                "'No default value'",
                "'h'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b5eed35476f44fe3a0789874222c9f22": {
        "code_string": "return response\n",
        "code_toks_joined": "return response <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "44f00b0fbb0f4b5a88882378dd85b8ed": {
        "code_string": "if migration_folder == '__pycache__':\n                     continue\n",
        "code_toks_joined": "if migration_folder == <STRING> : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'__pycache__'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f3602619d43241449909d80f29a3aadb": {
        "code_string": "for pattern in self.patterns['exclude']:\n             file_pattern, method_pattern = pattern.split(':', 1)\n             if fnmatch.fnmatch(method_file, file_pattern) and \\\n                fnmatch.fnmatch(method_name, method_pattern):\n                 return False\n",
        "code_toks_joined": "for pattern in self . patterns [ <STRING> ] : <NEWLINE> <INDENT> file_pattern , method_pattern = pattern . split ( <STRING> , 1 ) <NEWLINE> if fnmatch . fnmatch ( method_file , file_pattern ) and fnmatch . fnmatch ( method_name , method_pattern ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'exclude'",
                "':'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fca970e23c7c432d95076828cc659b21": {
        "code_string": "if request_format == 'html':\n         return render_template('view_album.html.jinja',\n                 images = image_structs,\n                 album=album)\n     else:\n         return images\n",
        "code_toks_joined": "if request_format == <STRING> : <NEWLINE> <INDENT> return render_template ( <STRING> , <NEWLINE> <INDENT> images = image_structs , <NEWLINE> album = album ) <NEWLINE> else : <NEWLINE> <DEDENT> return images <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'html'",
                "'view_album.html.jinja'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "702e029f3f9c4440b83e97a556a0eea1": {
        "code_string": "if request_format == 'html':\n         return render_template('view_album.html.jinja',\n                 images = image_structs,\n                 album=album)\n     else:\n         return images\n",
        "code_toks_joined": "if request_format == <STRING> : <NEWLINE> <INDENT> return render_template ( <STRING> , <NEWLINE> <INDENT> images = image_structs , <NEWLINE> album = album ) <NEWLINE> else : <NEWLINE> <DEDENT> return images <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'html'",
                "'view_album.html.jinja'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fd767efd139746828ed8b14e7e09a46a": {
        "code_string": "# create a temp dir and do the work\n     cwd = os.getcwd()\n     package_names = [x for x in packages_to_create]\n     package_names.sort()\n     for package_name in package_names:\n         package_version = package_names[package_name]\n         if normalize_package_name(package_name) in excluded_packages:\n             print('%s is excluded' % package_name)\n             continue\n         print('packaging %s...' % package_name)\n         if dry:\n             continue\n         temp_dir = mkdtemp(suffix='-multideb')\n         os.chdir(temp_dir)\n         prepare_package(package_name, package_version, deb_dest_dir, config_parser, allow_unsafe_download, verbose=verbose)\n         if not keep_temp:\n             shutil.rmtree(temp_dir)\n         else:\n             print('%s-%s: %s' % (package_name, package_version, temp_dir))\n     os.chdir(cwd)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> cwd = os . getcwd ( ) <NEWLINE> package_names = [ x for x in packages_to_create ] <NEWLINE> package_names . sort ( ) <NEWLINE> for package_name in package_names : <NEWLINE> <INDENT> package_version = package_names [ package_name ] <NEWLINE> if normalize_package_name ( package_name ) in excluded_packages : <NEWLINE> <INDENT> print ( <STRING> % package_name ) <NEWLINE> continue <NEWLINE> <DEDENT> print ( <STRING> % package_name ) <NEWLINE> if dry : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> temp_dir = mkdtemp ( suffix = <STRING> ) <NEWLINE> os . chdir ( temp_dir ) <NEWLINE> prepare_package ( package_name , package_version , deb_dest_dir , config_parser , allow_unsafe_download , verbose = verbose ) <NEWLINE> if not keep_temp : <NEWLINE> <INDENT> shutil . rmtree ( temp_dir ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> % ( package_name , package_version , temp_dir ) ) <NEWLINE> <DEDENT> <DEDENT> os . chdir ( cwd ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# create a temp dir and do the work"
            ],
            "<STRING>": [
                "'%s is excluded'",
                "'packaging %s...'",
                "'-multideb'",
                "'%s-%s: %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ae3a2eb80f2e4012b868cf5f7046c990": {
        "code_string": "self.api.update({'_id': document.id}, changeset, upsert=upsert)\n",
        "code_toks_joined": "self . api . update ( { <STRING> : document . id } , changeset , upsert = upsert ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'_id'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1f15565782ab44ae947ec2fa1110f5b5": {
        "code_string": "if tag != VERSION:\n             info = \"Git tag: {0} does not match the version of this app: {1}\".format(\n                 tag, VERSION\n             )\n             sys.exit(info)\n",
        "code_toks_joined": "if tag != VERSION : <NEWLINE> <INDENT> info = <STRING> . format ( <NEWLINE> <INDENT> tag , VERSION <NEWLINE> <DEDENT> ) <NEWLINE> sys . exit ( info ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Git tag: {0} does not match the version of this app: {1}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "161cd271b07a4361841685d5a40dee98": {
        "code_string": "@isa(\"jal\", RV32I, opcode=0b1101111)\n class InstructionJAL(InstructionJType):\n     def execute(self, model: Model):\n         model.state.intreg[self.rd] = model.state.pc + 4\n         model.state.pc = self.imm\n",
        "code_toks_joined": "@ isa ( <STRING> , RV32I , opcode = 0b1101111 ) <NEWLINE> <INDENT> class InstructionJAL ( InstructionJType ) : <NEWLINE> <INDENT> def execute ( self , model : Model ) : <NEWLINE> <INDENT> model . state . intreg [ self . rd ] = model . state . pc + 4 <NEWLINE> model . state . pc = self . imm <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"jal\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "71426bc007c1437c968e99900610f46c": {
        "code_string": "to_find = name_list[0]\n         found = None\n         for key, value in data.items():\n             if in_or_eq(to_find, key):\n                 found = self.find(name_list[1:], strict, value, misses)\n             else:\n                 found = self.find(name_list, strict, value, misses-1)\n             if found:\n                 return found\n",
        "code_toks_joined": "to_find = name_list [ 0 ] <NEWLINE> <INDENT> found = None <NEWLINE> for key , value in data . items ( ) : <NEWLINE> <INDENT> if in_or_eq ( to_find , key ) : <NEWLINE> <INDENT> found = self . find ( name_list [ 1 : ] , strict , value , misses ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> found = self . find ( name_list , strict , value , misses - 1 ) <NEWLINE> <DEDENT> if found : <NEWLINE> <INDENT> return found <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ad603a126cf441b4be0a16ed5c1f47c6": {
        "code_string": "data = read_csv(input, sep=sep, rm_comment=True)\n     if not header:\n         # Remove header\n         _ = data.pop(0)\n",
        "code_toks_joined": "data = read_csv ( input , sep = sep , rm_comment = True ) <NEWLINE> <INDENT> if not header : <NEWLINE> <COMMENT> <NL> <INDENT> _ = data . pop ( 0 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Remove header"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f3ba9c4fc6ef449995b2a17ad84813f6": {
        "code_string": "try:\n             test1 = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n             test2 = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n         except:\n             raise ValueError(\"Invalid datetime format; use 'YYYY-MM-DD hh:mm:ss'\")\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> test1 = datetime . strptime ( start , <STRING> ) <NEWLINE> test2 = datetime . strptime ( start , <STRING> ) <NEWLINE> except : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'%Y-%m-%d %H:%M:%S'",
                "'%Y-%m-%d %H:%M:%S'",
                "\"Invalid datetime format; use 'YYYY-MM-DD hh:mm:ss'\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fe08e38881c04ace816b8de1e6b9c007": {
        "code_string": "if (entity!='activities' or (entity in ['contacts', 'accounts'] and field in ['createdAt', 'updatedAt'])):\n             fieldDef = self.GetFields(entity=entity, fields=[field], cdoID=cdoID)\n",
        "code_toks_joined": "if ( entity != <STRING> or ( entity in [ <STRING> , <STRING> ] and field in [ <STRING> , <STRING> ] ) ) : <NEWLINE> <INDENT> fieldDef = self . GetFields ( entity = entity , fields = [ field ] , cdoID = cdoID ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'activities'",
                "'contacts'",
                "'accounts'",
                "'createdAt'",
                "'updatedAt'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ffaaec9c80b943f1b1a630e601db10a5": {
        "code_string": "if not accesskey or not secretkey or not repopwd or not endpoint or not bucket and not targetdir:\n         raise Exception('One of the required parameters (accesskey, secretkey, repopwd, endpoint, bucket, targetdir) is missing. ')\n",
        "code_toks_joined": "if not accesskey or not secretkey or not repopwd or not endpoint or not bucket and not targetdir : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'One of the required parameters (accesskey, secretkey, repopwd, endpoint, bucket, targetdir) is missing. '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "22bc975a13fa41378d0a2590bd04bcb8": {
        "code_string": "command = (['docker', 'run', '--rm',\n                     '-v', '%s:/var/simdata/' % tmp,\n                     'celliern/energy_plus:%s' % docker_tag,\n                     'EnergyPlus',\n                     \"-w\", \"/var/simdata/%s\" % weather_file.basename(),\n                     \"-p\", prefix,\n                     \"-d\", \"/var/simdata/\"] +\n                    ([\"-i\", \"/var/simdata/%s\" % idd_file.basename()]\n                     if idd_file is not None else []) +\n                    [\"-s\", \"d\",\n                     \"-r\",\n                     \"/var/simdata/%s\" % idf_file.basename()])\n         return command\n     else:\n         command = (['EnergyPlus',\n                     \"-w\", tmp / weather_file.basename(),\n                     \"-p\", prefix,\n                     \"-d\", tmp.abspath()] +\n                    ([\"-i\", tmp / idf_file.basename()]\n                     if idd_file is not None else []) +\n                    [\"-s\", \"d\",\n                     \"-r\",\n                     tmp / idf_file.basename()])\n         return command\n",
        "code_toks_joined": "command = ( [ <STRING> , <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> , <STRING> % tmp , <NEWLINE> <STRING> % docker_tag , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <STRING> % weather_file . basename ( ) , <NEWLINE> <STRING> , prefix , <NEWLINE> <STRING> , <STRING> ] + <NEWLINE> ( [ <STRING> , <STRING> % idd_file . basename ( ) ] <NEWLINE> if idd_file is not None else [ ] ) + <NEWLINE> [ <STRING> , <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> % idf_file . basename ( ) ] ) <NEWLINE> return command <NEWLINE> else : <NEWLINE> command = ( [ <STRING> , <NEWLINE> <STRING> , tmp / weather_file . basename ( ) , <NEWLINE> <STRING> , prefix , <NEWLINE> <STRING> , tmp . abspath ( ) ] + <NEWLINE> ( [ <STRING> , tmp / idf_file . basename ( ) ] <NEWLINE> if idd_file is not None else [ ] ) + <NEWLINE> [ <STRING> , <STRING> , <NEWLINE> <STRING> , <NEWLINE> tmp / idf_file . basename ( ) ] ) <NEWLINE> return command <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'docker'",
                "'run'",
                "'--rm'",
                "'-v'",
                "'%s:/var/simdata/'",
                "'celliern/energy_plus:%s'",
                "'EnergyPlus'",
                "\"-w\"",
                "\"/var/simdata/%s\"",
                "\"-p\"",
                "\"-d\"",
                "\"/var/simdata/\"",
                "\"-i\"",
                "\"/var/simdata/%s\"",
                "\"-s\"",
                "\"d\"",
                "\"-r\"",
                "\"/var/simdata/%s\"",
                "'EnergyPlus'",
                "\"-w\"",
                "\"-p\"",
                "\"-d\"",
                "\"-i\"",
                "\"-s\"",
                "\"d\"",
                "\"-r\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bb68fd896e384b648a07bafd89b961f9": {
        "code_string": "self.ping_timeout_job = gevent.spawn_later(ping_timeout, self.ping_timeout)\n",
        "code_toks_joined": "self . ping_timeout_job = gevent . spawn_later ( ping_timeout , self . ping_timeout ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6a726548951d4146b9a680db65aa0c71": {
        "code_string": "all_event = []\n     nr_event = 0\n     with open(filename, 'rb') as f:\n         while True:\n             nr_event += 1\n             if progress and nr_event % 10000 == 0:\n                 print('.', end='')\n             data = f.read(4)\n             if data == b'':\n                 # no more data available\n                 break\n             s = (((((data[3] << 8) + data[2]) << 8) + data[1]) << 8) + data[0]\n             finish_code = (s & (1 << 31)) >> 31\n             event_length = (s & ((1 << 14)-1) << 17) >> 17\n             header_length = (s & (0b11111 << 12)) >> 12\n             crate_id = (s & (0b1111 << 8)) >> 8\n             slot_id = (s & (0b1111 << 4)) >> 4\n             channel_nr = s & 0b1111\n             data = f.read(4)\n             eventtime_lo = (((((data[3] << 8) + data[2]) << 8) + data[1]) << 8) + data[0]\n             data = f.read(4)\n             s = (data[3] << 8) + data[2]\n             cfd_trigger_bits = (s & (0b111 << 13)) >> 13\n             cfd_fractional = s & ((1 << 13)-1)\n             eventtime_hi = (data[1] << 8) + data[0]\n             data = f.read(4)\n             s = (data[3] << 8) + data[2]\n             trace_flag = (s & (1 << 15)) >> 15\n             trace_length = s & ((1 << 16) - 1)\n             event_energy = (data[1] << 8) + data[0]\n             for i in range(header_length-4):\n                 data = f.read(4)\n             if event_length-header_length > 0:\n                 # read the whole trace in one go to make reading faster than reading byte by byte\n                 # about 300x faster than multiple f.read(2)\n                 trace = np.fromfile(f, '({},2)<u2'.format(event_length-header_length), count=1)[0, :, :]\n                 # need to swap columns and flatten array\n                 if not keep_trace:\n                     trace[:, 0], trace[:, 1] = trace[:, 1], trace[:, 0].copy()\n                     trace = trace.flatten()\n             else:\n                 trace = []\n             if not keep_trace:\n                 trace = []\n             TS = 2**32 * mpmath.mpf(eventtime_hi) + mpmath.mpf(eventtime_lo)\n             TS*= 10e-9\n             CFD_error = False\n             if cfd_trigger_bits == 7:\n                 CFD_error = True\n             else:\n                 TS += mpmath.mpf(cfd_trigger_bits-1 + cfd_fractional/8192)*2e-9\n             yield Event(channel_nr, crate_id, slot_id, TS, event_energy, trace, CFD_error)\n     return\n",
        "code_toks_joined": "all_event = [ ] <NEWLINE> <INDENT> nr_event = 0 <NEWLINE> with open ( filename , <STRING> ) as f : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> nr_event += 1 <NEWLINE> if progress and nr_event % 10000 == 0 : <NEWLINE> <INDENT> print ( <STRING> , end = <STRING> ) <NEWLINE> <DEDENT> data = f . read ( 4 ) <NEWLINE> if data == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> break <NEWLINE> <DEDENT> s = ( ( ( ( ( data [ 3 ] << 8 ) + data [ 2 ] ) << 8 ) + data [ 1 ] ) << 8 ) + data [ 0 ] <NEWLINE> finish_code = ( s & ( 1 << 31 ) ) >> 31 <NEWLINE> event_length = ( s & ( ( 1 << 14 ) - 1 ) << 17 ) >> 17 <NEWLINE> header_length = ( s & ( 0b11111 << 12 ) ) >> 12 <NEWLINE> crate_id = ( s & ( 0b1111 << 8 ) ) >> 8 <NEWLINE> slot_id = ( s & ( 0b1111 << 4 ) ) >> 4 <NEWLINE> channel_nr = s & 0b1111 <NEWLINE> data = f . read ( 4 ) <NEWLINE> eventtime_lo = ( ( ( ( ( data [ 3 ] << 8 ) + data [ 2 ] ) << 8 ) + data [ 1 ] ) << 8 ) + data [ 0 ] <NEWLINE> data = f . read ( 4 ) <NEWLINE> s = ( data [ 3 ] << 8 ) + data [ 2 ] <NEWLINE> cfd_trigger_bits = ( s & ( 0b111 << 13 ) ) >> 13 <NEWLINE> cfd_fractional = s & ( ( 1 << 13 ) - 1 ) <NEWLINE> eventtime_hi = ( data [ 1 ] << 8 ) + data [ 0 ] <NEWLINE> data = f . read ( 4 ) <NEWLINE> s = ( data [ 3 ] << 8 ) + data [ 2 ] <NEWLINE> trace_flag = ( s & ( 1 << 15 ) ) >> 15 <NEWLINE> trace_length = s & ( ( 1 << 16 ) - 1 ) <NEWLINE> event_energy = ( data [ 1 ] << 8 ) + data [ 0 ] <NEWLINE> for i in range ( header_length - 4 ) : <NEWLINE> <INDENT> data = f . read ( 4 ) <NEWLINE> <DEDENT> if event_length - header_length > 0 : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> trace = np . fromfile ( f , <STRING> . format ( event_length - header_length ) , count = 1 ) [ 0 , : , : ] <NEWLINE> <COMMENT> <NL> if not keep_trace : <NEWLINE> <INDENT> trace [ : , 0 ] , trace [ : , 1 ] = trace [ : , 1 ] , trace [ : , 0 ] . copy ( ) <NEWLINE> trace = trace . flatten ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> trace = [ ] <NEWLINE> <DEDENT> if not keep_trace : <NEWLINE> <INDENT> trace = [ ] <NEWLINE> <DEDENT> TS = 2 ** 32 * mpmath . mpf ( eventtime_hi ) + mpmath . mpf ( eventtime_lo ) <NEWLINE> TS *= 10e-9 <NEWLINE> CFD_error = False <NEWLINE> if cfd_trigger_bits == 7 : <NEWLINE> <INDENT> CFD_error = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> TS += mpmath . mpf ( cfd_trigger_bits - 1 + cfd_fractional / 8192 ) * 2e-9 <NEWLINE> <DEDENT> yield Event ( channel_nr , crate_id , slot_id , TS , event_energy , trace , CFD_error ) <NEWLINE> <DEDENT> <DEDENT> return <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'rb'",
                "'.'",
                "''",
                "b''",
                "'({},2)<u2'"
            ],
            "<COMMENT>": [
                "# no more data available",
                "# read the whole trace in one go to make reading faster than reading byte by byte",
                "# about 300x faster than multiple f.read(2)",
                "# need to swap columns and flatten array"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a051fe6a9fcb4158a854224bcd9ecec7": {
        "code_string": "return CFD, cfdtime, errors\n",
        "code_toks_joined": "return CFD , cfdtime , errors <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "49f53da2ca264e07b0512fa681afc789": {
        "code_string": "def to_namedtuple(name, d):\n     keys = list(d.keys())\n     namedtuple = collections.namedtuple(name, d)\n     return namedtuple(**d)\n",
        "code_toks_joined": "def to_namedtuple ( name , d ) : <NEWLINE> <INDENT> keys = list ( d . keys ( ) ) <NEWLINE> namedtuple = collections . namedtuple ( name , d ) <NEWLINE> return namedtuple ( ** d ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6c67f6ac140a4042ab12422742a43321": {
        "code_string": "def test_smoothline():\n     canvas = pyagg.Canvas(1000,500)\n     canvas.percent_space()\n     for x1,x2 in zip(range(-50, 100, 10), range(0, 150, 10)):\n         canvas.draw_line([x2,0, x1,100])\n     canvas.draw_line([10,10, 50,90, 90,10],\n                      smooth=True,\n                      fillcolor=(222,0,0),\n                      fillsize=2)\n     canvas.draw_text((50,50), \"Hello\", textfont=\"segoe print bold\", textsize=55)\n     return canvas\n",
        "code_toks_joined": "def test_smoothline ( ) : <NEWLINE> <INDENT> canvas = pyagg . Canvas ( 1000 , 500 ) <NEWLINE> canvas . percent_space ( ) <NEWLINE> for x1 , x2 in zip ( range ( - 50 , 100 , 10 ) , range ( 0 , 150 , 10 ) ) : <NEWLINE> <INDENT> canvas . draw_line ( [ x2 , 0 , x1 , 100 ] ) <NEWLINE> <DEDENT> canvas . draw_line ( [ 10 , 10 , 50 , 90 , 90 , 10 ] , <NEWLINE> <INDENT> smooth = True , <NEWLINE> fillcolor = ( 222 , 0 , 0 ) , <NEWLINE> fillsize = 2 ) <NEWLINE> <DEDENT> canvas . draw_text ( ( 50 , 50 ) , <STRING> , textfont = <STRING> , textsize = 55 ) <NEWLINE> return canvas <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Hello\"",
                "\"segoe print bold\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0e297511c6d740be927ba4cbba4a53a4": {
        "code_string": "if self.has_duplicates():\n             return super().contains_element(element)\n",
        "code_toks_joined": "if self . has_duplicates ( ) : <NEWLINE> <INDENT> return super ( ) . contains_element ( element ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "52e1b1cc4a0f4ce29fb76a5d7afffca1": {
        "code_string": "if intersection.area == box_list[0].area:\n             if intersection.area == PyAlgorithm.unionRects(rect_list).area:\n                 return intersection\n             else:\n                 return box(0,0,0,0)\n         else:\n             return intersection\n",
        "code_toks_joined": "if intersection . area == box_list [ 0 ] . area : <NEWLINE> <INDENT> if intersection . area == PyAlgorithm . unionRects ( rect_list ) . area : <NEWLINE> <INDENT> return intersection <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return box ( 0 , 0 , 0 , 0 ) <NEWLINE> else : <NEWLINE> <DEDENT> return intersection <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fa6c0839a3264e66ac7a9231084587df": {
        "code_string": "# Iterate over all families to process, i.e. a separate DP table is created\n \t\t\t# for each family.\n \t\t\tfor representative_sample, family in families.items():\n \t\t\t\tif len(family) == 1:\n \t\t\t\t\tlogger.info('---- Processing individual %s', sample)\n \t\t\t\telse:\n \t\t\t\t\tlogger.info('---- Processing family with individuals: %s', ','.join(family))\n \t\t\t\tmax_coverage_per_sample = max(1, max_coverage // len(family))\n \t\t\t\tlogger.info('Using maximum coverage per sample of %dX', max_coverage_per_sample)\n \t\t\t\ttrios = family_trios[representative_sample]\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> for representative_sample , family in families . items ( ) : <NEWLINE> <INDENT> if len ( family ) == 1 : <NEWLINE> <INDENT> logger . info ( <STRING> , sample ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . info ( <STRING> , <STRING> . join ( family ) ) <NEWLINE> <DEDENT> max_coverage_per_sample = max ( 1 , max_coverage // len ( family ) ) <NEWLINE> logger . info ( <STRING> , max_coverage_per_sample ) <NEWLINE> trios = family_trios [ representative_sample ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Iterate over all families to process, i.e. a separate DP table is created",
                "# for each family."
            ],
            "<STRING>": [
                "'---- Processing individual %s'",
                "'---- Processing family with individuals: %s'",
                "','",
                "'Using maximum coverage per sample of %dX'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fe06bc0f5a5242dab60a063b1783b429": {
        "code_string": "def eval_overlap(n1, n2):\n \t\"\"\"\n \tReturn a tuple containing the number of matches (resp.,\n \tmismatches) between a pair (n1,n2) of overlapping reads\n \t\"\"\"\n \thang1 = n2['begin'] - n1['begin']\n \toverlap = zip(n1['sites'][hang1:], n2['sites'])\n \tmatch, mismatch = (0, 0)\n \tfor (c1, c2) in overlap:\n \t\tif c1 in ['A', 'C', 'G', 'T'] and c1 in ['A', 'C', 'G', 'T']:\n \t\t\tif c1 == c2:\n \t\t\t\tmatch += 1\n \t\t\telse:\n \t\t\t\tmismatch += 1\n \treturn (match, mismatch)\n",
        "code_toks_joined": "def eval_overlap ( n1 , n2 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> hang1 = n2 [ <STRING> ] - n1 [ <STRING> ] <NEWLINE> overlap = zip ( n1 [ <STRING> ] [ hang1 : ] , n2 [ <STRING> ] ) <NEWLINE> match , mismatch = ( 0 , 0 ) <NEWLINE> for ( c1 , c2 ) in overlap : <NEWLINE> <INDENT> if c1 in [ <STRING> , <STRING> , <STRING> , <STRING> ] and c1 in [ <STRING> , <STRING> , <STRING> , <STRING> ] : <NEWLINE> <INDENT> if c1 == c2 : <NEWLINE> <INDENT> match += 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> mismatch += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return ( match , mismatch ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n \tReturn a tuple containing the number of matches (resp.,\n \tmismatches) between a pair (n1,n2) of overlapping reads\n \t\"\"\"",
                "'begin'",
                "'begin'",
                "'sites'",
                "'sites'",
                "'A'",
                "'C'",
                "'G'",
                "'T'",
                "'A'",
                "'C'",
                "'G'",
                "'T'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2db9253e3b9c4759afcd3dcde6529327": {
        "code_string": "if given_args_length < command_args_length:\n                 arg_index = command_args_length - given_args_length\n                 if arg_index >= len(comm['required_args']):\n                     arg_index = 0\n                 error = colorize(comm['required_args'][arg_index], 'blue') + ' is required'\n             elif given_args_length > max_args_length:\n                 error = '{} argument(s) were expected, but {} were given.'.format(\n                     command_args_length, given_args_length)\n             else:\n                 # Must be valid arguments list\n                 # Call the handler\n                 error = comm['handler'](*args)\n",
        "code_toks_joined": "if given_args_length < command_args_length : <NEWLINE> <INDENT> arg_index = command_args_length - given_args_length <NEWLINE> if arg_index >= len ( comm [ <STRING> ] ) : <NEWLINE> <INDENT> arg_index = 0 <NEWLINE> <DEDENT> error = colorize ( comm [ <STRING> ] [ arg_index ] , <STRING> ) + <STRING> <NEWLINE> elif given_args_length > max_args_length : <NEWLINE> error = <STRING> . format ( <NEWLINE> <INDENT> command_args_length , given_args_length ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> error = comm [ <STRING> ] ( * args ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'required_args'",
                "'required_args'",
                "'blue'",
                "' is required'",
                "'{} argument(s) were expected, but {} were given.'",
                "'handler'"
            ],
            "<COMMENT>": [
                "# Must be valid arguments list",
                "# Call the handler"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "46752a52839645f69223bafd921182ca": {
        "code_string": "logger.debug('Attempting to add record to zone %s: %s', zone_id, data)\n         response = self._send_request('POST', '/v1/user/self/zone/{0}/record'.format(domain), data)\n",
        "code_toks_joined": "logger . debug ( <STRING> , zone_id , data ) <NEWLINE> <INDENT> response = self . _send_request ( <STRING> , <STRING> . format ( domain ) , data ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Attempting to add record to zone %s: %s'",
                "'POST'",
                "'/v1/user/self/zone/{0}/record'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "427a708613d544ec84450b39620c3af5": {
        "code_string": "def _parse_filename(f_arg, d_arg, name_contains):\n     if f_arg is None:\n         for file in os.listdir(os.path.join(os.path.dirname(__file__), d_arg)):\n             if file.endswith(\".json\"):\n                 if name_contains in file:\n                     return file\n     return None\n",
        "code_toks_joined": "def _parse_filename ( f_arg , d_arg , name_contains ) : <NEWLINE> <INDENT> if f_arg is None : <NEWLINE> <INDENT> for file in os . listdir ( os . path . join ( os . path . dirname ( __file__ ) , d_arg ) ) : <NEWLINE> <INDENT> if file . endswith ( <STRING> ) : <NEWLINE> <INDENT> if name_contains in file : <NEWLINE> <INDENT> return file <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\".json\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ded6e6bef0554f1b8a40ea50ce982da6": {
        "code_string": "def handle_batch_place(self, frame):\n         for x in frame['payload']:\n             self.handle_place(frame)\n",
        "code_toks_joined": "def handle_batch_place ( self , frame ) : <NEWLINE> <INDENT> for x in frame [ <STRING> ] : <NEWLINE> <INDENT> self . handle_place ( frame ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'payload'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4cb7156447a84cb6b43a1eb3f3f1ff5f": {
        "code_string": "for member in pyson.evaluate(term.args[1], intention.scope):\n         agent.stack.append(choicepoint)\n",
        "code_toks_joined": "for member in pyson . evaluate ( term . args [ 1 ] , intention . scope ) : <NEWLINE> <INDENT> agent . stack . append ( choicepoint ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9018b16be29643689524bfaaa3efc934": {
        "code_string": "def _request(self, name, namespace, devId=None, payload={}):\n         headers = {\n             'Content-Type': 'application/json'\n         }\n         header = {\n             'name': name,\n             'namespace': namespace,\n             'payloadVersion': 1,\n         }\n         payload['accessToken'] = SESSION.accessToken\n         if namespace != 'discovery':\n             payload[\"devId\"] = devId\n         data = {\n             'header': header,\n             'payload': payload\n         }\n         response = requests.post(\n             (TUYACLOUDURL+'/homeassistant/skill').format(SESSION.region),\n             json = data\n         )\n         if not response.ok:\n             _LOGGER.warning(\"request error, status code is %d, device %s\", devId, response.status_code)\n             return\n         response_json = response.json()\n         if response_json['header']['code'] != 'SUCCESS':\n             _LOGGER.debug(\"control device error, error code is \" +\n                 response_json['header']['code'])\n         return response_json\n",
        "code_toks_joined": "def _request ( self , name , namespace , devId = None , payload = { } ) : <NEWLINE> <INDENT> headers = { <NEWLINE> <INDENT> <STRING> : <STRING> <NEWLINE> <DEDENT> } <NEWLINE> header = { <NEWLINE> <INDENT> <STRING> : name , <NEWLINE> <STRING> : namespace , <NEWLINE> <STRING> : 1 , <NEWLINE> <DEDENT> } <NEWLINE> payload [ <STRING> ] = SESSION . accessToken <NEWLINE> if namespace != <STRING> : <NEWLINE> <INDENT> payload [ <STRING> ] = devId <NEWLINE> <DEDENT> data = { <NEWLINE> <INDENT> <STRING> : header , <NEWLINE> <STRING> : payload <NEWLINE> <DEDENT> } <NEWLINE> response = requests . post ( <NEWLINE> <INDENT> ( TUYACLOUDURL + <STRING> ) . format ( SESSION . region ) , <NEWLINE> json = data <NEWLINE> <DEDENT> ) <NEWLINE> if not response . ok : <NEWLINE> <INDENT> _LOGGER . warning ( <STRING> , devId , response . status_code ) <NEWLINE> return <NEWLINE> <DEDENT> response_json = response . json ( ) <NEWLINE> if response_json [ <STRING> ] [ <STRING> ] != <STRING> : <NEWLINE> <INDENT> _LOGGER . debug ( <STRING> + <NEWLINE> <INDENT> response_json [ <STRING> ] [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> return response_json <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Content-Type'",
                "'application/json'",
                "'name'",
                "'namespace'",
                "'payloadVersion'",
                "'accessToken'",
                "'discovery'",
                "\"devId\"",
                "'header'",
                "'payload'",
                "'/homeassistant/skill'",
                "\"request error, status code is %d, device %s\"",
                "'header'",
                "'code'",
                "'SUCCESS'",
                "\"control device error, error code is \"",
                "'header'",
                "'code'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d4d7a1165f44e3dbaaa1dffad703901": {
        "code_string": "# MET Resolution\n         dimu_pt, dimu_phi, dimu_para, dimu_perp = create_metres(\n             event.METnoX, event.MuonSelection,\n         )\n         event.DiMuon_pt = dimu_pt\n         event.DiMuon_phi = dimu_phi\n         event.METnoX_diMuonParaProjPt = dimu_para\n         event.METnoX_diMuonPerpProjPt = dimu_perp\n         event.METnoX_diMuonParaProjPt_Minus_DiMuon_pt = dimu_para - dimu_pt\n         event.METnoX_diMuonPerpProjPt_Plus_DiMuon_pt = dimu_perp + dimu_pt\n         event.METnoX_diMuonParaProjPt_Div_DiMuon_pt = dimu_para / dimu_pt\n         event.METnoX_diMuonPerpProjPt_Plus_DiMuon_pt_Div_DiMuon_pt = (dimu_para + dimu_pt) / dimu_pt\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> dimu_pt , dimu_phi , dimu_para , dimu_perp = create_metres ( <NEWLINE> <INDENT> event . METnoX , event . MuonSelection , <NEWLINE> <DEDENT> ) <NEWLINE> event . DiMuon_pt = dimu_pt <NEWLINE> event . DiMuon_phi = dimu_phi <NEWLINE> event . METnoX_diMuonParaProjPt = dimu_para <NEWLINE> event . METnoX_diMuonPerpProjPt = dimu_perp <NEWLINE> event . METnoX_diMuonParaProjPt_Minus_DiMuon_pt = dimu_para - dimu_pt <NEWLINE> event . METnoX_diMuonPerpProjPt_Plus_DiMuon_pt = dimu_perp + dimu_pt <NEWLINE> event . METnoX_diMuonParaProjPt_Div_DiMuon_pt = dimu_para / dimu_pt <NEWLINE> event . METnoX_diMuonPerpProjPt_Plus_DiMuon_pt_Div_DiMuon_pt = ( dimu_para + dimu_pt ) / dimu_pt <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# MET Resolution"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5abda6006f1b4da0aa955f69d165da77": {
        "code_string": "# get libraries statistics using 1% of mapped read limit\n     libdata = fastq2insert_size(sys.stderr, fastq, fasta, mapq, threads, \\\n                                 limit/100, verbose)\n     # separate paired-end & mate pairs\n     ## also separate 300 and 600 paired-ends\n     libraries = []\n     # add libraries strating from lowest insert size\n     for fq1, fq2, ismedian, ismean, isstd, pairs in sorted(libdata, key=lambda x: x[3]):\n         # add new library set if \n         if not libraries or ismedian > 1.5*libraries[-1][4][0]:\n             # libnames, libFs, libRs, orientations, libIS, libISStDev\n             libraries.append([[], [], [], [], [], []])\n             i = 1\n         # add libname & fastq files\n         libraries[-1][0].append(\"lib%s\"%i)\n         libraries[-1][1].append(open(fq1))\n         libraries[-1][2].append(open(fq2))\n         # orientation\n         orientation = get_orientation(pairs, fq1, fq2)\n         libraries[-1][3].append(orientation)\n         # insert size information\n         libraries[-1][4].append(int(ismean))\n         stdfrac = isstd / ismean\n         # capture large stdev\n         if stdfrac > 0.66:\n             sys.stderr.write(\"[WARNING] Highly variable insert size (%.f +- %.2f) in %s - %s!\\n\"%(ismean, isstd, fq1, fq2))\n         # SSSPACE accepts stdfrac 0-1.0\n         if stdfrac > 1:\n             stdfrac = 1.0\n         libraries[-1][5].append(stdfrac)\n         # update counter\n         i += 1\n     return libraries\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> libdata = fastq2insert_size ( sys . stderr , fastq , fasta , mapq , threads , limit / 100 , verbose ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> libraries = [ ] <NEWLINE> <COMMENT> <NL> for fq1 , fq2 , ismedian , ismean , isstd , pairs in sorted ( libdata , key = lambda x : x [ 3 ] ) : <NEWLINE> <COMMENT> <NL> <INDENT> if not libraries or ismedian > 1.5 * libraries [ - 1 ] [ 4 ] [ 0 ] : <NEWLINE> <COMMENT> <NL> <INDENT> libraries . append ( [ [ ] , [ ] , [ ] , [ ] , [ ] , [ ] ] ) <NEWLINE> i = 1 <NEWLINE> <COMMENT> <NL> <DEDENT> libraries [ - 1 ] [ 0 ] . append ( <STRING> % i ) <NEWLINE> libraries [ - 1 ] [ 1 ] . append ( open ( fq1 ) ) <NEWLINE> libraries [ - 1 ] [ 2 ] . append ( open ( fq2 ) ) <NEWLINE> <COMMENT> <NL> orientation = get_orientation ( pairs , fq1 , fq2 ) <NEWLINE> libraries [ - 1 ] [ 3 ] . append ( orientation ) <NEWLINE> <COMMENT> <NL> libraries [ - 1 ] [ 4 ] . append ( int ( ismean ) ) <NEWLINE> stdfrac = isstd / ismean <NEWLINE> <COMMENT> <NL> if stdfrac > 0.66 : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> % ( ismean , isstd , fq1 , fq2 ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> if stdfrac > 1 : <NEWLINE> <INDENT> stdfrac = 1.0 <NEWLINE> <DEDENT> libraries [ - 1 ] [ 5 ] . append ( stdfrac ) <NEWLINE> <COMMENT> <NL> i += 1 <NEWLINE> <DEDENT> return libraries <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# get libraries statistics using 1% of mapped read limit",
                "# separate paired-end & mate pairs",
                "## also separate 300 and 600 paired-ends",
                "# add libraries strating from lowest insert size",
                "# add new library set if ",
                "# libnames, libFs, libRs, orientations, libIS, libISStDev",
                "# add libname & fastq files",
                "# orientation",
                "# insert size information",
                "# capture large stdev",
                "# SSSPACE accepts stdfrac 0-1.0",
                "# update counter"
            ],
            "<STRING>": [
                "\"lib%s\"",
                "\"[WARNING] Highly variable insert size (%.f +- %.2f) in %s - %s!\\n\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d7c4246b5d2446b2b15c5c8b852421c5": {
        "code_string": "return Mutation(\n         seq = str(seq_region),\n         start = start_pos,\n         stop = end_pos,\n         mutation_start = aa_position,\n         n_removed = n_aa_deleted,\n         n_inserted = n_aa_inserted,\n         annot = annot)\n",
        "code_toks_joined": "return Mutation ( <NEWLINE> <INDENT> seq = str ( seq_region ) , <NEWLINE> start = start_pos , <NEWLINE> stop = end_pos , <NEWLINE> mutation_start = aa_position , <NEWLINE> n_removed = n_aa_deleted , <NEWLINE> n_inserted = n_aa_inserted , <NEWLINE> annot = annot ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8104f2b003bc47778fa7cc5a0b59e045": {
        "code_string": "def __init__(self, text: str):\n         \"\"\"Find all the id, class, and el elements. Build a set of all of the\n         elements for matching, and store a score using the count of each\"\"\"\n         self.text = text\n         self.items = set()\n         count: Counter = Counter()\n         for key, val in self.parse(text):\n             self.items.add(val)\n             count[key] += 1\n             if key == \"el\":\n                 self.el = key\n",
        "code_toks_joined": "def __init__ ( self , text : str ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . text = text <NEWLINE> self . items = set ( ) <NEWLINE> count : Counter = Counter ( ) <NEWLINE> for key , val in self . parse ( text ) : <NEWLINE> <INDENT> self . items . add ( val ) <NEWLINE> count [ key ] += 1 <NEWLINE> if key == <STRING> : <NEWLINE> <INDENT> self . el = key <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Find all the id, class, and el elements. Build a set of all of the\n         elements for matching, and store a score using the count of each\"\"\"",
                "\"el\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f46dd7d03cca4bbf925e887c274f2f9a": {
        "code_string": "from pprint import pprint\n     with open(filename, 'r') as script:\n         script_content = script.readlines()\n     pprint(script_content)\n     pprint(script_content[0].rstrip())\n     pprint(script_content[0].strip())\n     if (script_content[0].rstrip() != OP_BASH_HEADER) or (script_content[0].rstrip() != OP_SH_HEADER):\n         script_content.insert(0, OP_SH_HEADER + \"\\n\\n\")\n     usr_exec_file = os.path.basename(new_sh_file)\n     pprint(script_content)\n     pprint(\"Path: \" + P_USR_LOCAL_BIN_DIR + usr_exec_file)\n     if os.path.exists(P_USR_LOCAL_BIN_DIR + usr_exec_file):\n         return False\n     else:\n         with open(P_USR_LOCAL_BIN_DIR + usr_exec_file, 'w') as sh_file:\n             for script_line in script_content:\n                 sh_file.write(script_line)\n         os.chmod(P_USR_LOCAL_BIN_DIR + usr_exec_file, 0o755)\n         return True\n",
        "code_toks_joined": "from pprint import pprint <NEWLINE> <INDENT> with open ( filename , <STRING> ) as script : <NEWLINE> <INDENT> script_content = script . readlines ( ) <NEWLINE> <DEDENT> pprint ( script_content ) <NEWLINE> pprint ( script_content [ 0 ] . rstrip ( ) ) <NEWLINE> pprint ( script_content [ 0 ] . strip ( ) ) <NEWLINE> if ( script_content [ 0 ] . rstrip ( ) != OP_BASH_HEADER ) or ( script_content [ 0 ] . rstrip ( ) != OP_SH_HEADER ) : <NEWLINE> <INDENT> script_content . insert ( 0 , OP_SH_HEADER + <STRING> ) <NEWLINE> <DEDENT> usr_exec_file = os . path . basename ( new_sh_file ) <NEWLINE> pprint ( script_content ) <NEWLINE> pprint ( <STRING> + P_USR_LOCAL_BIN_DIR + usr_exec_file ) <NEWLINE> if os . path . exists ( P_USR_LOCAL_BIN_DIR + usr_exec_file ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( P_USR_LOCAL_BIN_DIR + usr_exec_file , <STRING> ) as sh_file : <NEWLINE> <INDENT> for script_line in script_content : <NEWLINE> <INDENT> sh_file . write ( script_line ) <NEWLINE> <DEDENT> <DEDENT> os . chmod ( P_USR_LOCAL_BIN_DIR + usr_exec_file , 0o755 ) <NEWLINE> return True <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'r'",
                "\"\\n\\n\"",
                "\"Path: \"",
                "'w'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e8ea1e0c3eb043688a1f7f94b72effe9": {
        "code_string": "# default output range for int is full data type range\n     if np.issubdtype(dtype, np.integer):\n         if out_min is None:\n             out_min = np.iinfo(dtype).min\n         if in_max is None:\n             out_max = np.iinfo(dtype).max\n     # default range for float is 0:1\n     else:\n         if out_min is None:\n             out_min = 0.\n         if in_max is None:\n             out_max = 1.\n     # how to choose the data type to preserve data from overflow?\n     scaled = (img - in_min) * (out_max - out_min) / (in_max - in_min) + out_min\n     return saturate_cast(scaled, dtype)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if np . issubdtype ( dtype , np . integer ) : <NEWLINE> <INDENT> if out_min is None : <NEWLINE> <INDENT> out_min = np . iinfo ( dtype ) . min <NEWLINE> <DEDENT> if in_max is None : <NEWLINE> <INDENT> out_max = np . iinfo ( dtype ) . max <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if out_min is None : <NEWLINE> <INDENT> out_min = 0. <NEWLINE> <DEDENT> if in_max is None : <NEWLINE> <INDENT> out_max = 1. <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> scaled = ( img - in_min ) * ( out_max - out_min ) / ( in_max - in_min ) + out_min <NEWLINE> return saturate_cast ( scaled , dtype ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# default output range for int is full data type range",
                "# default range for float is 0:1",
                "# how to choose the data type to preserve data from overflow?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fa349ab9721848babdc9c0b6259f4713": {
        "code_string": "def clean(self, value):\n         cleaned_data = []\n         errors = []\n         value = filter(None, value)\n         for index, item in enumerate(value):\n             try:\n                 cleaned_data.append(self.base_field.clean(item))\n             except forms.ValidationError as error:\n                 errors.append(\n                     prefix_validation_error(\n                         error, self.error_messages[\"item_invalid\"], code=\"item_invalid\", params={\"nth\": index}\n                     )\n                 )\n         if errors:\n             raise forms.ValidationError(list(chain.from_iterable(errors)))\n         if cleaned_data and self.required:\n             raise forms.ValidationError(self.error_messages[\"required\"])\n         return cleaned_data\n",
        "code_toks_joined": "def clean ( self , value ) : <NEWLINE> <INDENT> cleaned_data = [ ] <NEWLINE> errors = [ ] <NEWLINE> value = filter ( None , value ) <NEWLINE> for index , item in enumerate ( value ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> cleaned_data . append ( self . base_field . clean ( item ) ) <NEWLINE> <DEDENT> except forms . ValidationError as error : <NEWLINE> <INDENT> errors . append ( <NEWLINE> <INDENT> prefix_validation_error ( <NEWLINE> <INDENT> error , self . error_messages [ <STRING> ] , code = <STRING> , params = { <STRING> : index } <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> if errors : <NEWLINE> <INDENT> raise forms . ValidationError ( list ( chain . from_iterable ( errors ) ) ) <NEWLINE> <DEDENT> if cleaned_data and self . required : <NEWLINE> <INDENT> raise forms . ValidationError ( self . error_messages [ <STRING> ] ) <NEWLINE> <DEDENT> return cleaned_data <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"item_invalid\"",
                "\"item_invalid\"",
                "\"nth\"",
                "\"required\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bf03ffbd72bd46bc88cb05334ad8ddc0": {
        "code_string": "widget.world.setToIdentity()\n         if event.buttons() == Qt.LeftButton:\n             self.set_x_rotation(widget, widget.xWorldRot + dy)\n             self.set_z_rotation(widget, widget.zWorldRot + dx)\n         elif event.buttons() == Qt.RightButton:\n             self.set_x_rotation(widget, widget.xWorldRot + dy)\n             self.set_y_rotation(widget, widget.yWorldRot - dx)\n         elif event.buttons() == Qt.MiddleButton:\n             # FIXME Dependent on aspect ratio\n             distance_x = 200 * abs(widget.zCameraPos + widget.centroid[2]) / widget.width()\n             distance_y = 200 * abs(widget.zCameraPos + widget.centroid[2]) / widget.height()\n             self.set_x_movement(widget, widget.xCameraPos + (distance_x * dx / 200.0))\n             self.set_y_movement(widget, widget.yCameraPos - (distance_y * dy / 200.0))\n",
        "code_toks_joined": "widget . world . setToIdentity ( ) <NEWLINE> <INDENT> if event . buttons ( ) == Qt . LeftButton : <NEWLINE> <INDENT> self . set_x_rotation ( widget , widget . xWorldRot + dy ) <NEWLINE> self . set_z_rotation ( widget , widget . zWorldRot + dx ) <NEWLINE> <DEDENT> elif event . buttons ( ) == Qt . RightButton : <NEWLINE> <INDENT> self . set_x_rotation ( widget , widget . xWorldRot + dy ) <NEWLINE> self . set_y_rotation ( widget , widget . yWorldRot - dx ) <NEWLINE> <DEDENT> elif event . buttons ( ) == Qt . MiddleButton : <NEWLINE> <COMMENT> <NL> <INDENT> distance_x = 200 * abs ( widget . zCameraPos + widget . centroid [ 2 ] ) / widget . width ( ) <NEWLINE> distance_y = 200 * abs ( widget . zCameraPos + widget . centroid [ 2 ] ) / widget . height ( ) <NEWLINE> self . set_x_movement ( widget , widget . xCameraPos + ( distance_x * dx / 200.0 ) ) <NEWLINE> self . set_y_movement ( widget , widget . yCameraPos - ( distance_y * dy / 200.0 ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# FIXME Dependent on aspect ratio"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "89f82a7f570f4849a132dbd43b1a9857": {
        "code_string": "# Fill buffers (see GLDrawable)\n         self.fill_buffer(_POSITION, 3, vertices, GLfloat, GL_FLOAT, self.vbos[_POSITION])\n         self.fill_buffer(_COLOR, 4, colors, GLfloat, GL_FLOAT, self.vbos[_POSITION])\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> self . fill_buffer ( _POSITION , 3 , vertices , GLfloat , GL_FLOAT , self . vbos [ _POSITION ] ) <NEWLINE> self . fill_buffer ( _COLOR , 4 , colors , GLfloat , GL_FLOAT , self . vbos [ _POSITION ] ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Fill buffers (see GLDrawable)"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bbd0c2eb5c8c407b9ef157cbe1029010": {
        "code_string": "e_transport_config = engine_backend_config\n     if engine_transport_config in (ENGINE_TRANSPORT_CONFIG, ENGINE_TRANSPORT_BRIDGE_CONFIG):\n         e_transport_config = None\n",
        "code_toks_joined": "e_transport_config = engine_backend_config <NEWLINE> <INDENT> if engine_transport_config in ( ENGINE_TRANSPORT_CONFIG , ENGINE_TRANSPORT_BRIDGE_CONFIG ) : <NEWLINE> <INDENT> e_transport_config = None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4a7d87dc02334d96a2f698150e452fca": {
        "code_string": "def submit_xml(self, xml):\n         data = {'XML': ET.tostring(xml),\n                 'API': self.API}\n         response = urllib2.urlopen(self.url, utf8urlencode(data))\n         root = ET.parse(response).getroot()\n         if root.tag == 'Error':\n             raise USPSXMLError(root)\n         error = root.find('.//Error')\n         if error is None:\n             raise USPSXMLError(error)\n         return root\n",
        "code_toks_joined": "def submit_xml ( self , xml ) : <NEWLINE> <INDENT> data = { <STRING> : ET . tostring ( xml ) , <NEWLINE> <INDENT> <STRING> : self . API } <NEWLINE> <DEDENT> response = urllib2 . urlopen ( self . url , utf8urlencode ( data ) ) <NEWLINE> root = ET . parse ( response ) . getroot ( ) <NEWLINE> if root . tag == <STRING> : <NEWLINE> <INDENT> raise USPSXMLError ( root ) <NEWLINE> <DEDENT> error = root . find ( <STRING> ) <NEWLINE> if error is None : <NEWLINE> <INDENT> raise USPSXMLError ( error ) <NEWLINE> <DEDENT> return root <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'XML'",
                "'API'",
                "'Error'",
                "'.//Error'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d8f2dd8474ec4b42997eae24c0b4229b": {
        "code_string": "def is_virtualenv():\n   return hasattr(sys, 'real_prefix') or (sys.prefix == sys.base_prefix)\n",
        "code_toks_joined": "def is_virtualenv ( ) : <NEWLINE> <INDENT> return hasattr ( sys , <STRING> ) or ( sys . prefix == sys . base_prefix ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'real_prefix'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c90c7e87ad154c18ad5a760ac3ce4324": {
        "code_string": "if (options.kaichu_jira_host\n             and options.pocket_change_username\n             and (options.pocket_change_password or options.pocket_change_token)\n             and options.kaichu_jira_app_key\n             and options.kaichu_jira_project_key):\n             try:\n                 KaichuManager.jira = JiraClient(options.pocket_change_host,\n                                                 options.kaichu_jira_host,\n                                                 options.kaichu_jira_app_key,\n                                                 options.pocket_change_username,\n                                                 options.pocket_change_password,\n                                                 options.pocket_change_token)\n             except ValueError:\n                 return False\n             else:\n                 return True\n         else:\n             return True\n",
        "code_toks_joined": "if ( options . kaichu_jira_host <NEWLINE> <INDENT> and options . pocket_change_username <NEWLINE> and ( options . pocket_change_password or options . pocket_change_token ) <NEWLINE> and options . kaichu_jira_app_key <NEWLINE> and options . kaichu_jira_project_key ) : <NEWLINE> try : <NEWLINE> <INDENT> KaichuManager . jira = JiraClient ( options . pocket_change_host , <NEWLINE> <INDENT> options . kaichu_jira_host , <NEWLINE> options . kaichu_jira_app_key , <NEWLINE> options . pocket_change_username , <NEWLINE> options . pocket_change_password , <NEWLINE> options . pocket_change_token ) <NEWLINE> <DEDENT> <DEDENT> except ValueError : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return True <NEWLINE> else : <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "26dd4568bbcc474ca6aa3887864abfc2": {
        "code_string": "def check_authkey(f):\n     def wrapper(*args, **kwargs):\n         if not args[0].authkey and not args[0].uid == args[1]:\n             args[0].authorize(args[1])\n         return f(*args, **kwargs)\n     return wrapper\n",
        "code_toks_joined": "def check_authkey ( f ) : <NEWLINE> <INDENT> def wrapper ( * args , ** kwargs ) : <NEWLINE> <INDENT> if not args [ 0 ] . authkey and not args [ 0 ] . uid == args [ 1 ] : <NEWLINE> <INDENT> args [ 0 ] . authorize ( args [ 1 ] ) <NEWLINE> <DEDENT> return f ( * args , ** kwargs ) <NEWLINE> <DEDENT> return wrapper <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0b849cf48206461795d9f4a9a9b72e8f": {
        "code_string": "# ---- Basic Resolvers ---- #\n # Resolve by Attribute\n attribute_resolver = KeyResolver(lambda k, v: getattr(k, v), (AttributeError, TypeError))\n",
        "code_toks_joined": "<COMMENT> <NL> <COMMENT> <NL> <INDENT> attribute_resolver = KeyResolver ( lambda k , v : getattr ( k , v ) , ( AttributeError , TypeError ) ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# ---- Basic Resolvers ---- #",
                "# Resolve by Attribute"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f8901570bb37409fb666b23b88246c50": {
        "code_string": "def launch_task(self, task_obj, *args, **kwargs):\n         tid_obj = self.register_task(task_obj)\n         task_obj.run(*args, **kwargs)\n         return tid_obj\n",
        "code_toks_joined": "def launch_task ( self , task_obj , * args , ** kwargs ) : <NEWLINE> <INDENT> tid_obj = self . register_task ( task_obj ) <NEWLINE> task_obj . run ( * args , ** kwargs ) <NEWLINE> return tid_obj <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fd3adf88f7024670ac6eac70ae52747e": {
        "code_string": "# shortcut for thin elements:\n         if float(self.elements[ix].length) == 0:\n             return y[x]\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if float ( self . elements [ ix ] . length ) == 0 : <NEWLINE> <INDENT> return y [ x ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# shortcut for thin elements:"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cf8dd8f818ad45e091a3942b15f1d11c": {
        "code_string": "conditions = self.get(key)\n         if not conditions:\n             # XXX: option to have default return value?\n             return False\n",
        "code_toks_joined": "conditions = self . get ( key ) <NEWLINE> <INDENT> if not conditions : <NEWLINE> <COMMENT> <NL> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# XXX: option to have default return value?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "418b8b618b1b41a1af981e7819b52efb": {
        "code_string": "admin.site.register(SwitchAdmin, Switch)",
        "code_toks_joined": "admin . site . register ( SwitchAdmin , Switch ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0ae8172303e442218b3d4d4f583cada3": {
        "code_string": "def _resolved(self, path, kwargs):\n         if not path:\n             return self\n         c = self\n         for name in path[:-1]:\n             that = self.resolvables.get(name)\n             that = Context(c) if that is None else that.resolve(c)\n",
        "code_toks_joined": "def _resolved ( self , path , kwargs ) : <NEWLINE> <INDENT> if not path : <NEWLINE> <INDENT> return self <NEWLINE> <DEDENT> c = self <NEWLINE> for name in path [ : - 1 ] : <NEWLINE> <INDENT> that = self . resolvables . get ( name ) <NEWLINE> that = Context ( c ) if that is None else that . resolve ( c ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e7b54412b0b848e98c6e1a86eef35c81": {
        "code_string": "def process_response(self, request, response):\n         if (not getattr('clienttrack_prohibit', request, False)) or not request.clienttrack_first_visit:\n             # even if clienttrack_prohibit is True, we we set the cookie for first time visitors. \n             if not request.clienttrack_first_visit:\n                 request.clienttrack_first_visit = time.time()\n             max_age = 3*365*24*60*60  # 3 years\n             expires_time = time.time() + max_age\n             expires = cookie_date(expires_time)\n             response.set_cookie('_hda', \"%d,%s,%f\" % (request.clienttrack_first_visit, request.clienttrack_uid, time.time()),\n                                 max_age=max_age, expires=expires)\n         return response\n",
        "code_toks_joined": "def process_response ( self , request , response ) : <NEWLINE> <INDENT> if ( not getattr ( <STRING> , request , False ) ) or not request . clienttrack_first_visit : <NEWLINE> <COMMENT> <NL> <INDENT> if not request . clienttrack_first_visit : <NEWLINE> <INDENT> request . clienttrack_first_visit = time . time ( ) <NEWLINE> <DEDENT> max_age = 3 * 365 * 24 * 60 * 60 <COMMENT> <NEWLINE> expires_time = time . time ( ) + max_age <NEWLINE> expires = cookie_date ( expires_time ) <NEWLINE> response . set_cookie ( <STRING> , <STRING> % ( request . clienttrack_first_visit , request . clienttrack_uid , time . time ( ) ) , <NEWLINE> <INDENT> max_age = max_age , expires = expires ) <NEWLINE> <DEDENT> <DEDENT> return response <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'clienttrack_prohibit'",
                "'_hda'",
                "\"%d,%s,%f\""
            ],
            "<COMMENT>": [
                "# even if clienttrack_prohibit is True, we we set the cookie for first time visitors. ",
                "# 3 years"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d4c7bcd0b9804c2ab3fe18bd5fd2f9c4": {
        "code_string": "svm = sns.catplot(x=x_axis_name, y=\"MeanProminence\", hue=hue, data=gobal_amplitude_data_table,\n                           hue_order=None,\n                           kind=kind, orient=None, color=fig_facecolor, palette=palette, ax=ax1)\n",
        "code_toks_joined": "svm = sns . catplot ( x = x_axis_name , y = <STRING> , hue = hue , data = gobal_amplitude_data_table , <NEWLINE> <INDENT> hue_order = None , <NEWLINE> kind = kind , orient = None , color = fig_facecolor , palette = palette , ax = ax1 ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"MeanProminence\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "57964e6137d046f7b10fe2fcc8de1d3d": {
        "code_string": "def verify_result(self, expected):\n         self.assertEqual(self.result, expected)\n",
        "code_toks_joined": "def verify_result ( self , expected ) : <NEWLINE> <INDENT> self . assertEqual ( self . result , expected ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0b6bf91241954326b61764e325dde1f5": {
        "code_string": "nx.write_graphml(graph, path)\n",
        "code_toks_joined": "nx . write_graphml ( graph , path ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "27af5ed4358345ef95f0b610bacb1375": {
        "code_string": "# build key and extract group\n         desired_key = model.Key.from_raw(flattened)\n         root = (ancestor for ancestor in desired_key.ancestry).next()\n         tail = (\n           desired_key.flatten(True)[0].replace(root.flatten(True)[0], '') or (\n             '__root__'))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> desired_key = model . Key . from_raw ( flattened ) <NEWLINE> root = ( ancestor for ancestor in desired_key . ancestry ) . next ( ) <NEWLINE> tail = ( <NEWLINE> <INDENT> desired_key . flatten ( True ) [ 0 ] . replace ( root . flatten ( True ) [ 0 ] , <STRING> ) or ( <NEWLINE> <INDENT> <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# build key and extract group"
            ],
            "<STRING>": [
                "''",
                "'__root__'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "78db78af03eb42faa7577a36b5fe16b6": {
        "code_string": "else:\n         logger.debug('performing a rmtree')\n         rmtree(path)\n         return True\n",
        "code_toks_joined": "else : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> rmtree ( path ) <NEWLINE> return True <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'performing a rmtree'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d8b59986d5ff4b2c82c05ee51fdd04a3": {
        "code_string": "if next_img.shape != img_shape:\n             raise ValueError(\"All images must have same size. Sizes: %s, %s\" %\n                              (img_shape, next_img.shape))\n         img_out += next_img\n         # Now only on overlap, take the previous's pixels\n         overlap_idxs = (img_out != 0) & (next_img != 0)\n         img_out[overlap_idxs] = img_out[overlap_idxs]\n",
        "code_toks_joined": "if next_img . shape != img_shape : <NEWLINE> <INDENT> raise ValueError ( <STRING> % <NEWLINE> <INDENT> ( img_shape , next_img . shape ) ) <NEWLINE> img_out += next_img <NEWLINE> <COMMENT> <NL> overlap_idxs = ( img_out != 0 ) & ( next_img != 0 ) <NEWLINE> img_out [ overlap_idxs ] = img_out [ overlap_idxs ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"All images must have same size. Sizes: %s, %s\""
            ],
            "<COMMENT>": [
                "# Now only on overlap, take the previous's pixels"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "34f762c10b3a4a438cd0916860f1a542": {
        "code_string": "def get_arti_berhub(self, soup=None):\n         \"\"\"Return list of dictionary berupa arti berhubungan\n         dengan arti utama dengan **kata-kunci**\n         *ind* adalah indonesia\n         *ara* adalah arti arabnya.\"\"\"\n         if soup is not None:\n             soup = self.soup\n",
        "code_toks_joined": "def get_arti_berhub ( self , soup = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if soup is not None : <NEWLINE> <INDENT> soup = self . soup <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Return list of dictionary berupa arti berhubungan\n         dengan arti utama dengan **kata-kunci**\n         *ind* adalah indonesia\n         *ara* adalah arti arabnya.\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cec3ba8be38f47eaba398b3a1dc92d31": {
        "code_string": "@extension_class(list)\n def Sort(self: Flow, by):\n     if not is_to_destruct(by):\n         by = destruct_func(by)\n     self.stream.sort(key=by)\n     return self\n",
        "code_toks_joined": "@ extension_class ( list ) <NEWLINE> <INDENT> def Sort ( self : Flow , by ) : <NEWLINE> <INDENT> if not is_to_destruct ( by ) : <NEWLINE> <INDENT> by = destruct_func ( by ) <NEWLINE> <DEDENT> self . stream . sort ( key = by ) <NEWLINE> return self <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5b20406707a6431dbaab772f9a0adec3": {
        "code_string": "while (len(targets) > 1) and (len(target) > 1) and (targets[0] == dests[0]):\n                 targets = targets[1:]\n                 dests = dests[1:]\n",
        "code_toks_joined": "while ( len ( targets ) > 1 ) and ( len ( target ) > 1 ) and ( targets [ 0 ] == dests [ 0 ] ) : <NEWLINE> <INDENT> targets = targets [ 1 : ] <NEWLINE> dests = dests [ 1 : ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8035e16513c14f9d9558ea988c5c4186": {
        "code_string": "if self.balloon is not None:\n             if (self.balloontop > 0) or (self.balloonbottom > 0):\n                 self.output = self.output.split('\\n')\n                 self.output = self.output[self.balloontop : ~(self.balloonbottom)]\n                 self.output = '\\n'.join(self.output)\n",
        "code_toks_joined": "if self . balloon is not None : <NEWLINE> <INDENT> if ( self . balloontop > 0 ) or ( self . balloonbottom > 0 ) : <NEWLINE> <INDENT> self . output = self . output . split ( <STRING> ) <NEWLINE> self . output = self . output [ self . balloontop : ~ ( self . balloonbottom ) ] <NEWLINE> self . output = <STRING> . join ( self . output ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'\\n'",
                "'\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "72776e6fad944701943dfd789ed081cb": {
        "code_string": "## Apply metadata restriction\n             if self.restriction is not None:\n                 logic = Metadata.makeRestrictionLogic(self.restriction)\n                 ponies = {}\n                 for ponydir in ponydirs:\n                     for pony in Metadata.restrictedPonies(ponydir, logic):\n                         if (pony in ponies) and not (pony in ponies): # XXX and (pony not in passed)\n                             ponies[pony] = ponydir + pony + '.pony'\n                 if len(ponies) > 0:\n                     oldponies = ponies\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if self . restriction is not None : <NEWLINE> <INDENT> logic = Metadata . makeRestrictionLogic ( self . restriction ) <NEWLINE> ponies = { } <NEWLINE> for ponydir in ponydirs : <NEWLINE> <INDENT> for pony in Metadata . restrictedPonies ( ponydir , logic ) : <NEWLINE> <INDENT> if ( pony in ponies ) and not ( pony in ponies ) : <COMMENT> <NEWLINE> <INDENT> ponies [ pony ] = ponydir + pony + <STRING> <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if len ( ponies ) > 0 : <NEWLINE> <INDENT> oldponies = ponies <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "## Apply metadata restriction",
                "# XXX and (pony not in passed)"
            ],
            "<STRING>": [
                "'.pony'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a642b6f975c841eeba7ad76e1d86e888": {
        "code_string": "if coord_ts is None:\n             if cur_corr > file_data[i - 1][CORR_KEY] and cur_corr > file_data[i + 1][CORR_KEY]:\n                 logger.info(\"Found local max '%f' at coordinate '%f'\", cur_corr, cur_coord)\n                 return -math.log10(inv_C_0 / sum_for_pka), cur_corr, cur_coord\n         else:\n             if cur_corr >= coord_ts:\n                 logger.info(\"Integrating to input TS coordinate '%f' with value , '%f'\", cur_coord, cur_corr)\n                 return -math.log10(inv_C_0 / sum_for_pka), cur_corr, cur_coord\n",
        "code_toks_joined": "if coord_ts is None : <NEWLINE> <INDENT> if cur_corr > file_data [ i - 1 ] [ CORR_KEY ] and cur_corr > file_data [ i + 1 ] [ CORR_KEY ] : <NEWLINE> <INDENT> logger . info ( <STRING> , cur_corr , cur_coord ) <NEWLINE> return - math . log10 ( inv_C_0 / sum_for_pka ) , cur_corr , cur_coord <NEWLINE> else : <NEWLINE> <DEDENT> if cur_corr >= coord_ts : <NEWLINE> <INDENT> logger . info ( <STRING> , cur_coord , cur_corr ) <NEWLINE> return - math . log10 ( inv_C_0 / sum_for_pka ) , cur_corr , cur_coord <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Found local max '%f' at coordinate '%f'\"",
                "\"Integrating to input TS coordinate '%f' with value , '%f'\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ddb3208a9bae4489bd35caf594045d15": {
        "code_string": "copy_field('email', True)\n         copy_field('first_name')\n         copy_field('last_name')\n         if access_token is None:\n             user.access_token = access_token\n",
        "code_toks_joined": "copy_field ( <STRING> , True ) <NEWLINE> <INDENT> copy_field ( <STRING> ) <NEWLINE> copy_field ( <STRING> ) <NEWLINE> if access_token is None : <NEWLINE> <INDENT> user . access_token = access_token <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'email'",
                "'first_name'",
                "'last_name'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "132e5a32182f472fbd9068d8163c14db": {
        "code_string": "try:\n         best_token = token_manager.get_access_token(user_id)\n     except UserToken.DoesNotExist:\n         pass\n     else:\n         if best_token.id not in processed_user_tokens:\n             logger.info('Retrying debug_all_tokens_for_user.')\n             debug_all_tokens_for_user.retry(args=[user_id],\n                                             countdown=45)\n         else:\n             logger.info('Deleting user tokens except best one.')\n             tokens_to_delete = sorted(processed_user_tokens)\n             tokens_to_delete.remove(best_token.id)\n             for token_id in processed_user_tokens:\n                 UserToken.objects.filter(id=token_id).update(deleted=True)\n",
        "code_toks_joined": "try : <NEWLINE> <INDENT> best_token = token_manager . get_access_token ( user_id ) <NEWLINE> except UserToken . DoesNotExist : <NEWLINE> pass <NEWLINE> else : <NEWLINE> if best_token . id not in processed_user_tokens : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> debug_all_tokens_for_user . retry ( args = [ user_id ] , <NEWLINE> <INDENT> countdown = 45 ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> tokens_to_delete = sorted ( processed_user_tokens ) <NEWLINE> tokens_to_delete . remove ( best_token . id ) <NEWLINE> for token_id in processed_user_tokens : <NEWLINE> <INDENT> UserToken . objects . filter ( id = token_id ) . update ( deleted = True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Retrying debug_all_tokens_for_user.'",
                "'Deleting user tokens except best one.'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8678e781956241aeb7354731f5e86b27": {
        "code_string": "qname = dns.name.from_text(theZone)\n     request = dns.message.make_query(qname, rdtype=dns.rdatatype.SOA, rdclass=dns.rdataclass.ANY)\n     request.use_edns(r.edns, r.ednsflags, r.payload)\n     request.want_dnssec(True)\n     response = None\n     nameservers = misc.authNS(theZone)\n     for nameserver in nameservers[:]:\n         try:\n             l.logDebug('Querying {} for SOA of {} via TCP'.format(theZone, nameserver))\n             response = dns.query.tcp(request, nameserver, 10)\n             rcode = response.rcode()\n             if rcode == 0: continue\n         except (socket.error, dns.exception.Timeout, dns.query.UnexpectedSource,\n                 dns.exception.FormError, EOFError, dns.resolver.NoAnswer,\n                 dns.resolver.NXDOMAIN):\n             pass\n         l.logError('NS {} did not answer SOA query for {}'.format(\n                                                     nameserver, theZone))\n",
        "code_toks_joined": "qname = dns . name . from_text ( theZone ) <NEWLINE> <INDENT> request = dns . message . make_query ( qname , rdtype = dns . rdatatype . SOA , rdclass = dns . rdataclass . ANY ) <NEWLINE> request . use_edns ( r . edns , r . ednsflags , r . payload ) <NEWLINE> request . want_dnssec ( True ) <NEWLINE> response = None <NEWLINE> nameservers = misc . authNS ( theZone ) <NEWLINE> for nameserver in nameservers [ : ] : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> l . logDebug ( <STRING> . format ( theZone , nameserver ) ) <NEWLINE> response = dns . query . tcp ( request , nameserver , 10 ) <NEWLINE> rcode = response . rcode ( ) <NEWLINE> if rcode == 0 : continue <NEWLINE> <DEDENT> except ( socket . error , dns . exception . Timeout , dns . query . UnexpectedSource , <NEWLINE> <INDENT> dns . exception . FormError , EOFError , dns . resolver . NoAnswer , <NEWLINE> dns . resolver . NXDOMAIN ) : <NEWLINE> pass <NEWLINE> <DEDENT> l . logError ( <STRING> . format ( <NEWLINE> <INDENT> nameserver , theZone ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Querying {} for SOA of {} via TCP'",
                "'NS {} did not answer SOA query for {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "9f3516a5fbc64eacb25fd24ba4c32330": {
        "code_string": "if png_icon_name in files:\n                 new_path = os.path.join(root, svg_icon_name)\n                 new_size = self._extract_icon_size_from_path(new_path)\n",
        "code_toks_joined": "if png_icon_name in files : <NEWLINE> <INDENT> new_path = os . path . join ( root , svg_icon_name ) <NEWLINE> new_size = self . _extract_icon_size_from_path ( new_path ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7d24faf65a0e4ffd9151457dda9ede6b": {
        "code_string": "content = []\n         if isinstance(file, list):\n             files = []\n             for filePattern in file:\n                 files += matchPattern(filePattern)\n         elif isinstance(file, str):\n             files = matchPattern(filePattern)\n         else:\n             raise RuntimeError(\"natlang.dataLoader.load [ERROR]: parameter \" +\n                                \"type\")\n",
        "code_toks_joined": "content = [ ] <NEWLINE> <INDENT> if isinstance ( file , list ) : <NEWLINE> <INDENT> files = [ ] <NEWLINE> for filePattern in file : <NEWLINE> <INDENT> files += matchPattern ( filePattern ) <NEWLINE> <DEDENT> <DEDENT> elif isinstance ( file , str ) : <NEWLINE> <INDENT> files = matchPattern ( filePattern ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"natlang.dataLoader.load [ERROR]: parameter \"",
                "\"type\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "dc6f8835538c45a0bdb1feb328b18b67": {
        "code_string": "if resource is None:\n       message = (\"Couldn't match version '{v}' to '{id}'; options: {vers}\"\n                  .format(v=target, id=resource.id, vers=\", \".join(options)))\n",
        "code_toks_joined": "if resource is None : <NEWLINE> <INDENT> message = ( <STRING> <NEWLINE> <INDENT> . format ( v = target , id = resource . id , vers = <STRING> . join ( options ) ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Couldn't match version '{v}' to '{id}'; options: {vers}\"",
                "\", \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e1cab3f2b16c45d9b726f208448f6efe": {
        "code_string": "restorer = capnp.Restorer(capability.TestSturdyRefObjectId, _restore)\n     server = capnp.RpcServer(loop, restorer, write_stream)\n     client = capnp.RpcClient(loop, read_stream)\n",
        "code_toks_joined": "restorer = capnp . Restorer ( capability . TestSturdyRefObjectId , _restore ) <NEWLINE> <INDENT> server = capnp . RpcServer ( loop , restorer , write_stream ) <NEWLINE> client = capnp . RpcClient ( loop , read_stream ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cac96ddd0b954e6db7d48ce997da90f6": {
        "code_string": "if rebuild:\n             if not hasattr(stack.top, 'sass_cache'):\n                 stack.top.sass_cache = {}\n             cache = stack.top.sass_cache\n",
        "code_toks_joined": "if rebuild : <NEWLINE> <INDENT> if not hasattr ( stack . top , <STRING> ) : <NEWLINE> <INDENT> stack . top . sass_cache = { } <NEWLINE> <DEDENT> cache = stack . top . sass_cache <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'sass_cache'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "409da267fea945f28380d2612da7e438": {
        "code_string": "return {\n         \"policy\": policy,\n         \"signature\": signature_b64,\n         \"key\": key,\n         \"AWSAccessKeyId\": access_key,\n         \"form_action\": bucket_url,\n         \"success_action_status\": \"201\",\n         \"acl\": \"public-read\",\n         \"Content-Type\": content_type\n     }\n",
        "code_toks_joined": "return { <NEWLINE> <INDENT> <STRING> : policy , <NEWLINE> <STRING> : signature_b64 , <NEWLINE> <STRING> : key , <NEWLINE> <STRING> : access_key , <NEWLINE> <STRING> : bucket_url , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : content_type <NEWLINE> } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"policy\"",
                "\"signature\"",
                "\"key\"",
                "\"AWSAccessKeyId\"",
                "\"form_action\"",
                "\"success_action_status\"",
                "\"201\"",
                "\"acl\"",
                "\"public-read\"",
                "\"Content-Type\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8db06db33c494122a91cbbf2e77bfb94": {
        "code_string": "for kInfo in kernelInfo:\n \t\t\tif isInPlace and numKernelsOdd and not inPlaceDone and kernelInfo.in_place_possible:\n \t\t\t\tcurrWrite = currRead\n \t\t\t\tinPlaceDone = True\n",
        "code_toks_joined": "for kInfo in kernelInfo : <NEWLINE> <INDENT> if isInPlace and numKernelsOdd and not inPlaceDone and kernelInfo . in_place_possible : <NEWLINE> <INDENT> currWrite = currRead <NEWLINE> inPlaceDone = True <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "06f904e9459f4bc599201db88313ccca": {
        "code_string": "flag_dict={\"-r\":\"Restart\",\"-h\":\"Hibernate\",\"-s\":\"Shutdown\"}\n def cancel():\n     c_input=input(\"Press any key to cancel process\")\n     sub.Popen(\"shutdown -a\",shell=True)\n def get_method():\n     flag=\"\"\n     get_method=input(\"Please Enter Method , Shutdown[1] , Hibernate[2] , Restart[3]\")\n     if get_method==\"2\":\n         flag=\"-h\"\n     elif flag==\"3\":\n         flag=\"-r\"\n     else:\n         flag=\"-s\"\n     return flag\n def print_time(sleep_list,method):\n     print(\"Your Computer Will Be \",flag_dict[method],\"In \",str(sleep_list[0]),\"Hour and\",str(sleep_list[1]),\"Minute\")\n def duration():\n     try:\n         get_hour=int(input(\"Please Enter Hour :\"))\n     except ValueError:\n         get_hour=0\n     try:\n         get_minute=int(input(\"Please Enter Minute :\"))\n     except ValueError:\n         get_minute=0\n     sleep_time=get_hour*3600+get_minute*60\n     flag=get_method()\n     print_time([get_hour,get_minute],flag)\n     sub.Popen(\"shutdown \"+flag+\" -f -t \"+str(sleep_time),shell=True)\n     cancel()\n",
        "code_toks_joined": "flag_dict = { <STRING> : <STRING> , <STRING> : <STRING> , <STRING> : <STRING> } <NEWLINE> <INDENT> def cancel ( ) : <NEWLINE> <INDENT> c_input = input ( <STRING> ) <NEWLINE> sub . Popen ( <STRING> , shell = True ) <NEWLINE> <DEDENT> def get_method ( ) : <NEWLINE> <INDENT> flag = <STRING> <NEWLINE> get_method = input ( <STRING> ) <NEWLINE> if get_method == <STRING> : <NEWLINE> <INDENT> flag = <STRING> <NEWLINE> <DEDENT> elif flag == <STRING> : <NEWLINE> <INDENT> flag = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> flag = <STRING> <NEWLINE> <DEDENT> return flag <NEWLINE> <DEDENT> def print_time ( sleep_list , method ) : <NEWLINE> <INDENT> print ( <STRING> , flag_dict [ method ] , <STRING> , str ( sleep_list [ 0 ] ) , <STRING> , str ( sleep_list [ 1 ] ) , <STRING> ) <NEWLINE> <DEDENT> def duration ( ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> get_hour = int ( input ( <STRING> ) ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> get_hour = 0 <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> get_minute = int ( input ( <STRING> ) ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> get_minute = 0 <NEWLINE> <DEDENT> sleep_time = get_hour * 3600 + get_minute * 60 <NEWLINE> flag = get_method ( ) <NEWLINE> print_time ( [ get_hour , get_minute ] , flag ) <NEWLINE> sub . Popen ( <STRING> + flag + <STRING> + str ( sleep_time ) , shell = True ) <NEWLINE> cancel ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"-r\"",
                "\"Restart\"",
                "\"-h\"",
                "\"Hibernate\"",
                "\"-s\"",
                "\"Shutdown\"",
                "\"Press any key to cancel process\"",
                "\"shutdown -a\"",
                "\"\"",
                "\"Please Enter Method , Shutdown[1] , Hibernate[2] , Restart[3]\"",
                "\"2\"",
                "\"-h\"",
                "\"3\"",
                "\"-r\"",
                "\"-s\"",
                "\"Your Computer Will Be \"",
                "\"In \"",
                "\"Hour and\"",
                "\"Minute\"",
                "\"Please Enter Hour :\"",
                "\"Please Enter Minute :\"",
                "\"shutdown \"",
                "\" -f -t \""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fd2f551f67c14851881818724dbd86e7": {
        "code_string": "e_transport_config = engine_backend_config\n     if engine_transport_config in (ENGINE_TRANSPORT_CONFIG, ENGINE_TRANSPORT_BRIDGE_CONFIG):\n         e_transport_config = None\n",
        "code_toks_joined": "e_transport_config = engine_backend_config <NEWLINE> <INDENT> if engine_transport_config in ( ENGINE_TRANSPORT_CONFIG , ENGINE_TRANSPORT_BRIDGE_CONFIG ) : <NEWLINE> <INDENT> e_transport_config = None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b0f22d1261df41549b3ddf0af6fb3ebe": {
        "code_string": "def query_ncbi_species(species_entry):\n     if species_entry is None:\n         return None\n     try:\n         species_entry = re.compile(species_entry)\n         with get_connection() as connection:\n             db = connection.get_database()  # Database name is ngs_runs\n             species_db = db.species  # Collection name is samples\n             result = species_db.find_one({\"organism\": species_entry}, {\"ncbi_species\": 1, \"_id\": 0})\n             group_result = species_db.find_one({\"group\": species_entry}, {\"ncbi_species\": 1, \"_id\": 0})\n             if result is not None:\n                 return result[\"ncbi_species\"]\n             elif group_result is not None:\n                 return result[\"ncbi_species\"]\n             else:\n                 return None\n     except Exception as e:\n         print(traceback.format_exc())\n         return None\n",
        "code_toks_joined": "def query_ncbi_species ( species_entry ) : <NEWLINE> <INDENT> if species_entry is None : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> species_entry = re . compile ( species_entry ) <NEWLINE> with get_connection ( ) as connection : <NEWLINE> <INDENT> db = connection . get_database ( ) <COMMENT> <NEWLINE> species_db = db . species <COMMENT> <NEWLINE> result = species_db . find_one ( { <STRING> : species_entry } , { <STRING> : 1 , <STRING> : 0 } ) <NEWLINE> group_result = species_db . find_one ( { <STRING> : species_entry } , { <STRING> : 1 , <STRING> : 0 } ) <NEWLINE> if result is not None : <NEWLINE> <INDENT> return result [ <STRING> ] <NEWLINE> <DEDENT> elif group_result is not None : <NEWLINE> <INDENT> return result [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> print ( traceback . format_exc ( ) ) <NEWLINE> return None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Database name is ngs_runs",
                "# Collection name is samples"
            ],
            "<STRING>": [
                "\"organism\"",
                "\"ncbi_species\"",
                "\"_id\"",
                "\"group\"",
                "\"ncbi_species\"",
                "\"_id\"",
                "\"ncbi_species\"",
                "\"ncbi_species\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "35c20efa5ab344f39014e91302608495": {
        "code_string": "@app.callback(\n     Output(\"rerun-form\", \"children\"),\n     [Input(\"run-name\", \"children\")]\n )\n def update_rerun_form(run_name):\n     run_name = run_name.split(\"/\")[0]\n     if run_name == \"\" or hasattr(keys, \"rerun\"):\n         return None\n",
        "code_toks_joined": "@ app . callback ( <NEWLINE> <INDENT> Output ( <STRING> , <STRING> ) , <NEWLINE> [ Input ( <STRING> , <STRING> ) ] <NEWLINE> ) <NEWLINE> def update_rerun_form ( run_name ) : <NEWLINE> run_name = run_name . split ( <STRING> ) [ 0 ] <NEWLINE> if run_name == <STRING> or hasattr ( keys , <STRING> ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"rerun-form\"",
                "\"children\"",
                "\"run-name\"",
                "\"children\"",
                "\"/\"",
                "\"\"",
                "\"rerun\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2843117cbe3949db9548f8f8badcbeb2": {
        "code_string": "def test_describe_then_description(self):\n         \"\"\"describe then description of IP object should return same str.\"\"\"\n         network = \"203.0.113.128/25\"\n         description_str = \"should be the same\"\n         self.address_space.describe(\n             description=description_str,\n             ip_parameter=network,\n         )\n         self.assertEqual(\n             self.address_space.description(\n                 network,\n             ),\n             description_str,\n         )\n         ipv6_address = \"2001:db8::2018:7:12\"\n         description_str = \"hey! an IPv6 Address\"\n         self.address_space.describe(\n             ip_parameter=ipv6_address,\n             description=description_str,\n         )\n         self.assertEqual(\n             self.address_space.description(\n                 ipv6_address,\n             ),\n             description_str,\n         )\n         zero_ipv4 = \"0.0.0.0\"\n         description_str = \"address 0 for ipv4\"\n         self.address_space.describe(\n             description=description_str,\n             ip_parameter=network,\n         )\n         self.assertEqual(\n             self.address_space.description(\n                 zero_ipv4,\n             ),\n             description_str,\n         )\n",
        "code_toks_joined": "def test_describe_then_description ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> network = <STRING> <NEWLINE> description_str = <STRING> <NEWLINE> self . address_space . describe ( <NEWLINE> <INDENT> description = description_str , <NEWLINE> ip_parameter = network , <NEWLINE> <DEDENT> ) <NEWLINE> self . assertEqual ( <NEWLINE> <INDENT> self . address_space . description ( <NEWLINE> <INDENT> network , <NEWLINE> <DEDENT> ) , <NEWLINE> description_str , <NEWLINE> <DEDENT> ) <NEWLINE> ipv6_address = <STRING> <NEWLINE> description_str = <STRING> <NEWLINE> self . address_space . describe ( <NEWLINE> <INDENT> ip_parameter = ipv6_address , <NEWLINE> description = description_str , <NEWLINE> <DEDENT> ) <NEWLINE> self . assertEqual ( <NEWLINE> <INDENT> self . address_space . description ( <NEWLINE> <INDENT> ipv6_address , <NEWLINE> <DEDENT> ) , <NEWLINE> description_str , <NEWLINE> <DEDENT> ) <NEWLINE> zero_ipv4 = <STRING> <NEWLINE> description_str = <STRING> <NEWLINE> self . address_space . describe ( <NEWLINE> <INDENT> description = description_str , <NEWLINE> ip_parameter = network , <NEWLINE> <DEDENT> ) <NEWLINE> self . assertEqual ( <NEWLINE> <INDENT> self . address_space . description ( <NEWLINE> <INDENT> zero_ipv4 , <NEWLINE> <DEDENT> ) , <NEWLINE> description_str , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"describe then description of IP object should return same str.\"\"\"",
                "\"203.0.113.128/25\"",
                "\"should be the same\"",
                "\"2001:db8::2018:7:12\"",
                "\"hey! an IPv6 Address\"",
                "\"0.0.0.0\"",
                "\"address 0 for ipv4\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b227df57481040b6974ca164fa360cec": {
        "code_string": "self.__parent_supernet[as_address] = supernet\n             children_of_as_network = (\n                 self.__children_ip_object.setdefault(as_network, set())\n             )\n             children_of_supernet = (\n                 self.__children_ip_object.setdefault(supernet, set())\n             )\n             # Supernet's child can be as_network's child\n             version = as_network.version\n             to_arrange = set()\n             for tentative_child in children_of_supernet:\n                 if (isinstance(tentative_child, IPAddressTuple)\n                         and tentative_child.version == version\n                         and tentative_child in as_network):\n                     to_arrange.add(tentative_child)\n                 elif (isinstance(tentative_child, IPNetworkTuple)\n                         and tentative_child.version == version\n                         and tentative_child.subnet_of(as_network)):\n                     to_arrange.add(tentative_child)\n             for child in to_arrange:\n                 self.__parent_supernet[child] = as_network\n                 children_of_as_network.add(child)\n                 children_of_supernet.remove(child)\n             children_of_supernet.add(as_address)\n         else:\n             raise TypeError(\"ip_parameter must be a valid IP parameter\")\n",
        "code_toks_joined": "self . __parent_supernet [ as_address ] = supernet <NEWLINE> <INDENT> children_of_as_network = ( <NEWLINE> <INDENT> self . __children_ip_object . setdefault ( as_network , set ( ) ) <NEWLINE> <DEDENT> ) <NEWLINE> children_of_supernet = ( <NEWLINE> <INDENT> self . __children_ip_object . setdefault ( supernet , set ( ) ) <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> version = as_network . version <NEWLINE> to_arrange = set ( ) <NEWLINE> for tentative_child in children_of_supernet : <NEWLINE> <INDENT> if ( isinstance ( tentative_child , IPAddressTuple ) <NEWLINE> <INDENT> and tentative_child . version == version <NEWLINE> and tentative_child in as_network ) : <NEWLINE> to_arrange . add ( tentative_child ) <NEWLINE> <DEDENT> elif ( isinstance ( tentative_child , IPNetworkTuple ) <NEWLINE> <INDENT> and tentative_child . version == version <NEWLINE> and tentative_child . subnet_of ( as_network ) ) : <NEWLINE> to_arrange . add ( tentative_child ) <NEWLINE> <DEDENT> <DEDENT> for child in to_arrange : <NEWLINE> <INDENT> self . __parent_supernet [ child ] = as_network <NEWLINE> children_of_as_network . add ( child ) <NEWLINE> children_of_supernet . remove ( child ) <NEWLINE> <DEDENT> children_of_supernet . add ( as_address ) <NEWLINE> else : <NEWLINE> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Supernet's child can be as_network's child"
            ],
            "<STRING>": [
                "\"ip_parameter must be a valid IP parameter\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cf415855265e481f8ef8cde8bec83fcf": {
        "code_string": "self.__parent_supernet[as_address] = supernet\n             children_of_as_network = (\n                 self.__children_ip_object.setdefault(as_network, set())\n             )\n             children_of_supernet = (\n                 self.__children_ip_object.setdefault(supernet, set())\n             )\n             # Supernet's child can be as_network's child\n             version = as_network.version\n             to_arrange = set()\n             for tentative_child in children_of_supernet:\n                 if (isinstance(tentative_child, IPAddressTuple)\n                         and tentative_child.version == version\n                         and tentative_child in as_network):\n                     to_arrange.add(tentative_child)\n                 elif (isinstance(tentative_child, IPNetworkTuple)\n                         and tentative_child.version == version\n                         and tentative_child.subnet_of(as_network)):\n                     to_arrange.add(tentative_child)\n             for child in to_arrange:\n                 self.__parent_supernet[child] = as_network\n                 children_of_as_network.add(child)\n                 children_of_supernet.remove(child)\n             children_of_supernet.add(as_network)\n         else:\n             raise TypeError(\"ip_parameter must be a valid IP parameter\")\n",
        "code_toks_joined": "self . __parent_supernet [ as_address ] = supernet <NEWLINE> <INDENT> children_of_as_network = ( <NEWLINE> <INDENT> self . __children_ip_object . setdefault ( as_network , set ( ) ) <NEWLINE> <DEDENT> ) <NEWLINE> children_of_supernet = ( <NEWLINE> <INDENT> self . __children_ip_object . setdefault ( supernet , set ( ) ) <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> version = as_network . version <NEWLINE> to_arrange = set ( ) <NEWLINE> for tentative_child in children_of_supernet : <NEWLINE> <INDENT> if ( isinstance ( tentative_child , IPAddressTuple ) <NEWLINE> <INDENT> and tentative_child . version == version <NEWLINE> and tentative_child in as_network ) : <NEWLINE> to_arrange . add ( tentative_child ) <NEWLINE> <DEDENT> elif ( isinstance ( tentative_child , IPNetworkTuple ) <NEWLINE> <INDENT> and tentative_child . version == version <NEWLINE> and tentative_child . subnet_of ( as_network ) ) : <NEWLINE> to_arrange . add ( tentative_child ) <NEWLINE> <DEDENT> <DEDENT> for child in to_arrange : <NEWLINE> <INDENT> self . __parent_supernet [ child ] = as_network <NEWLINE> children_of_as_network . add ( child ) <NEWLINE> children_of_supernet . remove ( child ) <NEWLINE> <DEDENT> children_of_supernet . add ( as_network ) <NEWLINE> else : <NEWLINE> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Supernet's child can be as_network's child"
            ],
            "<STRING>": [
                "\"ip_parameter must be a valid IP parameter\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f04d8f703d364412aa3f6032c6742b78": {
        "code_string": "def _convert_table(self, **kwargs):\n         self._table_elements = {\n             'head': list(),\n             'body': list(),\n             'foot': list(),\n             'col.cnt': 0,\n         }\n         # parse HTML here;\n         if isinstance(self._html_content, dict):\n             self._table_elements.update(self._html_content)\n         else:\n             obj = self._html_content\n             if isinstance(self._html_content, (basestring, unicode)):\n                 from bs4 import BeautifulSoup\n                 obj = BeautifulSoup(self._html_content)\n             html_head = getattr(obj, 'thead')\n             if html_head:\n                 for a_col in html_head.find_all('th'):\n                     tmp_col_head = {\n                         'value': a_col.get_text(strip=True),\n                     }\n                     self._table_elements['head'].append(tmp_col_head)\n             html_body = getattr(obj, 'tbody')\n             if html_body:\n                 for a_row in html_body.find_all('tr'):\n                     new_row = list()\n                     for a_cell in a_row.find_all('td'):\n                         tmp_cell = {\n                             'value': a_cell.get_text(strip=True),\n                         }\n                         new_row.append(tmp_cell)\n                     self._table_elements['body'].append(new_row)\n             html_foot = getattr(obj, 'tfoot')\n             if html_foot:\n                 for a_foot in html_body.find_all('td'):\n                     foot_cell = {\n                         'value': a_foot.get_text(strip=True),\n                     }\n                 self._table_elements['foot'].append(foot_cell)\n",
        "code_toks_joined": "def _convert_table ( self , ** kwargs ) : <NEWLINE> <INDENT> self . _table_elements = { <NEWLINE> <INDENT> <STRING> : list ( ) , <NEWLINE> <STRING> : list ( ) , <NEWLINE> <STRING> : list ( ) , <NEWLINE> <STRING> : 0 , <NEWLINE> <DEDENT> } <NEWLINE> <COMMENT> <NL> if isinstance ( self . _html_content , dict ) : <NEWLINE> <INDENT> self . _table_elements . update ( self . _html_content ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> obj = self . _html_content <NEWLINE> if isinstance ( self . _html_content , ( basestring , unicode ) ) : <NEWLINE> <INDENT> from bs4 import BeautifulSoup <NEWLINE> obj = BeautifulSoup ( self . _html_content ) <NEWLINE> <DEDENT> html_head = getattr ( obj , <STRING> ) <NEWLINE> if html_head : <NEWLINE> <INDENT> for a_col in html_head . find_all ( <STRING> ) : <NEWLINE> <INDENT> tmp_col_head = { <NEWLINE> <INDENT> <STRING> : a_col . get_text ( strip = True ) , <NEWLINE> <DEDENT> } <NEWLINE> self . _table_elements [ <STRING> ] . append ( tmp_col_head ) <NEWLINE> <DEDENT> <DEDENT> html_body = getattr ( obj , <STRING> ) <NEWLINE> if html_body : <NEWLINE> <INDENT> for a_row in html_body . find_all ( <STRING> ) : <NEWLINE> <INDENT> new_row = list ( ) <NEWLINE> for a_cell in a_row . find_all ( <STRING> ) : <NEWLINE> <INDENT> tmp_cell = { <NEWLINE> <INDENT> <STRING> : a_cell . get_text ( strip = True ) , <NEWLINE> <DEDENT> } <NEWLINE> new_row . append ( tmp_cell ) <NEWLINE> <DEDENT> self . _table_elements [ <STRING> ] . append ( new_row ) <NEWLINE> <DEDENT> <DEDENT> html_foot = getattr ( obj , <STRING> ) <NEWLINE> if html_foot : <NEWLINE> <INDENT> for a_foot in html_body . find_all ( <STRING> ) : <NEWLINE> <INDENT> foot_cell = { <NEWLINE> <INDENT> <STRING> : a_foot . get_text ( strip = True ) , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> self . _table_elements [ <STRING> ] . append ( foot_cell ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'head'",
                "'body'",
                "'foot'",
                "'col.cnt'",
                "'thead'",
                "'th'",
                "'value'",
                "'head'",
                "'tbody'",
                "'tr'",
                "'td'",
                "'value'",
                "'body'",
                "'tfoot'",
                "'td'",
                "'value'",
                "'foot'"
            ],
            "<COMMENT>": [
                "# parse HTML here;"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "466ca8f64a8e4c87a6605b7e8e9b62ae": {
        "code_string": "def addDeterminants(self, text, deter_rule, matches, match_begin, match_end, current_position):\n         deter_rule = deter_rule[FastCNER.END]\n         end = current_position if match_end == 0 else match_end\n         current_span = Span(match_begin + self.offset, end + self.offset,\n                             text[match_begin:end])\n         current_spans_list = []\n         overlap_checkers = self.overlap_checkers\n         for key in deter_rule.keys():\n             rule_id = deter_rule[key]\n             if self.logger is not None:\n                 self.logger.debug(\n                     'try add matched rule ({}-{})\\t{}'.format(match_begin, match_end, str(self.rule_store[rule_id])))\n             current_span.rule_id = rule_id\n             if key in matches:\n                 current_spans_list = matches[key]\n                 overlap_checker = overlap_checkers[key]\n                 overlapped_pos = overlap_checker.search(current_span.begin, current_span.end)\n                 if len(overlapped_pos) > 0:\n                     pos = overlapped_pos.pop().data\n                     overlapped_span = current_spans_list[pos]\n                     if not self.compareSpan(current_span, overlapped_span):\n                         continue\n                     current_spans_list[pos] = current_span\n                     overlap_checker.remove(Interval(current_span.begin, current_span.end))\n                     overlap_checker.add(current_span.begin, current_span.end, pos)\n                 else:\n                     overlap_checker.add(current_span.begin, current_span.end, len(current_spans_list))\n                     current_spans_list.append(current_span)\n             else:\n                 matches[key] = current_spans_list\n                 overlap_checker = IntervalTree()\n                 # quickset's search will include both lower and upper bounds, so minus one from the end.\n                 overlap_checker.add(current_span.begin, current_span.end - 1, len(current_spans_list))\n                 current_spans_list.append(current_span)\n                 overlap_checkers[key] = overlap_checker\n",
        "code_toks_joined": "def addDeterminants ( self , text , deter_rule , matches , match_begin , match_end , current_position ) : <NEWLINE> <INDENT> deter_rule = deter_rule [ FastCNER . END ] <NEWLINE> end = current_position if match_end == 0 else match_end <NEWLINE> current_span = Span ( match_begin + self . offset , end + self . offset , <NEWLINE> <INDENT> text [ match_begin : end ] ) <NEWLINE> <DEDENT> current_spans_list = [ ] <NEWLINE> overlap_checkers = self . overlap_checkers <NEWLINE> for key in deter_rule . keys ( ) : <NEWLINE> <INDENT> rule_id = deter_rule [ key ] <NEWLINE> if self . logger is not None : <NEWLINE> <INDENT> self . logger . debug ( <NEWLINE> <INDENT> <STRING> . format ( match_begin , match_end , str ( self . rule_store [ rule_id ] ) ) ) <NEWLINE> <DEDENT> <DEDENT> current_span . rule_id = rule_id <NEWLINE> if key in matches : <NEWLINE> <INDENT> current_spans_list = matches [ key ] <NEWLINE> overlap_checker = overlap_checkers [ key ] <NEWLINE> overlapped_pos = overlap_checker . search ( current_span . begin , current_span . end ) <NEWLINE> if len ( overlapped_pos ) > 0 : <NEWLINE> <INDENT> pos = overlapped_pos . pop ( ) . data <NEWLINE> overlapped_span = current_spans_list [ pos ] <NEWLINE> if not self . compareSpan ( current_span , overlapped_span ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> current_spans_list [ pos ] = current_span <NEWLINE> overlap_checker . remove ( Interval ( current_span . begin , current_span . end ) ) <NEWLINE> overlap_checker . add ( current_span . begin , current_span . end , pos ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> overlap_checker . add ( current_span . begin , current_span . end , len ( current_spans_list ) ) <NEWLINE> current_spans_list . append ( current_span ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> matches [ key ] = current_spans_list <NEWLINE> overlap_checker = IntervalTree ( ) <NEWLINE> <COMMENT> <NL> overlap_checker . add ( current_span . begin , current_span . end - 1 , len ( current_spans_list ) ) <NEWLINE> current_spans_list . append ( current_span ) <NEWLINE> overlap_checkers [ key ] = overlap_checker <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'try add matched rule ({}-{})\\t{}'"
            ],
            "<COMMENT>": [
                "# quickset's search will include both lower and upper bounds, so minus one from the end."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "bdbfacd1308e41fba25456d7915e46de": {
        "code_string": "def isCreate(self):\n         data_id = self.featureServerProxy.getID()\n         return data_id is not None and self.request.body != \"\" and self.request.method == \"POST\"\n",
        "code_toks_joined": "def isCreate ( self ) : <NEWLINE> <INDENT> data_id = self . featureServerProxy . getID ( ) <NEWLINE> return data_id is not None and self . request . body != <STRING> and self . request . method == <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"",
                "\"POST\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b30fbf4a9c80401691020874719b804e": {
        "code_string": "if url not in scrape_whitelist:\n             return \"URL domain is not in whitelist\"\n",
        "code_toks_joined": "if url not in scrape_whitelist : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"URL domain is not in whitelist\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0cb2b3ff5b7f4333ad4f9544f1a4f483": {
        "code_string": "def get_selected_station(self):\n         bits = self.spi.read_pin(self.station_bits)\n         result = 0\n         for index, value in enumerate(bits):\n             result = value * 2**index\n",
        "code_toks_joined": "def get_selected_station ( self ) : <NEWLINE> <INDENT> bits = self . spi . read_pin ( self . station_bits ) <NEWLINE> result = 0 <NEWLINE> for index , value in enumerate ( bits ) : <NEWLINE> <INDENT> result = value * 2 ** index <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a15baef47ee14f3a818dd0c977e9ba83": {
        "code_string": "proc_session = xnat_login.experiments[\n",
        "code_toks_joined": "proc_session = xnat_login . experiments [ <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "unbalanced (){}[]"
        }
    },
    "2db9860f2e2849d8bac7b7c6fbc821ec": {
        "code_string": "for dataset in subject.datasets:\n",
        "code_toks_joined": "for dataset in subject . datasets : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eb7d14ed71b54d4082ced95a1f36ac94": {
        "code_string": "def __init__(self, study_name, datasets, fields):\n         super(BaseArchiveSink, self).__init__(study_name, datasets,\n                                               fields)\n         # Add input datasets\n         for dataset in datasets:\n             assert isinstance(dataset, DatasetSpec)\n             self._add_trait(self.inputs, dataset.name + PATH_SUFFIX,\n                             PATH_TRAIT)\n         # Add input fields\n         for field in fields:\n             assert isinstance(dataset, FieldSpec)\n             self._add_trait(self.inputs, field.name + FIELD_SUFFIX,\n                             field.dtype)\n",
        "code_toks_joined": "def __init__ ( self , study_name , datasets , fields ) : <NEWLINE> <INDENT> super ( BaseArchiveSink , self ) . __init__ ( study_name , datasets , <NEWLINE> <INDENT> fields ) <NEWLINE> <COMMENT> <NL> <DEDENT> for dataset in datasets : <NEWLINE> <INDENT> assert isinstance ( dataset , DatasetSpec ) <NEWLINE> self . _add_trait ( self . inputs , dataset . name + PATH_SUFFIX , <NEWLINE> <INDENT> PATH_TRAIT ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for field in fields : <NEWLINE> <INDENT> assert isinstance ( dataset , FieldSpec ) <NEWLINE> self . _add_trait ( self . inputs , field . name + FIELD_SUFFIX , <NEWLINE> <INDENT> field . dtype ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Add input datasets",
                "# Add input fields"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8b7c03521880419aaa5825f27b0b225a": {
        "code_string": "if FIELDS_FNAME in dname:\n",
        "code_toks_joined": "if FIELDS_FNAME in dname : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7273440a47bb468a909ba4262e0c4340": {
        "code_string": "if (rx is None):\n         ptid_md = pd.DataFrame(data=rowmeta_dict,\n                            columns=rowmeta_dict.keys())\n         ptid_md = ptid_md.drop_duplicates()\n     else:\n         ptid_md = _generatePtidMetadata(wideform_df, id_list, rx)\n",
        "code_toks_joined": "if ( rx is None ) : <NEWLINE> <INDENT> ptid_md = pd . DataFrame ( data = rowmeta_dict , <NEWLINE> <INDENT> columns = rowmeta_dict . keys ( ) ) <NEWLINE> <DEDENT> ptid_md = ptid_md . drop_duplicates ( ) <NEWLINE> else : <NEWLINE> ptid_md = _generatePtidMetadata ( wideform_df , id_list , rx ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a0ed77503834404e825f7b2545225daf": {
        "code_string": "def callimpl(self):\n     # The truth table options are {AND, OR, XOR}.\n     # Other functions are negations of these, the 2 constants, or not symmetric.\n     # XOR sounds just like noise so it can't be that.\n     # AND and OR have the same frequency spectrum so either is good.\n     # We use OR as downstream it will prefer envelope shape over zero:\n     noiseflag = self.noiseflagreg.value\n     if not self.toneflagreg.value:\n       self.blockbuf.copybuf(self.tone(self.block))\n       if not noiseflag:\n         self.blockbuf.orbuf(self.noise(self.block))\n     elif noiseflag:\n       self.blockbuf.copybuf(self.noise(self.block))\n     else:\n       self.blockbuf.fill(0)\n",
        "code_toks_joined": "def callimpl ( self ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> noiseflag = self . noiseflagreg . value <NEWLINE> if not self . toneflagreg . value : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . tone ( self . block ) ) <NEWLINE> if not noiseflag : <NEWLINE> <INDENT> self . blockbuf . orbuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> <DEDENT> elif noiseflag : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . blockbuf . fill ( 0 ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# The truth table options are {AND, OR, XOR}.",
                "# Other functions are negations of these, the 2 constants, or not symmetric.",
                "# XOR sounds just like noise so it can't be that.",
                "# AND and OR have the same frequency spectrum so either is good.",
                "# We use OR as downstream it will prefer envelope shape over zero:"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "be3c239d75cf4aedbddfce7fca1429c1": {
        "code_string": "command = \"%s -c -s\" % (executable or 'cjsx')\n     if bare:\n         command = ' '.join((executable, '-b'))\n",
        "code_toks_joined": "command = <STRING> % ( executable or <STRING> ) <NEWLINE> <INDENT> if bare : <NEWLINE> <INDENT> command = <STRING> . join ( ( executable , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"%s -c -s\"",
                "'cjsx'",
                "' '",
                "'-b'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4408838e6e0747ef8dd03db0d7cf888b": {
        "code_string": "if nomina_object:\n                 xml.complemento.nominas = nominas\n                 xml.complemento.nomina =  nominas[0]\n             else:\n                 xml.complemento.nominas = []\n                 xml.complemento.nomina = None\n",
        "code_toks_joined": "if nomina_object : <NEWLINE> <INDENT> xml . complemento . nominas = nominas <NEWLINE> xml . complemento . nomina = nominas [ 0 ] <NEWLINE> else : <NEWLINE> xml . complemento . nominas = [ ] <NEWLINE> xml . complemento . nomina = None <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "21634f6c55f6480681c39b9799ab7553": {
        "code_string": "used_file = remote_dockerfile.split('BUILD_FROM=hassioaddons/')[1]\n         used_file = used_file.split('\\n')[0]\n",
        "code_toks_joined": "used_file = remote_dockerfile . split ( <STRING> ) [ 1 ] <NEWLINE> <INDENT> used_file = used_file . split ( <STRING> ) [ 0 ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'BUILD_FROM=hassioaddons/'",
                "'\\n'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0249d46f619f4e699f94e8de242881ad": {
        "code_string": "interfaces_lag = self.specific_parser.get_interfaces_lag(interfaces)\n         for ifname, lag in interfaces_lag.items():\n             try:\n                 real_lag_name = (\n                     self._search_key_case_insensitive(interfaces, lag)\n                 )\n             except KeyError:\n                 logger.error(\"%s not exist in polled interfaces\", ifname)\n                 continue\n",
        "code_toks_joined": "interfaces_lag = self . specific_parser . get_interfaces_lag ( interfaces ) <NEWLINE> <INDENT> for ifname , lag in interfaces_lag . items ( ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> real_lag_name = ( <NEWLINE> <INDENT> self . _search_key_case_insensitive ( interfaces , lag ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> logger . error ( <STRING> , ifname ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"%s not exist in polled interfaces\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b8514055bfa6496aa0c098f948088a1e": {
        "code_string": "class ProcessInlineFormsetView(ProcessFormView):\n     def post(self, request, *args, **kwargs):\n         form = self.get_form()\n         inline_formset = None\n         if form.is_valid():\n             obj = form.save(commit=False)\n             inline_formset = self.get_inline_formset()\n             if inline_formset.is_valid():\n                 form.save()\n                 inline_formset.save()\n                 return self.form_valid(form, inline_formset)\n         return self.form_invalid(form=form, inline_formset=inline_formset)\n",
        "code_toks_joined": "class ProcessInlineFormsetView ( ProcessFormView ) : <NEWLINE> <INDENT> def post ( self , request , * args , ** kwargs ) : <NEWLINE> <INDENT> form = self . get_form ( ) <NEWLINE> inline_formset = None <NEWLINE> if form . is_valid ( ) : <NEWLINE> <INDENT> obj = form . save ( commit = False ) <NEWLINE> inline_formset = self . get_inline_formset ( ) <NEWLINE> if inline_formset . is_valid ( ) : <NEWLINE> <INDENT> form . save ( ) <NEWLINE> inline_formset . save ( ) <NEWLINE> return self . form_valid ( form , inline_formset ) <NEWLINE> <DEDENT> <DEDENT> return self . form_invalid ( form = form , inline_formset = inline_formset ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "5f99a9a7e7d343f9a271eccb8610ad0d": {
        "code_string": "if smart:\n             mmesh = deepcopy(lmesh)\n             # quality_func = lambda mesh: mesh.Sizes()\n             if quality_assessor is None or quality_assessor == \"size\":\n                 if edim== 3:\n                     quality_func = lambda mesh: mesh.Volumes()\n                 elif edim == 2:\n                     quality_func = lambda mesh: mesh.Areas()\n                 else:\n                     quality_func = lambda mesh: mesh.Lengths()\n             elif quality_assessor == \"aspect_ratio\":\n                 quality_assessor = lambda mesh: mesh.AspectRatios()\n             elif quality_assessor == \"angle\":\n                 quality_func = lambda mesh: mesh.Angles()\n             else:\n                 raise ValueError('quality_assessor not undersood')\n",
        "code_toks_joined": "if smart : <NEWLINE> <INDENT> mmesh = deepcopy ( lmesh ) <NEWLINE> <COMMENT> <NL> if quality_assessor is None or quality_assessor == <STRING> : <NEWLINE> <INDENT> if edim == 3 : <NEWLINE> <INDENT> quality_func = lambda mesh : mesh . Volumes ( ) <NEWLINE> <DEDENT> elif edim == 2 : <NEWLINE> <INDENT> quality_func = lambda mesh : mesh . Areas ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> quality_func = lambda mesh : mesh . Lengths ( ) <NEWLINE> <DEDENT> <DEDENT> elif quality_assessor == <STRING> : <NEWLINE> <INDENT> quality_assessor = lambda mesh : mesh . AspectRatios ( ) <NEWLINE> <DEDENT> elif quality_assessor == <STRING> : <NEWLINE> <INDENT> quality_func = lambda mesh : mesh . Angles ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# quality_func = lambda mesh: mesh.Sizes()"
            ],
            "<STRING>": [
                "\"size\"",
                "\"aspect_ratio\"",
                "\"angle\"",
                "'quality_assessor not undersood'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fda81ded02ac4bc9ad60bc33b6ee1f67": {
        "code_string": "self._set_epochs(epochs=epochs_max)\n         if not refit_on_all:\n             simple_logger.info('Loading best model from checkpoint at %d epochs' % max_epoch)\n             self.model, self.model_checkpoint = self.model_checkpoint, None\n         else:\n             # refit on whole data\n             simple_logger.info('Refitting on whole train data for %d epochs' % max_epoch)\n             self.fit(train_obs)\n",
        "code_toks_joined": "self . _set_epochs ( epochs = epochs_max ) <NEWLINE> <INDENT> if not refit_on_all : <NEWLINE> <INDENT> simple_logger . info ( <STRING> % max_epoch ) <NEWLINE> self . model , self . model_checkpoint = self . model_checkpoint , None <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> simple_logger . info ( <STRING> % max_epoch ) <NEWLINE> self . fit ( train_obs ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'Loading best model from checkpoint at %d epochs'",
                "'Refitting on whole train data for %d epochs'"
            ],
            "<COMMENT>": [
                "# refit on whole data"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b4e1fa0613ce4acc950636fc5ba96f7d": {
        "code_string": "if os.path.exists(path):        # handles case of trailing /\n             os.mkdir(path)\n",
        "code_toks_joined": "if os . path . exists ( path ) : <COMMENT> <NEWLINE> <INDENT> os . mkdir ( path ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# handles case of trailing /"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a5e21335c1f44d95ad3e0a6afb64cf29": {
        "code_string": "@classmethod\n     def construct(cls, gas, rh, t_min, t_max, t_delta, exegete: Exegete):\n         cells = [ExegeteRenderingTRhCell(t, exegete.error(gas, t, rh))\n                  for t in range(t_min, t_max + 1, t_delta)]\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def construct ( cls , gas , rh , t_min , t_max , t_delta , exegete : Exegete ) : <NEWLINE> <INDENT> cells = [ ExegeteRenderingTRhCell ( t , exegete . error ( gas , t , rh ) ) <NEWLINE> <INDENT> for t in range ( t_min , t_max + 1 , t_delta ) ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6748fb5139b74ec085e1c93c8eb07531": {
        "code_string": "for rh in range(10, 91, 5):\n     for t in range(0, 46, 5):\n         interpretation = exegete.interpretation('NO2', text, t, rh)\n         print(\"rh: %2d t: %2d text: %3.1f interpretation: %3.1f\" % (rh, t, text, interpretation))\n",
        "code_toks_joined": "for rh in range ( 10 , 91 , 5 ) : <NEWLINE> <INDENT> for t in range ( 0 , 46 , 5 ) : <NEWLINE> <INDENT> interpretation = exegete . interpretation ( <STRING> , text , t , rh ) <NEWLINE> print ( <STRING> % ( rh , t , text , interpretation ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'NO2'",
                "\"rh: %2d t: %2d text: %3.1f interpretation: %3.1f\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "de160a012fdd44f999930eff1287e22a": {
        "code_string": "return bucket_list\n",
        "code_toks_joined": "return bucket_list <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3ccb12135fc0405ea2d5a74868e67088": {
        "code_string": "@classmethod\n     def persistence_location(cls, host):\n         return host.aws_dir(), cls.__FILENAME\n",
        "code_toks_joined": "@ classmethod <NEWLINE> <INDENT> def persistence_location ( cls , host ) : <NEWLINE> <INDENT> return host . aws_dir ( ) , cls . __FILENAME <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a1a0500c881f478c97d44df7f3a57b7c": {
        "code_string": "return elapsed_minutes > self.__config.unresponsive_minutes_allowed\n",
        "code_toks_joined": "return elapsed_minutes > self . __config . unresponsive_minutes_allowed <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4a54da59fde34da297160d71696ca37e": {
        "code_string": "# Generate README.rst if missing or out of date\n if not os.path.exists('README.rst') and is_older_than('README.rst', 'README.md'):\n     os.system('pandoc --from=markdown --to=rst --output=README.rst README.md')\n with open('README.rst') as file:\n     long_description = file.read()\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if not os . path . exists ( <STRING> ) and is_older_than ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> os . system ( <STRING> ) <NEWLINE> <DEDENT> with open ( <STRING> ) as file : <NEWLINE> <INDENT> long_description = file . read ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Generate README.rst if missing or out of date"
            ],
            "<STRING>": [
                "'README.rst'",
                "'README.rst'",
                "'README.md'",
                "'pandoc --from=markdown --to=rst --output=README.rst README.md'",
                "'README.rst'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b34b9dcd0203489fb28326205ebbfcca": {
        "code_string": "def get_tracks(self, paths):\n         iterators = []\n         for path in paths:\n             try:\n                 iterators.append(IterableTrackPaths(self.script.configuration, path))\n             except LibraryError as e:\n                 self.error(e)\n         return iterators\n",
        "code_toks_joined": "def get_tracks ( self , paths ) : <NEWLINE> <INDENT> iterators = [ ] <NEWLINE> for path in paths : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> iterators . append ( IterableTrackPaths ( self . script . configuration , path ) ) <NEWLINE> <DEDENT> except LibraryError as e : <NEWLINE> <INDENT> self . error ( e ) <NEWLINE> <DEDENT> <DEDENT> return iterators <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a39d5b4e1cba4e9eabdbdb7aa9075aee": {
        "code_string": "skip = False\n             for i, (value, docstring) in enumerate(zip(args, docstring_arguments)):\n                 if skip:\n                     skip = False\n                     continue\n                 if not \"const\" in docstring.split() and \"*\" in docstring:\n                     # char ** arguments are always guaranteed to be NULL\n                     # terminated\n                     if docstring.startswith(\"char\") and \"**\" in docstring.split():\n                         if value.value is None:\n                             return_value.append(None)\n                         elif any(map(value.value.startswith, [\"[\", \"{\"])):\n                             try:\n                                 return_value.append(json.loads(value.value))\n                             except json.decoder.JSONDecodeError:\n                                 return_value.append(value.value)\n                         else:\n                             return_value.append(value.value)\n                     elif \"**\" in docstring.split():\n                         if (i + 1) < len(docstring_arguments):\n                             next_docstring = docstring_arguments[i + 1]\n                             next_docstring = next_docstring.split()\n                             if (\n                                 \"size_t\" in next_docstring\n                                 and \"*\" in next_docstring\n                                 and \"uint8_t\" in docstring.split()\n                             ):\n                                 return_value.append(\n                                     to_bytearray(value.value, args[i + 1].value)\n                                 )\n                                 skip = True\n                                 continue\n                         return_value.append(value.value)\n                     else:\n                         return_value.append(value)\n",
        "code_toks_joined": "skip = False <NEWLINE> <INDENT> for i , ( value , docstring ) in enumerate ( zip ( args , docstring_arguments ) ) : <NEWLINE> <INDENT> if skip : <NEWLINE> <INDENT> skip = False <NEWLINE> continue <NEWLINE> <DEDENT> if not <STRING> in docstring . split ( ) and <STRING> in docstring : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if docstring . startswith ( <STRING> ) and <STRING> in docstring . split ( ) : <NEWLINE> <INDENT> if value . value is None : <NEWLINE> <INDENT> return_value . append ( None ) <NEWLINE> <DEDENT> elif any ( map ( value . value . startswith , [ <STRING> , <STRING> ] ) ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> return_value . append ( json . loads ( value . value ) ) <NEWLINE> <DEDENT> except json . decoder . JSONDecodeError : <NEWLINE> <INDENT> return_value . append ( value . value ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> return_value . append ( value . value ) <NEWLINE> <DEDENT> <DEDENT> elif <STRING> in docstring . split ( ) : <NEWLINE> <INDENT> if ( i + 1 ) < len ( docstring_arguments ) : <NEWLINE> <INDENT> next_docstring = docstring_arguments [ i + 1 ] <NEWLINE> next_docstring = next_docstring . split ( ) <NEWLINE> if ( <NEWLINE> <INDENT> <STRING> in next_docstring <NEWLINE> and <STRING> in next_docstring <NEWLINE> and <STRING> in docstring . split ( ) <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> return_value . append ( <NEWLINE> <INDENT> to_bytearray ( value . value , args [ i + 1 ] . value ) <NEWLINE> <DEDENT> ) <NEWLINE> skip = True <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> return_value . append ( value . value ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return_value . append ( value ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"const\"",
                "\"*\"",
                "\"char\"",
                "\"**\"",
                "\"[\"",
                "\"{\"",
                "\"**\"",
                "\"size_t\"",
                "\"*\"",
                "\"uint8_t\""
            ],
            "<COMMENT>": [
                "# char ** arguments are always guaranteed to be NULL",
                "# terminated"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "363fe67dad5f4452b9d54682128f2dfd": {
        "code_string": "# Set xlimit to appropriate limits..\n     newxlim = (ax_left.get_xlim()[0], xpos + 0.25)\n     ax_left.set_xlim(newxlim)\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> newxlim = ( ax_left . get_xlim ( ) [ 0 ] , xpos + 0.25 ) <NEWLINE> ax_left . set_xlim ( newxlim ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Set xlimit to appropriate limits.."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7f0fecaa589b4834aea666c84295bd11": {
        "code_string": "for i in range(0, len(bslist)):\n         bsi=bslist[i]\n         # array=list(bsi.items())[7][1] # Pull out the bootstrapped array.\n         array=bslist['diffarray']\n         ylims.append(array)\n",
        "code_toks_joined": "for i in range ( 0 , len ( bslist ) ) : <NEWLINE> <INDENT> bsi = bslist [ i ] <NEWLINE> <COMMENT> <NL> array = bslist [ <STRING> ] <NEWLINE> ylims . append ( array ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# array=list(bsi.items())[7][1] # Pull out the bootstrapped array."
            ],
            "<STRING>": [
                "'diffarray'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "505e92340527496c8a257cf1deb55c2e": {
        "code_string": "self.rotate_songs()\n             self.current_song = {}\n             self.current_song_json = \"{}\"\n             self.current_song_json_updated = str(time())\n         else:\n             song = r.json()\n             item = song[\"item\"]\n             # If song did not change - just update progress and JSON\n             if \"id\" in self.current_song and item[\"id\"] == self.current_song[\"id\"]:\n                 self.current_song_checks += 1\n                 # Update ETag if song's playback was manipulated\n                 if song[\"progress_ms\"] < self.current_song[\"progress\"] or song[\"progress_ms\"] - 10000 > self.current_song[\"progress\"]:\n                     self.current_song_json_updated = str(time())\n                     LISTEN_ALONG_API.set_current_playing_song(song_uri=song[\"uri\"], position_ms=song[\"progress_ms\"])\n",
        "code_toks_joined": "self . rotate_songs ( ) <NEWLINE> <INDENT> self . current_song = { } <NEWLINE> self . current_song_json = <STRING> <NEWLINE> self . current_song_json_updated = str ( time ( ) ) <NEWLINE> else : <NEWLINE> song = r . json ( ) <NEWLINE> item = song [ <STRING> ] <NEWLINE> <COMMENT> <NL> if <STRING> in self . current_song and item [ <STRING> ] == self . current_song [ <STRING> ] : <NEWLINE> <INDENT> self . current_song_checks += 1 <NEWLINE> <COMMENT> <NL> if song [ <STRING> ] < self . current_song [ <STRING> ] or song [ <STRING> ] - 10000 > self . current_song [ <STRING> ] : <NEWLINE> <INDENT> self . current_song_json_updated = str ( time ( ) ) <NEWLINE> LISTEN_ALONG_API . set_current_playing_song ( song_uri = song [ <STRING> ] , position_ms = song [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{}\"",
                "\"item\"",
                "\"id\"",
                "\"id\"",
                "\"id\"",
                "\"progress_ms\"",
                "\"progress\"",
                "\"progress_ms\"",
                "\"progress\"",
                "\"uri\"",
                "\"progress_ms\""
            ],
            "<COMMENT>": [
                "# If song did not change - just update progress and JSON",
                "# Update ETag if song's playback was manipulated"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8202f17247bf461f8d2e0642e056c43e": {
        "code_string": "users_json = ']'\n     return PlainTextResponse(content=users_json, media_type=\"application/json\")\n",
        "code_toks_joined": "users_json = <STRING> <NEWLINE> <INDENT> return PlainTextResponse ( content = users_json , media_type = <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "']'",
                "\"application/json\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a2084df14f404aa2a11382efd09e0012": {
        "code_string": "@staticmethod\n     def _set_song(user: ListenAlongUser, song_json: str) -> None:\n         if user.tokens:\n             status = SpotifyWebAPI.set_current_playing_song(song_json, user.tokens.access)\n             if user.public.status != status:\n                 user.public.status = status\n                 user.public_json = json.dumps(asdict(user.public))\n",
        "code_toks_joined": "@ staticmethod <NEWLINE> <INDENT> def _set_song ( user : ListenAlongUser , song_json : str ) -> None : <NEWLINE> <INDENT> if user . tokens : <NEWLINE> <INDENT> status = SpotifyWebAPI . set_current_playing_song ( song_json , user . tokens . access ) <NEWLINE> if user . public . status != status : <NEWLINE> <INDENT> user . public . status = status <NEWLINE> user . public_json = json . dumps ( asdict ( user . public ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c47ae3402b36444ab615deae5ce4808e": {
        "code_string": "class post_install(install):\n     def run(self):\n         install.run(self)\n         print(\"*** Executing post install actions:\")\n         # update mpd configuration if necessary\n         if '/tmp/mpd.fifo' in open('/etc/mpd.conf').read():\n             os.system(\"sudo cat /etc/fifo-mpd.conf >> /etc/mpd.conf\") \n             os.system(\"sudo service mpd restart\") \n         # update music display init script\n         os.system(\"sudo chmod +x /etc/init.d/pifidisplay\")\n         os.system(\"sudo update-rc.d pifidisplay defaults\")\n         os.system(\"sudo service pifidisplay restart\")\n         # update remote control init script\n         os.system(\"sudo chmod +x /etc/init.d/pifiremote\")\n         os.system(\"sudo update-rc.d pifiremote defaults\")\n         os.system(\"sudo service pifiremote restart\") \n",
        "code_toks_joined": "class post_install ( install ) : <NEWLINE> <INDENT> def run ( self ) : <NEWLINE> <INDENT> install . run ( self ) <NEWLINE> print ( <STRING> ) <NEWLINE> <COMMENT> <NL> if <STRING> in open ( <STRING> ) . read ( ) : <NEWLINE> <INDENT> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> <COMMENT> <NL> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> os . system ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"*** Executing post install actions:\"",
                "'/tmp/mpd.fifo'",
                "'/etc/mpd.conf'",
                "\"sudo cat /etc/fifo-mpd.conf >> /etc/mpd.conf\"",
                "\"sudo service mpd restart\"",
                "\"sudo chmod +x /etc/init.d/pifidisplay\"",
                "\"sudo update-rc.d pifidisplay defaults\"",
                "\"sudo service pifidisplay restart\"",
                "\"sudo chmod +x /etc/init.d/pifiremote\"",
                "\"sudo update-rc.d pifiremote defaults\"",
                "\"sudo service pifiremote restart\""
            ],
            "<COMMENT>": [
                "# update mpd configuration if necessary",
                "# update music display init script",
                "# update remote control init script"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "417543adf1ce4911bdc8e7eb6e442daa": {
        "code_string": "def echo_copyright():\n     \"\"\"Display a greeting message providing basic set of information.\"\"\"\n     cur_year = str(datetime.now().year)\n     year_range = '2018'\n     if cur_year != year_range:\n         year_range = '2018-{}'.format(year_range)\n     gpl3_notice_2018 = [\n         '{app_name} {version}'.format(\n             app_name=__BigName__,\n             version=dob_version,\n         ),\n         '',\n         'Copyright (C) {years} {author} <{email}>'.format(\n             years=year_range,\n             author=__author__,\n             email=__author_email__,\n         ),\n         # Be nice and call out the significant copyright holders from the years.\n         # (lb): What about Right to be forgotten?\n         'Copyright (C) 2015-2016 Eric Goller <elbenfreund@DenkenInEchtzeit.net>',\n         'Copyright (C) 2007-2014 Toms Baugis <toms.baugis@gmail.com>',\n         'Copyright (C) 2007-2008 Patryk Zawadzki <patrys at pld-linux.org>',\n         '',\n         _('This program comes with ABSOLUTELY NO WARRANTY. This is free software,'),\n         _('and you are welcome to redistribute it under certain conditions.'),\n         _(''),\n         _('Run `{} license` for details.').format(__arg0name__),\n     ]\n     notice = gpl3_notice_2018\n     for line in notice:\n         click_echo(line)\n",
        "code_toks_joined": "def echo_copyright ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cur_year = str ( datetime . now ( ) . year ) <NEWLINE> year_range = <STRING> <NEWLINE> if cur_year != year_range : <NEWLINE> <INDENT> year_range = <STRING> . format ( year_range ) <NEWLINE> <DEDENT> gpl3_notice_2018 = [ <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> app_name = __BigName__ , <NEWLINE> version = dob_version , <NEWLINE> <DEDENT> ) , <NEWLINE> <STRING> , <NEWLINE> <STRING> . format ( <NEWLINE> <INDENT> years = year_range , <NEWLINE> author = __author__ , <NEWLINE> email = __author_email__ , <NEWLINE> <DEDENT> ) , <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> _ ( <STRING> ) , <NEWLINE> _ ( <STRING> ) , <NEWLINE> _ ( <STRING> ) , <NEWLINE> _ ( <STRING> ) . format ( __arg0name__ ) , <NEWLINE> <DEDENT> ] <NEWLINE> notice = gpl3_notice_2018 <NEWLINE> for line in notice : <NEWLINE> <INDENT> click_echo ( line ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Display a greeting message providing basic set of information.\"\"\"",
                "'2018'",
                "'2018-{}'",
                "'{app_name} {version}'",
                "''",
                "'Copyright (C) {years} {author} <{email}>'",
                "'Copyright (C) 2015-2016 Eric Goller <elbenfreund@DenkenInEchtzeit.net>'",
                "'Copyright (C) 2007-2014 Toms Baugis <toms.baugis@gmail.com>'",
                "'Copyright (C) 2007-2008 Patryk Zawadzki <patrys at pld-linux.org>'",
                "''",
                "'This program comes with ABSOLUTELY NO WARRANTY. This is free software,'",
                "'and you are welcome to redistribute it under certain conditions.'",
                "''",
                "'Run `{} license` for details.'"
            ],
            "<COMMENT>": [
                "# Be nice and call out the significant copyright holders from the years.",
                "# (lb): What about Right to be forgotten?"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0309942e525e4a87a1797b45167c697c": {
        "code_string": "if len(line.split()) >= 1:\n                 if '--' in line.split()[0]:\n                     flag = 1\n",
        "code_toks_joined": "if len ( line . split ( ) ) >= 1 : <NEWLINE> <INDENT> if <STRING> in line . split ( ) [ 0 ] : <NEWLINE> <INDENT> flag = 1 <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'--'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "79f96160b33b417f85618ca881b6fb8c": {
        "code_string": "# Base parser assume `row` is already a list or tuple\n     def parse(self, row):\n         output = []\n         for i, value in enumerate(row):\n             parsed = getattr(self, 'parse_{}'.format(i), noop)(value)\n             if isinstance(parsed, (list, tuple)):\n                 output.extend(parsed)\n             else:\n                 output.append(parsed)\n         return parsed\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> def parse ( self , row ) : <NEWLINE> <INDENT> output = [ ] <NEWLINE> for i , value in enumerate ( row ) : <NEWLINE> <INDENT> parsed = getattr ( self , <STRING> . format ( i ) , noop ) ( value ) <NEWLINE> if isinstance ( parsed , ( list , tuple ) ) : <NEWLINE> <INDENT> output . extend ( parsed ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output . append ( parsed ) <NEWLINE> <DEDENT> <DEDENT> return parsed <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# Base parser assume `row` is already a list or tuple"
            ],
            "<STRING>": [
                "'parse_{}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "87c5751ffafa41b4b0a5bb6e3dc635c6": {
        "code_string": "def safe_apply(\n         self, apply_on: str, func: Callable, apply_to: str = None\n     ) -> pd.Series:\n         try:\n             apply_to = apply_to or apply_on\n             self.df[apply_on] = self.df[apply_on].apply(func)\n         except KeyError as ke:\n             logger.debug(\n                 MSG_PARSER_CHECK.format(op_name=self.operator.name, col_name=apply_on)\n             )\n         except Exception as e:\n             logger.debug(\n                 MSG_PARSER_ERROR.format(op_name=self.operator.name, e=e), exc_info=e\n             )\n         return self\n",
        "code_toks_joined": "def safe_apply ( <NEWLINE> <INDENT> self , apply_on : str , func : Callable , apply_to : str = None <NEWLINE> ) -> pd . Series : <NEWLINE> try : <NEWLINE> <INDENT> apply_to = apply_to or apply_on <NEWLINE> self . df [ apply_on ] = self . df [ apply_on ] . apply ( func ) <NEWLINE> <DEDENT> except KeyError as ke : <NEWLINE> <INDENT> logger . debug ( <NEWLINE> <INDENT> MSG_PARSER_CHECK . format ( op_name = self . operator . name , col_name = apply_on ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> logger . debug ( <NEWLINE> <INDENT> MSG_PARSER_ERROR . format ( op_name = self . operator . name , e = e ) , exc_info = e <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return self <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f989b2edb59a408b9559f11f3a04ddda": {
        "code_string": "for PATH in PATHS:\n         if not os.path.isfile(PATHS):\n             print(f\"`{PATH}` dosyaya ait de\u011fil.\")\n             continue\n",
        "code_toks_joined": "for PATH in PATHS : <NEWLINE> <INDENT> if not os . path . isfile ( PATHS ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "f\"`{PATH}` dosyaya ait de\u011fil.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7ecb7689bd3044de873557bb33d38c73": {
        "code_string": "GPIO.setmode(pin_mode)\n         if pin_rst is None:\n             GPIO.setup(pin_rst, GPIO.OUT)\n             GPIO.output(pin_rst, 1)\n         GPIO.setup(pin_irq, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n         GPIO.add_event_detect(pin_irq, GPIO.FALLING,\n                 callback=self.irq_callback)\n         if pin_ce != 0:\n             GPIO.setup(pin_ce, GPIO.OUT)\n             GPIO.output(pin_ce, 1)\n         self.init()\n",
        "code_toks_joined": "GPIO . setmode ( pin_mode ) <NEWLINE> <INDENT> if pin_rst is None : <NEWLINE> <INDENT> GPIO . setup ( pin_rst , GPIO . OUT ) <NEWLINE> GPIO . output ( pin_rst , 1 ) <NEWLINE> <DEDENT> GPIO . setup ( pin_irq , GPIO . IN , pull_up_down = GPIO . PUD_UP ) <NEWLINE> GPIO . add_event_detect ( pin_irq , GPIO . FALLING , <NEWLINE> <INDENT> callback = self . irq_callback ) <NEWLINE> <DEDENT> if pin_ce != 0 : <NEWLINE> <INDENT> GPIO . setup ( pin_ce , GPIO . OUT ) <NEWLINE> GPIO . output ( pin_ce , 1 ) <NEWLINE> <DEDENT> self . init ( ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "291a0485e95f463c9dd4550e28ef43dd": {
        "code_string": "if (with_forecast):\n                     forecast_url = forecast_urls[provider]\n                     r = requests.get(\n                         forecast_url.format(location, units, api_key))\n                     f = json.loads(r.text)\n                     if (c['response']['features']['forecast'] != 1):\n                         return {'e': 'Unable to load forecast'}\n                 else:\n                     f = None\n             except requests.exceptions.ConnectionError as e:\n                 return {'e': 'Connection error'}\n             return {'c': c, 'f': f}\n         else:\n             return {'e': 'Unable to parse location'}\n",
        "code_toks_joined": "if ( with_forecast ) : <NEWLINE> <INDENT> forecast_url = forecast_urls [ provider ] <NEWLINE> r = requests . get ( <NEWLINE> <INDENT> forecast_url . format ( location , units , api_key ) ) <NEWLINE> <DEDENT> f = json . loads ( r . text ) <NEWLINE> if ( c [ <STRING> ] [ <STRING> ] [ <STRING> ] != 1 ) : <NEWLINE> <INDENT> return { <STRING> : <STRING> } <NEWLINE> else : <NEWLINE> <DEDENT> f = None <NEWLINE> except requests . exceptions . ConnectionError as e : <NEWLINE> return { <STRING> : <STRING> } <NEWLINE> return { <STRING> : c , <STRING> : f } <NEWLINE> else : <NEWLINE> return { <STRING> : <STRING> } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'response'",
                "'features'",
                "'forecast'",
                "'e'",
                "'Unable to load forecast'",
                "'e'",
                "'Connection error'",
                "'c'",
                "'f'",
                "'e'",
                "'Unable to parse location'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c10a1f4ff7c347a3bf3806a2bc28a9f6": {
        "code_string": "logger2.info(step.name)\n             args = step.args.replace(\n                 'patientname', tmpdir + patientname).replace('reference', ref).replace('samplename', samplename)\n             cmdver = step.version.replace('.', '_')\n             javacmds = ['GATK', 'picard', 'SnpSift', 'snpEff']\n             if any(javacmd in step.command for javacmd in javacmds):\n                 cmd = 'java -jar -Xmx12G -Djava.io.tmpdir=%s ' % tmpdir + cfg.binPath + step.command + '/' + step.command + '_' + cmdver \\\n                       + '.jar ' + step.subcommand\n             else:\n                 cmd = cfg.binPath + step.command + '/' + \\\n                       step.command + '_' + cmdver + ' ' + step.subcommand\n             if 'HaplotypeCaller' in cmd:\n                 cmdstr = cmd + ' ' + args + ' ' + ' -I ' + inputfile + ' ' + outputfile\n             else:\n                 cmdstr = cmd + ' ' + args + ' ' + ' ' + inputfile + ' ' + outputfile\n                 print(cmd)\n             cmd = shlex.split(cmdstr)\n",
        "code_toks_joined": "logger2 . info ( step . name ) <NEWLINE> <INDENT> args = step . args . replace ( <NEWLINE> <INDENT> <STRING> , tmpdir + patientname ) . replace ( <STRING> , ref ) . replace ( <STRING> , samplename ) <NEWLINE> <DEDENT> cmdver = step . version . replace ( <STRING> , <STRING> ) <NEWLINE> javacmds = [ <STRING> , <STRING> , <STRING> , <STRING> ] <NEWLINE> if any ( javacmd in step . command for javacmd in javacmds ) : <NEWLINE> <INDENT> cmd = <STRING> % tmpdir + cfg . binPath + step . command + <STRING> + step . command + <STRING> + cmdver + <STRING> + step . subcommand <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cmd = cfg . binPath + step . command + <STRING> + step . command + <STRING> + cmdver + <STRING> + step . subcommand <NEWLINE> <DEDENT> if <STRING> in cmd : <NEWLINE> <INDENT> cmdstr = cmd + <STRING> + args + <STRING> + <STRING> + inputfile + <STRING> + outputfile <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cmdstr = cmd + <STRING> + args + <STRING> + <STRING> + inputfile + <STRING> + outputfile <NEWLINE> print ( cmd ) <NEWLINE> <DEDENT> cmd = shlex . split ( cmdstr ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'patientname'",
                "'reference'",
                "'samplename'",
                "'.'",
                "'_'",
                "'GATK'",
                "'picard'",
                "'SnpSift'",
                "'snpEff'",
                "'java -jar -Xmx12G -Djava.io.tmpdir=%s '",
                "'/'",
                "'_'",
                "'.jar '",
                "'/'",
                "'_'",
                "' '",
                "'HaplotypeCaller'",
                "' '",
                "' '",
                "' -I '",
                "' '",
                "' '",
                "' '",
                "' '",
                "' '"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cefe7dadf9d24d14ac11f4a721ef49da": {
        "code_string": "return dict_in\n",
        "code_toks_joined": "return dict_in <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3ed048ff5a0f4554bd84c96862f81d74": {
        "code_string": "# With sem (standard error of the mean).\n         if with_sem:\n             df_sem = (df[column]\n                       .groupby('datetime')\n                       .mean()\n                       .resample(groupby)\n                       .apply(sem)\n                       if groupby == 'S' and groupby != '1S' else\n                       df[column].groupby('datetime').apply(sem))\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> if with_sem : <NEWLINE> <INDENT> df_sem = ( df [ column ] <NEWLINE> <INDENT> . groupby ( <STRING> ) <NEWLINE> . mean ( ) <NEWLINE> . resample ( groupby ) <NEWLINE> . apply ( sem ) <NEWLINE> if groupby == <STRING> and groupby != <STRING> else <NEWLINE> df [ column ] . groupby ( <STRING> ) . apply ( sem ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# With sem (standard error of the mean)."
            ],
            "<STRING>": [
                "'datetime'",
                "'S'",
                "'1S'",
                "'datetime'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ab7b9e1604b94405a690b4895bd15aff": {
        "code_string": "def _request(self, method, endpoint, params=None, **kwargs):\n         _params = {'access_token': self.access_token}\n         if params:\n             _params.update(params)\n         response = requests.request(method, self.BASE_URL + endpoint, params=params, **kwargs)\n         return self._parse(response)\n",
        "code_toks_joined": "def _request ( self , method , endpoint , params = None , ** kwargs ) : <NEWLINE> <INDENT> _params = { <STRING> : self . access_token } <NEWLINE> if params : <NEWLINE> <INDENT> _params . update ( params ) <NEWLINE> <DEDENT> response = requests . request ( method , self . BASE_URL + endpoint , params = params , ** kwargs ) <NEWLINE> return self . _parse ( response ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'access_token'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8bf66ee9adb845fbb98a9952579c83e4": {
        "code_string": "linelist = [element for element in line if element in illegal]\n     line = _np.array(linelist)\n     return line\n",
        "code_toks_joined": "linelist = [ element for element in line if element in illegal ] <NEWLINE> <INDENT> line = _np . array ( linelist ) <NEWLINE> return line <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b9091d9eb6e3417f853eda501fb7bb4e": {
        "code_string": "def _splice_volumes(self, vollist):\n         namespace = self.get_user_namespace()\n         already_vols = []\n         if self.volumes:\n             already_vols = [x[\"name\"] for x in self.volumes]\n             self.log.debug(\"Already_vols: %r\" % already_vols)\n         for vol in vollist:\n             volname = self._get_volume_name_for_mountpoint(vol[\"mountpoint\"])\n             shortname = vol[\"mountpoint\"][1:].replace(\"/\", \"-\")\n             if volname in already_vols:\n                 self.log.info(\n                     \"Volume '{}' already exists for pod.\".format(volname))\n                 continue\n             k8s_vol = vol[\"k8s_vol\"]\n             if k8s_vol:\n                 # Create shadow PV and namespaced PVC for volume\n                 kvol = self._get_nfs_volume(k8s_vol)\n                 ns_vol = self._replicate_nfs_pv_with_suffix(\n                     kvol, namespace)\n                 self._create_pvc_for_pv(ns_vol)\n             mode = \"ReadOnlyMany\"\n             vmro = True\n             if vol[\"mode\"] == \"rw\":\n                 mode = \"ReadWriteMany\"\n                 vmro = False\n             vvol = {\n                 \"name\": shortname,\n             }\n             if k8s_vol:\n                 pvcvs = V1PersistentVolumeClaimVolumeSource(\n                     claim_name=ns_vol.metadata.name,\n                     read_only=vmro\n                 )\n                 vvol[\"persistent_volume_claim\"] = pvcvs\n             else:\n                 vvol[\"nfs\"] = {\n                     \"server\": vol[\"host\"],\n                     \"path\": vol[\"export\"],\n                     \"accessModes\": [mode]\n                 }\n",
        "code_toks_joined": "def _splice_volumes ( self , vollist ) : <NEWLINE> <INDENT> namespace = self . get_user_namespace ( ) <NEWLINE> already_vols = [ ] <NEWLINE> if self . volumes : <NEWLINE> <INDENT> already_vols = [ x [ <STRING> ] for x in self . volumes ] <NEWLINE> self . log . debug ( <STRING> % already_vols ) <NEWLINE> <DEDENT> for vol in vollist : <NEWLINE> <INDENT> volname = self . _get_volume_name_for_mountpoint ( vol [ <STRING> ] ) <NEWLINE> shortname = vol [ <STRING> ] [ 1 : ] . replace ( <STRING> , <STRING> ) <NEWLINE> if volname in already_vols : <NEWLINE> <INDENT> self . log . info ( <NEWLINE> <INDENT> <STRING> . format ( volname ) ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> k8s_vol = vol [ <STRING> ] <NEWLINE> if k8s_vol : <NEWLINE> <COMMENT> <NL> <INDENT> kvol = self . _get_nfs_volume ( k8s_vol ) <NEWLINE> ns_vol = self . _replicate_nfs_pv_with_suffix ( <NEWLINE> <INDENT> kvol , namespace ) <NEWLINE> <DEDENT> self . _create_pvc_for_pv ( ns_vol ) <NEWLINE> <DEDENT> mode = <STRING> <NEWLINE> vmro = True <NEWLINE> if vol [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> mode = <STRING> <NEWLINE> vmro = False <NEWLINE> <DEDENT> vvol = { <NEWLINE> <INDENT> <STRING> : shortname , <NEWLINE> <DEDENT> } <NEWLINE> if k8s_vol : <NEWLINE> <INDENT> pvcvs = V1PersistentVolumeClaimVolumeSource ( <NEWLINE> <INDENT> claim_name = ns_vol . metadata . name , <NEWLINE> read_only = vmro <NEWLINE> <DEDENT> ) <NEWLINE> vvol [ <STRING> ] = pvcvs <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> vvol [ <STRING> ] = { <NEWLINE> <INDENT> <STRING> : vol [ <STRING> ] , <NEWLINE> <STRING> : vol [ <STRING> ] , <NEWLINE> <STRING> : [ mode ] <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"name\"",
                "\"Already_vols: %r\"",
                "\"mountpoint\"",
                "\"mountpoint\"",
                "\"/\"",
                "\"-\"",
                "\"Volume '{}' already exists for pod.\"",
                "\"k8s_vol\"",
                "\"ReadOnlyMany\"",
                "\"mode\"",
                "\"rw\"",
                "\"ReadWriteMany\"",
                "\"name\"",
                "\"persistent_volume_claim\"",
                "\"nfs\"",
                "\"server\"",
                "\"host\"",
                "\"path\"",
                "\"export\"",
                "\"accessModes\""
            ],
            "<COMMENT>": [
                "# Create shadow PV and namespaced PVC for volume"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "d8869c53c7b3444ab41f08e4531d00f8": {
        "code_string": "class Task(TaskModel):\n     \"\"\"\n     Provides :meth:`__init__` for :class:`TaskModel` so the model can\n     be instanced with initial values.\n     \"\"\"\n     def __init__(self, job, frame, parent_task=None, state=None,\n                  priority=None, attempts=None, agent=None):\n         # build parent job id\n         if not modelfor(job, TABLE_JOB):\n             jobid = job.jobid\n             if jobid is None:\n                 raise ValueError(\"`job` with null id provided\")\n         elif isinstance(job, int):\n             jobid = job\n         else:\n             raise ValueError(\"failed to determine job id\")\n",
        "code_toks_joined": "class Task ( TaskModel ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , job , frame , parent_task = None , state = None , <NEWLINE> <INDENT> priority = None , attempts = None , agent = None ) : <NEWLINE> <COMMENT> <NL> if not modelfor ( job , TABLE_JOB ) : <NEWLINE> jobid = job . jobid <NEWLINE> if jobid is None : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> elif isinstance ( job , int ) : <NEWLINE> jobid = job <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n     Provides :meth:`__init__` for :class:`TaskModel` so the model can\n     be instanced with initial values.\n     \"\"\"",
                "\"`job` with null id provided\"",
                "\"failed to determine job id\""
            ],
            "<COMMENT>": [
                "# build parent job id"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "21d81fd3921a4769850d414c14bcf748": {
        "code_string": "if attempts is None:\n             self.attempts = attempts\n",
        "code_toks_joined": "if attempts is None : <NEWLINE> <INDENT> self . attempts = attempts <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b4b79b46aebf42e19cbbbd4abb02b9aa": {
        "code_string": "m = re.match(self.inputs[f], os.path.basename(filename))\n             if m is not None:\n                 # does the file exist locally\n                 if os.path.exists(f):\n                     self.local_in[f] = filename\n                 else:\n                     self.remote_in[f] = filename\n",
        "code_toks_joined": "m = re . match ( self . inputs [ f ] , os . path . basename ( filename ) ) <NEWLINE> <INDENT> if m is not None : <NEWLINE> <COMMENT> <NL> <INDENT> if os . path . exists ( f ) : <NEWLINE> <INDENT> self . local_in [ f ] = filename <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . remote_in [ f ] = filename <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# does the file exist locally"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1eb5047cab6c40d68fa99a77ed834e6f": {
        "code_string": "for name in self.const:\n             args = self.const[name]\n             names = [codegen.getName() for _ in args]\n             codegen.inFunction()\n             if len(args) > 0:\n                 codegen.append(\"function \"+self.package+\"_\"+name+\"(\")\n                 codegen.append(\",\".join(names))\n                 codegen.append(\"){return [\"+str(count)+\",\"+\",\".join(names)+\"]}\")\n             else:\n                 codegen.append(\"var \"+self.package+\"_\"+name+\"=[\"+str(count)+\",\"+\",\".join(names)+\"];\")\n",
        "code_toks_joined": "for name in self . const : <NEWLINE> <INDENT> args = self . const [ name ] <NEWLINE> names = [ codegen . getName ( ) for _ in args ] <NEWLINE> codegen . inFunction ( ) <NEWLINE> if len ( args ) > 0 : <NEWLINE> <INDENT> codegen . append ( <STRING> + self . package + <STRING> + name + <STRING> ) <NEWLINE> codegen . append ( <STRING> . join ( names ) ) <NEWLINE> codegen . append ( <STRING> + str ( count ) + <STRING> + <STRING> . join ( names ) + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> codegen . append ( <STRING> + self . package + <STRING> + name + <STRING> + str ( count ) + <STRING> + <STRING> . join ( names ) + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"function \"",
                "\"_\"",
                "\"(\"",
                "\",\"",
                "\"){return [\"",
                "\",\"",
                "\",\"",
                "\"]}\"",
                "\"var \"",
                "\"_\"",
                "\"=[\"",
                "\",\"",
                "\",\"",
                "\"];\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "261bbdeeeccd409099e0235423855439": {
        "code_string": "filters.append(query.c.key == tag.lower())\n",
        "code_toks_joined": "filters . append ( query . c . key == tag . lower ( ) ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "55e0a1bdf2c643289818bd5f9f037317": {
        "code_string": "values = list(pushes.value)\n     title = \"-\".join(\n         map(\n             str,\n             [\n                 sig.framework,\n                 sig.suite,\n                 sig.test,\n                 sig.platform,\n                 sig.repository,\n                 about_deviant.overall_dev_status,\n             ],\n         )\n     )\n     # EG https://treeherder.mozilla.org/perf.html#/graphs?highlightAlerts=1&series=mozilla-central,fee739b45f7960e4a520d8e0bd781dd9d0a3bec4,1,10&timerange=31536000\n     url = \"https://treeherder.mozilla.org/perf.html#/graphs?\" + value2url_param({\n         \"highlightAlerts\": 1,\n         \"series\": [sig.repository, sig.id, 1, coalesce(sig.framework, sig.framework_id)],\n         \"timerange\": 31536000,\n     })\n",
        "code_toks_joined": "values = list ( pushes . value ) <NEWLINE> <INDENT> title = <STRING> . join ( <NEWLINE> <INDENT> map ( <NEWLINE> <INDENT> str , <NEWLINE> [ <NEWLINE> <INDENT> sig . framework , <NEWLINE> sig . suite , <NEWLINE> sig . test , <NEWLINE> sig . platform , <NEWLINE> sig . repository , <NEWLINE> about_deviant . overall_dev_status , <NEWLINE> <DEDENT> ] , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> url = <STRING> + value2url_param ( { <NEWLINE> <INDENT> <STRING> : 1 , <NEWLINE> <STRING> : [ sig . repository , sig . id , 1 , coalesce ( sig . framework , sig . framework_id ) ] , <NEWLINE> <STRING> : 31536000 , <NEWLINE> <DEDENT> } ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"-\"",
                "\"https://treeherder.mozilla.org/perf.html#/graphs?\"",
                "\"highlightAlerts\"",
                "\"series\"",
                "\"timerange\""
            ],
            "<COMMENT>": [
                "# EG https://treeherder.mozilla.org/perf.html#/graphs?highlightAlerts=1&series=mozilla-central,fee739b45f7960e4a520d8e0bd781dd9d0a3bec4,1,10&timerange=31536000"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b3b35655364d4c2eb35e730a759ddf25": {
        "code_string": "datasetPath = os.path.join(self.Path,self.DatasetName,self.Name)   \n             #check if dataset exists\n             if not arcpy.Exists(datasetPath):\n                 raise Exception(datasetPath +\" doesn't exist\")\n             #test for schema lock, before continue\n             trys=0\n             while arcpy.TestSchemaLock(datasetPath) or trys>6:\n                 time.sleep(10)\n                 trys+=1\n             #next\n",
        "code_toks_joined": "datasetPath = os . path . join ( self . Path , self . DatasetName , self . Name ) <NEWLINE> <COMMENT> <NL> <INDENT> if not arcpy . Exists ( datasetPath ) : <NEWLINE> <INDENT> raise Exception ( datasetPath + <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> trys = 0 <NEWLINE> while arcpy . TestSchemaLock ( datasetPath ) or trys > 6 : <NEWLINE> <INDENT> time . sleep ( 10 ) <NEWLINE> trys += 1 <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#check if dataset exists",
                "#test for schema lock, before continue",
                "#next"
            ],
            "<STRING>": [
                "\" doesn't exist\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b30a395c402a4989b519df8f6d36c8cc": {
        "code_string": "def is_prefixed(value, prefix):\n     return value.startswith(\n         force_bytes(prefix) if is_bytes(prefix) else force_text(prefix)\n     )\n",
        "code_toks_joined": "def is_prefixed ( value , prefix ) : <NEWLINE> <INDENT> return value . startswith ( <NEWLINE> <INDENT> force_bytes ( prefix ) if is_bytes ( prefix ) else force_text ( prefix ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "b7a0432290a74945b80f7cd5bebbbd37": {
        "code_string": "if is_array(block.get(\"transactions\")):\n         for item in block[\"transactions\"]:\n             if is_string(item):\n                 item = outputTransactionFormatter(item)\n",
        "code_toks_joined": "if is_array ( block . get ( <STRING> ) ) : <NEWLINE> <INDENT> for item in block [ <STRING> ] : <NEWLINE> <INDENT> if is_string ( item ) : <NEWLINE> <INDENT> item = outputTransactionFormatter ( item ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"transactions\"",
                "\"transactions\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ec6661332d854d74934898f2e954cc43": {
        "code_string": "def login(request):\n     \"Displays the login form and handles the login action.\"\n     manipulator = AuthenticationForm(request)\n     redirect_to = request.REQUEST.get(REDIRECT_FIELD_NAME, '')\n     if request.POST:\n         errors = manipulator.get_validation_errors(request.POST)\n         if not errors:\n             # Light security check -- make sure redirect_to isn't garbage.\n             if not redirect_to or '://' in redirect_to or ' ' in redirect_to:\n                 redirect_to = '/accounts/profile/'\n             request.session[users.SESSION_KEY] = manipulator.get_user_id()\n             return HttpResponseRedirect(redirect_to)\n     else:\n         errors = {}\n     response = HttpResponse()\n     response.session.set_test_cookie()\n     t = template_loader.get_template('registration/login')\n     c = Context(request, {\n         'form': formfields.FormWrapper(manipulator, request.POST, errors),\n         REDIRECT_FIELD_NAME: redirect_to,\n         'site_name': sites.get_current().name,\n     })\n     response.write(t.render(c))\n     return response\n",
        "code_toks_joined": "def login ( request ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> manipulator = AuthenticationForm ( request ) <NEWLINE> redirect_to = request . REQUEST . get ( REDIRECT_FIELD_NAME , <STRING> ) <NEWLINE> if request . POST : <NEWLINE> <INDENT> errors = manipulator . get_validation_errors ( request . POST ) <NEWLINE> if not errors : <NEWLINE> <COMMENT> <NL> <INDENT> if not redirect_to or <STRING> in redirect_to or <STRING> in redirect_to : <NEWLINE> <INDENT> redirect_to = <STRING> <NEWLINE> <DEDENT> request . session [ users . SESSION_KEY ] = manipulator . get_user_id ( ) <NEWLINE> return HttpResponseRedirect ( redirect_to ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> errors = { } <NEWLINE> <DEDENT> response = HttpResponse ( ) <NEWLINE> response . session . set_test_cookie ( ) <NEWLINE> t = template_loader . get_template ( <STRING> ) <NEWLINE> c = Context ( request , { <NEWLINE> <INDENT> <STRING> : formfields . FormWrapper ( manipulator , request . POST , errors ) , <NEWLINE> REDIRECT_FIELD_NAME : redirect_to , <NEWLINE> <STRING> : sites . get_current ( ) . name , <NEWLINE> <DEDENT> } ) <NEWLINE> response . write ( t . render ( c ) ) <NEWLINE> return response <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"Displays the login form and handles the login action.\"",
                "''",
                "'://'",
                "' '",
                "'/accounts/profile/'",
                "'registration/login'",
                "'form'",
                "'site_name'"
            ],
            "<COMMENT>": [
                "# Light security check -- make sure redirect_to isn't garbage."
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7a2fb19c38b24cae8b0a5e464ca8b338": {
        "code_string": "def W(self):\n         \"ISO-8601 week number of year, weeks starting on Monday\"\n         # Algorithm from http://www.personal.ecu.edu/mccartyr/ISOwdALG.txt\n         week_number = None\n         jan1_weekday = self.data.replace(month=1, day=1).weekday() + 1\n         weekday = self.data.weekday() + 1\n         day_of_year = self.z()\n         if day_of_year <= (8 - jan1_weekday) and jan1_weekday > 4:\n             if jan1_weekday == 5 or (jan1_weekday == 6 and isleap(self.data.year-1)):\n                 week_number = 53\n             else:\n                 week_number = 52\n         else:\n             if isleap(self.data.year):\n                 i = 366\n             else:\n                 i = 365\n             if (i - day_of_year) < (4 - weekday):\n                 week_number = 1\n             else:\n                 j = day_of_year + (7 - weekday) + (jan1_weekday - 1)\n                 week_number = j / 7\n                 if jan1_weekday > 4:\n                     week_number -= 1\n         return week_number\n",
        "code_toks_joined": "def W ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> week_number = None <NEWLINE> jan1_weekday = self . data . replace ( month = 1 , day = 1 ) . weekday ( ) + 1 <NEWLINE> weekday = self . data . weekday ( ) + 1 <NEWLINE> day_of_year = self . z ( ) <NEWLINE> if day_of_year <= ( 8 - jan1_weekday ) and jan1_weekday > 4 : <NEWLINE> <INDENT> if jan1_weekday == 5 or ( jan1_weekday == 6 and isleap ( self . data . year - 1 ) ) : <NEWLINE> <INDENT> week_number = 53 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> week_number = 52 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if isleap ( self . data . year ) : <NEWLINE> <INDENT> i = 366 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> i = 365 <NEWLINE> <DEDENT> if ( i - day_of_year ) < ( 4 - weekday ) : <NEWLINE> <INDENT> week_number = 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> j = day_of_year + ( 7 - weekday ) + ( jan1_weekday - 1 ) <NEWLINE> week_number = j / 7 <NEWLINE> if jan1_weekday > 4 : <NEWLINE> <INDENT> week_number -= 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return week_number <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"ISO-8601 week number of year, weeks starting on Monday\""
            ],
            "<COMMENT>": [
                "# Algorithm from http://www.personal.ecu.edu/mccartyr/ISOwdALG.txt"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "2799f861a5854e738c36e79f893e04b6": {
        "code_string": "def test_cache_page_old_style(self):\n         \"\"\"\n         Test that we can call cache_page the old way\n         \"\"\"\n         def my_view(request):\n             return \"response\"\n         my_view_cached = cache_page(123, my_view)\n         self.assertEqual(my_view_cached(HttpRequest()), \"response\")\n",
        "code_toks_joined": "def test_cache_page_old_style ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def my_view ( request ) : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> my_view_cached = cache_page ( 123 , my_view ) <NEWLINE> self . assertEqual ( my_view_cached ( HttpRequest ( ) ) , <STRING> ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Test that we can call cache_page the old way\n         \"\"\"",
                "\"response\"",
                "\"response\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fc3900add4cf4872bbeac395501a2e1a": {
        "code_string": "def _get_key(self, full_path):\n         \"\"\"Build a checksum used to identify this filepath\"\"\"\n         full_path_checksum = hashlib.sha1(full_path).digest()\n         return full_path\n",
        "code_toks_joined": "def _get_key ( self , full_path ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> full_path_checksum = hashlib . sha1 ( full_path ) . digest ( ) <NEWLINE> return full_path <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Build a checksum used to identify this filepath\"\"\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "face7b171f164f878cd2270b48e6ca1a": {
        "code_string": "for m_instance, name in SWIFT_STATS.iteritems():\n         if m_instance in stats:\n             metric = collectd.Values()\n             metric.plugin = 'swift_stat'\n             metric.interval = INTERVAL\n             metric.type = 'gauge'\n             metric.type_instance = m_instance\n             metric.values = [stats[m_instance]]\n             metric.dispatch()\n         else:\n             collectd.error('swift_stat: Can not find: {}'.format(m_instance))\n",
        "code_toks_joined": "for m_instance , name in SWIFT_STATS . iteritems ( ) : <NEWLINE> <INDENT> if m_instance in stats : <NEWLINE> <INDENT> metric = collectd . Values ( ) <NEWLINE> metric . plugin = <STRING> <NEWLINE> metric . interval = INTERVAL <NEWLINE> metric . type = <STRING> <NEWLINE> metric . type_instance = m_instance <NEWLINE> metric . values = [ stats [ m_instance ] ] <NEWLINE> metric . dispatch ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> collectd . error ( <STRING> . format ( m_instance ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'swift_stat'",
                "'gauge'",
                "'swift_stat: Can not find: {}'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "569a9852f27446638481a4785765ced4": {
        "code_string": "#index bam\n             samtools_index_command = (\n                 settings[\"samtools_call\"], \"index\",\n                 length_split_bam\n                 )\n",
        "code_toks_joined": "<COMMENT> <NL> <INDENT> samtools_index_command = ( <NEWLINE> <INDENT> settings [ <STRING> ] , <STRING> , <NEWLINE> length_split_bam <NEWLINE> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "#index bam"
            ],
            "<STRING>": [
                "\"samtools_call\"",
                "\"index\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6cb99dd2240c460c99dab5805e0d1d05": {
        "code_string": "featureCounts_command =(\n                         settings[\"featureCounts_call\"],\n                         #\"-T\", str(settings[\"CPUs\"]),\n                         #\"-G\", settings[\"genome\"],\n                         \"-M\", \"-O\", \"-s 1\", \"-F SAF\",\n                         \"--nonOverlap\", str(settings[\"non_overlap\"]),\n                         \"--nonOverlapFeature\", str(settings[\"non_overlap\"]),\n ##                        \"--fracOverlap\", str(overlap[i]),\n ##                        \"--fracOverlapFeature\", str(overlap[i]),                                             \n                         \"-a\", input_SAF,\n                         \"-o\", output_SAF,\n                         input_bam\n #                        input_bam, \"2>\", featurecounts_info\n                         )\n         with open(output_SAF) as f_out:\n             for line in f_out:\n                 print(line)\n",
        "code_toks_joined": "featureCounts_command = ( <NEWLINE> <INDENT> settings [ <STRING> ] , <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <STRING> , str ( settings [ <STRING> ] ) , <NEWLINE> <STRING> , str ( settings [ <STRING> ] ) , <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <STRING> , input_SAF , <NEWLINE> <STRING> , output_SAF , <NEWLINE> input_bam <NEWLINE> <COMMENT> <NL> ) <NEWLINE> with open ( output_SAF ) as f_out : <NEWLINE> for line in f_out : <NEWLINE> print ( line ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"featureCounts_call\"",
                "\"-M\"",
                "\"-O\"",
                "\"-s 1\"",
                "\"-F SAF\"",
                "\"--nonOverlap\"",
                "\"non_overlap\"",
                "\"--nonOverlapFeature\"",
                "\"non_overlap\"",
                "\"-a\"",
                "\"-o\""
            ],
            "<COMMENT>": [
                "#\"-T\", str(settings[\"CPUs\"]),",
                "#\"-G\", settings[\"genome\"],",
                "##                        \"--fracOverlap\", str(overlap[i]),",
                "##                        \"--fracOverlapFeature\", str(overlap[i]),                                             ",
                "#                        input_bam, \"2>\", featurecounts_info"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0e343905a0804171a7fe7fc9a608b389": {
        "code_string": "elif pos == consensus_seq[i]:\n",
        "code_toks_joined": "elif pos == consensus_seq [ i ] : <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3ac5c64c81f5461cac5c3ee410a346df": {
        "code_string": "pshow = sp.add_parser('show', help=\"Show the current 6-digit token\")\n     m = pshow.add_mutually_exclusive_group()\n     m.add_argument('-s', '--secret',\n                    help=\"Specify the token secret on the command line (base32 encoded)\")\n     m.add_argument('-f', '--dotfile', type=PathType(exists=True), default=os.path.expanduser('~/.vipaccess'),\n                    help=\"File in which the credential is stored (default ~/.vipaccess\")\n     m.add_argument('-v', '--verbose', action='store_true')\n     pshow.set_defaults(func=show)\n",
        "code_toks_joined": "pshow = sp . add_parser ( <STRING> , help = <STRING> ) <NEWLINE> <INDENT> m = pshow . add_mutually_exclusive_group ( ) <NEWLINE> m . add_argument ( <STRING> , <STRING> , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> m . add_argument ( <STRING> , <STRING> , type = PathType ( exists = True ) , default = os . path . expanduser ( <STRING> ) , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> m . add_argument ( <STRING> , <STRING> , action = <STRING> ) <NEWLINE> pshow . set_defaults ( func = show ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'show'",
                "\"Show the current 6-digit token\"",
                "'-s'",
                "'--secret'",
                "\"Specify the token secret on the command line (base32 encoded)\"",
                "'-f'",
                "'--dotfile'",
                "'~/.vipaccess'",
                "\"File in which the credential is stored (default ~/.vipaccess\"",
                "'-v'",
                "'--verbose'",
                "'store_true'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "7fbf8aef756c4f5db053674a567d2183": {
        "code_string": "if callback:\n             callbacks.remove(callback)\n             if len(callbacks) == 0:\n                 del self._subscribers[(channel, pattern)]\n",
        "code_toks_joined": "if callback : <NEWLINE> <INDENT> callbacks . remove ( callback ) <NEWLINE> if len ( callbacks ) == 0 : <NEWLINE> <INDENT> del self . _subscribers [ ( channel , pattern ) ] <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "72c7c12c155946f383804a5e7bb91057": {
        "code_string": "@property\n     def ip_addrs(self):\n         \"\"\" get a list of all current IP addresses of device\n         \"\"\"\n         self._logger.info(\"retreive IP addresses\")\n         out = self.cmd(\" | \".join([\n             \"ifconfig -a\",\n             \"grep 'inet addr'\",\n             \"awk -F: '{print $2}'\",\n             \"awk '{print $1}'\",]))\n         ips = out.split(\"\\n\")\n         if out and not out.startswith(\"127.0.0.1\"):\n             self._logger.debug(\"found IP addresses:\" + str(out))\n             return out.split('\\n')\n         else:\n             raise NoIPException(\"couldn't receive any IP address:'{}'\".format(\n                 ips))\n",
        "code_toks_joined": "@ property <NEWLINE> <INDENT> def ip_addrs ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _logger . info ( <STRING> ) <NEWLINE> out = self . cmd ( <STRING> . join ( [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , ] ) ) <NEWLINE> <DEDENT> ips = out . split ( <STRING> ) <NEWLINE> if out and not out . startswith ( <STRING> ) : <NEWLINE> <INDENT> self . _logger . debug ( <STRING> + str ( out ) ) <NEWLINE> return out . split ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NoIPException ( <STRING> . format ( <NEWLINE> <INDENT> ips ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\" get a list of all current IP addresses of device\n         \"\"\"",
                "\"retreive IP addresses\"",
                "\" | \"",
                "\"ifconfig -a\"",
                "\"grep 'inet addr'\"",
                "\"awk -F: '{print $2}'\"",
                "\"awk '{print $1}'\"",
                "\"\\n\"",
                "\"127.0.0.1\"",
                "\"found IP addresses:\"",
                "'\\n'",
                "\"couldn't receive any IP address:'{}'\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e3cee4eed9dd4c5ab580cd74bbbfe97d": {
        "code_string": "def set_page_and_response_if_appropriate(self):\n         if isinstance(request.routing_exception, NotFound) and current_page:\n             return self.page_view()\n",
        "code_toks_joined": "def set_page_and_response_if_appropriate ( self ) : <NEWLINE> <INDENT> if isinstance ( request . routing_exception , NotFound ) and current_page : <NEWLINE> <INDENT> return self . page_view ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "1d299209390345d28b6933e532e745ef": {
        "code_string": "def _get_missing_params_from_robot_variables(self, param_dict):\n         for testlink_param, robot_variable in robot_report_params.items():\n             setdefault_if_not_none(param_dict, testlink_param, self._get_param_from_robot(testlink_param))\n",
        "code_toks_joined": "def _get_missing_params_from_robot_variables ( self , param_dict ) : <NEWLINE> <INDENT> for testlink_param , robot_variable in robot_report_params . items ( ) : <NEWLINE> <INDENT> setdefault_if_not_none ( param_dict , testlink_param , self . _get_param_from_robot ( testlink_param ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "fba4e05cf286423cbb73d494a6c493e1": {
        "code_string": "def maybe_swap_spatial_dims(ds, namex='x', namey='y'):\n     swaps = {}\n     lx, rx = ds.indexes[namex][[0, -1]]\n     uy, ly = ds.indexes[namex][[0, -1]]\n",
        "code_toks_joined": "def maybe_swap_spatial_dims ( ds , namex = <STRING> , namey = <STRING> ) : <NEWLINE> <INDENT> swaps = { } <NEWLINE> lx , rx = ds . indexes [ namex ] [ [ 0 , - 1 ] ] <NEWLINE> uy , ly = ds . indexes [ namex ] [ [ 0 , - 1 ] ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'x'",
                "'y'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e057331df5da4df2bece02f667c8fa2f": {
        "code_string": "@staticmethod\n     def write_output(content, basename):\n         \"\"\"\n         Leverage django-compressor's JsCompressor to write the Javascript, making use of configured filters\n         and other settings.\n         \"\"\"\n         # Compress it\n         compressor = JsCompressor()\n         filtered = compressor.filter(content, method='input', kind='js')\n         output = compressor.filter_output(filtered)\n         path = compressor.get_filepath(output, basename=basename)\n         # Write it\n         compressor.storage.save(path, ContentFile(content.encode(compressor.charset)))\n         return mark_safe(compressor.storage.url(path))\n",
        "code_toks_joined": "@ staticmethod <NEWLINE> <INDENT> def write_output ( content , basename ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> compressor = JsCompressor ( ) <NEWLINE> filtered = compressor . filter ( content , method = <STRING> , kind = <STRING> ) <NEWLINE> output = compressor . filter_output ( filtered ) <NEWLINE> path = compressor . get_filepath ( output , basename = basename ) <NEWLINE> <COMMENT> <NL> compressor . storage . save ( path , ContentFile ( content . encode ( compressor . charset ) ) ) <NEWLINE> return mark_safe ( compressor . storage . url ( path ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"\n         Leverage django-compressor's JsCompressor to write the Javascript, making use of configured filters\n         and other settings.\n         \"\"\"",
                "'input'",
                "'js'"
            ],
            "<COMMENT>": [
                "# Compress it",
                "# Write it"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e7a2b22dffab4adfb3198eef9695f552": {
        "code_string": "with ExitStack() as stack:\n             files = [stack.enter_context(open(fname, 'r')) for fname in self.file_list]\n             ii=0\n             while True:\n                 try:\n                     timestamps, channel_data = self._reader.read_block(file=files[ii], block_size=block_size)\n                     if 0 < len(timestamps) < block_size:\n                         # block_size could not be filled from current file, so advance to next file\n                         ii+=1\n                         timestamps_, channel_data_ = self._reader.read_block(file=files[ii], block_size=block_size-len(timestamps))\n                         if not isinstance(timestamps, list):\n                             raise TypeError(\"timestamps MUST be a list!\")\n                         timestamps = timestamps + timestamps_ # list concatenation\n                         channel_data = np.hstack((channel_data, channel_data_))\n                     if timestamps:\n                         yield timestamps, channel_data_\n                     else:\n                         ii+=1\n                 except IndexError:\n                     return\n",
        "code_toks_joined": "with ExitStack ( ) as stack : <NEWLINE> <INDENT> files = [ stack . enter_context ( open ( fname , <STRING> ) ) for fname in self . file_list ] <NEWLINE> ii = 0 <NEWLINE> while True : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> timestamps , channel_data = self . _reader . read_block ( file = files [ ii ] , block_size = block_size ) <NEWLINE> if 0 < len ( timestamps ) < block_size : <NEWLINE> <COMMENT> <NL> <INDENT> ii += 1 <NEWLINE> timestamps_ , channel_data_ = self . _reader . read_block ( file = files [ ii ] , block_size = block_size - len ( timestamps ) ) <NEWLINE> if not isinstance ( timestamps , list ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT> timestamps = timestamps + timestamps_ <COMMENT> <NEWLINE> channel_data = np . hstack ( ( channel_data , channel_data_ ) ) <NEWLINE> <DEDENT> if timestamps : <NEWLINE> <INDENT> yield timestamps , channel_data_ <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ii += 1 <NEWLINE> <DEDENT> <DEDENT> except IndexError : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'r'",
                "\"timestamps MUST be a list!\""
            ],
            "<COMMENT>": [
                "# block_size could not be filled from current file, so advance to next file",
                "# list concatenation"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a8f9fb6dca944fd8b5dc2faf0049f82f": {
        "code_string": "super().__setitem__(key, (len(key), value))\n",
        "code_toks_joined": "super ( ) . __setitem__ ( key , ( len ( key ) , value ) ) <NEWLINE>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "cad64aa7e26145ef967aacaf9b09c4a2": {
        "code_string": "argString = str( self.__convertDictItemsToStr( args ) )\n \t\targStringb64 = b64encode( bytes( argString, \"utf-8\" ) ).decode( \"utf-8\" )\n \t\tsignature = hmac.new(\n \t\t\t\tbytes( self.__private, 'utf-8' ),\n \t\t\t\tbytes( argString, 'utf-8' ),\n \t\t\t\tsha384 )\n \t\theaderPayload = {\n \t\t\t'X-GEMINI-APIKEY': self.__public,\n \t\t\t'X-GEMINI-PAYLOAD': argStringb64,\n \t\t\t'X-GEMINI-SIGNATURE': signature.hexdigest()\n \t\t}\n",
        "code_toks_joined": "argString = str ( self . __convertDictItemsToStr ( args ) ) <NEWLINE> <INDENT> argStringb64 = b64encode ( bytes ( argString , <STRING> ) ) . decode ( <STRING> ) <NEWLINE> signature = hmac . new ( <NEWLINE> <INDENT> bytes ( self . __private , <STRING> ) , <NEWLINE> bytes ( argString , <STRING> ) , <NEWLINE> sha384 ) <NEWLINE> <DEDENT> headerPayload = { <NEWLINE> <INDENT> <STRING> : self . __public , <NEWLINE> <STRING> : argStringb64 , <NEWLINE> <STRING> : signature . hexdigest ( ) <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"utf-8\"",
                "\"utf-8\"",
                "'utf-8'",
                "'utf-8'",
                "'X-GEMINI-APIKEY'",
                "'X-GEMINI-PAYLOAD'",
                "'X-GEMINI-SIGNATURE'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "0c4b16292eee4498886c0ef7f2de48b5": {
        "code_string": "def _notify_thread_subscribers(self, thread, notification_text, comment=None):\n         forum = thread.getForum()\n         di = self._thread_info(thread)\n         if comment is not None:\n             di['commenturl'] = comment.absolute_url()\n             di['commenttext'] = safe_unicode(comment.getText())\n         subscriptions = getUtility(ISubscriptions)\n         subscribers = set(subscriptions.subscribers_for(thread)) | set(subscriptions.subscribers_for(forum))\n         mdtool = getToolByName(comment, 'portal_memberdata')\n         keys = mdtool.propertyIds()\n         for mdata in subscribers:\n             if (comment is not None) and (mdata.getId() == comment.Creator()):\n                 continue\n             di.update([(k, str(mdata.getProperty(k)).decode(self._encoding())) for k in keys])\n             di['salutation'] = self._salutation_for_member(di)\n             self._notify(di, notification_text % di)\n             log.info('notified subscriber {subscriber}'.format(subscriber=di.get('email')))\n",
        "code_toks_joined": "def _notify_thread_subscribers ( self , thread , notification_text , comment = None ) : <NEWLINE> <INDENT> forum = thread . getForum ( ) <NEWLINE> di = self . _thread_info ( thread ) <NEWLINE> if comment is not None : <NEWLINE> <INDENT> di [ <STRING> ] = comment . absolute_url ( ) <NEWLINE> di [ <STRING> ] = safe_unicode ( comment . getText ( ) ) <NEWLINE> <DEDENT> subscriptions = getUtility ( ISubscriptions ) <NEWLINE> subscribers = set ( subscriptions . subscribers_for ( thread ) ) | set ( subscriptions . subscribers_for ( forum ) ) <NEWLINE> mdtool = getToolByName ( comment , <STRING> ) <NEWLINE> keys = mdtool . propertyIds ( ) <NEWLINE> for mdata in subscribers : <NEWLINE> <INDENT> if ( comment is not None ) and ( mdata . getId ( ) == comment . Creator ( ) ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> di . update ( [ ( k , str ( mdata . getProperty ( k ) ) . decode ( self . _encoding ( ) ) ) for k in keys ] ) <NEWLINE> di [ <STRING> ] = self . _salutation_for_member ( di ) <NEWLINE> self . _notify ( di , notification_text % di ) <NEWLINE> log . info ( <STRING> . format ( subscriber = di . get ( <STRING> ) ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'commenturl'",
                "'commenttext'",
                "'portal_memberdata'",
                "'salutation'",
                "'notified subscriber {subscriber}'",
                "'email'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "8823766f993449b49b8fb794b1a9d1a7": {
        "code_string": "def validate_request(schema):\n     def decorator(view_func):\n         @wraps(view_func)\n         def _wrapped_view(view, *args, **kwargs):\n             request = view.request\n             context = DrfUtils.get_request_parameters(request)\n             context = SchemaValidator.validate(context, schema)\n             kwargs['context'] = context\n             return view_func(request, *args, **kwargs)\n",
        "code_toks_joined": "def validate_request ( schema ) : <NEWLINE> <INDENT> def decorator ( view_func ) : <NEWLINE> <INDENT> @ wraps ( view_func ) <NEWLINE> def _wrapped_view ( view , * args , ** kwargs ) : <NEWLINE> <INDENT> request = view . request <NEWLINE> context = DrfUtils . get_request_parameters ( request ) <NEWLINE> context = SchemaValidator . validate ( context , schema ) <NEWLINE> kwargs [ <STRING> ] = context <NEWLINE> return view_func ( request , * args , ** kwargs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'context'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "14011ca4b07149aaa54ed9f7679b60d1": {
        "code_string": "def __init__(self, code, db=os.getenv(\"HOME\") + '/.inmetdb.hdf',\n                  local=False):\n         if code in inmet.sites.index.values:\n             self.code = code\n             self.cod_OMM = inmet.sites.loc[code].cod_OMM\n             self.inicio_operacao = inmet.sites.loc[code].inicio_operacao\n             self.lat = inmet.sites.loc[code].lat\n             self.lon = inmet.sites.loc[code].lon\n             self.alt = inmet.sites.loc[code].alt\n         self.dados = get_from_ldb(code, db, local)\n",
        "code_toks_joined": "def __init__ ( self , code , db = os . getenv ( <STRING> ) + <STRING> , <NEWLINE> <INDENT> local = False ) : <NEWLINE> if code in inmet . sites . index . values : <NEWLINE> self . code = code <NEWLINE> self . cod_OMM = inmet . sites . loc [ code ] . cod_OMM <NEWLINE> self . inicio_operacao = inmet . sites . loc [ code ] . inicio_operacao <NEWLINE> self . lat = inmet . sites . loc [ code ] . lat <NEWLINE> self . lon = inmet . sites . loc [ code ] . lon <NEWLINE> self . alt = inmet . sites . loc [ code ] . alt <NEWLINE> self . dados = get_from_ldb ( code , db , local ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"HOME\"",
                "'/.inmetdb.hdf'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "3edb460c81f143d2a41a0261a748baf0": {
        "code_string": "def trajectories(ret,trajs,**kw):\n     getkw=mk_getkw(trajdefaults, kw);\n     x,y = getkw(\"coords\");\n     if not test(kw, \"no_resize\"):\n         xlim, ylim = ret['axes'].get_xlim(), ret['axes'].get_ylim();\n     if not test(kw,\"color_quantity\"):\n         plotit = lambda itr: ret['axes'].plot(\n             itr[x], itr[y],\n             lw=0.1,\n             c=getkw('color'),alpha=getkw('alpha'));\n         pass;\n     else:\n         cf = getkw('color_quantity');\n         if type(cf) == str:\n             cf = lambda itr: itr[cf];\n         plotit = lambda itr: ret['axes'].scatter(\n             itr[x], itr[y],\n             c=cf(itr),\n             lw=getkw('lw'),\n             s=getkw('size'),\n             cmap=getkw('cmap'));\n     for itr in np.rollaxis(trajs,1):\n         plotit(itr);\n     if not test(kw, \"no_resize\"):\n         ret['axes'].set_xlim(xlim);\n         ret['axes'].set_ylim(ylim);\n",
        "code_toks_joined": "def trajectories ( ret , trajs , ** kw ) : <NEWLINE> <INDENT> getkw = mk_getkw ( trajdefaults , kw ) ; <NEWLINE> x , y = getkw ( <STRING> ) ; <NEWLINE> if not test ( kw , <STRING> ) : <NEWLINE> <INDENT> xlim , ylim = ret [ <STRING> ] . get_xlim ( ) , ret [ <STRING> ] . get_ylim ( ) ; <NEWLINE> <DEDENT> if not test ( kw , <STRING> ) : <NEWLINE> <INDENT> plotit = lambda itr : ret [ <STRING> ] . plot ( <NEWLINE> <INDENT> itr [ x ] , itr [ y ] , <NEWLINE> lw = 0.1 , <NEWLINE> c = getkw ( <STRING> ) , alpha = getkw ( <STRING> ) ) ; <NEWLINE> <DEDENT> pass ; <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cf = getkw ( <STRING> ) ; <NEWLINE> if type ( cf ) == str : <NEWLINE> <INDENT> cf = lambda itr : itr [ cf ] ; <NEWLINE> <DEDENT> plotit = lambda itr : ret [ <STRING> ] . scatter ( <NEWLINE> <INDENT> itr [ x ] , itr [ y ] , <NEWLINE> c = cf ( itr ) , <NEWLINE> lw = getkw ( <STRING> ) , <NEWLINE> s = getkw ( <STRING> ) , <NEWLINE> cmap = getkw ( <STRING> ) ) ; <NEWLINE> <DEDENT> <DEDENT> for itr in np . rollaxis ( trajs , 1 ) : <NEWLINE> <INDENT> plotit ( itr ) ; <NEWLINE> <DEDENT> if not test ( kw , <STRING> ) : <NEWLINE> <INDENT> ret [ <STRING> ] . set_xlim ( xlim ) ; <NEWLINE> ret [ <STRING> ] . set_ylim ( ylim ) ; <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"coords\"",
                "\"no_resize\"",
                "'axes'",
                "'axes'",
                "\"color_quantity\"",
                "'axes'",
                "'color'",
                "'alpha'",
                "'color_quantity'",
                "'axes'",
                "'lw'",
                "'size'",
                "'cmap'",
                "\"no_resize\"",
                "'axes'",
                "'axes'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "82491a923729485e8fe5895c3acc1342": {
        "code_string": "class xnd(_xnd):\n     def __new__(cls, value=None, type=None, levels=None):\n         if type is None:\n             if levels is not None:\n                 args = ', '.join(\"'%s'\" % l if l is not None else 'NA' for l in levels)\n                 t = \"%d * categorical(%s)\" % (len(value), args)\n                 type = ndt(t)\n             else:\n                 type = typeof(value)\n         else:\n             if levels is not None:\n                 raise TypeError(\n                     \"the 'type' and 'levels' arguments are mutually exclusive\")\n             elif isinstance(type, str):\n                 type = ndt(type)\n         return _xnd(value, type)\n",
        "code_toks_joined": "class xnd ( _xnd ) : <NEWLINE> <INDENT> def __new__ ( cls , value = None , type = None , levels = None ) : <NEWLINE> <INDENT> if type is None : <NEWLINE> <INDENT> if levels is not None : <NEWLINE> <INDENT> args = <STRING> . join ( <STRING> % l if l is not None else <STRING> for l in levels ) <NEWLINE> t = <STRING> % ( len ( value ) , args ) <NEWLINE> type = ndt ( t ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> type = typeof ( value ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if levels is not None : <NEWLINE> <INDENT> raise TypeError ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif isinstance ( type , str ) : <NEWLINE> <INDENT> type = ndt ( type ) <NEWLINE> <DEDENT> <DEDENT> return _xnd ( value , type ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "', '",
                "\"'%s'\"",
                "'NA'",
                "\"%d * categorical(%s)\"",
                "\"the 'type' and 'levels' arguments are mutually exclusive\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "db6bef679476451ab6b20c81b7bf4213": {
        "code_string": "if not pack_transfer.is_dest_location_to_confirm(move, scanned_location):\n             if confirmation:\n                 # keep the move in sync otherwise we would have a move line outside\n                 # the dest location of the move\n                 move.location_dest_id = scanned_location.id\n             else:\n                 return self._response_for_location_need_confirm()\n",
        "code_toks_joined": "if not pack_transfer . is_dest_location_to_confirm ( move , scanned_location ) : <NEWLINE> <INDENT> if confirmation : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> move . location_dest_id = scanned_location . id <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . _response_for_location_need_confirm ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# keep the move in sync otherwise we would have a move line outside",
                "# the dest location of the move"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "e26073cedc3e4872b409939a75dfdfc7": {
        "code_string": "def is_dest_location_to_confirm(self, move, scanned_location):\n         \"\"\"Destination that could be used but need confirmation\"\"\"\n         move_dest_location = move.move_line_ids[0].location_dest_id\n         zone_locations = self.env[\"stock.location\"].search(\n             [(\"id\", \"child_of\", move_dest_location.id)]\n         )\n         return scanned_location in zone_locations\n",
        "code_toks_joined": "def is_dest_location_to_confirm ( self , move , scanned_location ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> move_dest_location = move . move_line_ids [ 0 ] . location_dest_id <NEWLINE> zone_locations = self . env [ <STRING> ] . search ( <NEWLINE> <INDENT> [ ( <STRING> , <STRING> , move_dest_location . id ) ] <NEWLINE> <DEDENT> ) <NEWLINE> return scanned_location in zone_locations <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"\"\"Destination that could be used but need confirmation\"\"\"",
                "\"stock.location\"",
                "\"id\"",
                "\"child_of\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "eca97c1280174b339685737b102a270c": {
        "code_string": "if not pack_transfer.is_dest_location_to_confirm(move, scanned_location):\n             if confirmation:\n                 # keep the move in sync otherwise we would have a move line outside\n                 # the dest location of the move\n                 move.location_dest_id = scanned_location.id\n             else:\n                 return self._response_for_location_need_confirm()\n",
        "code_toks_joined": "if not pack_transfer . is_dest_location_to_confirm ( move , scanned_location ) : <NEWLINE> <INDENT> if confirmation : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> move . location_dest_id = scanned_location . id <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . _response_for_location_need_confirm ( ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<COMMENT>": [
                "# keep the move in sync otherwise we would have a move line outside",
                "# the dest location of the move"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "a9f1385cbb6c461793e02c3003ee2abe": {
        "code_string": "class SelectDestPackageMixin:\n     def _assert_response_select_dest_package(\n         self, response, picking, selected_lines, packages, message=None\n     ):\n         self.assert_response(\n             response,\n             next_state=\"select_dest_package\",\n             data={\n                 \"picking\": {\n                     \"id\": picking.id,\n                     \"name\": picking.name,\n                     \"note\": \"\",\n                     \"origin\": \"\",\n                     \"line_count\": len(picking.move_line_ids),\n                     \"partner\": {\"id\": self.customer.id, \"name\": self.customer.name},\n                 },\n                 \"packages\": [\n                     self._package_data(picking, package) for package in packages\n                 ],\n                 \"selected_move_lines\": [\n                     self._move_line_data(ml) for ml in selected_lines.sorted()\n                 ],\n             },\n             message=message,\n         )\n",
        "code_toks_joined": "class SelectDestPackageMixin : <NEWLINE> <INDENT> def _assert_response_select_dest_package ( <NEWLINE> <INDENT> self , response , picking , selected_lines , packages , message = None <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> self . assert_response ( <NEWLINE> <INDENT> response , <NEWLINE> next_state = <STRING> , <NEWLINE> data = { <NEWLINE> <INDENT> <STRING> : { <NEWLINE> <INDENT> <STRING> : picking . id , <NEWLINE> <STRING> : picking . name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : len ( picking . move_line_ids ) , <NEWLINE> <STRING> : { <STRING> : self . customer . id , <STRING> : self . customer . name } , <NEWLINE> <DEDENT> } , <NEWLINE> <STRING> : [ <NEWLINE> <INDENT> self . _package_data ( picking , package ) for package in packages <NEWLINE> <DEDENT> ] , <NEWLINE> <STRING> : [ <NEWLINE> <INDENT> self . _move_line_data ( ml ) for ml in selected_lines . sorted ( ) <NEWLINE> <DEDENT> ] , <NEWLINE> <DEDENT> } , <NEWLINE> message = message , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"select_dest_package\"",
                "\"picking\"",
                "\"id\"",
                "\"name\"",
                "\"note\"",
                "\"\"",
                "\"origin\"",
                "\"\"",
                "\"line_count\"",
                "\"partner\"",
                "\"id\"",
                "\"name\"",
                "\"packages\"",
                "\"selected_move_lines\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "511519f3065d41b1a8ff2a5f8e340eb0": {
        "code_string": "new_moves = self.browse(chain.from_iterable(new_move_per_location.values()))\n         return self + new_moves\n",
        "code_toks_joined": "new_moves = self . browse ( chain . from_iterable ( new_move_per_location . values ( ) ) ) <NEWLINE> <INDENT> return self + new_moves <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "6df8979acb2544df95217e7abfacfa10": {
        "code_string": "for attr_name in loss_like_attr_names:\n             tag_name = \"{base_name}/{attr_name}\".format(\n                 base_name=self.base_name, attr_name=attr_name\n             )\n             attr = getattr(dc_value, attr_name)\n             if isinstance(attr, Tensor):\n                 value = attr.item()\n             else:\n                 value = attr\n             self.writer.add_scalar(tag_name, scalar_value=value, global_step=step)\n             self.values[attr_name].append(dc_value)\n",
        "code_toks_joined": "for attr_name in loss_like_attr_names : <NEWLINE> <INDENT> tag_name = <STRING> . format ( <NEWLINE> <INDENT> base_name = self . base_name , attr_name = attr_name <NEWLINE> <DEDENT> ) <NEWLINE> attr = getattr ( dc_value , attr_name ) <NEWLINE> if isinstance ( attr , Tensor ) : <NEWLINE> <INDENT> value = attr . item ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> value = attr <NEWLINE> <DEDENT> self . writer . add_scalar ( tag_name , scalar_value = value , global_step = step ) <NEWLINE> self . values [ attr_name ] . append ( dc_value ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"{base_name}/{attr_name}\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "4621577aa225479596ddee75d957345a": {
        "code_string": "for config_data_point in self._config[\"datapoints\"]:\n             dp = DataPoint(config_data_point, _config_methods, self._logger, self._mqtt_client,\n                            self._no_data_behavior)\n             self._purges.append(dp.purge_old_values)\n             for method in dp.methods:\n                 process = method.process\n                 cost = method.execution_points_estimation()\n                 self._logger.info(\"DataPointManager - adding process '{}' with cost '{}'.\".\n                                   format(process.__name__, cost))\n                 self._processes.append((process, cost))\n             self._data_points.append(dp)\n",
        "code_toks_joined": "for config_data_point in self . _config [ <STRING> ] : <NEWLINE> <INDENT> dp = DataPoint ( config_data_point , _config_methods , self . _logger , self . _mqtt_client , <NEWLINE> <INDENT> self . _no_data_behavior ) <NEWLINE> <DEDENT> self . _purges . append ( dp . purge_old_values ) <NEWLINE> for method in dp . methods : <NEWLINE> <INDENT> process = method . process <NEWLINE> cost = method . execution_points_estimation ( ) <NEWLINE> self . _logger . info ( <STRING> . <NEWLINE> <INDENT> format ( process . __name__ , cost ) ) <NEWLINE> <DEDENT> self . _processes . append ( ( process , cost ) ) <NEWLINE> <DEDENT> self . _data_points . append ( dp ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\"datapoints\"",
                "\"DataPointManager - adding process '{}' with cost '{}'.\""
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "ae609a2f49b546babe87fbc3f48d308e": {
        "code_string": "for domain, data in domains.items():\n             service_name = domain.get('http')\n             if not service_name:\n                 continue\n             strowger.set_http_route(domain, service_name)\n",
        "code_toks_joined": "for domain , data in domains . items ( ) : <NEWLINE> <INDENT> service_name = domain . get ( <STRING> ) <NEWLINE> if not service_name : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> strowger . set_http_route ( domain , service_name ) <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'http'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f863ba62a30c4d5084af1be0d09e77b3": {
        "code_string": "def connectMongo():\n     client=pymongo.MongoClient(aa.mongouri)\n     shouts=client.aaserver.shouts.find({})\n     shouts_=[shout for shout in shouts]\n     return shouts\n",
        "code_toks_joined": "def connectMongo ( ) : <NEWLINE> <INDENT> client = pymongo . MongoClient ( aa . mongouri ) <NEWLINE> shouts = client . aaserver . shouts . find ( { } ) <NEWLINE> shouts_ = [ shout for shout in shouts ] <NEWLINE> return shouts <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "f49fb9d3480d44e89352cfdea557e7c8": {
        "code_string": "def output(self, out_prefix):\n         if (not out_prefix):\n             tax_filepath = default_taxonomy_file\n             tag_filepath = default_tagging_file\n             exp_filepath = default_expansion_file\n         else:\n             tax_filepath = out_prefix + \".taxonomy\"\n             tag_filepath = out_prefix + \".tagging\"\n             exp_filepath = out_prefix + \".expansion\"\n         taxonomy.to_file(tax_filepath)\n         log.info('[-] Output %d taxonomy tags to %s' % (\n                         len(taxonomy), tax_filepath))\n         tagging.expand_all_destinations()\n         tagging.to_file(tag_filepath)\n         log.info('[-] Output %d tagging rules to %s' % (\n                         len(tagging), tax_filepath))\n         expansion.to_file(exp_filepath)\n         log.info('[-] Output %d expansion rules to %s' % (\n                         len(expansion), exp_filepath))\n",
        "code_toks_joined": "def output ( self , out_prefix ) : <NEWLINE> <INDENT> if ( not out_prefix ) : <NEWLINE> <INDENT> tax_filepath = default_taxonomy_file <NEWLINE> tag_filepath = default_tagging_file <NEWLINE> exp_filepath = default_expansion_file <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> tax_filepath = out_prefix + <STRING> <NEWLINE> tag_filepath = out_prefix + <STRING> <NEWLINE> exp_filepath = out_prefix + <STRING> <NEWLINE> <DEDENT> taxonomy . to_file ( tax_filepath ) <NEWLINE> log . info ( <STRING> % ( <NEWLINE> <INDENT> len ( taxonomy ) , tax_filepath ) ) <NEWLINE> <DEDENT> tagging . expand_all_destinations ( ) <NEWLINE> tagging . to_file ( tag_filepath ) <NEWLINE> log . info ( <STRING> % ( <NEWLINE> <INDENT> len ( tagging ) , tax_filepath ) ) <NEWLINE> <DEDENT> expansion . to_file ( exp_filepath ) <NEWLINE> log . info ( <STRING> % ( <NEWLINE> <INDENT> len ( expansion ) , exp_filepath ) ) <NEWLINE> <DEDENT> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "\".taxonomy\"",
                "\".tagging\"",
                "\".expansion\"",
                "'[-] Output %d taxonomy tags to %s'",
                "'[-] Output %d tagging rules to %s'",
                "'[-] Output %d expansion rules to %s'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "adf2a07e6f3c4d3e9faf8501ca50a474": {
        "code_string": "evolution = evolve.run_evolution(vertices, terminals, 5, n=16, n1=2, n2=8)\n     genes = evolve.get_best_individual(evolution)\n     tst = evolve.get_best_terminal_steiner_tree(vertices, terminals, genes)\n     w1 = tst['weight'].sum()\n     w2 = evolution.at[len(evolution)-1,'weight']\n",
        "code_toks_joined": "evolution = evolve . run_evolution ( vertices , terminals , 5 , n = 16 , n1 = 2 , n2 = 8 ) <NEWLINE> <INDENT> genes = evolve . get_best_individual ( evolution ) <NEWLINE> tst = evolve . get_best_terminal_steiner_tree ( vertices , terminals , genes ) <NEWLINE> w1 = tst [ <STRING> ] . sum ( ) <NEWLINE> w2 = evolution . at [ len ( evolution ) - 1 , <STRING> ] <NEWLINE> <DEDENT>",
        "anonymize_dict": {
            "<STRING>": [
                "'weight'",
                "'weight'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "181245349d144e9188fc352e4f915ebf": {
        "code_string": "rogues = reflections.select(reflections['id'] == 1701)\n",
        "code_toks_joined": "rogues = reflections . select ( reflections [ <STRING> ] == 1701 ) <NEWLINE>",
        "anonymize_dict": {
            "<STRING>": [
                "'id'"
            ]
        },
        "err_obj": {
            "msg": "invalid syntax"
        }
    },
    "c216934c1b344b9fbaee03c508b2f731": {
        "code_string": "if ntr:\n             integrater.integrater_reset_reindex_operator()\n             need_to_return = True\n",
        "code_toks_joined": "if ntr : <NEWLINE> <INDENT> integrater . integrater_reset_reindex_operator ( ) <NEWLINE> need_to_return = True <NEWLINE> <DEDENT>",
        "anonymize_dict": {},
        "err_obj": {
            "msg": "invalid syntax"
        }
    }
}