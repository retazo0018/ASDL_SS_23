def visualize_attention ( self , img_data , attentions , output , label , flag_incorrect ) : <NEWLINE> <INDENT> if flag_incorrect : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> filename = label . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) <NEWLINE> if len ( filename ) == 0 : <NEWLINE> <INDENT> filename = <STRING> <NEWLINE> <DEDENT> output_dir = os . path . join ( output_dir , filename ) <NEWLINE> if not os . path . exists ( output_dir ) : <NEWLINE> <INDENT> os . makedirs ( output_dir ) <NEWLINE> <DEDENT> with open ( os . path . join ( output_dir , <STRING> ) , <STRING> ) as fword : <NEWLINE> <INDENT> fword . write ( output + <STRING> ) <NEWLINE> fword . write ( label ) <NEWLINE> file_img_data = BytesIO ( img_data ) <NEWLINE> img = Image . open ( file_img_data ) <NEWLINE> w , h = img . size <NEWLINE> mh = 32 <NEWLINE> mw = math . floor ( 1. * w / h * mh ) <NEWLINE> img = img . resize ( <NEWLINE> <INDENT> ( mw , mh ) , <NEWLINE> Image . ANTIALIAS ) <NEWLINE> <DEDENT> img_data = np . asarray ( img , dtype = np . uint8 ) <NEWLINE> for idx in xrange ( len ( output ) ) : <NEWLINE> <INDENT> output_filename = os . path . join ( output_dir , <STRING> % ( idx ) ) <NEWLINE> attention = attentions [ idx ] [ : ( int ( mw / 4 ) - 1 ) ] <NEWLINE> attention_orig = np . zeros ( mw ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> if i / 4 - 1 > 0 and i / 4 - 1 < len ( attention ) : <NEWLINE> <INDENT> attention_orig [ i ] = attention [ int ( i / 4 ) - 1 ] <NEWLINE> <DEDENT> <DEDENT> attention_orig = np . convolve ( attention_orig , [ 0.199547 , 0.200226 , 0.200454 , 0.200226 , 0.199547 ] , mode = <STRING> ) <NEWLINE> attention_orig = np . maximum ( attention_orig , 0.3 ) <NEWLINE> attention_out = np . zeros ( ( h , mw ) ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> attention_out [ : , i ] = attention_orig [ i ] <NEWLINE> <DEDENT> if len ( img_data . shape ) == 3 : <NEWLINE> <INDENT> attention_out = attention_out [ : , : , np . newaxis ] <NEWLINE> <DEDENT> img_out_data = img_data * attention_out <NEWLINE> img_out = Image . fromarray ( img_out_data . astype ( np . uint8 ) ) <NEWLINE> img_out . save ( output_filename ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def visualize_attention ( self , img_data , attentions , output , label , flag_incorrect ) : <NEWLINE> <INDENT> if flag_incorrect : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output_dir = os . path . join ( self . output_dir , <STRING> ) <NEWLINE> <DEDENT> filename = label . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) <NEWLINE> if len ( filename ) == 0 : <NEWLINE> <INDENT> filename = <STRING> <NEWLINE> <DEDENT> output_dir = os . path . join ( output_dir , filename ) <NEWLINE> if not os . path . exists ( output_dir ) : <NEWLINE> <INDENT> os . makedirs ( output_dir ) <NEWLINE> <DEDENT> with open ( os . path . join ( output_dir , <STRING> ) , <STRING> ) as fword : <NEWLINE> <INDENT> fword . write ( output + <STRING> ) <NEWLINE> fword . write ( label ) <NEWLINE> file_img_data = BytesIO ( img_data ) <NEWLINE> img = Image . open ( file_img_data ) <NEWLINE> w , h = img . size <NEWLINE> mh = 32 <NEWLINE> mw = math . floor ( 1. * w / h * mh ) <NEWLINE> img = img . resize ( <NEWLINE> <INDENT> ( mw , mh ) , <NEWLINE> Image . ANTIALIAS ) <NEWLINE> <DEDENT> img_data = np . asarray ( img , dtype = np . uint8 ) <NEWLINE> for idx in xrange ( len ( output ) ) : <NEWLINE> <INDENT> output_filename = os . path . join ( output_dir , <STRING> % ( idx ) ) <NEWLINE> attention = attentions [ idx ] [ : ( int ( mw / 4 ) - 1 ) ] <NEWLINE> attention_orig = np . zeros ( mw ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> if i / 4 - 1 > 0 and i / 4 - 1 < len ( attention ) : <NEWLINE> <INDENT> attention_orig [ i ] = attention [ int ( i / 4 ) - 1 ] <NEWLINE> <DEDENT> <DEDENT> attention_orig = np . convolve ( attention_orig , [ 0.199547 , 0.200226 , 0.200454 , 0.200226 , 0.199547 ] , mode = <STRING> ) <NEWLINE> attention_orig = np . maximum ( attention_orig , 0.3 ) <NEWLINE> attention_out = np . zeros ( ( mh , mw ) ) <NEWLINE> for i in xrange ( mw ) : <NEWLINE> <INDENT> attention_out [ : , i ] = attention_orig [ i ] <NEWLINE> <DEDENT> if len ( img_data . shape ) == 3 : <NEWLINE> <INDENT> attention_out = attention_out [ : , : , np . newaxis ] <NEWLINE> <DEDENT> img_out_data = img_data * attention_out <NEWLINE> img_out = Image . fromarray ( img_out_data . astype ( np . uint8 ) ) <NEWLINE> img_out . save ( output_filename ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
self . decoder_inputs = [ ] <NEWLINE> <INDENT> self . target_weights = [ ] <NEWLINE> for i in xrange ( self . decoder_size + 1 ) : <NEWLINE> <INDENT> self . decoder_inputs . append ( <NEWLINE> <INDENT> tf . tile ( [ 1 ] , [ num_images ] ) <NEWLINE> <DEDENT> ) <NEWLINE> if i < self . decoder_size : <NEWLINE> <INDENT> self . target_weights . append ( tf . tile ( [ 1. ] , [ num_images ] ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . target_weights . append ( tf . tile ( [ 0. ] , [ num_images ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
timestruct = datetime ( year , month , day , hour + hourdiff ) . isoformat ( ) + <STRING> <NEWLINE> <INDENT> data [ <STRING> ] [ <STRING> ] = timestruct <NEWLINE> data [ <STRING> ] [ <STRING> ] = hourduration * 3600 <NEWLINE> data [ <STRING> ] [ <STRING> ] = projectid <NEWLINE> data [ <STRING> ] [ <STRING> ] = <STRING> <NEWLINE> data [ <STRING> ] [ <STRING> ] = billable <NEWLINE> <DEDENT>
def extract ( self , sourcepc , neighborhood , targetpc , targetindex , volume ) : <NEWLINE> <INDENT> t2a , t2c = utils . get_features ( targetpc , self . requires ( ) , targetindex ) <NEWLINE> x , y , z = utils . get_point ( targetpc , targetindex ) <NEWLINE> return t2c - t2a - z <COMMENT> <NEWLINE> <DEDENT>
def test_GetPointCloudPointFeatures ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> pc = test_tools . generate_test_point_cloud ( ) <NEWLINE> cols = 0.5 * ( pc [ keys . point ] [ <STRING> ] [ <STRING> ] + pc [ keys . point ] [ <STRING> ] [ <STRING> ] ) <NEWLINE> flavs = 0.5 * ( pc [ keys . point ] [ <STRING> ] [ <STRING> ] - pc [ keys . point ] [ <STRING> ] [ <STRING> ] ) <NEWLINE> pc [ keys . point ] [ <STRING> ] = { <STRING> : <STRING> , <STRING> : cols } <NEWLINE> pc [ keys . point ] [ <STRING> ] = { <STRING> : <STRING> , <STRING> : flavs } <NEWLINE> x , y , z = utils . get_point ( pc , 2 ) <NEWLINE> c , f = utils . get_features ( pc , ( <STRING> , <STRING> ) , 2 ) <NEWLINE> self . assertEqual ( c , 0.5 * ( x + y ) ) <NEWLINE> self . assertEqual ( f , 0.5 * ( x - y ) ) <NEWLINE> <DEDENT>
self . assertEqual ( <STRING> , <NEWLINE> <INDENT> target_point_cloud [ keys . provenance ] [ 1 ] [ <STRING> ] ) <NEWLINE> <DEDENT>
self . assertEqual ( <STRING> , <NEWLINE> <INDENT> target_point_cloud [ keys . provenance ] [ - 1 ] [ <STRING> ] ) <NEWLINE> <DEDENT>
if end == sys . maxsize : <NEWLINE> <INDENT> return self . _data [ self . _lb + start : self . _ub ] <NEWLINE> elif self . _lb + end > self . _ub : <NEWLINE> <INDENT> raise IndexError ( ) <NEWLINE> else : <NEWLINE> <DEDENT> return self . _data [ self . _lb + start : self . _lb + end ] <NEWLINE> <DEDENT>
if ( Energy_needed_from_battery > 0 ) : <NEWLINE> <COMMENT> <NL> <INDENT> if ( self . _lastPonctualObservation [ 0 ] * self . battery_size > Energy_needed_from_battery ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . _lastPonctualObservation [ 0 ] = self . _lastPonctualObservation [ 0 ] - Energy_needed_from_battery / self . battery_size / self . battery_eta <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> reward -= ( Energy_needed_from_battery - self . _lastPonctualObservation [ 0 ] * self . battery_size ) * 2 <COMMENT> <NEWLINE> self . _lastPonctualObservation [ 0 ] = 0 <NEWLINE> elif ( Energy_needed_from_battery < 0 ) : <NEWLINE> <COMMENT> <NL> <DEDENT> self . _lastPonctualObservation [ 0 ] = min ( 1. , self . _lastPonctualObservation [ 0 ] - Energy_needed_from_battery / self . battery_size * self . battery_eta ) <NEWLINE> <DEDENT>
@ viz_reg_test <NEWLINE> <INDENT> def test_boxplot_melted ( ) : <NEWLINE> <INDENT> return ar . boxplot ( data . iris ( ) , <STRING> , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def readrequest ( ) : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> idx = buf . find ( <STRING> , pos [ 0 ] ) <NEWLINE> if idx >= 0 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> readmore ( ) <NEWLINE> <DEDENT> head = buf [ pos [ 0 ] : idx ] <NEWLINE> pos [ 0 ] = idx + 4 <NEWLINE> lines = iter ( head . decode ( <STRING> ) . split ( <STRING> ) ) <NEWLINE> status = next ( lines ) <NEWLINE> headers = { } <NEWLINE> last_header = None <NEWLINE> for line in lines : <NEWLINE> <INDENT> if line . startswith ( ( <STRING> , <STRING> ) ) : <NEWLINE> <INDENT> if last_header is not None : <NEWLINE> <INDENT> headers [ last_header ] += line <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise EOFError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif <STRING> in line : <NEWLINE> <INDENT> k , v = line . split ( <STRING> , 1 ) <NEWLINE> k = k . strip ( ) <NEWLINE> if k in headers : <NEWLINE> <INDENT> headers [ k ] += <STRING> + v . strip ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> headers [ k ] = v . strip ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise EOFError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> clen = int ( headers . get ( <STRING> , <STRING> ) ) <NEWLINE> if clen < 0 : <NEWLINE> <INDENT> raise EOFError ( <STRING> ) <NEWLINE> <DEDENT> while pos [ 0 ] + clen > len ( buf ) : <NEWLINE> <INDENT> readmore ( ) <NEWLINE> <DEDENT> return status , headers , buf [ pos [ 0 ] : pos [ 0 ] + clen ] <NEWLINE> <DEDENT>
with pytest . raises ( ValueError ) as errinfo : <NEWLINE> <INDENT> bb . io . save ( df_anno_simple , parser , <STRING> ) <NEWLINE> assert <STRING> in str ( errinfo . value ) <NEWLINE> <DEDENT>
def hex_ranges ( h3_address_list , ring_size ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> num_hexagons = len ( h3_address_list ) <NEWLINE> array_len = num_hexagons * libh3 . maxKringSize ( ring_size ) <NEWLINE> HexArray = c_long * num_hexagons <NEWLINE> KringArray = c_long * array_len <NEWLINE> <COMMENT> <NL> hex_array = HexArray ( <NEWLINE> <INDENT> * [ string_to_h3 ( h3_address ) for h3_address in h3_address_list ] ) <NEWLINE> <DEDENT> krings = KringArray ( ) <NEWLINE> success = libh3 . hexRanges ( <NEWLINE> <INDENT> hex_array , <NEWLINE> num_hexagons , <NEWLINE> ring_size , <NEWLINE> krings , <NEWLINE> <DEDENT> ) <NEWLINE> if success != 0 : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> out = { } <NEWLINE> for i in range ( 0 , num_hexagons ) : <NEWLINE> <INDENT> h3_address = h3_address_list [ i ] <NEWLINE> hex_range_list = [ ] <NEWLINE> out [ h3_address ] = hex_range_list <NEWLINE> for j in range ( 0 , ring_size + 1 ) : <NEWLINE> <INDENT> hex_range_list . append ( set ( [ ] ) ) <NEWLINE> <DEDENT> for j in range ( 0 , int ( array_len / num_hexagons ) ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> ring_index = int ( <NEWLINE> <INDENT> math . floor ( <NEWLINE> <INDENT> ( 1 + math . sqrt ( 1 + 8 * math . ceil ( j / 6.0 ) ) ) / 2 ) ) - 1 <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> hex_range_list [ ring_index ] . add ( <NEWLINE> <INDENT> h3_to_string ( krings [ i * array_len + j ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return out <NEWLINE> <DEDENT>
with closing ( os . fdopen ( os . open ( log_technical_terms_to_path , <NEWLINE> <INDENT> os . O_RDWR | os . O_CREAT ) , <NEWLINE> <STRING> ) ) as terms_file : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> terms = set ( terms_file . read ( ) . splitlines ( ) ) <COMMENT> <NEWLINE> terms_file . seek ( 0 ) <COMMENT> <NEWLINE> terms_file . truncate ( 0 ) <COMMENT> <NEWLINE> tech_terms = freduce ( lambda x , y : x | y , <NEWLINE> _drain ( log_technical_terms_to_queue ) ) <NEWLINE> terms_file . write ( <STRING> . join ( list ( terms | <COMMENT> <NEWLINE> <INDENT> set ( tech_terms ) ) ) ) <NEWLINE> <DEDENT> <DEDENT>
def intersect ( obj1 , obj2 ) : <NEWLINE> <INDENT> if not ( isinstance ( obj1 , Vector ) and isinstance ( obj2 , Vector ) ) : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> obj1 . reproject ( obj2 . srs ) <NEWLINE> <DEDENT>
if pattern : <NEWLINE> <INDENT> tests = [ f for f in docfiles if f . find ( pattern ) == 0 ] <NEWLINE> else : <NEWLINE> tests = docfiles <NEWLINE> <DEDENT>
def halton ( base ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def value ( index ) : <NEWLINE> <INDENT> result = 0.0 <NEWLINE> f = 1.0 / base <NEWLINE> i = index <NEWLINE> while i > 0 : <NEWLINE> <INDENT> result += f * ( i % base ) <NEWLINE> i = i // base <NEWLINE> f = f / base <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT> i = 1 <NEWLINE> while i > 0 : <NEWLINE> <INDENT> yield value ( i ) <NEWLINE> i += 1 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> open_kwds = { } <NEWLINE> if sys . version_info >= ( 3 , ) : <NEWLINE> <INDENT> open_kwds [ <STRING> ] = <STRING> <NEWLINE> <DEDENT> <DEDENT>
class TaggedItemManager ( models . Manager ) : <NEWLINE> <INDENT> def get_by_model ( self , Model , tags ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> tags = get_tag_list ( tags ) <NEWLINE> if len ( tags ) == 1 : <NEWLINE> <INDENT> tag = tags [ 0 ] <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . get_intersection_by_model ( Model , tags ) <NEWLINE> <DEDENT> ctype = ContentType . objects . get_for_model ( Model ) <NEWLINE> rel_table = backend . quote_name ( self . model . _meta . db_table ) <NEWLINE> return Model . objects . extra ( <NEWLINE> <INDENT> tables = [ self . model . _meta . db_table ] , <COMMENT> <NEWLINE> where = [ <NEWLINE> <INDENT> <STRING> % rel_table , <NEWLINE> <STRING> % rel_table , <NEWLINE> <STRING> % ( backend . quote_name ( Model . _meta . db_table ) , <NEWLINE> <INDENT> backend . quote_name ( Model . _meta . pk . column ) , <NEWLINE> rel_table ) <NEWLINE> <DEDENT> <DEDENT> ] , <NEWLINE> params = [ ctype . id , tag . id ] , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
class TaggedItemManager ( models . Manager ) : <NEWLINE> <INDENT> def get_by_model ( self , Model , tags ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> tags = get_tag_list ( tags ) <NEWLINE> if len ( tags ) == 1 : <NEWLINE> <INDENT> tag = tags [ 0 ] <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . get_intersection_by_model ( Model , tags ) <NEWLINE> <DEDENT> ctype = ContentType . objects . get_for_model ( Model ) <NEWLINE> rel_table = backend . quote_name ( self . model . _meta . db_table ) <NEWLINE> return Model . objects . extra ( <NEWLINE> <INDENT> tables = [ self . model . _meta . db_table ] , <COMMENT> <NEWLINE> where = [ <NEWLINE> <INDENT> <STRING> % rel_table , <NEWLINE> <STRING> % rel_table , <NEWLINE> <STRING> % ( backend . quote_name ( Model . _meta . db_table ) , <NEWLINE> <INDENT> backend . quote_name ( Model . _meta . pk . column ) , <NEWLINE> rel_table ) <NEWLINE> <DEDENT> <DEDENT> ] , <NEWLINE> params = [ ctype . id , tag . id ] , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
if data is None : <NEWLINE> <INDENT> if exc is not None : <NEWLINE> <INDENT> raise exc ( qs ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def upload ( self ) : <NEWLINE> <INDENT> self . fitting = 0 <NEWLINE> self . save ( ) <NEWLINE> lore . io . upload ( self . model_path ( ) , self . remote_model_path ( ) ) <NEWLINE> <DEDENT>
def upload ( self ) : <NEWLINE> <INDENT> super ( Base , self ) . upload ( ) <NEWLINE> lore . io . upload ( self . weights_path ( ) , self . remote_weights_path ( ) ) <NEWLINE> <DEDENT>
if self . towers > 1 : <NEWLINE> <INDENT> result = numpy . mean ( result , axis = 0 ) . squeeze ( axis = 1 ) <NEWLINE> <DEDENT>
if cluster not in clusters : <NEWLINE> <COMMENT> <NL> <INDENT> clusters . append ( cluster ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if column not in columns : <NEWLINE> columns . append ( column ) <NEWLINE> <DEDENT>
def adjust_cluster ( self , cluster , rowscores , cutoff , limit ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def max_row_in_column ( matrix , column ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sm = matrix . submatrix_by_name ( wh , [ matrix . column_names [ column ] ] ) <NEWLINE> sm_values = sm . values <NEWLINE> max_row = 0 <NEWLINE> max_score = sys . float_info . min <NEWLINE> for row in range ( sm . num_rows ( ) ) : <NEWLINE> <INDENT> if sm_values [ row ] [ 0 ] > max_score : <NEWLINE> <INDENT> max_score = sm_values [ row ] [ 0 ] <NEWLINE> max_row = row <NEWLINE> <DEDENT> <DEDENT> return sm . row_names [ max_row ] <NEWLINE> <DEDENT> <DEDENT>
def setUp ( self ) : <COMMENT> <NEWLINE> <INDENT> <STRING> <NEWLINE> if not os . path . exists ( <STRING> ) : <NEWLINE> <INDENT> os . mkdir ( <STRING> ) <NEWLINE> <DEDENT> self . service = mo . MicrobesOnline ( <STRING> , mo . MICROBES_ONLINE_BASE_URL ) <NEWLINE> <DEDENT>
h , m , s = convert_elapsed_time ( training_time ) <NEWLINE> <INDENT> logger . info ( <STRING> <NEWLINE> <INDENT> . format ( h , m , s ) ) <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> logger . info ( outputs_ ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> logger . info ( targets ) <NEWLINE> <DEDENT>
if point_xy : <NEWLINE> <INDENT> x_point = self . workspace . get_content_point ( x_uuid , point_xy ) <NEWLINE> format_str , unit_str , x_point = self . document . convert_units ( x_uuid , x_point ) <NEWLINE> y_point = self . workspace . get_content_point ( y_uuid , point_xy ) <NEWLINE> format_str , unit_str , y_point = self . document . convert_units ( y_uuid , y_point ) <NEWLINE> else : <NEWLINE> x_point = None <NEWLINE> y_point = None <NEWLINE> <DEDENT>
asset_map [ <STRING> ] = vmuuid <NEWLINE>
rec = params . record_cache [ record . record_uid ] <NEWLINE> <INDENT> data . update ( json . loads ( rec [ <STRING> ] . decode ( ) ) ) <NEWLINE> if <STRING> in rec : <NEWLINE> <INDENT> extra . update ( json . loads ( rec [ <STRING> ] . decode ( ) ) ) <NEWLINE> <DEDENT> if <STRING> in rec : <NEWLINE> <INDENT> udata . update ( rec [ <STRING> ] ) <NEWLINE> <DEDENT> unencrypted_key = rec [ <STRING> ] <NEWLINE> record_object [ <STRING> ] = rec [ <STRING> ] <NEWLINE> if rec . get ( <STRING> ) : <NEWLINE> <INDENT> if params . debug : print ( <STRING> ) <NEWLINE> record_object [ <STRING> ] = encrypt_aes ( params . data_key , unencrypted_key ) <NEWLINE> else : <NEWLINE> <DEDENT> if params . debug : print ( <STRING> ) <NEWLINE> unencrypted_key = os . urandom ( 32 ) <NEWLINE> record_object [ <STRING> ] = encrypt_aes ( unencrypted_key , params . data_key ) <NEWLINE> record_object [ <STRING> ] = 0 <NEWLINE> <DEDENT>
svg_text = game . get_svg_str ( ) <NEWLINE> <INDENT> html_text = HTML_WRAPPER . format ( title = game_id , filename = svg_filename ) <NEWLINE> <DEDENT>
def test_get_fee ( ) : <NEWLINE> <INDENT> assert get_fee ( fast = True ) >= get_fee ( fast = False ) <NEWLINE> <DEDENT>
org_info = wa . organisms . show_organism ( org_id ) <NEWLINE> <INDENT> if <STRING> not in org_info : <NEWLINE> <INDENT> time . sleep ( 1 ) <NEWLINE> org_info = wa . organisms . show_organism ( org_id ) <NEWLINE> <DEDENT> <DEDENT>
class TestFK ( unittest . TestCase ) : <NEWLINE> <INDENT> def setUp ( self ) : <NEWLINE> <INDENT> self . creature = creature . creature ( creature_type = creature_type ) <NEWLINE> all_zeros = [ 0 for i in range ( 0 , self . creature . config . joints_number ) ] <NEWLINE> one_move = [ 0 for i in range ( 0 , self . creature . config . joints_number ) ] <NEWLINE> one_move [ 5 ] = np . pi / 4 <NEWLINE> one_move [ 6 ] = - np . pi / 2 <NEWLINE> one_move [ 4 ] = - np . pi / 2 <NEWLINE> self . test_pos = all_zeros <NEWLINE> <DEDENT> <DEDENT>
def run ( self ) : <NEWLINE> <INDENT> self . running = True <NEWLINE> while self . running : <NEWLINE> <INDENT> if len ( self . timers ) > 0 : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . _wait ( self . timers [ - 1 ] . next_fire_time ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> self . _error ( e ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> self . assertEqual ( pipeline_obj . steps [ - 1 ] [ - 1 ] . n_neighbors , pmml_obj . NearestNeighborModel [ 0 ] . numberOfNeighbors ) <NEWLINE> <DEDENT>
self . start_time = self . distances . arrival - self . config . offsets [ 0 ] <NEWLINE> <INDENT> self . end_time = self . distances . arrival + self . config . offsets [ 1 ] <NEWLINE> <DEDENT>
if preview_path : <NEWLINE> <INDENT> mimetype = magic . from_file ( preview_path , mime = True ) . lower ( ) <NEWLINE> if mimetype in [ ExportMimeType . PNG , ExportMimeType . PDF ] : <NEWLINE> <INDENT> return preview_path <NEWLINE> <DEDENT> <DEDENT>
if RuleMods . EXPAND in mods or RuleMods . EXPAND1 in mods : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> def p_rule ( self , p ) : <NEWLINE> <INDENT> subtree = [ ] <NEWLINE> for child in p [ 1 : ] : <NEWLINE> <INDENT> if isinstance ( child , self . tree_class ) and child . head in self . rules_to_expand : <NEWLINE> <INDENT> subtree . extend ( child . tail ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> subtree . append ( child ) <NEWLINE> <DEDENT> <DEDENT> p [ 0 ] = self . tree_class ( rule_name , subtree , skip_adjustments = True ) if len ( subtree ) != 1 else subtree [ 0 ] <NEWLINE> else : <NEWLINE> <DEDENT> def p_rule ( self , p ) : <NEWLINE> <INDENT> p [ 0 ] = self . tree_class ( rule_name , p [ 1 : ] , skip_adjustments = True ) <NEWLINE> p_rule . __doc__ = rule_def <NEWLINE> setattr ( self , <STRING> % ( rule_name , ) , types . MethodType ( p_rule , self ) ) <NEWLINE> <DEDENT> <DEDENT>
def colorStr ( text , color = WHITE ) : <NEWLINE> <INDENT> if has_colors : <NEWLINE> <INDENT> seq = <STRING> % ( 30 + color ) + text + <STRING> <NEWLINE> return seq <NEWLINE> sys . stdout . write ( seq + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return text <NEWLINE> sys . stdout . write ( text + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
( type , len ) = ofp_action_header ( message , cursor ) <NEWLINE> <INDENT> field = message [ cursor . offset : offset + len ] <NEWLINE> cursor . offset = offset + len <NEWLINE> return namedtuple ( <STRING> , <NEWLINE> <INDENT> <STRING> ) ( type , len , field ) <NEWLINE> <DEDENT> <DEDENT>
def _align ( length ) : <NEWLINE> <INDENT> return ( length + 7 ) // 8 * 8 <NEWLINE> <DEDENT>
def _find_completion ( fuser , relation ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for fuser_relation in fuser . fusion_graph . get_relations ( relation . row_type , <NEWLINE> <INDENT> relation . col_type ) : <NEWLINE> if fuser_relation . _id == relation . _id : <NEWLINE> return fuser . complete ( relation ) <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>
assert isinstance ( ams_netid_st , SAmsNetId ) <NEWLINE>
in_channels = convolutions [ 0 ] [ 0 ] <NEWLINE> <INDENT> self . fc1 = Linear ( embed_dim , in_channels , dropout = dropout ) <NEWLINE> self . projections = nn . ModuleList ( ) <NEWLINE> self . convolutions = nn . ModuleList ( ) <NEWLINE> for ( out_channels , kernel_size ) in convolutions : <NEWLINE> <INDENT> pad = ( kernel_size - 1 ) / 2 <NEWLINE> self . projections . append ( Linear ( in_channels , out_channels ) <NEWLINE> <INDENT> if in_channels != out_channels else None ) <NEWLINE> <DEDENT> self . convolutions . append ( <NEWLINE> <INDENT> ConvTBC ( in_channels , out_channels * 2 , kernel_size , padding = pad , <NEWLINE> <INDENT> dropout = dropout ) ) <NEWLINE> <DEDENT> <DEDENT> in_channels = out_channels <NEWLINE> <DEDENT> self . fc2 = Linear ( in_channels , embed_dim ) <NEWLINE> <DEDENT>
def forward ( self , input , incremental_state = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> bsz , seq_len = input . size ( ) <NEWLINE> max_pos = self . padding_idx + 1 + seq_len <NEWLINE> if max_pos > self . weights . size ( 0 ) : <NEWLINE> <INDENT> self . weights = SinusoidalPositionalEmbedding . get_embedding ( <NEWLINE> <INDENT> max_pos , <NEWLINE> self . embedding_dim , <NEWLINE> self . padding_idx , <NEWLINE> <DEDENT> ) . type_as ( self . weights ) <NEWLINE> <DEDENT> weights = Variable ( self . weights ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> max_epoch = args . max_epoch or math . inf <NEWLINE> max_update = args . max_update or math . inf <NEWLINE> lr = trainer . get_lr ( ) <NEWLINE> train_meter = StopwatchMeter ( ) <NEWLINE> train_meter . start ( ) <NEWLINE> valid_losses = [ None ] <NEWLINE> valid_subsets = args . valid_subset . split ( <STRING> ) <NEWLINE> while lr > args . min_lr and epoch_itr . epoch < max_epoch and trainer . get_num_updates ( ) < max_update : <NEWLINE> <COMMENT> <NL> <INDENT> train ( args , trainer , task , epoch_itr ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> max_epoch = args . max_epoch or math . inf <NEWLINE> max_update = args . max_update or math . inf <NEWLINE> lr = trainer . get_lr ( ) <NEWLINE> train_meter = StopwatchMeter ( ) <NEWLINE> train_meter . start ( ) <NEWLINE> valid_losses = [ None ] <NEWLINE> valid_subsets = args . valid_subset . split ( <STRING> ) <NEWLINE> while lr > args . min_lr and epoch_itr . epoch < max_epoch and trainer . get_num_updates ( ) < max_update : <NEWLINE> <COMMENT> <NL> <INDENT> train ( args , trainer , task , epoch_itr ) <NEWLINE> <DEDENT> <DEDENT>
sample_size = sample [ <STRING> ] . size ( 0 ) if self . args . sentence_avg else sample [ <STRING> ] <NEWLINE> <INDENT> logging_output = { <NEWLINE> <INDENT> <STRING> : utils . item ( loss . data ) if reduce else loss . data , <NEWLINE> <STRING> : utils . item ( nll_loss . data ) if reduce else nll_loss . data , <NEWLINE> <STRING> : sample [ <STRING> ] , <NEWLINE> <STRING> : sample_size , <NEWLINE> <DEDENT> } <NEWLINE> return loss , sample_size , logging_output <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> attn = torch . rand ( bbsz , tgt_len , src_len ) <NEWLINE> <DEDENT>
def step_update ( self , num_updates ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if num_updates < self . args . warmup_updates : <NEWLINE> <INDENT> self . lr = self . args . warmup_init_lr + num_updates * self . lr_step <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> curr_updates = num_updates - self . args . warmup_updates <NEWLINE> if self . t_mult != 1 : <NEWLINE> <INDENT> i = math . floor ( math . log ( 1 - curr_updates / self . period * ( 1 - self . t_mult ) , self . t_mult ) ) <NEWLINE> t_i = self . t_mult ** i * self . period <NEWLINE> t_curr = curr_updates - ( 1 - self . t_mult ** i ) / ( 1 - self . t_mult ) * self . period <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> i = math . floor ( curr_updates / self . period ) <NEWLINE> t_i = self . period <NEWLINE> t_curr = curr_updates - ( self . period * i ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def __get_safe_conn ( self , retry_count ) : <NEWLINE> <INDENT> self . current_size += 1 <NEWLINE> c = self . unuse_list . pop ( ) <NEWLINE> if self . ping_check : <NEWLINE> <INDENT> now = int ( time ( ) ) <NEWLINE> timeout = now <NEWLINE> if isinstance ( self . ping_check , int ) : <NEWLINE> <INDENT> timeout = timeout - self . ping_check <NEWLINE> <DEDENT> if not hasattr ( c , <STRING> ) : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> if c . __ping_check_timestamp < timeout : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> c . ping ( ) <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> self . current_size -= 1 <NEWLINE> if retry_count < 10 : c = self . __get_conn ( retry_count + 1 ) <NEWLINE> <DEDENT> <DEDENT> if c : self . inuse_list . add ( c ) <NEWLINE> return c <NEWLINE> <DEDENT>
def copy ( self , new_path , replace = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if replace or not get_file ( new_path ) . exists ( ) : <NEWLINE> <INDENT> self . key . copy ( self . key . bucket , new_path ) <NEWLINE> return True <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
if <STRING> in os . environ : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> elif self . charm_name : <NEWLINE> if charm_name == self . charm_name : <NEWLINE> <INDENT> charm_branch = os . getcwd ( ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if has_perf_mod : <NEWLINE> <INDENT> _set_cache_ = _set_cache_brute_ <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> _set_cache_ = _set_cache_too_slow_without_c <NEWLINE> <DEDENT> <DEDENT>
step = peda . intsize ( ) <NEWLINE> <INDENT> if not peda . is_address ( address ) : <COMMENT> <NEWLINE> <INDENT> msg ( <STRING> % address , <STRING> ) <NEWLINE> return <NEWLINE> for i in range ( count ) : <NEWLINE> <INDENT> if not peda . execute ( <STRING> % ( <STRING> if step == 8 else <STRING> , address + i * step ) ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> return <NEWLINE> <DEDENT> <DEDENT>
def set_statics ( self ) : <NEWLINE> <INDENT> if not os . path . exists ( self . results_dir ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> shutil . copytree ( os . path . join ( self . templates_dir , <STRING> ) , os . path . join ( self . results_dir , <STRING> ) ) <NEWLINE> shutil . copytree ( os . path . join ( self . templates_dir , <STRING> ) , os . path . join ( self . results_dir , <STRING> ) ) <NEWLINE> shutil . copytree ( os . path . join ( self . templates_dir , <STRING> ) , os . path . join ( self . results_dir , <STRING> ) ) <NEWLINE> <DEDENT> except OSError : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> <DEDENT>
bundle = entangled_interface . local_attach_to_tangle ( pb , gta [ <STRING> ] , gta [ <STRING> ] , mwm ) <NEWLINE>
def edit_rwhois ( self , abuse_email = None , address1 = None , address2 = None , <NEWLINE> <INDENT> city = None , company_name = None , country = None , <NEWLINE> first_name = None , last_name = None , postal_code = None , <NEWLINE> private_residence = None , state = None ) : <NEWLINE> <STRING> <NEWLINE> update = { } <NEWLINE> for key , value in [ ( <STRING> , abuse_email ) , <NEWLINE> <INDENT> ( <STRING> , address1 ) , <NEWLINE> ( <STRING> , address2 ) , <NEWLINE> ( <STRING> , city ) , <NEWLINE> ( <STRING> , company_name ) , <NEWLINE> ( <STRING> , country ) , <NEWLINE> ( <STRING> , first_name ) , <NEWLINE> ( <STRING> , last_name ) , <NEWLINE> ( <STRING> , private_residence ) , <NEWLINE> ( <STRING> , state ) , <NEWLINE> ( <STRING> , postal_code ) ] : <NEWLINE> if value is not None : <NEWLINE> update [ key ] = value <NEWLINE> <DEDENT> <DEDENT>
@ click . command ( ) <NEWLINE> <INDENT> @ click . argument ( <STRING> ) <NEWLINE> @ click . option ( <STRING> , <NEWLINE> <INDENT> type = click . Choice ( [ <STRING> , <STRING> , <STRING> ] ) , <NEWLINE> help = <STRING> , <NEWLINE> required = True ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , <NEWLINE> <INDENT> is_flag = True , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT> @ environment . pass_env <NEWLINE> def cli ( env , target , firewall_type , high_availability ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> <DEDENT>
base_type_name = <STRING> <NEWLINE> <INDENT> package = storage_utils . get_package ( self , storage_type ) <NEWLINE> if storage_type == <STRING> : <NEWLINE> <INDENT> complex_type = base_type_name + <STRING> <NEWLINE> prices = [ <NEWLINE> <INDENT> storage_utils . find_performance_price ( <NEWLINE> <INDENT> package , <NEWLINE> <STRING> <NEWLINE> ) , <NEWLINE> <DEDENT> storage_utils . find_performance_space_price ( package , size ) , <NEWLINE> storage_utils . find_performance_iops_price ( package , size , iops ) , <NEWLINE> <DEDENT> ] <NEWLINE> <DEDENT> elif storage_type == <STRING> : <NEWLINE> <INDENT> complex_type = base_type_name + <STRING> <NEWLINE> prices = [ <NEWLINE> <INDENT> storage_utils . find_endurance_price ( package , <STRING> ) , <NEWLINE> storage_utils . find_endurance_price ( <NEWLINE> <INDENT> package , <NEWLINE> <STRING> <NEWLINE> ) , <NEWLINE> <DEDENT> storage_utils . find_endurance_space_price ( <NEWLINE> <INDENT> package , <NEWLINE> size , <NEWLINE> tier_level <NEWLINE> ) , <NEWLINE> <DEDENT> storage_utils . find_endurance_tier_price ( package , tier_level ) , <NEWLINE> <DEDENT> ] <NEWLINE> if snapshot_size is not None : <NEWLINE> <INDENT> prices . append ( storage_utils . find_snapshot_space_price ( <NEWLINE> <INDENT> package , snapshot_size , tier_level ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise exceptions . SoftLayerError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if private_vlan or public_vlan : <NEWLINE> <INDENT> network_components = self . _create_network_components ( public_vlan , private_vlan , <NEWLINE> <INDENT> private_subnet , public_subnet ) <NEWLINE> <DEDENT> data . update ( network_components ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> member_table = formatting . Table ( [ <STRING> , <STRING> , <STRING> ] , title = <STRING> ) <NEWLINE> guests = group . get ( <STRING> , [ ] ) <NEWLINE> for guest in guests : <NEWLINE> <INDENT> real_guest = guest . get ( <STRING> ) <NEWLINE> member_table . add_row ( [ <NEWLINE> <INDENT> real_guest . get ( <STRING> ) , real_guest . get ( <STRING> ) , utils . clean_time ( real_guest . get ( <STRING> ) ) <NEWLINE> <DEDENT> ] ) <NEWLINE> <DEDENT> env . fout ( member_table ) <NEWLINE> <DEDENT>
return ui , loader . to_be_bound ( ) <NEWLINE>
@ step ( <STRING> ) <NEWLINE> <INDENT> @ persona_vars <NEWLINE> def click_link_in_email ( context , address ) : <NEWLINE> <INDENT> mails = context . mail . user_messages ( address ) <NEWLINE> assert mails , <STRING> <NEWLINE> mail = email . message_from_string ( mails [ 0 ] ) <NEWLINE> links = URL_RE . findall ( str ( mail ) . replace ( <STRING> , <STRING> ) ) <NEWLINE> assert links , <STRING> <NEWLINE> url = links [ 0 ] <NEWLINE> context . browser . visit ( url ) <NEWLINE> <DEDENT> <DEDENT>
train_corpus = GoldCorpus ( train_json_path , dev_json_path ) <NEWLINE> <INDENT> test_corpus = GoldCorpus ( train_json_path , test_json_path ) <NEWLINE> <DEDENT>
with open ( os . path . join ( model_output_dir , <STRING> ) ) as metric_file : <NEWLINE> <INDENT> json . dump ( scorer_onto_retrained . scores , metric_file ) <NEWLINE> if __name__ == <STRING> : <NEWLINE> parser = argparse . ArgumentParser ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if generate_linking_classifier_training_data : <NEWLINE> <INDENT> for candidates , mention_types_for_mention in zip ( candidates_by_mention , mention_types_by_mention ) : <NEWLINE> <INDENT> for candidate_id , candidate in candidates . items ( ) : <NEWLINE> <INDENT> classifier_example = linker . classifier_example ( candidate_id , candidate , mention_text , mention_types_for_mention ) <NEWLINE> classifier_example [ <STRING> ] = int ( gold_entity . umls_id == candidate_id ) <NEWLINE> linking_classifier_training_data . append ( classifier_example ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
unit = unit or self . select_build_worker ( p ) <NEWLINE> <INDENT> if unit is None or not self . can_afford ( building ) : <NEWLINE> <INDENT> return ActionResult . Error <NEWLINE> <DEDENT> return await self . do ( unit . build ( building , p ) ) <NEWLINE> <DEDENT>
def circle_intersection ( self , p : <STRING> , r : Union [ int , float ] ) -> Set [ <STRING> ] : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert self != p <NEWLINE> distanceBetweenPoints = self . distance_to ( p ) <NEWLINE> assert r >= distanceBetweenPoints / 2 <NEWLINE> <COMMENT> <NL> remainingDistanceFromCenter = ( r ** 2 - ( distanceBetweenPoints / 2 ) ** 2 ) ** 0.5 <NEWLINE> <COMMENT> <NL> offsetToCenter = Point2 ( ( ( p . x - self . x ) / 2 , ( p . y - self . y ) / 2 ) ) <NEWLINE> center = self . offset ( offsetToCenter ) <NEWLINE> <DEDENT>
def circle_intersection ( self , p : <STRING> , r : Union [ int , float ] ) -> Set [ <STRING> ] : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert self != p <NEWLINE> distanceBetweenPoints = self . distance_to ( p ) <NEWLINE> assert r >= distanceBetweenPoints / 2 <NEWLINE> <COMMENT> <NL> remainingDistanceFromCenter = ( r ** 2 - ( distanceBetweenPoints / 2 ) ** 2 ) ** 0.5 <NEWLINE> <COMMENT> <NL> offsetToCenter = Point2 ( ( ( p . x - self . x ) / 2 , ( p . y - self . y ) / 2 ) ) <NEWLINE> center = self . offset ( offsetToCenter ) <NEWLINE> <DEDENT>
def take ( self , n : int ) -> <STRING> : <NEWLINE> <INDENT> if self . amount <= n : <NEWLINE> <INDENT> return self <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . subgroup ( self [ : n ] ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> sample_file = os . path . join ( project_dir , <STRING> ) <NEWLINE> settings_file = os . path . join ( home_config_dir , <STRING> ) <NEWLINE> if not os . path . exists ( home_config_dir ) : <NEWLINE> <INDENT> os . makedirs ( home_config_dir ) <NEWLINE> <DEDENT> copyfile ( sample_file , settings_file ) <NEWLINE> print ( <STRING> . format ( repr ( settings_file ) ) ) <NEWLINE> return settings_file <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if self . options . get ( <STRING> , None ) or self . options . get ( <STRING> , None ) : <NEWLINE> <INDENT> assert self . options [ <STRING> ] < self . options [ <STRING> ] <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = True <NEWLINE> self . options [ <STRING> ] = False <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if self . options . get ( <STRING> , None ) and self . options . get ( <STRING> , None ) : <NEWLINE> <INDENT> assert self . options [ <STRING> ] < self . options [ <STRING> ] <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = self . utcify ( self . options [ <STRING> ] ) <NEWLINE> self . options [ <STRING> ] = True <NEWLINE> self . options [ <STRING> ] = False <NEWLINE> <DEDENT> <DEDENT>
for i in range ( len ( received ) ) : <NEWLINE> <INDENT> if expected [ i ] [ <STRING> ] == received [ i ] [ <STRING> ] : <NEWLINE> <INDENT> self . assertEqual ( expected [ i ] [ <STRING> ] , received [ i ] [ <STRING> ] ) <NEWLINE> self . assertEqual ( expected [ i ] [ <STRING> ] , received [ i ] [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT>
with pytest . raises ( Exception ) : <NEWLINE> <INDENT> view_func2 ( ) <NEWLINE> assert len ( User . query . all ( ) ) == 0 <NEWLINE> <DEDENT>
def prebuildcleanup ( top , parent ) : <NEWLINE> <INDENT> preclean = { } <NEWLINE> preclean_patterns = { <STRING> : <STRING> , <STRING> : <STRING> } <NEWLINE> for element in top : <NEWLINE> <INDENT> if element . tag == <STRING> : <NEWLINE> <INDENT> preclean [ <STRING> ] = ( element . text == <STRING> ) <NEWLINE> <DEDENT> elif element . tag == <STRING> : <NEWLINE> <INDENT> for subelement in element : <NEWLINE> <INDENT> if subelement . tag != <STRING> : <NEWLINE> <INDENT> raise NotImplementedError ( <STRING> <NEWLINE> <INDENT> <STRING> % subelement . tag ) <NEWLINE> <DEDENT> <DEDENT> if subelement . find ( <STRING> ) is not None and subelement . find ( <STRING> ) is not None : <NEWLINE> <INDENT> rule_type = subelement . find ( <STRING> ) . text . lower ( ) <NEWLINE> rule_patt = subelement . find ( <STRING> ) . text <NEWLINE> preclean_patterns [ rule_type ] = rule_patt <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif element . tag == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> pass <NEWLINE> <DEDENT> elif element . tag == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NotImplementedError ( <STRING> <NEWLINE> <INDENT> <STRING> % element . tag ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if haveSampleColumn : <NEWLINE> <INDENT> if not all ( [ patient in sample for sample , patient in zip ( clinicalSampleDF [ sampleId ] , clinicalSampleDF [ patientId ] ) ] ) : <NEWLINE> <INDENT> total_error += <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> sample_patients = clinicalSampleDF [ patientId ] [ clinicalSampleDF [ patientId ] != <STRING> ] <NEWLINE> patient_patients = clinicalDF [ patientId ] [ clinicalDF [ patientId ] != <STRING> ] <NEWLINE> <COMMENT> <NL> if not all ( sample_patients . isin ( patient_patients ) ) : <NEWLINE> <INDENT> total_error += <STRING> % <STRING> . join ( clinicalSampleDF [ sampleId ] [ ~ clinicalSampleDF [ patientId ] . isin ( clinicalDF [ patientId ] ) ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> if not all ( patient_patients . isin ( sample_patients ) ) : <NEWLINE> <COMMENT> <NL> <INDENT> warning += <STRING> % <STRING> . join ( clinicalDF [ patientId ] [ ~ clinicalDF [ patientId ] . isin ( clinicalSampleDF [ patientId ] ) ] ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def createMafDatabase ( syn , databaseToSynIdMappingDf , testing = False , staging = False ) : <NEWLINE> <INDENT> mafDatabaseSynId = process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) <NEWLINE> mafDatabaseEnt = syn . get ( mafDatabaseSynId ) <NEWLINE> mafCols = list ( syn . getTableColumns ( mafDatabaseSynId ) ) <NEWLINE> schema = synapseclient . Schema ( name = <STRING> % time . time ( ) , columns = mafCols , parent = process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) ) <NEWLINE> schema . primaryKey = mafDatabaseEnt . primaryKey <NEWLINE> newMafDb = syn . store ( schema ) <NEWLINE> <COMMENT> <NL> databaseToSynIdMappingDf [ <STRING> ] [ 0 ] = newMafDb . id <NEWLINE> syn . store ( synapseclient . Table ( process_functions . getDatabaseSynId ( syn , <STRING> , test = testing ) , databaseToSynIdMappingDf ) ) <NEWLINE> if not staging and not testing : <NEWLINE> <COMMENT> <NL> <INDENT> databaseToSynIdMapping = syn . tableQuery ( <STRING> ) <NEWLINE> databaseToSynIdMappingDf = databaseToSynIdMapping . asDataFrame ( ) <NEWLINE> databaseToSynIdMappingDf [ <STRING> ] [ 0 ] = newMafDb . id <NEWLINE> syn . store ( synapseclient . Table ( <STRING> , databaseToSynIdMappingDf ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> mafDatabaseEnt . parentId = <STRING> <NEWLINE> mafDatabaseEnt . name = <STRING> + mafDatabaseEnt . name <NEWLINE> syn . store ( mafDatabaseEnt ) <NEWLINE> mafDatabaseSynId = newMafDb . id <NEWLINE> <COMMENT> <NL> syn . setPermissions ( mafDatabaseSynId , 3326313 , [ ] ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> haveColumn = process_functions . checkColExist ( clinicalDF , <STRING> ) <NEWLINE> if haveColumn : <NEWLINE> <INDENT> if not all ( [ i != <STRING> for i in clinicalDF [ <STRING> ] ] ) : <NEWLINE> <INDENT> warning += <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> seqAssayIds = clinicalDF . SEQ_ASSAY_ID [ clinicalDF . SEQ_ASSAY_ID != <STRING> ] <NEWLINE> allSeqAssays = seqAssayIds . unique ( ) <NEWLINE> notNormalized = [ ] <NEWLINE> not_caps = [ ] <NEWLINE> for seqassay in allSeqAssays : <NEWLINE> <COMMENT> <NL> <INDENT> if not seqassay . upper ( ) . startswith ( self . center ) : <NEWLINE> <INDENT> not_caps . append ( seqassay ) <NEWLINE> <DEDENT> <DEDENT> if len ( not_caps ) > 0 : <NEWLINE> <INDENT> total_error += <STRING> % <STRING> . join ( not_caps ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> total_error += <STRING> <NEWLINE> <DEDENT> <DEDENT>
if len ( samples ) == 1 : <NEWLINE> <INDENT> tumor = samples [ 0 ] <NEWLINE> normal = <STRING> <NEWLINE> elif len ( samples ) == 2 : <NEWLINE> <COMMENT> <NL> tumor = samples [ 0 ] <NEWLINE> normal = samples [ 1 ] <NEWLINE> else : <NEWLINE> tumor = <STRING> <NEWLINE> normal = <STRING> <NEWLINE> <COMMENT> <NL> if tumor == <STRING> : <NEWLINE> tumorName = vcfName . replace ( <STRING> , <STRING> ) <NEWLINE> else : <NEWLINE> tumorName = tumor <NEWLINE> newMAFPath = newVCFPath + <STRING> <NEWLINE> if os . path . isfile ( newMAFPath ) : <NEWLINE> mafFiles . append ( newMAFPath ) <NEWLINE> else : <NEWLINE> command = [ <STRING> , os . path . join ( vcf2mafPath , <STRING> ) , <NEWLINE> <INDENT> <STRING> , newVCFPath , <NEWLINE> <STRING> , newMAFPath , <NEWLINE> <STRING> , veppath , <NEWLINE> <STRING> , vepdata , <NEWLINE> <STRING> , <STRING> , <NEWLINE> <STRING> , tumorName , <NEWLINE> <STRING> , normal , <NEWLINE> <STRING> , tumor , <NEWLINE> <COMMENT> <NL> <STRING> , os . path . join ( vcf2mafPath , <STRING> ) ] <NEWLINE> <DEDENT> if reference is not None : <NEWLINE> <INDENT> command . extend ( [ <STRING> , reference ] ) <NEWLINE> <DEDENT> subprocess . check_call ( command ) <NEWLINE> if ( os . path . isfile ( newMAFPath ) ) : <NEWLINE> <INDENT> mafFiles . append ( newMAFPath ) <NEWLINE> <DEDENT> <DEDENT>
allFiles = pd . DataFrame ( allFiles , columns = [ <STRING> , <STRING> ] ) <NEWLINE> <COMMENT> <NL> <INDENT> if allFiles . empty : <NEWLINE> <INDENT> logger . info ( <STRING> % center ) <NEWLINE> return ( [ ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> if process != <STRING> : <NEWLINE> <INDENT> addToQuery = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> addToQuery = <STRING> <NEWLINE> <DEDENT> validationStatus = syn . tableQuery ( <STRING> % ( process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) , center , addToQuery ) ) <NEWLINE> errorTracker = syn . tableQuery ( <STRING> % ( process_functions . getDatabaseSynId ( syn , <STRING> , databaseToSynIdMappingDf = databaseToSynIdMappingDf ) , center , addToQuery ) ) <NEWLINE> <COMMENT> <NL> validationStatusDf = validationStatus . asDataFrame ( ) <NEWLINE> errorTrackerDf = errorTracker . asDataFrame ( ) <NEWLINE> validated = allFiles . apply ( lambda x : validateFile ( syn , validationStatusDf , errorTrackerDf , center , thread , x , testing , oncotreeLink ) , axis = 1 ) <NEWLINE> inputValidStatus = [ ] <NEWLINE> invalidErrors = [ ] <NEWLINE> for inputStat , invalErrors in validated : <NEWLINE> <INDENT> inputValidStatus . extend ( inputStat ) <NEWLINE> if invalErrors is not None : <NEWLINE> <INDENT> invalidErrors . extend ( invalErrors ) <NEWLINE> <DEDENT> <DEDENT> inputValidStatus = pd . DataFrame ( inputValidStatus , columns = [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> duplicatedFiles = inputValidStatus [ inputValidStatus [ <STRING> ] . duplicated ( keep = False ) ] <NEWLINE> cbsSegBool = [ os . path . basename ( i ) . endswith ( <STRING> ) or os . path . basename ( i ) . endswith ( <STRING> ) for i in inputValidStatus [ <STRING> ] ] <NEWLINE> cbsSegFiles = inputValidStatus [ cbsSegBool ] <NEWLINE> if len ( cbsSegFiles ) > 1 : <NEWLINE> <INDENT> duplicatedFiles = duplicatedFiles . append ( cbsSegFiles ) <NEWLINE> <DEDENT> clinical_bool = [ <STRING> in i for i in inputValidStatus [ <STRING> ] ] <NEWLINE> clinical_files = inputValidStatus [ clinical_bool ] <NEWLINE> if len ( clinical_files ) > 2 : <NEWLINE> <INDENT> duplicatedFiles = duplicatedFiles . append ( clinical_files ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
status_list = check_file_status [ <STRING> ] <NEWLINE> <INDENT> error_list = check_file_status [ <STRING> ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> validator = validate . GenieValidationHelper ( syn = syn , center = center , <NEWLINE> <INDENT> filepathlist = filepaths , <NEWLINE> format_registry = format_registry , <NEWLINE> testing = testing ) <NEWLINE> <DEDENT> filetype = validator . file_type <NEWLINE> if check_file_status [ <STRING> ] : <NEWLINE> <INDENT> valid , message , filetype = validator . validate_single_file ( <NEWLINE> <INDENT> oncotree_link = oncotree_link , nosymbol_check = False ) <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> input_status_list , invalid_errors_list = _get_status_and_error_list ( <NEWLINE> <INDENT> valid , message , entities ) <NEWLINE> <COMMENT> <NL> <DEDENT> if not invalid_errors_list : <NEWLINE> <INDENT> _send_validation_error_email ( syn , filenames , message , file_users ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> input_status_list = [ <NEWLINE> <INDENT> [ ent . id , path , ent . md5 , status , filename , entity_date_to_timestamp ( ent . properties . modifiedOn ) ] <NEWLINE> for ent , path , status , filename in <NEWLINE> zip ( entities , filepaths , status_list , filenames ) ] <NEWLINE> <DEDENT> invalid_errors_list = [ <NEWLINE> <INDENT> [ entity . id , error , filename ] <NEWLINE> for entity , error , filename in <NEWLINE> zip ( entities , error_list , filenames ) ] <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for input_status in input_status_list : <NEWLINE> <INDENT> input_status . extend ( [ filetype , center ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> for invalid_errors in invalid_errors_list : <NEWLINE> <INDENT> invalid_errors . extend ( [ filetype , center ] ) <NEWLINE> <DEDENT> return ( input_status_list , invalid_errors_list ) <NEWLINE> <DEDENT>
if sum ( q_inds ) : <NEWLINE> <INDENT> inds_to_send = q_inds [ np . where ( H [ <STRING> ] [ q_inds ] == max ( H [ <STRING> ] [ q_inds ] ) ) [ 0 ] ] <NEWLINE> Work [ i ] = { <STRING> : sim_specs [ <STRING> ] [ 0 ] , <NEWLINE> <INDENT> <STRING> : sim_specs [ <STRING> ] , <NEWLINE> <STRING> : [ ] , <NEWLINE> <STRING> : H [ sim_specs [ <STRING> ] ] [ inds_to_send ] , <NEWLINE> <STRING> : sim_specs [ <STRING> ] , <NEWLINE> <STRING> : { <STRING> : <STRING> , <STRING> : inds_to_send } , <NEWLINE> } <NEWLINE> <DEDENT> <DEDENT>
comm . send ( obj = data_out , dest = 0 , tag = tag_out ) <NEWLINE>
def kill_pending ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for req in self . _outbox : <NEWLINE> <INDENT> if not req . Test ( ) : <NEWLINE> <INDENT> req . Cancel ( ) <NEWLINE> <DEDENT> <DEDENT> self . _outbox = [ ] <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def get_slurm_nodelist ( node_list_env ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> fullstr = os . environ [ node_list_env ] <NEWLINE> if not fullstr : <NEWLINE> <INDENT> return [ ] <NEWLINE> <DEDENT> part_splitstr = fullstr . split ( <STRING> ) <NEWLINE> if len ( part_splitstr ) == 1 : <COMMENT> <NEWLINE> <INDENT> splitstr = fullstr . split ( <STRING> , 1 ) <NEWLINE> if len ( splitstr ) == 1 : <COMMENT> <NEWLINE> <INDENT> return splitstr <NEWLINE> <DEDENT> prefix = splitstr [ 0 ] <NEWLINE> nidstr = splitstr [ 1 ] . strip ( <STRING> ) <NEWLINE> nidlst = EnvResources . _noderange_append ( prefix , nidstr ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> splitgroups = [ str . split ( <STRING> , 1 ) for str in part_splitstr ] <NEWLINE> prefixgroups = [ group [ 0 ] for group in splitgroups ] <NEWLINE> nodegroups = [ group [ 1 ] . strip ( <STRING> ) for group in splitgroups ] <NEWLINE> nidlst = [ ] <NEWLINE> for i in range ( len ( prefixgroups ) ) : <NEWLINE> <INDENT> prefix = prefixgroups [ i ] <NEWLINE> nidstr = nodegroups [ i ] <NEWLINE> nidlst . extend ( EnvResources . _noderange_append ( prefix , nidstr ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
last_size = persis_info . get ( <STRING> ) <NEWLINE> <INDENT> if len ( H ) : <NEWLINE> <COMMENT> <NL> <INDENT> if ( alloc_specs [ <STRING> ] . get ( <STRING> ) <NEWLINE> <INDENT> and not all ( np . logical_or ( H [ <STRING> ] [ last_size : ] , <NEWLINE> <INDENT> H [ <STRING> ] [ last_size : ] ) ) ) : <NEWLINE> <DEDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> if len ( persis_info [ lw ] [ <STRING> ] ) : <NEWLINE> <INDENT> runs_needing_to_advance = np . zeros ( len ( persis_info [ lw ] [ <STRING> ] ) , dtype = bool ) <NEWLINE> for run , inds in enumerate ( persis_info [ lw ] [ <STRING> ] . values ( ) ) : <NEWLINE> <INDENT> runs_needing_to_advance [ run ] = np . all ( H [ <STRING> ] [ inds ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
last_size = persis_info . get ( <STRING> ) <NEWLINE> <INDENT> if len ( H ) : <NEWLINE> <COMMENT> <NL> <INDENT> if ( alloc_specs [ <STRING> ] . get ( <STRING> ) <NEWLINE> <INDENT> and not all ( np . logical_or ( H [ <STRING> ] [ last_size : ] , <NEWLINE> <INDENT> H [ <STRING> ] [ last_size : ] ) ) ) : <NEWLINE> <DEDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> if len ( persis_info [ lw ] [ <STRING> ] ) : <NEWLINE> <INDENT> runs_needing_to_advance = np . zeros ( len ( persis_info [ lw ] [ <STRING> ] ) , dtype = bool ) <NEWLINE> for run , inds in enumerate ( persis_info [ lw ] [ <STRING> ] . values ( ) ) : <NEWLINE> <INDENT> runs_needing_to_advance [ run ] = H [ <STRING> ] [ inds [ - 1 ] ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> still_working = ~ H [ <STRING> ] <NEWLINE> if alloc_specs [ <STRING> ] . get ( <STRING> ) and np . any ( still_working ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> H_o = add_to_Out ( np . zeros ( 1 , dtype = gen_specs [ <STRING> ] ) , x , 0 , <NEWLINE> <INDENT> gen_specs [ <STRING> ] [ <STRING> ] , gen_specs [ <STRING> ] [ <STRING> ] , local = True , active = True ) <NEWLINE> <DEDENT> tag , Work , calc_in = sendrecv_mgr_worker_msg ( comm , H_o ) <NEWLINE> if tag in [ STOP_TAG , PERSIS_STOP ] : <NEWLINE> <INDENT> nlopt . forced_stop . message = <STRING> + str ( Work ) <NEWLINE> raise nlopt . forced_stop <NEWLINE> <DEDENT> <DEDENT>
return H_o , persis_info , Work <NEWLINE>
def prepare_to_run_command ( self , cmd ) : <NEWLINE> <INDENT> self . LOG . debug ( <STRING> . format ( cmd . cmd_name ) ) <NEWLINE> self . timer . start ( ) <NEWLINE> os . umask ( self . options . umask ) <NEWLINE> self . LOG . debug ( <STRING> . format ( <NEWLINE> <INDENT> self . options . environment ) ) <NEWLINE> <DEDENT> self . environment = self . options . environment <NEWLINE> self . secrets_basedir = self . options . secrets_basedir <NEWLINE> <COMMENT> <NL> if cmd . cmd_name != <STRING> : <NEWLINE> <INDENT> SecretsEnvironment . permissions_check ( <NEWLINE> <INDENT> self . secrets_basedir , <NEWLINE> verbose_level = self . options . verbose_level , <NEWLINE> ) <NEWLINE> <DEDENT> self . secrets_file = self . options . secrets_file <NEWLINE> self . secrets = SecretsEnvironment ( <NEWLINE> <INDENT> environment = self . environment , <NEWLINE> secrets_basedir = self . secrets_basedir , <NEWLINE> secrets_file = self . secrets_file , <NEWLINE> export_env_vars = self . options . export_env_vars , <NEWLINE> verbose_level = self . options . verbose_level , <NEWLINE> env_var_prefix = self . options . env_var_prefix , <NEWLINE> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for m in self . history . alive_models ( t - 1 ) : <NEWLINE> <INDENT> particles , w = self . history . get_distribution ( m , t - 1 ) <NEWLINE> self . transitions [ m ] . fit ( particles , w ) <NEWLINE> <DEDENT>
@ app . route ( <STRING> ) <NEWLINE> <INDENT> def abc_model ( abc_id , model_id , t ) : <NEWLINE> <INDENT> history = app . config [ <STRING> ] <NEWLINE> history . id = abc_id <NEWLINE> if t == <STRING> : <NEWLINE> <INDENT> t = history . max_t <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> t = int ( t ) <NEWLINE> <DEDENT> df , w = history . get_distribution ( model_id , t ) <NEWLINE> df [ <STRING> ] = w <NEWLINE> tabs = [ ] <NEWLINE> <DEDENT> <DEDENT>
for h in histories : <NEWLINE> <INDENT> for t in range ( 4 ) : <NEWLINE> <INDENT> for m in range ( 5 ) : <NEWLINE> <INDENT> pop = pops [ ( h , m , t ) ] <NEWLINE> expected_particles_list = [ p . parameter for p in pop ] <NEWLINE> pars_df , w = h . get_distribution ( m , t ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> assert np . isclose ( w . sum ( ) , 1 ) <NEWLINE> for part_nr in range ( len ( expected_particles_list ) ) : <NEWLINE> <INDENT> expected_par = expected_particles_list [ part_nr ] <NEWLINE> actual_par = pars_df . iloc [ part_nr ] <NEWLINE> assert expected_par . a == actual_par . a <NEWLINE> assert expected_par . b == actual_par . b <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
yield instr ( arg ) <NEWLINE>
if opcode in ( opcodes . OP_CHECKSIG , opcodes . OP_CHECKSIGVERIFY ) : <NEWLINE> <COMMENT> <NL> <INDENT> op_checksig ( stack , signature_for_hash_type_f , expected_hash_type , script [ begin_code_hash : ] , flags ) <NEWLINE> if opcode == opcodes . OP_CHECKSIGVERIFY : <NEWLINE> <INDENT> if not bool_from_script_bytes ( stack . pop ( ) ) : <NEWLINE> <INDENT> raise ScriptError ( <STRING> % ( pc - 1 ) ) <NEWLINE> <DEDENT> <DEDENT> continue <NEWLINE> <DEDENT>
if len ( output_order ) == 0 : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> elif len ( output_dict ) == 1 : <NEWLINE> print ( output_dict [ output_order [ 0 ] [ 0 ] ] ) <NEWLINE> else : <NEWLINE> dump_output ( output_dict , output_order ) <NEWLINE> <DEDENT>
def deterministic_generate_k ( generator_order , secret_exponent , val , hash_f = hashlib . sha256 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> n = generator_order <NEWLINE> bln = bit_length ( n ) <NEWLINE> order_size = ( bln + 7 ) // 8 <NEWLINE> hash_size = hash_f ( ) . digest_size <NEWLINE> v = <STRING> * hash_size <NEWLINE> k = <STRING> * hash_size <NEWLINE> priv = intstream . to_bytes ( secret_exponent , length = order_size ) <NEWLINE> shift = 8 * hash_size - bln <NEWLINE> if shift > 0 : <NEWLINE> <INDENT> val >>= shift <NEWLINE> <DEDENT> if val >= n : <NEWLINE> <INDENT> val -= n <NEWLINE> <DEDENT> h1 = intstream . to_bytes ( val , length = order_size ) <NEWLINE> k = hmac . new ( k , v + <STRING> + priv + h1 , hash_f ) . digest ( ) <NEWLINE> v = hmac . new ( k , v , hash_f ) . digest ( ) <NEWLINE> k = hmac . new ( k , v + <STRING> + priv + h1 , hash_f ) . digest ( ) <NEWLINE> v = hmac . new ( k , v , hash_f ) . digest ( ) <NEWLINE> <DEDENT>
def test_get_host_speed_rank ( self ) : <NEWLINE> <INDENT> r = util . get_host_speed_rank ( [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ] ) <NEWLINE> assert len ( r ) <= 1 <NEWLINE> with pytest . raises ( ValueError ) : <NEWLINE> <INDENT> util . get_host_speed_rank ( [ <STRING> ] ) <NEWLINE> <DEDENT> assert util . get_host_speed_rank ( timeout = 0 ) == [ ] <NEWLINE> <DEDENT>
if args . id is not None : <NEWLINE> <INDENT> consensus . id = args . id <NEWLINE> elif args . idLambda is not None : <NEWLINE> idLambda = eval ( args . idLambda ) <NEWLINE> consensus . id = idLambda ( consensus . id ) <NEWLINE> <DEDENT>
account_numbers . append ( arn . account_number ) <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> arn = ARN ( princ_aws ) <NEWLINE> if arn . error : <NEWLINE> <INDENT> self . add_issue ( 3 , <STRING> , snsitem , notes = princ_aws ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> account_numbers . append ( arn . account_number ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if fallback is None and return_value is None : <NEWLINE> <INDENT> raise KeyError ( <STRING> % ( option , section ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> assert len ( actual_all_tasks . keys ( ) ) == 1 <NEWLINE> assert actual_all_tasks == expected_all_tasks <NEWLINE> mocked_get_required_params . assert_called_once_with ( <NEWLINE> <INDENT> region_name , launch_details . get ( <STRING> ) , launch_details . get ( <STRING> ) , launch_details . get ( <STRING> ) , account_id <NEWLINE> <DEDENT> ) <NEWLINE> mocked_get_parameters_for_launch . assert_called_once_with ( <NEWLINE> <INDENT> required_parameters , <NEWLINE> deployment_map , <NEWLINE> manifest , <NEWLINE> launch_details , <NEWLINE> account_id , <NEWLINE> launch_details . get ( <STRING> , constants . PROVISIONED ) , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
ymin = log10 ( ymin ) <NEWLINE>
class DecodedSoundFile : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , name : str , nchannels : int , sample_rate : int , sample_width : int , <NEWLINE> <INDENT> sample_format : int , samples : array . array ) -> None : <NEWLINE> self . name = name <NEWLINE> self . nchannels = nchannels <NEWLINE> self . sample_rate = sample_rate <NEWLINE> self . sample_width = sample_width <NEWLINE> self . sample_format = sample_format <COMMENT> <NEWLINE> self . sample_format_name = ffi . string ( lib . ma_get_format_name ( sample_format ) ) . decode ( ) <NEWLINE> self . samples = samples <NEWLINE> self . num_frames = len ( samples ) // self . nchannels <NEWLINE> self . duration = self . num_frames / self . sample_rate <NEWLINE> <DEDENT> <DEDENT>
def test_if_constant_bool ( self ) : <NEWLINE> <INDENT> a = True <NEWLINE> b = array ( [ 1 , 2 ] ) <NEWLINE> c = array ( [ 3 , 4 ] ) <NEWLINE> res = evaluate ( <STRING> ) <NEWLINE> assert_equal ( res , b ) <NEWLINE> a = False <NEWLINE> res = evaluate ( <STRING> ) <NEWLINE> assert_equal ( res , c ) <NEWLINE> <DEDENT>
async def _sender_loop ( self ) : <NEWLINE> <INDENT> while not self . _ender . done ( ) : <NEWLINE> <INDENT> if len ( self . _queue ) != 0 : <NEWLINE> <INDENT> await self . _send ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> k_to_delete = [ ] <NEWLINE> for k in ddata . keys ( ) : <NEWLINE> <INDENT> if k . endswith ( <STRING> ) : <NEWLINE> <INDENT> k_stripped = k . replace ( <STRING> , <STRING> ) <NEWLINE> if k_stripped in smp : <NEWLINE> <INDENT> smp [ k_stripped ] = pd . Categorical . from_codes ( <NEWLINE> <INDENT> codes = smp [ k_stripped ] . values , <NEWLINE> categories = ddata [ k ] ) <NEWLINE> <DEDENT> <DEDENT> if k_stripped in var : <NEWLINE> <INDENT> var [ k_stripped ] = pd . Categorical . from_codes ( <NEWLINE> <INDENT> codes = var [ k_stripped ] . values , <NEWLINE> categories = ddata [ k ] ) <NEWLINE> <DEDENT> <DEDENT> k_to_delete . append ( k ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if write_obsm_varm : <NEWLINE> <INDENT> for key in adata . obsm . keys ( ) : <NEWLINE> <INDENT> col_attrs [ key ] = adata . obsm [ key ] <NEWLINE> <DEDENT> for key in adata . varm . keys ( ) : <NEWLINE> <INDENT> row_attrs [ key ] = adata . varm [ key ] <NEWLINE> elif len ( adata . obsm . keys ( ) ) > 0 or len ( adata . varm . keys ( ) ) > 0 : <NEWLINE> <DEDENT> logger . warning ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
markers = ut . find_corr_genes ( gene , txt ) . flatten ( ) <NEWLINE> <INDENT> _ , i = np . unique ( markers , return_index = True ) <NEWLINE> markers = markers [ np . sort ( i ) ] <NEWLINE> self . marker_genes [ self . stab . selected_index ] = markers <NEWLINE> <DEDENT>
V = [ ] ; M = [ ] <NEWLINE> <INDENT> for s in S : <NEWLINE> <INDENT> if not case_sensitive : <NEWLINE> <INDENT> s = s . lower ( ) <NEWLINE> <DEDENT> for i in range ( len ( vec ) ) : <NEWLINE> <INDENT> if case_sensitive : <NEWLINE> <INDENT> st = vec [ i ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> st = vec [ i ] . lower ( ) <NEWLINE> <DEDENT> b = st . find ( s ) <NEWLINE> if not invert and b != - 1 or invert and b == - 1 : <NEWLINE> <INDENT> m . append ( i ) <NEWLINE> <DEDENT> <DEDENT> if len ( m ) > 0 : <NEWLINE> <INDENT> V . append ( vec [ np . array ( m ) ] ) ; M . append ( np . array ( m ) ) <NEWLINE> <DEDENT> <DEDENT> if len ( V ) > 0 : <NEWLINE> <INDENT> i = len ( V ) <NEWLINE> V = np . concatenate ( V ) ; M = np . concatenate ( M ) ; <NEWLINE> if i > 1 : <NEWLINE> <INDENT> ix = np . sort ( np . unique ( M , return_index = True ) [ 1 ] ) <NEWLINE> V = V [ ix ] ; M = M [ ix ] ; <NEWLINE> <DEDENT> return V , M <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return - 1 , - 1 <NEWLINE> <DEDENT> <DEDENT>
def build_help_menus ( self ) : <NEWLINE> <INDENT> menu_help = tk . Menu ( self . menubar , tearoff = 0 ) <NEWLINE> self . menubar . add_cascade ( menu = menu_help , label = <STRING> , underline = 0 ) <NEWLINE> menu_help . add_command ( label = <STRING> ) <NEWLINE> menu_help . add_separator ( ) <NEWLINE> if not getattr ( sys , <STRING> , False ) : <NEWLINE> <INDENT> menu_help . add_command ( label = <STRING> , command = lambda : self . conf . upgrade_package ( logger = self . logger ) ) <NEWLINE> menu_help . add_separator ( ) <NEWLINE> <DEDENT> menu_help . add_command ( label = <STRING> , command = self . about_msg ) <NEWLINE> <DEDENT>
nb_batches = ( len ( tgt_list ) + mb_size - 1 ) // mb_size <NEWLINE> <INDENT> for num_batch in six . moves . range ( nb_batches ) : <NEWLINE> <INDENT> tgt_batch , arg_sort = utils . make_batch_tgt ( tgt_list [ num_batch * nb_batches : ( num_batch + 1 ) * nb_batches ] , <NEWLINE> <INDENT> eos_idx = eos_idx , gpu = gpu , volatile = <STRING> , need_arg_sort = True ) <NEWLINE> <DEDENT> scores , attn = scorer ( tgt_batch ) <NEWLINE> scores , _ = scores <NEWLINE> scores = scores . data <NEWLINE> <DEDENT> <DEDENT>
nb_batches = ( len ( tgt_list ) + mb_size - 1 ) // mb_size <NEWLINE> <INDENT> for num_batch in six . moves . range ( nb_batches ) : <NEWLINE> <INDENT> tgt_batch , arg_sort = utils . make_batch_tgt ( tgt_list [ num_batch * nb_batches : ( num_batch + 1 ) * nb_batches ] , <NEWLINE> <INDENT> eos_idx = eos_idx , gpu = gpu , volatile = <STRING> , need_arg_sort = True ) <NEWLINE> <DEDENT> scores , attn = scorer ( tgt_batch ) <NEWLINE> scores , _ = scores <NEWLINE> scores = scores . data <NEWLINE> <DEDENT> <DEDENT>
constraints_list = [ ] <NEWLINE> <INDENT> for sentence_src in src : <NEWLINE> <COMMENT> <NL> <INDENT> seq_src = src_pp . convert ( sentence_src , stats = stats_src ) <NEWLINE> if make_constraints is not None : <NEWLINE> <INDENT> constraints_fn = make_constraints ( sentence_src , seq_src ) <NEWLINE> constraints_list . append ( constraints_fn ) <NEWLINE> <DEDENT> res . append ( seq_src ) <NEWLINE> <DEDENT> if make_constraints is not None : <NEWLINE> <INDENT> return res , stats_src , constraints_list <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return res , stats_src <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if sids_intersection <= 0 : <NEWLINE> <INDENT> option_parser . error ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for g , rows in report_rows : <NEWLINE> <INDENT> if g : <NEWLINE> <INDENT> sheet1 . write ( row_index , 0 , <STRING> % g , stylebold ) <NEWLINE> row_index += 1 <NEWLINE> <DEDENT> for row in list ( rows ) : <NEWLINE> <INDENT> if row . is_value ( ) : <NEWLINE> <INDENT> for index , x in enumerate ( row ) : <NEWLINE> <INDENT> if isinstance ( x . value , ( list , tuple ) ) : <NEWLINE> <INDENT> xvalue = <STRING> . join ( [ <STRING> % v for v in x . value ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> xvalue = x . text ( ) <NEWLINE> <DEDENT> sheet1 . write ( row_index , index , xvalue , stylevalue ) <NEWLINE> <DEDENT> row_index += 1 <NEWLINE> <DEDENT> elif row . is_caption : <NEWLINE> <INDENT> for index , x in enumerate ( row ) : <NEWLINE> <INDENT> if not isinstance ( x , ( unicode , str ) ) : <NEWLINE> <INDENT> sheet1 . write ( row_index , index , x . text ( ) , stylebold ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sheet1 . write ( row_index , index , x , stylebold ) <NEWLINE> <DEDENT> <DEDENT> row_index += 1 <NEWLINE> <DEDENT> elif row . is_total : <NEWLINE> <INDENT> for index , x in enumerate ( row ) : <NEWLINE> <INDENT> sheet1 . write ( row_index , index , x . text ( ) , stylebold ) <NEWLINE> sheet1 . write ( row_index + 1 , index , <STRING> ) <NEWLINE> <DEDENT> row_index += 2 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
time_now = time . time ( ) <NEWLINE> <INDENT> if time_now > timestamp_last + 300 : <COMMENT> <NEWLINE> <INDENT> difficulty2 = percentage ( 97 , difficulty ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> difficulty2 = difficulty <NEWLINE> <DEDENT> <DEDENT>
if nodes_ban_reset and len ( connection_pool ) <= len ( banlist ) and int ( time . time ( ) - reset_time ) > 60 * 10 : <COMMENT> <NEWLINE> <INDENT> app_log . warning ( <STRING> . format ( len ( connection_pool ) , len ( banlist ) ) ) <NEWLINE> del banlist [ : ] <NEWLINE> banlist . extend ( config . banlist ) <COMMENT> <NEWLINE> del warning_list [ : ] <NEWLINE> del tried [ : ] <NEWLINE> reset_time = time . time ( ) <NEWLINE> <DEDENT>
html . append ( <STRING> ) <NEWLINE> <INDENT> html . append ( <STRING> . format ( data_total ) ) <NEWLINE> html . append ( <STRING> . format ( tx_count ) ) <NEWLINE> html . append ( <STRING> . format ( tx_count / 500 ) ) <NEWLINE> html . append ( <STRING> . format ( transferred_total ) ) <NEWLINE> <DEDENT>
if block_height >= 427000 : <COMMENT> <NEWLINE> <INDENT> execute ( c , <STRING> ) <NEWLINE> result = c . fetchone ( ) <NEWLINE> timestamp_last = Decimal ( result [ 1 ] ) <NEWLINE> block_height = int ( result [ 0 ] ) <NEWLINE> timestamp_before_last = Decimal ( c . fetchone ( ) [ 1 ] ) <NEWLINE> <DEDENT>
address = blake2b ( pubkey . to_string ( ) , digest_size = 20 ) . hexdigest ( ) <NEWLINE>
address = blake2b ( privkey . to_string ( ) , digest_size = 20 ) . hexdigest ( ) <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> balance_pre = ledger_balance3 ( db_address , c , balances ) <NEWLINE> <COMMENT> <NL> balance = quantize_eight ( balance_pre - block_debit_address ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>
if self . __output_filter == <STRING> : <NEWLINE> <INDENT> self . __output += self . __p . communicate ( ) [ 0 ] . decode ( <STRING> ) <COMMENT> <NEWLINE> else : <NEWLINE> if str ( self . __p . communicate ( ) [ 0 ] . decode ( <STRING> ) ) . find ( self . __output_filter ) == - 1 : <NEWLINE> <INDENT> self . __output += self . __p . communicate ( ) [ 0 ] . decode ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
@ figsize . setter <NEWLINE> <INDENT> def figsize ( self , size : Tuple [ float , float ] ) : <NEWLINE> <INDENT> if ( <NEWLINE> <INDENT> not isinstance ( size , Sequence ) <NEWLINE> or len ( size ) != 2 <NEWLINE> or not all ( isinstance ( x , Real ) and x > 0 for x in size ) <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> size = tuple ( size ) <NEWLINE> if self . figsize != size : <NEWLINE> <INDENT> self . _pltkwargs [ <STRING> ] = size <NEWLINE> if self . _fig is not None : <NEWLINE> <INDENT> self . _fig . set_size_inches ( size ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def populateSubstitutionGroups ( self , elements ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> length = len ( elements ) <NEWLINE> for i , element in enumerate ( elements ) : <NEWLINE> <INDENT> if <STRING> in element . getAttrs ( ) : <NEWLINE> <INDENT> substitutionGroup = element . getAttrs ( ) [ <STRING> ] <NEWLINE> base = element . getBase ( ) <NEWLINE> self . opts . lang . abstract_type_map [ substitutionGroup ] = base <NEWLINE> self . opts . lang . substitutionGroup_map [ base ] = substitutionGroup <NEWLINE> <DEDENT> <DEDENT> for i , element in enumerate ( elements ) : <NEWLINE> <INDENT> if self . opts . lang . hasSubstitutionGroup ( element . getName ( ) ) : <NEWLINE> <INDENT> substitutionGroupName = self . opts . lang . substitutionGroup ( element . getName ( ) ) <NEWLINE> self . substitutionElement_map [ substitutionGroupName ] = element <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> if len ( self . opts . lang . getSubstitutionTypes ( ) ) > 0 : <NEWLINE> <INDENT> config . METADATA_OBJECT_IGNORE . remove ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def validate ( self ) : <NEWLINE> <INDENT> sizeC = int ( self . data [ <STRING> ] [ <STRING> ] ) <NEWLINE> assert ( len ( self . data [ <STRING> ] ) <= sizeC ) , str ( self . data ) <NEWLINE> channel_samples = sum ( [ int ( x . data [ <STRING> ] ) <NEWLINE> <INDENT> for x in self . data [ <STRING> ] ] ) <NEWLINE> <DEDENT> assert channel_samples <= sizeC , str ( self . data ) <NEWLINE> return self . data <NEWLINE> <DEDENT>
rightNodeSummary = self . tree_node_summary ( w_r , y_r , <NEWLINE> <INDENT> min_samples_treatment = min_samples_treatment , <NEWLINE> n_reg = n_reg , <NEWLINE> parentNodeSummary = currentNodeSummary ) <NEWLINE> <DEDENT>
after_first_commit = get_log_version ( cwd ) <NEWLINE>
if realdirpath not in scm_dirs : <NEWLINE> <COMMENT> <NL> <INDENT> dirnames [ : ] = [ ] <NEWLINE> continue <NEWLINE> if os . path . islink ( dirpath ) and not os . path . relpath ( <NEWLINE> realdirpath , realpath <NEWLINE> ) . startswith ( os . pardir ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> res . append ( os . path . join ( path , os . path . relpath ( dirpath , path ) ) ) <NEWLINE> dirnames [ : ] = [ ] <NEWLINE> continue <NEWLINE> if realdirpath in seen : <NEWLINE> <COMMENT> <NL> dirnames [ : ] = [ ] <NEWLINE> continue <NEWLINE> dirnames [ : ] = [ dn for dn in dirnames if not _link_not_in_scm ( dn ) ] <NEWLINE> for filename in filenames : <NEWLINE> if _link_not_in_scm ( filename ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> fullfilename = os . path . join ( dirpath , filename ) <NEWLINE> if os . path . normcase ( os . path . realpath ( fullfilename ) ) in scm_files : <NEWLINE> <INDENT> res . append ( os . path . join ( path , os . path . relpath ( fullfilename , realpath ) ) ) <NEWLINE> seen . add ( realdirpath ) <NEWLINE> return res <NEWLINE> <DEDENT> <DEDENT>
if ( fn [ <STRING> ] >= fs [ <STRING> ] ) : <NEWLINE>
else : <NEWLINE> <INDENT> _logger . debug ( <STRING> . format ( result , delivery_state ) ) <NEWLINE> message . state = constants . MessageState . SendComplete <NEWLINE> message . _response = errors . MessageAlreadySettled ( ) <COMMENT> <NEWLINE> if message . on_send_complete : <NEWLINE> message . on_send_complete ( result , exception ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> clock_rate = op . clock_rates [ mb_num ] <NEWLINE> if clock_rate is not None : <NEWLINE> <INDENT> u . set_clock_rate ( clock_rate , mb_num ) <NEWLINE> <DEDENT> op . clock_rates [ mb_num ] = u . get_clock_rate ( mb_num ) <NEWLINE> <DEDENT>
i = 0 <NEWLINE> <INDENT> while i <= w + x : <NEWLINE> <INDENT> virtual . set_position ( ( i , 0 ) ) <NEWLINE> regulator . sleep ( ) <NEWLINE> i += 1 <NEWLINE> <DEDENT> <DEDENT>
def error ( <NEWLINE> <INDENT> self , <NEWLINE> statement , <NEWLINE> message = None , <NEWLINE> variable = None , <NEWLINE> line = None , <NEWLINE> column = None , <NEWLINE> ) : <NEWLINE> if not message : <NEWLINE> <INDENT> message = self . assign_msg <NEWLINE> <DEDENT> if not variable : <NEWLINE> <INDENT> variable = statement . id <NEWLINE> <DEDENT> if not line : <NEWLINE> <INDENT> line = statement . lineno <NEWLINE> <DEDENT> if not column : <NEWLINE> <INDENT> column = statement . col_offset <NEWLINE> <DEDENT> <DEDENT>
if isinstance ( obj , h5py . Dataset ) : <NEWLINE> <INDENT> detail = <STRING> . format ( <NEWLINE> <INDENT> dt = fmt_dtype ( obj . id . get_type ( ) ) , <NEWLINE> shape = fmt_shape ( obj . shape ) , <NEWLINE> <DEDENT> ) <NEWLINE> if obj . id . get_create_plist ( ) . get_layout ( ) == h5py . h5d . VIRTUAL : <NEWLINE> <INDENT> detail += <STRING> <NEWLINE> elif isinstance ( obj , h5py . Group ) : <NEWLINE> <DEDENT> if max_depth >= 1 : <NEWLINE> <INDENT> children += [ self . group_item_node ( obj , key , max_depth - 1 ) <NEWLINE> <INDENT> for key in obj ] <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> detail = <STRING> <NEWLINE> else : <NEWLINE> <DEDENT> detail = <STRING> <NEWLINE> <DEDENT>
def _sort_query ( self , query , sort , order ) : <NEWLINE> <INDENT> criteria = [ ] <NEWLINE> for field in self . _list_fields : <NEWLINE> <INDENT> if field . id ( ) == sort : <NEWLINE> <INDENT> criterion = field . sort_column ( ) <NEWLINE> if order == <STRING> : <NEWLINE> <INDENT> criterion = desc ( criterion ) <NEWLINE> <DEDENT> criteria . append ( criterion ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _verify_checkout ( <NEWLINE> <INDENT> replica : Replica , token : typing . Optional [ str ] , file_metadata : dict , blob_path : str , <NEWLINE> ) -> typing . Tuple [ str , bool ] : <NEWLINE> decoded_token : dict <NEWLINE> if token is None : <NEWLINE> execution_id = start_file_checkout ( replica , blob_path ) <NEWLINE> start_time = time . time ( ) <NEWLINE> attempts = 0 <NEWLINE> <DEDENT>
@ elasticsearch_retry ( logger , timeout ) <NEWLINE> <INDENT> def remove_bundle ( self , bundle : Bundle , tombstone : Tombstone ) : <NEWLINE> <INDENT> elasticsearch_retry . add_context ( tombstone = tombstone , bundle = bundle ) <NEWLINE> doc = BundleDocument . from_bundle ( bundle ) <NEWLINE> tombstone_doc = BundleTombstoneDocument . from_tombstone ( tombstone ) <NEWLINE> modified , index_name = doc . entomb ( tombstone_doc , dryrun = self . dryrun ) <NEWLINE> if self . notify or modified and self . notify is None : <NEWLINE> <INDENT> self . _notify ( tombstone_doc , index_name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if type ( c ) . __bases__ [ 0 ] == cmp . component : <NEWLINE> <INDENT> msg = ( <STRING> <NEWLINE> <INDENT> <STRING> + <NEWLINE> str ( type ( c ) ) + <STRING> ) <NEWLINE> <DEDENT> raise TypeError ( msg ) <NEWLINE> else : <NEWLINE> msg = ( <STRING> <NEWLINE> <INDENT> <STRING> + <NEWLINE> str ( type ( c ) . __bases__ [ 0 ] ) + <STRING> ) <NEWLINE> <DEDENT> raise TypeError ( msg ) <NEWLINE> return True <NEWLINE> return True <NEWLINE> <DEDENT>
message = CallRequestMessage ( <NEWLINE> <INDENT> service = self . service , <NEWLINE> args = [ safebytes ( arg_1 ) , arg_2 , arg_3 ] , <NEWLINE> ) <NEWLINE> <DEDENT>
if self . _number_of_images >= IMAGES_NUM_LIMIT * ( i + 1 ) : <NEWLINE> <INDENT> num = IMAGES_NUM_LIMIT <NEWLINE> else : <NEWLINE> num = ( self . _number_of_images % IMAGES_NUM_LIMIT ) or self . _number_of_images <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> with open ( credfile , <STRING> ) as f : <NEWLINE> <INDENT> creds = json . load ( f ) <NEWLINE> except json . JSONDecodeError as e : <NEWLINE> <DEDENT> logger . error ( <NEWLINE> <INDENT> <STRING> . format ( self , credfile ) <NEWLINE> <DEDENT> ) <NEWLINE> raise e <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> data_manager_site = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : None <NEWLINE> <DEDENT> } , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : None , <NEWLINE> <STRING> : 8 <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> } <NEWLINE> config_base [ <STRING> ] . append ( data_manager_site ) <NEWLINE> <DEDENT>
app_fut = dfk . submit ( func , app_args = args , <NEWLINE> <INDENT> executors = self . executors , <NEWLINE> fn_hash = self . func_hash , <NEWLINE> cache = self . cache , <NEWLINE> ignore_for_cache = self . ignore_for_cache , <NEWLINE> app_kwargs = invocation_kwargs ) <NEWLINE> <DEDENT>
windows = mw . get_sliding_window_boundaries ( start_time = st , stop_time = et , window_duration = ws , step_size = ss ) <NEWLINE> <INDENT> chunk_windows_mask = ( windows [ : , 0 ] >= data_start_indicator ) & ( windows [ : , 0 ] < data_stop_indicator ) <NEWLINE> chunk_windows = windows [ chunk_windows_mask , : ] <NEWLINE> if len ( chunk_windows ) == 0 : <NEWLINE> <INDENT> return pd . DataFrame ( ) <NEWLINE> <DEDENT> result_data = mw . apply_to_sliding_windows ( df = combined_data , sliding_windows = chunk_windows , window_operations = features , operation_names = feature_names , return_dataframe = True ) <NEWLINE> return result_data <NEWLINE> <DEDENT>
class Drawing : <NEWLINE> <INDENT> def __init__ ( self , tagreader ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _dxfversion = <STRING> <COMMENT> <NEWLINE> self . encoding = <STRING> <COMMENT> <NEWLINE> self . filename = None <COMMENT> <NEWLINE> self . entitydb = EntityDB ( ) <NEWLINE> self . sections = Sections ( tagreader , self ) <NEWLINE> self . _dxfversion = self . header [ <STRING> ] <NEWLINE> self . encoding = self . _get_encoding ( ) <NEWLINE> nexthandle = int ( self . header . get ( <STRING> , <STRING> ) , 16 ) <NEWLINE> self . handlegenerator = HandleGenerator ( startvalue = nexthandle ) <NEWLINE> self . dxfengine = dxfengine ( self . _dxfversion , self ) <NEWLINE> <DEDENT> <DEDENT>
if name in self : <NEWLINE> <INDENT> super ( ) . delete ( name ) <NEWLINE> else : <NEWLINE> raise DXFValueError ( <STRING> ) <NEWLINE> <DEDENT>
def set_edge_visibilty ( self , num , status = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not status : <NEWLINE> <INDENT> self . dxf . invisible = self . dxf . invisible | ( 1 << num ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . dxf . invisible = self . dxf . invisible & ~ ( 1 << num ) <NEWLINE> <DEDENT> <DEDENT>
if block_ref . has_uniform_scaling and xscale < 0 : <NEWLINE> <COMMENT> <NL> <INDENT> has_non_uniform_scaling = False <NEWLINE> <DEDENT>
if vertices : <NEWLINE> <INDENT> if not last_vertex . isclose ( vertices [ 0 ] ) : <NEWLINE> <INDENT> vertices . append ( last_vertex ) <NEWLINE> <DEDENT> self . out . draw_filled_polygon ( vertices , properties ) <NEWLINE> <DEDENT>
return sum ( <NEWLINE> <INDENT> ( p2 . x - p1 . x ) * ( p2 . y + p1 . y ) <NEWLINE> for p1 , p2 in zip ( vertices , vertices [ 1 : ] ) <NEWLINE> ) > 0 <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if image_thumbnail : <NEWLINE> <INDENT> images . append ( image_thumbnail ) <NEWLINE> <COMMENT> <NL> for index , image in enumerate ( images ) : <NEWLINE> <COMMENT> <NL> <INDENT> image = image . partition ( <STRING> ) [ 0 ] <NEWLINE> image_type = mimetypes . guess_type ( image ) [ 0 ] <NEWLINE> if image_type is not None : <NEWLINE> <INDENT> image_type = <STRING> + image_type . split ( <STRING> ) [ - 1 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> image_type = <STRING> <COMMENT> <NEWLINE> <COMMENT> <NL> <DEDENT> if image_type == <STRING> and image_type not in image : <NEWLINE> <INDENT> image_type = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> image = <STRING> . join ( image . partition ( image_type ) [ : 2 ] ) <NEWLINE> images [ index ] = image . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> <DEDENT> self . _images = images <NEWLINE> return self . _images <NEWLINE> <DEDENT>
outputs = model ( images_val ) <NEWLINE> <INDENT> val_loss = loss_fn ( input = outputs , target = labels_val ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if scale_weight is None : <COMMENT> <NEWLINE> <INDENT> n_inp = len ( input ) <NEWLINE> scale = 0.4 <NEWLINE> scale_weight = torch . pow ( scale * torch . ones ( n_inp ) , torch . arange ( n_inp ) . float ( ) ) . to ( <NEWLINE> <INDENT> target . device <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if len ( _tmatrix_disconnected . closed_sets ( C ) ) < nstates : <NEWLINE> <INDENT> msm_prior = 0.001 <NEWLINE> B = msm_prior * np . eye ( C . shape [ 0 ] ) <COMMENT> <NEWLINE> B += msmtools . estimation . prior_neighbor ( C , alpha = msm_prior ) <COMMENT> <NEWLINE> C_post = C + B <COMMENT> <NEWLINE> P_for_pcca = _tmatrix_disconnected . estimate_P ( C_post , reversible = True ) <NEWLINE> <DEDENT> elif reversible : <COMMENT> <NEWLINE> <INDENT> P_for_pcca = P_msm <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> P_for_pcca = _tmatrix_disconnected . estimate_P ( C , reversible = True ) <NEWLINE> <DEDENT> <DEDENT>
def test_findall ( self ) : <NEWLINE> <INDENT> mgr = self . manager <NEWLINE> o1 = fakes . FakeEntity ( ) <NEWLINE> o1 . some_att = <STRING> <NEWLINE> o2 = fakes . FakeEntity ( ) <NEWLINE> o2 . some_att = <STRING> <NEWLINE> o3 = fakes . FakeEntity ( ) <NEWLINE> o3 . some_att = <STRING> <NEWLINE> sav = mgr . list <NEWLINE> mgr . list = Mock ( return_value = [ o1 , o2 , o3 ] ) <NEWLINE> ret = mgr . findall ( some_att = <STRING> ) <NEWLINE> self . assertTrue ( o1 in ret ) <NEWLINE> self . assertFalse ( o2 in ret ) <NEWLINE> self . assertTrue ( o3 in ret ) <NEWLINE> mgr . list = sav <NEWLINE> <DEDENT>
self . __read_page ( r . json ( ) , self . __cmd . start_date , self . __cmd . end_date ) <NEWLINE> <INDENT> page += 1 <NEWLINE> params [ <STRING> ] = str ( page ) <NEWLINE> <DEDENT>
def get_vCloudDirector ( self , serviceId , vdcId ) : <NEWLINE> <INDENT> vdcReference = self . get_vdcReference ( serviceId , vdcId ) <NEWLINE> if vdcReference [ 0 ] == True : <NEWLINE> <COMMENT> <NL> <INDENT> vCloudSession = self . create_vCloudSession ( vdcReference [ 1 ] ) <NEWLINE> if vCloudSession : <NEWLINE> <INDENT> vcd = VCD ( vCloudSession , serviceId , vdcId ) <NEWLINE> return vcd <NEWLINE> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>
policy_references = self . _fetch_compute_policies ( ) <NEWLINE> <INDENT> policy_list = [ ] <NEWLINE> for policy_reference in policy_references . VdcComputePolicyReference : <NEWLINE> <INDENT> policy_list . append ( policy_reference ) <NEWLINE> <DEDENT> return policy_list <NEWLINE> <DEDENT>
def validate_params ( ctx , param , value ) : <NEWLINE> <INDENT> if any ( [ <STRING> not in item for item in value ] ) : <NEWLINE> <INDENT> raise click . BadParameter ( <STRING> ) <NEWLINE> <DEDENT> return dict ( [ tuple ( item . split ( <STRING> , 1 ) ) for item in value ] ) <NEWLINE> <DEDENT>
@ click . command ( help = <STRING> ) <NEWLINE> <INDENT> @ click . argument ( <STRING> , nargs = 1 ) <NEWLINE> @ click . argument ( <STRING> , nargs = 1 ) <NEWLINE> @ click . option ( <STRING> , metavar = <STRING> , help = <STRING> , default = <STRING> , type = click . Choice ( [ <STRING> , <STRING> ] ) ) <NEWLINE> def credentials_add ( domain , credentials_string , auth ) : <NEWLINE> <INDENT> if auth == <STRING> : <NEWLINE> <INDENT> header = credentials_string <NEWLINE> <DEDENT> elif auth == <STRING> : <NEWLINE> <INDENT> header = <STRING> + b64encode ( credentials_string ) <NEWLINE> <DEDENT> credentials = get_credentials ( ) <NEWLINE> credentials [ domain ] = header <NEWLINE> set_credentials ( credentials ) <NEWLINE> <DEDENT> <DEDENT>
def temporary_store_decorator ( config_files_directory = default_config_files_directory , file_name = None ) : <NEWLINE> <INDENT> parser = SafeConfigParser ( ) <NEWLINE> config_local_ini = os . path . join ( config_files_directory , <STRING> ) <NEWLINE> config_ini = os . path . join ( config_files_directory , <STRING> ) <NEWLINE> read_config_file_name = parser . read ( [ config_ini , config_local_ini ] ) <NEWLINE> tmp_directory = parser . get ( <STRING> , <STRING> ) <NEWLINE> assert tmp_directory is not None , <STRING> . format ( tmp_directory , read_config_file_name ) <NEWLINE> assert os . path . isabs ( tmp_directory ) , <STRING> . format ( tmp_directory , read_config_file_name ) <NEWLINE> if not os . path . isdir ( tmp_directory ) : <NEWLINE> <INDENT> <STRING> . format ( tmp_directory , read_config_file_name ) <NEWLINE> os . makedirs ( tmp_directory ) <NEWLINE> <DEDENT> <DEDENT>
if not v and update_installed is not None : <NEWLINE> <INDENT> v = latestSuitableVersion ( name , version_required , registry = <STRING> ) <NEWLINE> <DEDENT>
if util . canBuildNatively ( ) : <NEWLINE> <INDENT> forAllReporterTests ( generateTest ) <NEWLINE> else : <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT>
def register_command ( self , func , * args , ** kwargs ) : <NEWLINE> <INDENT> wrapped = functools . partial ( self . _dispatch , <STRING> , func ) <NEWLINE> self . commands [ func . __name__ ] = Command ( self , wrapped , * args , ** kwargs ) <NEWLINE> <DEDENT>
if self . can ( self . client . state . me , Permissions . MANAGE_MESSAGES ) and len ( messages ) > 2 : <NEWLINE> <INDENT> for chunk in chunks ( message_ids , 100 ) : <NEWLINE> <INDENT> self . client . api . channels_messages_delete_bulk ( self . id , chunk ) <NEWLINE> else : <NEWLINE> <DEDENT> for msg in messages : <NEWLINE> <INDENT> self . delete_message ( msg ) <NEWLINE> <DEDENT> <DEDENT>
args = get_cli_arguments ( ) <NEWLINE> <INDENT> parse_args = args . parse_args ( ) <NEWLINE> if parse_args . cli : <NEWLINE> <INDENT> cli = Cli ( args ) <NEWLINE> <COMMENT> <NL> exit ( 0 if cli . status else 1 ) <NEWLINE> <DEDENT> <DEDENT>
def get_driver ( self ) : <NEWLINE> <INDENT> driver_path = self . _driver_path ( ) <NEWLINE> if not is_file ( driver_path ) : <NEWLINE> <INDENT> self . download_drivder ( ) <NEWLINE> <DEDENT> self . is_win ( ) or chmod ( driver_path , 0o755 ) <NEWLINE> driver = webdriver . Chrome ( executable_path = driver_path ) <NEWLINE> driver . set_window_size ( 500 , 600 ) <NEWLINE> return driver <NEWLINE> <DEDENT>
def _translate_gapped ( seq , * args , ** kwds ) : <NEWLINE> <INDENT> if isinstance ( seq , SeqRecord ) : <NEWLINE> <INDENT> s = str ( seq . seq ) <NEWLINE> <DEDENT> elif isinstance ( seq , Seq ) : <NEWLINE> <INDENT> s = str ( seq ) <NEWLINE> <DEDENT> elif isinstance ( seq , str ) : <NEWLINE> <INDENT> s = seq <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> gaps = 0 <NEWLINE> lwr = 0 <NEWLINE> protein = <STRING> <NEWLINE> for i in range ( 0 , len ( s ) , 3 ) : <NEWLINE> <INDENT> j = min ( i + 3 , len ( s ) ) <NEWLINE> if s [ i : j ] == <STRING> [ : j - i ] : <NEWLINE> <INDENT> if not gaps : <NEWLINE> <INDENT> protein += _translate ( s [ lwr : i ] ) <NEWLINE> <DEDENT> gaps += 1 <NEWLINE> <DEDENT> elif gaps : <NEWLINE> <INDENT> protein += <STRING> * gaps <NEWLINE> gaps = 0 <NEWLINE> lwr = i <NEWLINE> <DEDENT> <DEDENT> if gaps : <NEWLINE> <INDENT> protein += <STRING> * gaps <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> protein += _translate ( s [ lwr : len ( seq ) ] ) <NEWLINE> <DEDENT> return protein <NEWLINE> <DEDENT>
old_sigalrm = signal . signal ( signal . SIGALRM , on_alarm ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> signal . alarm ( 1 ) <NEWLINE> sync_wait_reapable ( sleeper . pid ) <NEWLINE> assert sleeper . wait ( timeout = 1 ) == - 9 <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> if sleeper . returncode is None : <NEWLINE> <INDENT> sleeper . kill ( ) <NEWLINE> sleeper . wait ( ) <NEWLINE> <DEDENT> signal . signal ( signal . SIGALRM , old_sigalrm ) <NEWLINE> <DEDENT> <DEDENT>
deprecated_thing ( ) <NEWLINE> <INDENT> filename , lineno = _here ( ) <NEWLINE> assert len ( recwarn_always ) == 1 <NEWLINE> got = recwarn_always . pop ( TrioDeprecationWarning ) <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert <STRING> in got . message . args [ 0 ] <NEWLINE> assert got . filename == filename <NEWLINE> assert got . lineno == lineno - 1 <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> def ki_protection_enabled ( frame ) : <NEWLINE> <INDENT> while frame is not None : <NEWLINE> <INDENT> if LOCALS_KEY_KI_PROTECTION_ENABLED in frame . f_locals : <NEWLINE> <INDENT> return frame . f_locals [ LOCALS_KEY_KI_PROTECTION_ENABLED ] <NEWLINE> <DEDENT> if frame . f_code . co_name == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> frame = frame . f_back <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT>
if mode == <STRING> : <NEWLINE> <INDENT> import numpy as np <NEWLINE> array = np . empty ( ( height , width ) ) <NEWLINE> for row in range ( height ) : <NEWLINE> <INDENT> for column in range ( width ) : <NEWLINE> <INDENT> latitude = latitude_from + float ( row ) / height * ( latitude_to - latitude_from ) <NEWLINE> longitude = longitude_from + float ( column ) / width * ( longitude_to - longitude_from ) <NEWLINE> elevation = self . get_elevation ( latitude , longitude ) <NEWLINE> array [ row , column ] = elevation <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for row in range ( height ) : <NEWLINE> <INDENT> for column in range ( width ) : <NEWLINE> <INDENT> latitude = latitude_from + float ( row ) / height * ( latitude_to - latitude_from ) <NEWLINE> longitude = longitude_from + float ( column ) / width * ( longitude_to - longitude_from ) <NEWLINE> elevation = self . get_elevation ( latitude , longitude ) <NEWLINE> if elevation == None : <NEWLINE> <INDENT> color = unknown_color <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> elevation_coef = ( elevation - min_elevation ) / float ( max_elevation ) <NEWLINE> if elevation_coef < 0 : elevation_coef = 0 <NEWLINE> if elevation_coef > 1 : elevation_coef = 1 <NEWLINE> color = mod_utils . get_color_between ( min_color , max_color , elevation_coef ) <NEWLINE> if elevation <= 0 : <NEWLINE> <INDENT> color = zero_color <NEWLINE> <DEDENT> <DEDENT> draw . point ( ( column , height - row ) , color ) <NEWLINE> <DEDENT> <DEDENT>
def readf ( nb_file ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> file , ext = os . path . splitext ( nb_file ) <NEWLINE> if ext not in notebook_extensions : <NEWLINE> <INDENT> raise TypeError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> . format ( nb_file , <NEWLINE> <INDENT> notebook_extensions ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> with io . open ( nb_file , encoding = <STRING> ) as fp : <NEWLINE> <INDENT> return read ( fp , as_version = 4 , ext = ext ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> cm . rename ( tmp_nbpy , <STRING> ) <NEWLINE> assert not os . path . isfile ( str ( tmpdir . join ( tmp_ipynb ) ) ) <NEWLINE> assert not os . path . isfile ( str ( tmpdir . join ( tmp_nbpy ) ) ) <NEWLINE> <DEDENT>
def explicit_start_marker ( self , source ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . metadata : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if all ( [ line . startswith ( <STRING> ) for line in self . source ] ) : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if CellReader ( self . ext ) . read ( source ) [ 1 ] < len ( source ) : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT>
header = comment_lines ( header , text_format . header_prefix ) <NEWLINE> <INDENT> if lines_to_next_cell is None and notebook . cells : <NEWLINE> <INDENT> lines_to_next_cell = pep8_lines_between_cells ( header , notebook . cells [ 0 ] , ext ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> lines_to_next_cell = 1 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> assert os . path . getmtime ( tmp_ipynb ) <= os . path . getmtime ( tmp_py ) <NEWLINE> <DEDENT>
compare_notebooks ( nb2 , nb1 ) <NEWLINE>
compare_notebooks ( nb2 , nb1 ) <NEWLINE>
compare_notebooks ( nb2 , nb1 , <STRING> ) <NEWLINE>
if isinstance ( error , TextIOBase ) : <NEWLINE> <INDENT> self . _err = error <NEWLINE> else : <NEWLINE> self . _err = TextIOWrapper ( error ) <NEWLINE> <DEDENT>
if not ignore_options : <NEWLINE> <INDENT> options = ( <NEWLINE> <INDENT> Path ( <STRING> ) , <NEWLINE> failover , <COMMENT> <NEWLINE> <DEDENT> ) <NEWLINE> for option in options : <NEWLINE> <INDENT> if ( option . exists ( ) and <NEWLINE> <INDENT> os . access ( option , os . R_OK ) and <NEWLINE> option . stat ( ) . st_size > 0 ) : <NEWLINE> <INDENT> return option <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if batches >= 1 : <NEWLINE>
def test_estimate_virtual_op_num2 ( self ) : <NEWLINE> <INDENT> account = self . account <NEWLINE> h_all_raw = [ ] <NEWLINE> for h in account . history ( raw_output = False ) : <NEWLINE> <INDENT> h_all_raw . append ( h ) <NEWLINE> <DEDENT> last_block = h_all_raw [ 0 ] [ <STRING> ] <NEWLINE> i = 1 <NEWLINE> for op in h_all_raw [ 1 : ] : <NEWLINE> <INDENT> new_block = op [ <STRING> ] <NEWLINE> block_num = last_block + int ( ( new_block - last_block ) / 2 ) <NEWLINE> op_num = account . estimate_virtual_op_num ( block_num , stop_diff = 0.1 , max_count = 100 ) <NEWLINE> if op_num > 0 : <NEWLINE> <INDENT> op_num -= 1 <NEWLINE> <DEDENT> self . assertTrue ( op_num <= i ) <NEWLINE> i += 1 <NEWLINE> last_block = new_block <NEWLINE> <DEDENT> <DEDENT>
client = get_plugin ( <STRING> ) <NEWLINE> <INDENT> client . send_message ( topic = self . topic , msg = response , host = self . host , <NEWLINE> <INDENT> port = self . port , username = self . username , <NEWLINE> password = self . password , tls_cafile = self . tls_cafile , <NEWLINE> tls_certfile = self . tls_certfile , <NEWLINE> tls_keyfile = self . tls_keyfile , <NEWLINE> tls_version = self . tls_version , <NEWLINE> tls_ciphers = self . tls_ciphers ) <NEWLINE> <DEDENT> <DEDENT>
self . _last_read = sensors <NEWLINE> <INDENT> return ret <NEWLINE> <DEDENT>
with self . _init_lock : <NEWLINE> <INDENT> if self . _initialized and GPIO . getmode ( ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT>
def process_data ( self , data , new_data ) : <NEWLINE> <INDENT> if new_data : <NEWLINE> <INDENT> self . bus . post ( SensorDataChangeEvent ( data = new_data , source = self . plugin or self . __class__ . __name__ ) ) <NEWLINE> <DEDENT> <DEDENT>
if hasattr ( nb . metadata . latex_doc , <STRING> ) : <NEWLINE> <INDENT> bib = nb . metadata . latex_doc . bibliography <NEWLINE> bib = self . resolve_path ( bib , self . metapath ) <NEWLINE> if not os . path . exists ( bib ) : <NEWLINE> <INDENT> logging . warning ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( bib ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> external_files . append ( bib ) <NEWLINE> resources [ <STRING> ] = bib <NEWLINE> <DEDENT> <DEDENT>
elif name == <STRING> : <NEWLINE> <INDENT> if self . thread : <NEWLINE> <INDENT> mailcountstring = <STRING> % self . thread . get_total_messages ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> mailcountstring = <STRING> <NEWLINE> <DEDENT> mailcountstring = pad ( mailcountstring ) <NEWLINE> width = len ( mailcountstring ) <NEWLINE> mailcount_w = AttrFlipWidget ( urwid . Text ( mailcountstring ) , <NEWLINE> <INDENT> struct [ <STRING> ] ) <NEWLINE> <DEDENT> part = mailcount_w <NEWLINE> elif name == <STRING> : <NEWLINE> if self . thread : <NEWLINE> <INDENT> authors = self . thread . get_authors_string ( ) or <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> authors = <STRING> <NEWLINE> <DEDENT> authorsstring = pad ( authors , shorten_author_string ) <NEWLINE> authors_w = AttrFlipWidget ( urwid . Text ( authorsstring ) , <NEWLINE> <INDENT> struct [ <STRING> ] ) <NEWLINE> <DEDENT> width = len ( authorsstring ) <NEWLINE> part = authors_w <NEWLINE> <DEDENT>
def distance ( self , s0 , s1 ) : <NEWLINE> <INDENT> if s0 is None : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT> if s1 is None : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT> if s0 == s1 : <NEWLINE> <INDENT> return 0.0 <NEWLINE> <DEDENT> if len ( s0 ) == 0 : <NEWLINE> <INDENT> return len ( s1 ) <NEWLINE> <DEDENT> if len ( s1 ) == 0 : <NEWLINE> <INDENT> return len ( s0 ) <NEWLINE> <DEDENT> <DEDENT>
def check_new_log ( ) : <NEWLINE> <INDENT> log2 = open ( logpath ) . readlines ( ) <NEWLINE> return len ( log2 ) > 0 and log2 [ 0 ] . endswith ( <STRING> ) <NEWLINE> wait_for ( check_new_log ) <NEWLINE> <DEDENT>
