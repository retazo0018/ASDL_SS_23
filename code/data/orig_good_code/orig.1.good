def read ( self , io_in : BufferedIOBase , otherfields : Dict [ str , Any ] ) -> Optional [ Dict [ str , Any ] ] : <NEWLINE> <INDENT> vals = { } <NEWLINE> for field in self . fields : <NEWLINE> <INDENT> val = field . fieldtype . read ( io_in , vals ) <NEWLINE> if val is None : <NEWLINE> <COMMENT> <NL> <INDENT> if field == self . fields [ 0 ] : <NEWLINE> <INDENT> return None <NEWLINE> <COMMENT> <NL> <DEDENT> if field . option is not None : <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> raise ValueError ( <STRING> . format ( self , field ) ) <NEWLINE> <DEDENT> vals [ field . name ] = val <NEWLINE> <DEDENT> <DEDENT>
if self . tested == PyFunceble . CONFIGURATION [ <STRING> ] : <NEWLINE> <INDENT> url_to_get = <STRING> % self . tested <NEWLINE> else : <NEWLINE> url_to_get = self . tested <NEWLINE> <DEDENT>
if not path_to_config . endswith ( directory_separator ) : <NEWLINE> <INDENT> self . path_to_config += directory_separator <NEWLINE> <DEDENT>
if ( <NEWLINE> <INDENT> number_of_tested == 0 <NEWLINE> or list_to_test [ number_of_tested - 1 ] == list_to_test [ - 1 ] <NEWLINE> or number_of_tested >= len ( list_to_test ) <NEWLINE> ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>
if not is_cloned_version and ( <NEWLINE> <INDENT> PyFunceble . CONFIGURATION . db_type not in not_supported_db_types <NEWLINE> ) : <NEWLINE> destination_dir_instance . delete ( ) <NEWLINE> <DEDENT>
if not content : <NEWLINE> <INDENT> content = <STRING> <NEWLINE> continue <NEWLINE> <DEDENT>
with bz2 . BZ2File ( bad_pkg_path , <STRING> ) as f : <NEWLINE> <INDENT> f . write ( <STRING> . encode ( ) ) <NEWLINE> assert bad_pkg_name in os . listdir ( bad_pkg_root ) <NEWLINE> conda_mirror . _validate_packages ( repodata , bad_pkg_root ) <NEWLINE> assert bad_pkg_name not in os . listdir ( bad_pkg_root ) <NEWLINE> <DEDENT>
def handle_message ( self , msg ) : <NEWLINE> <INDENT> if self . categories . intersection ( msg . categories ) and msg . level >= self . level : <NEWLINE> <INDENT> self . notify ( msg ) <NEWLINE> <DEDENT> <DEDENT>
def _check_dates ( self , master , files ) : <NEWLINE> <INDENT> mtime = os . path . getmtime ( master ) <NEWLINE> for f in files : <NEWLINE> <INDENT> fpath = media_url_to_filepath ( f ) <NEWLINE> if os . path . getmtime ( fpath ) >= mtime : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>
if status != 200 : <NEWLINE> <INDENT> raise WebSocketProxyException ( <NEWLINE> <INDENT> <STRING> % status ) <NEWLINE> <DEDENT> <DEDENT>
with open ( filePathBase + <STRING> , <STRING> , encoding = <STRING> ) as toFile : <NEWLINE> <INDENT> write_header ( glos , toFile , frontBackMatter ) <NEWLINE> for entryI , entry in enumerate ( glos ) : <NEWLINE> <INDENT> if entry . isData ( ) : <NEWLINE> <INDENT> entry . save ( myResDir ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _readersEntryGen ( self ) -> Iterator [ BaseEntry ] : <NEWLINE> <INDENT> for reader in self . _readers : <NEWLINE> <INDENT> wordCount = 0 <NEWLINE> progressbar = False <NEWLINE> if self . ui and self . _progressbar : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> wordCount = len ( reader ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> log . exception ( <STRING> ) <NEWLINE> <DEDENT> if wordCount >= 0 : <NEWLINE> <INDENT> progressbar = True <NEWLINE> <DEDENT> <DEDENT> if progressbar : <NEWLINE> <INDENT> self . progressInit ( <STRING> ) <NEWLINE> <DEDENT> wcThreshold = wordCount // 200 + 1 <NEWLINE> lastPos = 0 <NEWLINE> try : <NEWLINE> <INDENT> for index , entry in enumerate ( reader ) : <NEWLINE> <INDENT> yield entry <NEWLINE> if progressbar : <NEWLINE> <INDENT> if entry is None or wordCount > 0 : <NEWLINE> <INDENT> if index % wcThreshold == 0 : <NEWLINE> <INDENT> self . progress ( index , wordCount ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> bp = entry . byteProgress ( ) <NEWLINE> if bp and bp [ 0 ] > lastPos + 10000 : <NEWLINE> <INDENT> self . progress ( bp [ 0 ] , bp [ 1 ] , unit = <STRING> ) <NEWLINE> lastPos = bp [ 0 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> reader . close ( ) <NEWLINE> <DEDENT> if progressbar : <NEWLINE> <INDENT> self . progressEnd ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def __next__ ( self ) -> BaseEntry : <NEWLINE> <INDENT> self . _pos += 1 <NEWLINE> try : <NEWLINE> <INDENT> return self . _pendingEntries . pop ( 0 ) <NEWLINE> <DEDENT> except IndexError : <NEWLINE> <INDENT> pass <NEWLINE> <COMMENT> <NL> <DEDENT> try : <NEWLINE> <INDENT> wordDefi = self . nextPair ( ) <NEWLINE> <DEDENT> except StopIteration as e : <NEWLINE> <INDENT> if self . _fileIndex < self . _fileCount - 1 : <NEWLINE> <INDENT> if self . openNextFile ( ) : <NEWLINE> <INDENT> return self . __next__ ( ) <NEWLINE> <DEDENT> <DEDENT> self . _wordCount = self . _pos <NEWLINE> raise e <NEWLINE> <DEDENT> if not wordDefi : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> word , defi = wordDefi <NEWLINE> <COMMENT> <NL> return Entry ( word , defi ) <NEWLINE> <DEDENT>
if list_observed_ip : <NEWLINE> <INDENT> bodie [ <STRING> ] = list ( set ( list_observed_ip ) ) <NEWLINE> <DEDENT>
return result <NEWLINE>
def get_blocks_container_html ( self , errors = None ) : <NEWLINE> <INDENT> help_text = getattr ( self . meta , <STRING> , None ) <NEWLINE> if isinstance ( errors , StreamBlockValidationError ) : <NEWLINE> <INDENT> non_block_errors = ( <NEWLINE> <INDENT> ( ) if errors is None <NEWLINE> else errors . as_data ( ) [ 0 ] . params . get ( NON_FIELD_ERRORS , ( ) ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> non_block_errors = errors <NEWLINE> <DEDENT> if help_text or non_block_errors : <NEWLINE> <INDENT> return render_to_string ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> { <NEWLINE> <INDENT> <STRING> : help_text , <NEWLINE> <STRING> : non_block_errors , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
elif qmean == <STRING> : <COMMENT> <NEWLINE> <INDENT> pca = sklearn . decomposition . PCA ( n_components = self . K , copy = True , whiten = True ) <NEWLINE> pca . fit ( s . concatenate ( self . data , axis = 1 ) . T ) <NEWLINE> qmean = pca . components_ . T <NEWLINE> <DEDENT>
def _actuallyPause ( ) : <NEWLINE> <INDENT> fount = self . _siphon . _tdrain . fount <NEWLINE> self . _siphon . _pending . suspend ( ) <NEWLINE> if fount is not None : <NEWLINE> <INDENT> pbpc = fount . pauseFlow ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pbpc = NoPause ( ) <NEWLINE> <DEDENT> self . _siphon . _pauseBecausePauseCalled = pbpc <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> orig_field = db_field . translated_field <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> orig_formfield = self . formfield_for_dbfield ( orig_field , ** kwargs ) <NEWLINE> field . widget = deepcopy ( orig_formfield . widget ) <NEWLINE> if db_field . null and isinstance ( field . widget , ( forms . TextInput , forms . Textarea ) ) : <NEWLINE> <INDENT> field . widget = ClearableWidgetWrapper ( field . widget ) <NEWLINE> <DEDENT> css_classes = field . widget . attrs . get ( <STRING> , <STRING> ) . split ( <STRING> ) <NEWLINE> css_classes . append ( <STRING> ) <NEWLINE> <COMMENT> <NL> css_classes . append ( build_css_class ( db_field . name , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
UI_element_group . name = xml_element . attrib [ <STRING> ] <NEWLINE> <INDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . timedelay = float ( xml_element . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . parent_string = xml_element . attrib [ <STRING> ] <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . start_func = self . get_func_by_name ( xml_element . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . stop_func = self . get_func_by_name ( xml_element . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> if xml_element . attrib . has_key ( <STRING> ) : <NEWLINE> <INDENT> UI_element_group . identifier_string = xml_element . attrib [ <STRING> ] <NEWLINE> UI_element_group . identifier = identifier_parser . parse ( UI_element_group . identifier_string , lexer = identifier_lexer ) <NEWLINE> <DEDENT> <DEDENT>
def test_volume_path_with_non_ascii_directory ( self ) : <NEWLINE> <INDENT> volume = <STRING> <NEWLINE> container_path = config . resolve_volume_path ( <STRING> , volume , <STRING> ) <NEWLINE> self . assertEqual ( container_path , volume ) <NEWLINE> <DEDENT>
def test_volume_path_with_non_ascii_directory ( self ) : <NEWLINE> <INDENT> volume = <STRING> <NEWLINE> container_path = config . resolve_volume_path ( <STRING> , volume , <STRING> ) <NEWLINE> self . assertEqual ( container_path , volume ) <NEWLINE> <DEDENT>
def _gen_asset_class ( sym ) : <NEWLINE> <INDENT> sym_class = str ( sym ) . split ( <STRING> ) <NEWLINE> if len ( sym_class ) > 1 : <NEWLINE> <INDENT> return sym_class [ 1 ] <NEWLINE> <DEDENT> return <STRING> <NEWLINE> <DEDENT>
if self . tied_weights : <NEWLINE> <INDENT> for i in range ( len ( self . weights ) - 1 , - 1 , - 1 ) : <NEWLINE> <INDENT> h = self . hiddens [ - 1 ] <NEWLINE> a , b = self . weights [ i ] . get_value ( borrow = True ) . shape <NEWLINE> logging . info ( <STRING> , i , b , a ) <NEWLINE> o = theano . shared ( np . zeros ( ( a , ) , FLOAT ) , name = <STRING> . format ( i ) ) <NEWLINE> self . preacts . append ( TT . dot ( h , self . weights [ i ] . T ) + o ) <NEWLINE> func = self . _output_func if i == 0 else self . _hidden_func <NEWLINE> self . hiddens . append ( func ( self . preacts [ - 1 ] ) ) <NEWLINE> <DEDENT> <DEDENT>
size = kwargs . get ( <STRING> , kwargs . get ( <STRING> , 32 ) ) <NEWLINE> <INDENT> self . callable = None <NEWLINE> self . batches = None <NEWLINE> if len ( data ) == 1 and isinstance ( data [ 0 ] , collections . Callable ) : <NEWLINE> <INDENT> self . callable = data [ 0 ] <NEWLINE> if not self . number_batches : <NEWLINE> <INDENT> self . number_batches = size <NEWLINE> <DEDENT> logging . info ( <STRING> , <NEWLINE> <INDENT> self . label , self . number_batches ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> shape = data [ 0 ] . shape <NEWLINE> axis = kwargs . get ( <STRING> , 1 if len ( shape ) == 3 else 0 ) <NEWLINE> slices = [ slice ( None ) , slice ( None ) ] <NEWLINE> self . batches = [ ] <NEWLINE> i = 0 <NEWLINE> while i + size <= shape [ axis ] : <NEWLINE> <INDENT> slices [ axis ] = slice ( i , i + size ) <NEWLINE> self . batches . append ( [ d [ tuple ( slices ) ] for d in data ] ) <NEWLINE> i += size <NEWLINE> <DEDENT> self . shuffle ( ) <NEWLINE> if not self . number_batches : <NEWLINE> <INDENT> self . number_batches = len ( self . batches ) <NEWLINE> <DEDENT> logging . info ( <STRING> , <NEWLINE> <INDENT> self . label , self . number_batches , len ( self . batches ) , <NEWLINE> <STRING> . join ( str ( x . shape ) for x in self . batches [ 0 ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
cstr_parts = [ ] <NEWLINE> <INDENT> if dsn : <NEWLINE> <INDENT> cstr_parts . append ( <STRING> % dsn ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> cstr_parts . append ( <STRING> % driver ) <NEWLINE> if ms_drivers . match ( driver ) or driver == <STRING> and options . get ( <STRING> , False ) : <NEWLINE> <INDENT> if port : <NEWLINE> <INDENT> host += <STRING> % port <NEWLINE> <DEDENT> cstr_parts . append ( <STRING> % host ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cstr_parts . append ( <STRING> % host ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if on_rtd : <COMMENT> <NEWLINE> <INDENT> import sphinx_rtd_theme <NEWLINE> html_theme = <STRING> <NEWLINE> html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] <NEWLINE> else : <NEWLINE> html_theme = <STRING> <NEWLINE> <DEDENT>
positions = [ ] <NEWLINE> <INDENT> ix = 0 <NEWLINE> taken_positions = set ( fixed_positions . values ( ) ) <NEWLINE> for i in range ( n_plots ) : <NEWLINE> <INDENT> if i in fixed_positions : <NEWLINE> <INDENT> positions . append ( fixed_positions [ i ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> row , col = ix // n_cols , ix % n_cols <NEWLINE> if ( row , col ) not in taken_positions : <NEWLINE> <INDENT> positions . append ( ( row , col ) ) <NEWLINE> taken_positions . add ( ( row , col ) ) <NEWLINE> i += 1 <NEWLINE> break <NEWLINE> <DEDENT> ix += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
report . add_error ( CoordinateConsistencyError ( child_tag , child . id , file_id , <NEWLINE>
class ASTCompAndOr ( ASTNode ) : <NEWLINE> <INDENT> op = None <NEWLINE> def __init__ ( self , comp , comps ) : <NEWLINE> <INDENT> super ( ASTCompAndOr , self ) . __init__ ( comp . token ) <NEWLINE> self . comps = [ comp ] <NEWLINE> for c in comps : <NEWLINE> <INDENT> self . comps . append ( c ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
total_examples = 0 <NEWLINE> <INDENT> tf . enable_eager_execution ( ) <NEWLINE> task = t5 . data . TaskRegistry . get ( FLAGS . task ) <NEWLINE> files = task . tfds_dataset . files ( FLAGS . split ) <NEWLINE> def _example_to_string ( ex ) : <NEWLINE> <INDENT> key_to_string = { } <NEWLINE> for k in ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> if k in ex : <NEWLINE> <INDENT> v = ex [ k ] . numpy ( ) <NEWLINE> key_to_string [ k ] = ( <NEWLINE> <INDENT> <STRING> . join ( str ( i ) for i in v ) if FLAGS . tokenize <NEWLINE> else v . decode ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> key_to_string [ k ] = <STRING> <NEWLINE> <DEDENT> <DEDENT> return FLAGS . format_string . format ( ** key_to_string ) <NEWLINE> <DEDENT> <DEDENT>
self [ <STRING> ] = collection <NEWLINE> <INDENT> self [ <STRING> ] = field <NEWLINE> self [ <STRING> ] = coercion_type_repr <NEWLINE> <DEDENT>
if ( len ( modules ) < 2 or <NEWLINE> <INDENT> modules [ 0 ] . value != <STRING> or modules [ 1 ] . value != <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT>
def poll ( self ) : <NEWLINE> <INDENT> flags = [ ] <NEWLINE> if self . sending : <NEWLINE> <INDENT> self . sending = False <NEWLINE> slist = [ ( self . sock , ) , ( self . sock , ) , ( self . sock , ) ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> slist = [ ( self . sock , ) , ( ) , ( self . sock , ) ] <NEWLINE> <DEDENT> timeout = self . timer . get_timeout ( ) <NEWLINE> if timeout >= 0 : <NEWLINE> <INDENT> slist . append ( timeout ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> rlist , wlist , xlist = select . select ( * slist ) <NEWLINE> <DEDENT> except select . error as e : <NEWLINE> <INDENT> print ( str ( e ) ) <NEWLINE> rlist = [ ] <NEWLINE> wlist = [ ] <NEWLINE> xlist = [ ] <NEWLINE> <DEDENT> if rlist : flags . append ( <STRING> ) <NEWLINE> if wlist : flags . append ( <STRING> ) <NEWLINE> if xlist : flags . append ( <STRING> ) <NEWLINE> return flags <NEWLINE> <DEDENT>
@ pl_announce ( <STRING> ) <NEWLINE> <INDENT> class SettingsPlugin : <NEWLINE> <INDENT> def __init__ ( self , ploader , kwargs ) : <NEWLINE> <INDENT> settings = get_settings ( kwargs . get ( <STRING> , { } ) , kwargs ) <NEWLINE> plugin_list = settings . get ( <STRING> , DefaultPlugins ) <NEWLINE> plugins = [ ] <NEWLINE> plugin_settings = { } <NEWLINE> for plugin in plugin_list : <NEWLINE> <INDENT> plugins . append ( plugin [ 1 ] ) <NEWLINE> plugin_settings [ plugin [ 1 ] ] = settings . get ( plugin [ 0 ] , { } ) <NEWLINE> <DEDENT> ploader . provides ( <STRING> , PloaderFetch ( plugins , plugin_settings ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
class StartPlugin : <NEWLINE> <INDENT> def __init__ ( self , ploader , settings ) : <NEWLINE> <INDENT> self . settings = utils . get_settings ( default_settings , settings ) <NEWLINE> self . event = ploader . requires ( <STRING> ) <NEWLINE> self . net = ploader . requires ( <STRING> ) <NEWLINE> self . auth = ploader . requires ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def _find_left_index ( ss_waypoints , s ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for i in range ( 1 , len ( ss_waypoints ) ) : <NEWLINE> <INDENT> if ss_waypoints [ i - 1 ] <= s and s < ss_waypoints [ i ] : <NEWLINE> <INDENT> return i - 1 <NEWLINE> <DEDENT> <DEDENT> return len ( ss_waypoints ) - 2 <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> ret = _rdparm . rdparm ( fname ) <NEWLINE> <DEDENT> except TypeError : <NEWLINE> <COMMENT> <NL> <INDENT> raise <NEWLINE> return self . rdparm_old ( open ( fname , <STRING> ) . readlines ( ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> parm_data , parm_comments , formats , unkflg , flag_list , version = ret <NEWLINE> <COMMENT> <NL> self . parm_data = parm_data <NEWLINE> self . parm_comments = parm_comments <NEWLINE> for key in formats : <NEWLINE> <INDENT> self . formats [ key ] = FortranFormat ( formats [ key ] ) <NEWLINE> <DEDENT> self . flag_list = flag_list <NEWLINE> self . version = version <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> for flag in unkflg : <NEWLINE> <INDENT> rawdata = self . parm_data [ flag ] <NEWLINE> self . parm_data [ flag ] = [ ] <NEWLINE> for line in rawdata : <NEWLINE> <INDENT> self . parm_data [ flag ] . extend ( self . formats [ flag ] . read ( line ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if not isinstance ( timeout , ( int , long , float ) ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> self . timeout = timeout <NEWLINE> <DEDENT>
if order : <NEWLINE> <INDENT> params . append ( ( <STRING> , order ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if <STRING> . format ( Cal . coefficients ) != <STRING> . format ( coefficients ) : <NEWLINE> <INDENT> logging . debug ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
for index , parent_field_name in enumerate ( parent_field_names ) : <NEWLINE> <INDENT> dictobj = dictobj . get ( parent_field_name ) <NEWLINE> if not dictobj : <NEWLINE> <COMMENT> <NL> <INDENT> return handle_missing_field ( <NEWLINE> <INDENT> <STRING> . join ( parent_field_names [ : index + 1 ] ) <NEWLINE> <DEDENT> ) <NEWLINE> if final_field_name not in dictobj : <NEWLINE> <DEDENT> return handle_missing_field ( field_name ) <NEWLINE> return dictobj . get ( final_field_name ) <NEWLINE> <DEDENT>
psw . grid ( row = 0 , column = 3 , sticky = tkinter . W ) <NEWLINE> <INDENT> pbndl = Label ( paramf , text = <STRING> , width = 10 , anchor = <STRING> ) <NEWLINE> pbndl . grid ( row = 1 , column = 0 , sticky = tkinter . E ) <NEWLINE> pbnde1 = Entry ( paramf , width = 8 ) <NEWLINE> pbnde2 = Entry ( paramf , width = 8 ) <NEWLINE> pbnde1 . grid ( row = 1 , column = 1 , sticky = tkinter . W ) <NEWLINE> pbnde2 . grid ( row = 1 , column = 2 , sticky = tkinter . W ) <NEWLINE> lbnd = xrsdkit . param_bound_defaults [ param_nm ] [ 0 ] <NEWLINE> ubnd = xrsdkit . param_bound_defaults [ param_nm ] [ 1 ] <NEWLINE> if xrsdkit . contains_param ( self . inputs [ <STRING> ] , pop_nm , param_nm ) : <NEWLINE> <INDENT> lbnd = self . inputs [ <STRING> ] [ pop_nm ] [ <STRING> ] [ param_nm ] [ 0 ] <NEWLINE> ubnd = self . inputs [ <STRING> ] [ pop_nm ] [ <STRING> ] [ param_nm ] [ 1 ] <NEWLINE> <COMMENT> <NL> <DEDENT> pbnde1 . insert ( 0 , str ( lbnd ) ) <NEWLINE> pbnde2 . insert ( 0 , str ( ubnd ) ) <NEWLINE> pexpl = Label ( paramf , text = <STRING> , width = 10 , anchor = <STRING> ) <NEWLINE> pexpl . grid ( row = 2 , column = 0 , sticky = tkinter . E ) <NEWLINE> pexpe = Entry ( paramf , width = 16 ) <NEWLINE> if xrsdkit . contains_param ( self . inputs [ <STRING> ] , pop_nm , param_nm ) : <NEWLINE> <INDENT> pexpe . insert ( 0 , self . inputs [ <STRING> ] , pop_nm , param_nm ) <NEWLINE> <COMMENT> <NL> <DEDENT> pexpe . grid ( row = 2 , column = 1 , columnspan = 3 , sticky = tkinter . E + tkinter . W ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> paramf . grid ( row = 4 + nstgs + ip , column = 0 , columnspan = 4 , sticky = tkinter . E + tkinter . W ) <NEWLINE> <DEDENT>
def getinfo ( self , path , namespaces = None ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . check ( ) <NEWLINE> namespaces = namespaces or ( ) <NEWLINE> _path = self . validatepath ( path ) <NEWLINE> sys_path = self . getsyspath ( _path ) <NEWLINE> _lstat = None <NEWLINE> with convert_os_errors ( <STRING> , path ) : <NEWLINE> <INDENT> _stat = os . stat ( fsencode ( sys_path ) ) <NEWLINE> if <STRING> in namespaces : <NEWLINE> <INDENT> _lstat = os . lstat ( fsencode ( sys_path ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
pages -= 1 <NEWLINE> <INDENT> if pages <= 0 : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT>
def test_search_members_1 ( self ) : <NEWLINE> <INDENT> self . client . login ( username = <STRING> , password = <STRING> ) <NEWLINE> member_1 = self . foo_list . subscribe ( <NEWLINE> <INDENT> <STRING> , pre_verified = True , pre_confirmed = True , <NEWLINE> pre_approved = True ) <NEWLINE> <DEDENT> member_2 = self . foo_list . subscribe ( <NEWLINE> <INDENT> <STRING> , pre_verified = True , pre_confirmed = True , <NEWLINE> pre_approved = True ) <NEWLINE> <DEDENT> response = self . client . get ( reverse ( <NEWLINE> <INDENT> <STRING> , args = [ <STRING> , <STRING> ] ) , <NEWLINE> { <STRING> : <STRING> } ) <NEWLINE> <DEDENT> self . assertEqual ( response . status_code , 200 ) <NEWLINE> self . assertEqual ( len ( response . context [ <STRING> ] ) , 2 ) <NEWLINE> self . assertContains ( response , member_1 . email ) <NEWLINE> self . assertContains ( response , member_2 . email ) <NEWLINE> response = self . client . get ( reverse ( <NEWLINE> <INDENT> <STRING> , args = [ <STRING> , <STRING> ] ) , <NEWLINE> { <STRING> : <STRING> } ) <NEWLINE> <DEDENT> self . assertEqual ( response . status_code , 200 ) <NEWLINE> self . assertEqual ( len ( response . context [ <STRING> ] ) , 1 ) <NEWLINE> self . assertContains ( response , member_1 . email ) <NEWLINE> self . assertNotContains ( response , member_2 . email ) <NEWLINE> response = self . client . get ( reverse ( <NEWLINE> <INDENT> <STRING> , args = [ <STRING> , <STRING> ] ) , <NEWLINE> { <STRING> : <STRING> } ) <NEWLINE> <DEDENT> self . assertEqual ( response . status_code , 200 ) <NEWLINE> self . assertEqual ( len ( response . context [ <STRING> ] ) , 0 ) <NEWLINE> self . assertNotContains ( response , member_1 . email ) <NEWLINE> self . assertNotContains ( response , member_2 . email ) <NEWLINE> <DEDENT>
sign = <STRING> if total_minutes >= 0 else <STRING> <NEWLINE> <INDENT> total_minutes = abs ( total_minutes ) <NEWLINE> hour , minute = divmod ( total_minutes , 60 ) <NEWLINE> <DEDENT>
def calculate_anomaly_score ( self , log ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> dist_smallest = np . inf <NEWLINE> for x in range ( self . model . shape [ 0 ] ) : <NEWLINE> <INDENT> for y in range ( self . model . shape [ 1 ] ) : <NEWLINE> <INDENT> dist = np . linalg . norm ( self . model [ x ] [ y ] - log ) <NEWLINE> if dist < dist_smallest : <NEWLINE> <INDENT> dist_smallest = dist <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return dist_smallest <NEWLINE> <DEDENT>
if isinstance ( cieobs , str ) : <NEWLINE> <INDENT> cmfs = _CMF [ cieobs ] [ <STRING> ] <NEWLINE> else : <NEWLINE> cmfs = cieobs <NEWLINE> cmfs = cmfs [ : , cmfs [ 1 : ] . sum ( axis = 0 ) > 0 ] <COMMENT> <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> for node in doctree . traverse ( ItemLink ) : <NEWLINE> <INDENT> for source in node [ <STRING> ] : <NEWLINE> <INDENT> for target in node [ <STRING> ] : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> env . traceability_collection . add_relation ( source , node [ <STRING> ] , target ) <NEWLINE> <DEDENT> except TraceabilityException as err : <NEWLINE> <INDENT> report_warning ( env , err , env . docname , self . lineno ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> node . replace_self ( [ ] ) <NEWLINE> <DEDENT> <DEDENT>
def run ( self , args ) : <NEWLINE> <INDENT> if len ( args ) > 1 and <STRING> == args [ 0 ] : <NEWLINE> <INDENT> highlight_type = self . get_highlight_type ( args [ 1 ] ) <NEWLINE> if not highlight_type : return <NEWLINE> clear_file_format_cache ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> highlight_type = self . get_highlight_type ( args [ 0 ] ) <NEWLINE> if not highlight_type : return <NEWLINE> if <STRING> == highlight_type : highlight_type = <STRING> <NEWLINE> pass <NEWLINE> <DEDENT> self . debugger . settings [ <STRING> ] = highlight_type <NEWLINE> show_cmd = self . proc . commands [ <STRING> ] <NEWLINE> show_cmd . run ( [ <STRING> , <STRING> ] ) <NEWLINE> return <NEWLINE> pass <NEWLINE> <DEDENT>
class TestSkip ( unittest . TestCase ) : <NEWLINE> <INDENT> @ unittest . skipIf ( <STRING> in os . environ , <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> def test_skip ( self ) : <NEWLINE> <DEDENT>
b1 = 0x80 | ( opcode & 0x0f ) <COMMENT> <NEWLINE> <INDENT> payload_len = len ( buf ) <NEWLINE> if payload_len <= 125 : <NEWLINE> <INDENT> header = struct . pack ( <STRING> , b1 , payload_len ) <NEWLINE> <DEDENT> elif payload_len > 125 and payload_len < 65536 : <NEWLINE> <INDENT> header = struct . pack ( <STRING> , b1 , 126 , payload_len ) <NEWLINE> <DEDENT> elif payload_len >= 65536 : <NEWLINE> <INDENT> header = struct . pack ( <STRING> , b1 , 127 , payload_len ) <NEWLINE> <DEDENT> <DEDENT>
if ( collect_dynamic is False ) : <NEWLINE> <INDENT> dyn_state . append ( self . _states ) ; <NEWLINE> dyn_time . append ( time ) ; <NEWLINE> <DEDENT>
if distance <= self . _threshold : <NEWLINE> <INDENT> self . __append_to_cluster ( index_cluster , index_point , point ) ; <NEWLINE> elif distance > self . _threshold2 : <NEWLINE> self . __allocate_cluster ( index_point , point ) ; <NEWLINE> <DEDENT>
if expected_cluster_length is not None : <NEWLINE> <INDENT> assertion . eq ( len ( expected_cluster_length ) , len ( centers ) ) <NEWLINE> <DEDENT>
assert car1 < car2 <NEWLINE> <INDENT> assert car2 > car3 <NEWLINE> assert car3 < car2 <NEWLINE> assert car4 > car3 <NEWLINE> assert car1 . antecedent <= transaction1 <NEWLINE> assert car2 . antecedent <= transaction1 <NEWLINE> assert car3 . antecedent <= transaction1 <NEWLINE> assert not car4 . antecedent <= transaction1 <NEWLINE> assert sorted_cars [ 0 ] == car4 <NEWLINE> <DEDENT>
t1 . merge ( t2 ) <NEWLINE> <INDENT> self . assertEqual ( t1 . ugettext ( <STRING> ) , <STRING> ) <NEWLINE> self . assertEqual ( t1 . ugettext ( <STRING> ) , <STRING> ) <NEWLINE> <DEDENT>
if len ( cache_path ) == 0 : <NEWLINE> <INDENT> return False , <STRING> <NEWLINE> if len ( cache ) == 0 : <NEWLINE> return False , <STRING> <NEWLINE> try : <NEWLINE> if not path . isdir ( cls . directory ) : <NEWLINE> <INDENT> makedirs ( cls . directory ) <NEWLINE> <DEDENT> with open ( cls . file_path ( cache_path ) , <STRING> ) as f : <NEWLINE> <INDENT> size = f . write ( cache ) <NEWLINE> return True , size <NEWLINE> except Exception as e : <NEWLINE> <DEDENT> return False , str ( e ) <NEWLINE> return False , <STRING> <NEWLINE> <DEDENT>
def status_info ( self ) : <NEWLINE> <INDENT> print ( <STRING> . format ( len ( self . busying ) ) ) <NEWLINE> urls = [ ] <NEWLINE> for i , url in enumerate ( self . busying ) : <NEWLINE> <INDENT> if i >= 3 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> urls . append ( url ) <NEWLINE> <DEDENT> <DEDENT>
boxes = [ ] <NEWLINE> <INDENT> for i , c in enumerate ( class_values ) : <NEWLINE> <INDENT> box = patches . Rectangle ( ( 0 , 0 ) , 20 , 10 , linewidth = .4 , edgecolor = colors [ <STRING> ] , <NEWLINE> <INDENT> facecolor = color_map [ c ] , label = class_names [ i ] ) <NEWLINE> <DEDENT> boxes . append ( box ) <NEWLINE> <DEDENT> <DEDENT>
lcolor = rcolor = colors [ <STRING> ] <NEWLINE> <INDENT> lpw = rpw = <STRING> <NEWLINE> if node . left . id in highlight_path : <NEWLINE> <INDENT> lcolor = colors [ <STRING> ] <NEWLINE> lpw = <STRING> <NEWLINE> <DEDENT> if node . right . id in highlight_path : <NEWLINE> <INDENT> rcolor = colors [ <STRING> ] <NEWLINE> rpw = <STRING> <NEWLINE> <DEDENT> edges . append ( <STRING> ) <NEWLINE> edges . append ( <STRING> ) <NEWLINE> edges . append ( <STRING> ) <NEWLINE> <DEDENT>
if getattr ( aq_base ( context ) , <STRING> , None ) : <NEWLINE> <INDENT> cont = False <NEWLINE> else : <NEWLINE> context = aq_parent ( context ) <NEWLINE> <DEDENT>
@ property <NEWLINE> <INDENT> def primary_opening_today ( self ) : <NEWLINE> <INDENT> today = date . today ( ) <NEWLINE> cache_key = self . get_opening_today_cache_key ( today ) <NEWLINE> times = cache . get ( cache_key ) <NEWLINE> if times is None : <NEWLINE> <INDENT> opening_times = self . primary_opening_times <NEWLINE> if opening_times : <NEWLINE> <INDENT> specific_times = utils . first_true ( opening_times , lambda x : x . get ( <STRING> ) == today ) <NEWLINE> times = specific_times or utils . first_true ( opening_times , lambda x : x . get ( <STRING> ) == today . weekday ( ) ) <NEWLINE> cache . set ( cache_key , times , 60 * 60 * 24 ) <NEWLINE> <DEDENT> <DEDENT> return times <NEWLINE> <DEDENT> <DEDENT>
def handle_ctrlchan ( self , nick , msg , send , send_raw ) : <NEWLINE> <INDENT> cmd = msg . split ( ) <NEWLINE> if cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> send_raw ( <STRING> . join ( cmd [ 1 : ] ) ) <NEWLINE> <DEDENT> elif cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . kick_enabled = False <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . disabled_mods . append ( cmd [ 2 ] ) <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . kick_enabled = True <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> if cmd [ 1 ] == <STRING> : <NEWLINE> <INDENT> self . disabled_mods = [ i for i in self . disabled_mods if i != cmd [ 2 ] ] <NEWLINE> send ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> elif cmd [ 0 ] == <STRING> : <NEWLINE> <INDENT> if cmd [ 1 ] == <STRING> and cmd [ 2 ] == <STRING> : <NEWLINE> <INDENT> send ( str ( self . disabled_mods ) ) <NEWLINE> <DEDENT> if cmd [ 1 ] == <STRING> and cmd [ 2 ] == <STRING> : <NEWLINE> <INDENT> send ( str ( [ i for i in self . modules if i not in self . disabled_mods ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def groundwater_constraint ( evaluation , simulation ) : <NEWLINE> <INDENT> if ( evaluation [ 0 ] - 0.1 <= simulation [ 0 ] ) and ( simulation [ 0 ] <= evaluation [ 0 ] + 0.1 ) : <NEWLINE> <INDENT> return 1.0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return 0.0 <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , sate_id , source ) : <NEWLINE> <INDENT> super ( TLEPredictor , self ) . __init__ ( sate_id , source ) <NEWLINE> self . _iterations = 0 <NEWLINE> <DEDENT>
self . novel_id = urlparse ( self . novel_url ) . path . split ( <STRING> ) [ - 1 ] <NEWLINE> <INDENT> logger . info ( <STRING> , self . novel_id ) <NEWLINE> <DEDENT>
def make_id ( name ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> r = get_rand_string ( 12 ) <NEWLINE> if len ( name ) > 22 : <NEWLINE> <INDENT> name = name [ : 22 ] <NEWLINE> <DEDENT> return name + <STRING> + r <NEWLINE> <DEDENT>
def int_to_decimal_str ( integer ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> int_string = str ( integer ) <NEWLINE> if len ( int_string ) <= 2 : <NEWLINE> <INDENT> return <STRING> + int_string . zfill ( 2 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return int_string [ : - 2 ] + <STRING> + int_string [ - 2 : ] <NEWLINE> <DEDENT> <DEDENT>
if not threshold == <STRING> : <NEWLINE> <INDENT> msg = <STRING> <NEWLINE> warnings . warn ( msg , DeprecationWarning ) <NEWLINE> threshold_rel = threshold <NEWLINE> <COMMENT> <NL> corner_threshold = max ( np . max ( image . ravel ( ) ) * threshold_rel , threshold_abs ) <NEWLINE> image_t = ( image > corner_threshold ) * 1 <NEWLINE> <DEDENT>
if offset == None : <NEWLINE> <INDENT> if not all ( [ d % 2 == 1 for d in selem . shape ] ) : <NEWLINE> <INDENT> ValueError ( <STRING> ) <NEWLINE> <DEDENT> offset = np . array ( [ d // 2 for d in selem . shape ] ) <NEWLINE> <COMMENT> <NL> selem [ [ slice ( d , d + 1 ) for d in offset ] ] = False <NEWLINE> <DEDENT>
def get_relationship ( self , rel_type , rel_key ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> values = self . get ( RELATIONSHIP_CF , ENDPOINT_NAME_TEMPLATE % ( rel_type , rel_key ) ) <NEWLINE> <DEDENT> except NotFoundException : <NEWLINE> <INDENT> raise NodeNotFoundException ( ) <NEWLINE> <DEDENT> source_node_key = None <NEWLINE> source_node_type = None <NEWLINE> source_attributes = { } <NEWLINE> for column in values . keys ( ) : <NEWLINE> <INDENT> value = values [ column ] <NEWLINE> if column == <STRING> : <NEWLINE> <INDENT> source_node_type = value <NEWLINE> <DEDENT> elif column == <STRING> : <NEWLINE> <INDENT> source_node_key = value <NEWLINE> <DEDENT> elif column . startswith ( <STRING> ) : <NEWLINE> <INDENT> source_attributes [ column [ 8 : ] ] = value <NEWLINE> <DEDENT> <DEDENT> source = prim . Node ( self , source_node_type , source_node_key , source_attributes ) <NEWLINE> rel_key = RELATIONSHIP_KEY_PATTERN % ( rel_type , rel_key ) <NEWLINE> return self . get_outgoing_relationship ( rel_type , source , ( rel_key , values ) ) <NEWLINE> <DEDENT>
def _recursive_directory_find ( path : Path , directory_name : str ) -> str : <NEWLINE> <INDENT> if str ( path ) == expanduser ( <STRING> ) : <NEWLINE> <INDENT> raise FileNotFoundError ( ) <NEWLINE> <DEDENT> joined_with_driectory = path . joinpath ( directory_name ) <NEWLINE> if joined_with_driectory . is_dir ( ) : <NEWLINE> <INDENT> return str ( path ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return _recursive_directory_find ( path . parent , directory_name ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if <STRING> in ann . sandbox . keys ( ) : <NEWLINE> <INDENT> for sliceop in ann . sandbox [ <STRING> ] : <NEWLINE> <COMMENT> <NL> <INDENT> tmpfiles = [ ] <NEWLINE> audio_files = [ audio_outfile ] + ann . sandbox . scaper . isolated_events_audio_path <NEWLINE> with _close_temp_files ( tmpfiles ) : <NEWLINE> <INDENT> for audio_file in audio_files : <NEWLINE> <COMMENT> <NL> <INDENT> tmpfiles . append ( <NEWLINE> <INDENT> tempfile . NamedTemporaryFile ( suffix = <STRING> , delete = False ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> tfm = sox . Transformer ( ) <NEWLINE> tfm . trim ( sliceop [ <STRING> ] , sliceop [ <STRING> ] ) <NEWLINE> tfm . build ( audio_file , tmpfiles [ - 1 ] . name ) <NEWLINE> <COMMENT> <NL> shutil . copyfile ( tmpfiles [ - 1 ] . name , audio_file ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
done = set ( ) <NEWLINE> <INDENT> dirs = [ ] <NEWLINE> for m in members : <NEWLINE> <INDENT> inf = self . getinfo ( m ) <NEWLINE> dst = self . _extract_one ( inf , path , pwd , not inf . is_dir ( ) ) <NEWLINE> if inf . is_dir ( ) : <NEWLINE> <INDENT> if dst not in done : <NEWLINE> <INDENT> dirs . append ( ( dst , inf ) ) <NEWLINE> done . add ( dst ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if dirs : <NEWLINE> <INDENT> dirs . sort ( reverse = True ) <NEWLINE> for dst , inf in dirs : <NEWLINE> <INDENT> self . _set_attrs ( inf , dst ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> x , y = 0 , 0 <NEWLINE> previous = 0 <NEWLINE> for c in text : <NEWLINE> <INDENT> face . load_char ( c ) <NEWLINE> bitmap = slot . bitmap <NEWLINE> top = slot . bitmap_top <NEWLINE> left = slot . bitmap_left <NEWLINE> w , h = bitmap . width , bitmap . rows <NEWLINE> y = height - baseline - top <NEWLINE> kerning = face . get_kerning ( previous , c ) <NEWLINE> x += ( kerning . x >> 6 ) <NEWLINE> Z [ y : y + h , x : x + w ] += numpy . array ( bitmap . buffer ) . reshape ( h , w ) <NEWLINE> x += ( slot . advance . x >> 6 ) <NEWLINE> previous = c <NEWLINE> <DEDENT> <DEDENT>
if isinstance ( v1 , list ) and isinstance ( v2 , list ) : <NEWLINE> <INDENT> v1 . extend ( v2 ) <NEWLINE> return v1 <NEWLINE> <DEDENT>
p_t = p - self . lr * m_b_t / ( T . sqrt ( v_b_t ) + self . epsilon ) <NEWLINE>
if len ( self . items_buffer ) >= self . settings . get ( <STRING> , 500 ) : <NEWLINE> <INDENT> self . send_items ( ) <NEWLINE> self . items_buffer = [ ] <NEWLINE> <DEDENT>
if read_lengths is None and psite_offsets is not None : <NEWLINE> <INDENT> sys . exit ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if read_lengths is not None and psite_offsets is not None : <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> psite_offsets = [ <NEWLINE> <INDENT> int ( x . strip ( ) ) for x in psite_offsets . strip ( ) . split ( <STRING> ) <NEWLINE> <DEDENT> ] <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> if len ( read_lengths ) != len ( psite_offsets ) : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> if not all ( x >= 0 for x in psite_offsets ) : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> if not all ( x > y for ( x , y ) in zip ( read_lengths , psite_offsets ) ) : <NEWLINE> <INDENT> sys . exit ( <STRING> ) <NEWLINE> <DEDENT> psite_offsets = dict ( zip ( read_lengths , psite_offsets ) ) <NEWLINE> if stranded == <STRING> : <NEWLINE> stranded = <STRING> <NEWLINE> detect_orfs ( bam , ribocop_index , prefix , stranded , read_lengths , <NEWLINE> <INDENT> psite_offsets , report_all ) <NEWLINE> <DEDENT> <DEDENT>
to_write = <STRING> . join ( columns ) <NEWLINE> <INDENT> formatter = <STRING> * ( len ( columns ) - 1 ) + <STRING> <NEWLINE> for orf in tqdm ( candidate_orfs ) : <NEWLINE> <INDENT> coordinate = <STRING> . join ( <NEWLINE> <INDENT> [ <STRING> . format ( iv . start , iv . end ) for iv in orf . intervals ] ) <NEWLINE> <DEDENT> to_write += formatter . format ( orf . oid , orf . category , orf . tid , orf . ttype , <NEWLINE> <INDENT> orf . gid , orf . gname , orf . gtype , orf . chrom , <NEWLINE> orf . strand , coordinate ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if re . search ( <STRING> , args . outfile ) is not None : <NEWLINE> <INDENT> sscs_singleton_bam = pysam . AlignmentFile ( <STRING> . format ( args . outfile . split ( <STRING> ) [ 0 ] ) , <NEWLINE> <INDENT> <STRING> , template = sscs_bam ) <NEWLINE> <DEDENT> dcs_header = <STRING> <NEWLINE> sc_header = <STRING> <NEWLINE> else : <NEWLINE> sscs_singleton_bam = pysam . AlignmentFile ( <STRING> . format ( args . outfile . split ( <STRING> ) [ 0 ] ) , <NEWLINE> <INDENT> <STRING> , template = sscs_bam ) <NEWLINE> <DEDENT> dcs_header = <STRING> <NEWLINE> sc_header = <STRING> <NEWLINE> <DEDENT>
for attr in fields : <NEWLINE> <INDENT> model = getattr ( attr , <STRING> , None ) <NEWLINE> if ( model and issubclass ( model , base_cls ) and <NEWLINE> <INDENT> model is not generic_cls and getattr ( model , <STRING> , True ) ) : <NEWLINE> <COMMENT> <NL> if not model . objects . is_generic ( ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> fk = model . _meta . get_field ( <STRING> ) <NEWLINE> if ctype == get_content_type ( fk . remote_field . model ) : <NEWLINE> <INDENT> return model <NEWLINE> return generic_cls <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def import_symbol ( sym , here = None , sep = <STRING> , ns = None ) : <NEWLINE> <INDENT> if ns is not None and sep not in sym : <NEWLINE> <INDENT> sym = <STRING> . format ( ns , sym ) <NEWLINE> <DEDENT> module_path , fn_name = sym . rsplit ( sep , 2 ) <NEWLINE> try : <NEWLINE> <INDENT> module = import_module ( module_path , here = here , sep = sep ) <NEWLINE> return getattr ( module , fn_name ) <NEWLINE> <DEDENT> except ( ImportError , AttributeError ) as e : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> . format ( sym , e ) ) <NEWLINE> raise <NEWLINE> <DEDENT> <DEDENT>
for header_name , header_list in serialized [ <STRING> ] . items ( ) : <NEWLINE> <INDENT> if isinstance ( header_list , list ) : <NEWLINE> <INDENT> for header_value in header_list : <NEWLINE> <INDENT> header_dict . add ( header_name , header_value ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> header_dict . add ( header_name , header_list ) <NEWLINE> r . headers = CaseInsensitiveDict ( header_dict ) <NEWLINE> <DEDENT> <DEDENT>
@ strict_globals ( deque = deque , itemgetter = og_itemgetter ) <NEWLINE> <INDENT> def itemgetter ( iterable , indexes ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> indexes = indexes if isinstance ( indexes , tuple ) else tuple ( indexes ) <NEWLINE> assert all ( isinstance ( i , int ) for i in indexes ) , <STRING> <NEWLINE> positive_indexes = [ i for i in indexes if i >= 0 ] <NEWLINE> negative_indexes = [ i for i in indexes if i < 0 ] <NEWLINE> out = { } <NEWLINE> if len ( negative_indexes ) : <NEWLINE> <COMMENT> <NL> <INDENT> negative_index_buffer = deque ( maxlen = min ( indexes ) * - 1 ) <NEWLINE> for i , x in enumerate ( iterable ) : <NEWLINE> <INDENT> if i in positive_indexes : <NEWLINE> <INDENT> out [ i ] = x <NEWLINE> <DEDENT> negative_index_buffer . append ( x ) <NEWLINE> <DEDENT> out . update ( { ni : negative_index_buffer [ ni ] for ni in negative_indexes } ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> out . update ( { i : x for i , x in enumerate ( iterable ) if i in positive_indexes } ) <NEWLINE> <DEDENT> return itemgetter ( * indexes ) ( out ) <NEWLINE> <DEDENT> <DEDENT>
cv = StratifiedKFold ( <NEWLINE> <INDENT> n_splits = 5 , <NEWLINE> shuffle = True , <NEWLINE> random_state = 8 <NEWLINE> ) <NEWLINE> preds = [ ] <NEWLINE> trues_list = [ ] <NEWLINE> for train_index , test_index in cv . split ( secondary_features , y ) : <NEWLINE> X_train , X_test = secondary_features [ train_index ] , secondary_features [ test_index ] <NEWLINE> y_train , y_test = y [ train_index ] , y [ test_index ] <NEWLINE> est = est . fit ( X_train , y_train ) <NEWLINE> preds . append ( <NEWLINE> <INDENT> getattr ( est , stacked_ensemble . base_learner_origin . <NEWLINE> <INDENT> meta_feature_generator ) ( X_test ) <NEWLINE> <DEDENT> <DEDENT> ) <NEWLINE> trues_list . append ( y_test ) <NEWLINE> preds = np . concatenate ( preds , axis = 0 ) <NEWLINE> y_true = np . concatenate ( trues_list ) <NEWLINE> <DEDENT>
self . update ( ) <NEWLINE> <INDENT> if not messagebox . askyesno ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> <NEWLINE> <STRING> , <NEWLINE> icon = <STRING> , parent = self ) : <NEWLINE> print ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>
class System ( system . System ) : <NEWLINE> <INDENT> def __init__ ( self , target ) : <NEWLINE> <INDENT> system . System . __init__ ( self ) <NEWLINE> <COMMENT> <NL> self . help = <STRING> <NEWLINE> self . valid = True <NEWLINE> <COMMENT> <NL> self . add_module_depend ( [ <STRING> ] ) <NEWLINE> self . add_export_flag ( <STRING> , <STRING> ) <NEWLINE> if env . get_isolate_system ( ) == True : <NEWLINE> <INDENT> self . add_header_file ( [ <NEWLINE> <INDENT> <STRING> <NEWLINE> ] , <NEWLINE> destination_path = <STRING> , <NEWLINE> recursive = True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def sanity_check ( self , outprefix ) : <NEWLINE> <INDENT> variants_only_removed = self . _remove_bad_genes ( self . seq_dicts [ <STRING> ] , outprefix + <STRING> ) <NEWLINE> presence_absence_removed = self . _remove_bad_genes ( self . seq_dicts [ <STRING> ] , outprefix + <STRING> ) <NEWLINE> self . _filter_bad_variant_data ( outprefix + <STRING> , presence_absence_removed , variants_only_removed ) <NEWLINE> <DEDENT>
if self . show_known_het and ( cluster , variant ) in all_het_snps and key + <STRING> not in rows [ filename ] [ cluster ] : <NEWLINE> <INDENT> rows [ filename ] [ cluster ] [ key + <STRING> ] = <STRING> <NEWLINE> <DEDENT>
for d in cluster . data : <NEWLINE> <INDENT> if d [ <STRING> ] == best_hit [ <STRING> ] : <NEWLINE> <INDENT> if d [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> het_data . append ( d [ <STRING> ] ) <NEWLINE> depths = [ int ( x ) for x in d [ <STRING> ] . split ( <STRING> ) ] <NEWLINE> depths . sort ( ) <NEWLINE> het_pc = round ( 100.0 * depths [ - 1 ] / sum ( depths ) , 2 ) <NEWLINE> if results [ <STRING> ] == <STRING> or results [ <STRING> ] > het_pc : <NEWLINE> <INDENT> results [ <STRING> ] = het_pc <NEWLINE> if len ( het_data ) : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> results [ <STRING> ] = <STRING> . join ( het_data ) <NEWLINE> <DEDENT>
if __name__ == <STRING> : <NEWLINE> <INDENT> if not check ( ) : <NEWLINE> <INDENT> sys . exit ( 1 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sys . exit ( 0 ) <NEWLINE> <DEDENT> <DEDENT>
return credential_response_schema . dumps ( credential_response ) <NEWLINE>
if not os . path . exists ( local_path ) : <NEWLINE> <INDENT> logging . error ( <STRING> , local_path ) <NEWLINE> <DEDENT>
class Tan ( AD ) : <NEWLINE> <INDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . tan = Sin ( func ) / Cos ( func ) <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . tan . cal ( x , dOrder ) <NEWLINE> class Tanh ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . tanh = - 1j * Tan ( func * 1j ) <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . tanh . cal ( x , dOrder ) <NEWLINE> <STRING> <NEWLINE> class Conjugate ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return np . conjugate ( self . func . cal ( x , dOrder ) ) <NEWLINE> class Real ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . func = func <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . func . cal ( x , dOrder ) . real <NEWLINE> class Imaginary ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> self . func = func <NEWLINE> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . func . cal ( x , dOrder ) . imag <NEWLINE> class Absolute ( AD ) : <NEWLINE> <DEDENT> def __init__ ( self , func ) : <NEWLINE> <INDENT> self . func = func <NEWLINE> self . abs = ( Real ( func ) ** 2. + Imaginary ( func ) ** 2. ) ** 0.5 <NEWLINE> try : <NEWLINE> <INDENT> self . dependent = func . dependent [ : ] <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> self . dependent = [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> def cal ( self , x , dOrder ) : <NEWLINE> <INDENT> return self . abs . cal ( x , dOrder ) <NEWLINE> <STRING> <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> v = next ( ( v for v , bias in original . items ( ) if bias ) ) <NEWLINE> <DEDENT> except StopIteration : <NEWLINE> <COMMENT> <NL> <INDENT> scalar = 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> scalar = poly [ v ] / original [ v ] <NEWLINE> <DEDENT> <DEDENT>
self . assertEqual ( bqm , new ) <NEWLINE> <INDENT> self . assertEqual ( new . info , { <STRING> : 5 } ) <NEWLINE> <DEDENT>
rvals = np . empty ( 2 * r ) <NEWLINE> <INDENT> rvals [ 0 : r ] = range ( - r , 0 ) <NEWLINE> rvals [ r : ] = range ( 1 , r + 1 ) <NEWLINE> qdata = rnd . choice ( rvals , size = len ( irow ) ) <NEWLINE> <DEDENT>
response_json = responses_data [ 0 ] <NEWLINE> <INDENT> assert <STRING> not in response_json <NEWLINE> assert response_json [ <STRING> ] == request1 [ <STRING> ] <NEWLINE> assert response_json [ <STRING> ] == 5 <NEWLINE> <DEDENT>
mail_params = { <NEWLINE> <INDENT> <STRING> : blocks [ <STRING> ] . strip ( ) , <NEWLINE> <STRING> : blocks [ <STRING> ] <NEWLINE> } <NEWLINE> for ob in optional_blocks : <NEWLINE> if ob in blocks : <NEWLINE> <INDENT> if ob == <STRING> and blocks [ ob ] . lower ( ) not in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> mail_params [ ob ] = blocks [ ob ] <NEWLINE> return mail_params <NEWLINE> <DEDENT> <DEDENT>
feature_dict = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : bin_labels_left , <NEWLINE> <STRING> : bin_labels_right , <NEWLINE> <STRING> : model_graph , <NEWLINE> <STRING> : bounds , <NEWLINE> } <NEWLINE> feature_list . append ( feature_dict ) <NEWLINE> density_list . append ( { } ) <NEWLINE> <DEDENT>
def _validate_range ( self , start , minimum , maximum ) : <NEWLINE> <INDENT> if maximum <= minimum : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> _ ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> if start < minimum or start > maximum : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> _ ( <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> rnrange = maximum - minimum <NEWLINE> return rnrange <NEWLINE> <DEDENT>
@ given ( bitmap_cls ) <NEWLINE> <INDENT> def test_shrink_to_fit ( self , cls ) : <NEWLINE> <INDENT> bm1 = BitMap ( ) <NEWLINE> size = 1000 <NEWLINE> for i in range ( size ) : <NEWLINE> <INDENT> bm1 . add ( i ) <NEWLINE> <DEDENT> bm2 = cls ( bm1 , optimize = False ) <NEWLINE> self . assertGreater ( bm2 . shrink_to_fit ( ) , 0 ) <NEWLINE> self . assertEqual ( bm2 . shrink_to_fit ( ) , 0 ) <NEWLINE> bm3 = cls ( bm1 , optimize = True ) <NEWLINE> self . assertEqual ( bm3 . shrink_to_fit ( ) , 0 ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> save_dir = maindir + <STRING> <NEWLINE> input_builder ( csvfile , save_dir , ebasis_dir , <NEWLINE> <INDENT> initial_coords_dict , title . replace ( <STRING> , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> save_dir = maindir + <STRING> <NEWLINE> input_builder ( csvfile , save_dir , ebasis_dir , <NEWLINE> <INDENT> initial_coords_dict , title . replace ( <STRING> , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> save_dir = maindir + <STRING> <NEWLINE> input_builder ( csvfile , ebasis_dir , save_dir , <NEWLINE> <INDENT> initial_coords_dict , title . replace ( <STRING> , <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> book = load_workbook ( sheetsdir + dir + <STRING> ) <NEWLINE> with pd . ExcelWriter ( sheetsdir + dir + <STRING> , engine = <STRING> ) as writer : <NEWLINE> <INDENT> writer . book = book <NEWLINE> writer . sheets = dict ( ( ws . title , ws ) for ws in book . worksheets ) <NEWLINE> if opt in df : <NEWLINE> <INDENT> df [ opt ] . to_excel ( writer , sheet_name = opt , startrow = 6 ) <NEWLINE> <DEDENT> if hes in df : <NEWLINE> <INDENT> df [ hes ] . to_excel ( writer , sheet_name = hes , startrow = 6 ) <NEWLINE> <DEDENT> if ram in df : <NEWLINE> <INDENT> df [ ram ] . to_excel ( writer , sheet_name = ram , startrow = 6 ) <NEWLINE> <DEDENT> if vsc in df : <NEWLINE> <INDENT> df [ vsc ] . to_excel ( writer , sheet_name = vsc , startrow = 6 ) <NEWLINE> <DEDENT> if cmp in df : <NEWLINE> <INDENT> df [ cmp ] . to_excel ( writer , sheet_name = cmp , startrow = 6 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> with sqlite3 . connect ( tmp_path ) as conn : <NEWLINE> <INDENT> seq ( elements ) . to_sqlite3 ( conn , insert_sql ) <NEWLINE> result = seq . sqlite3 ( conn , <STRING> ) . to_list ( ) <NEWLINE> self . assertListEqual ( elements , result ) <NEWLINE> <DEDENT> <DEDENT>
def feat_map ( x ) : <NEWLINE> <INDENT> scale = 2 * B . pi / B . cast ( period , x . dtype ) <NEWLINE> return B . concatenate ( ( B . sin ( x * scale ) , <NEWLINE> <INDENT> B . cos ( x * scale ) ) , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT>
def label ( self , data ) : <NEWLINE> <INDENT> core_labels , n_labels = label ( data >= self . max_intensity ) <NEWLINE> ws_labels = watershed ( data . max ( ) - data , markers = core_labels , mask = data >= self . min_intensity ) <NEWLINE> return ws_labels <NEWLINE> <DEDENT>
def label ( self , data ) : <NEWLINE> <INDENT> core_labels , n_labels = label ( data <= self . max_intensity ) <NEWLINE> ws_labels = watershed ( data . max ( ) - data , markers = core_labels , mask = data >= self . min_intensity ) <NEWLINE> return ws_labels <NEWLINE> <DEDENT>
def label ( self , data ) : <NEWLINE> <INDENT> core_labels , n_labels = label ( data >= self . max_intensity ) <NEWLINE> ws_labels = watershed ( data . max ( ) - data , markers = core_labels , mask = data >= self . min_intensity ) <NEWLINE> return ws_labels <NEWLINE> <DEDENT>
total_bandwidth = 480 <NEWLINE> <INDENT> num_obs = 8 <COMMENT> <NEWLINE> subband_width = 60 <COMMENT> <NEWLINE> num_subb = total_bandwidth // subband_width <NEWLINE> subband_dict = collections . defaultdict ( list ) <COMMENT> <NEWLINE> img_list = [ ] <NEWLINE> start_freq = 940 <NEWLINE> <DEDENT>
for dm , drop in ( dm1 , a ) , ( dm2 , b ) , ( dm2 , c ) : <NEWLINE> <INDENT> self . assertEqual ( DROPStates . COMPLETED , dm . get_drop_property ( sessionId , drop . uid , <STRING> ) ) <NEWLINE> self . assertEqual ( a . checksum , int ( droputils . allDropContents ( c ) ) ) <NEWLINE> <DEDENT>
def _ngas_and_fs_io ( self , command ) : <NEWLINE> <INDENT> a = NgasDROP ( <STRING> , <STRING> ) <COMMENT> <NEWLINE> b = DockerApp ( <STRING> , <STRING> , image = <STRING> , command = command ) <NEWLINE> c = FileDROP ( <STRING> , <STRING> ) <NEWLINE> b . addInput ( a ) <NEWLINE> b . addOutput ( c ) <NEWLINE> with DROPWaiterCtx ( self , c , 100 ) : <NEWLINE> <INDENT> a . setCompleted ( ) <NEWLINE> <DEDENT> self . assertEqual ( six . b ( a . dataURL ) , droputils . allDropContents ( c ) ) <NEWLINE> <DEDENT>
def check_log_dir ( self , log_dir ) : <NEWLINE> <INDENT> possible_logs = [ <NEWLINE> os . path . join ( log_dir , <STRING> , <STRING> ) , <NEWLINE> os . path . join ( log_dir , <STRING> , <STRING> ) <NEWLINE> ] <NEWLINE> for dim_log_f in possible_logs : <NEWLINE> <INDENT> if ( os . path . exists ( dim_log_f ) ) : <NEWLINE> <INDENT> self . _dim_log_f = [ dim_log_f ] <NEWLINE> if ( dim_log_f == possible_logs [ 0 ] ) : <NEWLINE> <INDENT> cluster_log = os . path . join ( log_dir , <STRING> , <STRING> ) <NEWLINE> if ( os . path . exists ( cluster_log ) ) : <NEWLINE> <INDENT> self . _dim_log_f . append ( cluster_log ) <NEWLINE> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>
def add_node ( self , u ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> kwargs = dict ( ) <NEWLINE> if ( self . _tmp_max_dop is None ) : <NEWLINE> <INDENT> self . _tmp_max_dop = dict ( ) <NEWLINE> <DEDENT> self_global_dag = self . _global_dag <NEWLINE> for _w_attr in self . _w_attr : <NEWLINE> <INDENT> u_aw = self_global_dag . node [ u ] . get ( _w_attr , 1 ) <NEWLINE> kwargs [ _w_attr ] = u_aw <NEWLINE> <DEDENT> kwargs [ <STRING> ] = self_global_dag . node [ u ] . get ( <STRING> , 5 ) <NEWLINE> self . _dag . add_node ( u , ** kwargs ) <NEWLINE> for k in self . _w_attr : <NEWLINE> <INDENT> self . _tmp_max_dop [ k ] = get_max_weighted_antichain ( self . _dag , w_attr = k ) [ 0 ] <NEWLINE> <DEDENT> self . _max_dop = self . _tmp_max_dop <NEWLINE> <DEDENT>
elif algo == ALGO_MIN_NUM_PARTS : <NEWLINE> <INDENT> time_greedy = 1 - time_greedy / 100.0 <COMMENT> <NEWLINE> pgt = MinNumPartsPGTP ( pgt , deadline , num_partitions , partition_label , max_cpu , merge_parts = could_merge , optimistic_factor = time_greedy ) <NEWLINE> <DEDENT>
def check_and_add ( ip ) : <NEWLINE> <INDENT> ntries = retry <NEWLINE> while ntries : <NEWLINE> <INDENT> if check_host ( ip , port , timeout = timeout , check_with_session = check_with_session ) : <NEWLINE> <INDENT> logger . info ( <STRING> , ip , port ) <NEWLINE> return ip <NEWLINE> <DEDENT> logger . warning ( <STRING> , ip , port ) <NEWLINE> ntries -= 1 <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>
def timed_import ( module_name ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> start = time . time ( ) <NEWLINE> module = importlib . import_module ( module_name ) <NEWLINE> logger . info ( <STRING> , module_name , time . time ( ) - start ) <NEWLINE> return module <NEWLINE> <DEDENT>
elif has_energy_recuperation and gear_box_power_in < 0 : <NEWLINE> <INDENT> status = 2 <NEWLINE> <DEDENT>
elif node_id in dists : <COMMENT> <NEWLINE> <INDENT> if dist < dists [ node_id ] : <COMMENT> <NEWLINE> <INDENT> raise DispatcherError ( self , <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> elif node_id not in seen or dist < seen [ node_id ] : <COMMENT> <NEWLINE> <DEDENT> <DEDENT> seen [ node_id ] = dist <COMMENT> <NEWLINE> <DEDENT>
elif node_id in dists : <COMMENT> <NEWLINE> <INDENT> if dist < dists [ node_id ] : <COMMENT> <NEWLINE> <INDENT> raise DispatcherError ( self , <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> elif node_id not in seen or dist < seen [ node_id ] : <COMMENT> <NEWLINE> <DEDENT> <DEDENT> seen [ node_id ] = dist <COMMENT> <NEWLINE> <DEDENT>
alternator_current = calculate_alternator_current ( <NEWLINE> <INDENT> alternator_status , on_engine , gear_box_power_in , <NEWLINE> alternator_current_model , engine_start_current , <NEWLINE> battery_state_of_charge , acceleration ) <NEWLINE> <DEDENT>
<STRING> : positive , <NEWLINE> <INDENT> <STRING> : positive , <NEWLINE> <STRING> : positive , <NEWLINE> <STRING> : positive , <NEWLINE> <STRING> : _bool , <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if self . check_wait_in ( node [ <STRING> ] , dsp_id ) : <NEWLINE> <INDENT> return False <COMMENT> <NEWLINE> <DEDENT> <DEDENT>
pn = np . array ( ( full_load_speeds , full_load_powers ) ) <NEWLINE> <INDENT> max_speed_at_max_power , max_power = pn [ : , np . argmax ( pn [ 1 ] ) ] <NEWLINE> pn [ 1 ] /= max_power <NEWLINE> idle = idle_engine_speed [ 0 ] <NEWLINE> pn [ 0 ] = ( pn [ 0 ] - idle ) / ( max_speed_at_max_power - idle ) <NEWLINE> <DEDENT>
p , s = calibrate_model_params ( co2_error_function_on_emissions , p ) <NEWLINE> <INDENT> success . append ( ( s , copy . deepcopy ( p ) ) ) <NEWLINE> _set_attr ( p , vary ) <NEWLINE> <DEDENT>
<STRING> : _type ( type = And ( Use ( tuple ) , ( _type ( float ) , ) ) , <NEWLINE> <INDENT> length = 3 , <NEWLINE> read = read ) , <NEWLINE> <STRING> : function , <NEWLINE> <STRING> : tuplefloat2 , <NEWLINE> <STRING> : function , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : _bag_phases ( read = read ) , <NEWLINE> <STRING> : <NEWLINE> _type ( type = And ( Use ( tuple ) , ( And ( Use ( tuple ) , ( _type ( float ) , ) ) , ) ) , <NEWLINE> read = read ) , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : <NEWLINE> _type ( type = And ( Use ( tuple ) , ( And ( Use ( tuple ) , ( _type ( float ) , ) ) , ) ) , <NEWLINE> read = read ) , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_int , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : tuplefloat , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array_int , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array_bool , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array_sorted , <NEWLINE> <STRING> : np_array_greater_than_minus_one , <NEWLINE> _compare_str ( <STRING> ) : np_array_greater_than_minus_one , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> <STRING> : np_array , <NEWLINE> } <NEWLINE> <DEDENT>
def test_calculate_torque_out ( self ) : <NEWLINE> <INDENT> wp , es , gbs = self . wp , self . es , self . ws <NEWLINE> self . assertEquals ( <NEWLINE> <INDENT> list ( calculate_gear_box_torques ( wp , gbs , es , 10 ) ) , list ( self . tgb ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if step is not None : <NEWLINE> <INDENT> if step <= 0 : <NEWLINE> <INDENT> progr_var . set ( - step ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> progr_var . set ( progr_var . get ( ) + step ) <NEWLINE> <DEDENT> <DEDENT>
def load_interpreter_for_model ( nlp , config , persisted_path ) : <NEWLINE> <INDENT> metadata = DataRouter . read_model_metadata ( persisted_path , config ) <NEWLINE> return DataRouter . create_interpreter ( metadata , nlp ) <NEWLINE> <DEDENT>
def test_wit_data ( ) : <NEWLINE> <INDENT> td = load_data ( <STRING> , <STRING> ) <NEWLINE> assert td . entity_examples != [ ] <NEWLINE> assert td . intent_examples == [ ] <NEWLINE> assert td . entity_synonyms == { } <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> tuned_parameters = [ { <STRING> : [ 1 , 2 , 5 , 10 , 20 , 100 ] , <STRING> : [ str ( <STRING> ) ] } ] <NEWLINE> cv_splits = max ( 2 , min ( MAX_CV_FOLDS , np . min ( np . bincount ( y ) ) // 5 ) ) <COMMENT> <NEWLINE> <DEDENT>
logger . info ( <STRING> + <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> self . num_intent_examples , len ( different_intents ) ) + <NEWLINE> <DEDENT> <STRING> . format ( list_to_str ( different_intents ) ) + <NEWLINE> <STRING> . format ( <NEWLINE> <INDENT> self . num_entity_examples , len ( different_entities ) ) + <NEWLINE> <DEDENT> <STRING> . format ( list_to_str ( different_entities ) ) ) <NEWLINE> <DEDENT>
def get_example ( self , example_in_md ) : <NEWLINE> <INDENT> entities = [ ] <NEWLINE> utter = example_in_md <NEWLINE> for regex in [ ent_regex , ent_regex_with_value ] : <NEWLINE> <INDENT> utter = re . sub ( regex , <STRING> , utter ) <COMMENT> <NEWLINE> ent_matches = re . finditer ( regex , example_in_md ) <NEWLINE> for matchNum , match in enumerate ( ent_matches ) : <NEWLINE> <INDENT> if <STRING> in match . groupdict ( ) : <NEWLINE> <INDENT> entity_value_in_utter = match . groupdict ( ) [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> entity_value_in_utter = match . groupdict ( ) [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return config . component_config_from_pipeline ( name , <NEWLINE> <INDENT> self . get ( <STRING> , [ ] ) , <NEWLINE> defaults ) <NEWLINE> <DEDENT> <DEDENT>
def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return config . component_config_from_pipeline ( name , <NEWLINE> <INDENT> self . get ( <STRING> , [ ] ) , <NEWLINE> defaults ) <NEWLINE> <DEDENT> <DEDENT>
def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return config . component_config_from_pipeline ( name , <NEWLINE> <INDENT> self . get ( <STRING> , [ ] ) , <NEWLINE> defaults ) <NEWLINE> <DEDENT> <DEDENT>
def for_component ( self , name , defaults = None ) : <NEWLINE> <INDENT> return component_config_from_pipeline ( name , self . pipeline , defaults ) <NEWLINE> <DEDENT>
parser = argparse . ArgumentParser ( <NEWLINE> <INDENT> description = <STRING> ) <NEWLINE> parent_parser = argparse . ArgumentParser ( add_help = False ) <NEWLINE> add_args_to_parser ( parent_parser ) <NEWLINE> cli . arguments . add_model_and_story_group ( parent_parser , <NEWLINE> <INDENT> allow_pretrained_model = False ) <NEWLINE> utils . add_logging_option_arguments ( parent_parser ) <NEWLINE> subparsers = parser . add_subparsers ( help = <STRING> , dest = <STRING> ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> <DEDENT> <DEDENT>
parser = argparse . ArgumentParser ( <NEWLINE> <INDENT> description = <STRING> ) <NEWLINE> parent_parser = argparse . ArgumentParser ( add_help = False ) <NEWLINE> add_args_to_parser ( parent_parser ) <NEWLINE> cli . arguments . add_model_and_story_group ( parent_parser , <NEWLINE> <INDENT> allow_pretrained_model = False ) <NEWLINE> utils . add_logging_option_arguments ( parent_parser ) <NEWLINE> subparsers = parser . add_subparsers ( help = <STRING> , dest = <STRING> ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> subparsers . add_parser ( <STRING> , <NEWLINE> help = <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> , <NEWLINE> parents = [ parent_parser ] ) <NEWLINE> <DEDENT> <DEDENT>
def test_generate_training_data_original_and_augmented_trackers ( <NEWLINE> <INDENT> default_domain ) : <NEWLINE> training_trackers = training . load_data ( <NEWLINE> <STRING> , default_domain , <NEWLINE> augmentation_factor = 3 <NEWLINE> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> original_trackers = [ <NEWLINE> t <NEWLINE> for t in training_trackers if not <NEWLINE> <INDENT> hasattr ( t , <STRING> ) or not t . is_augmented <NEWLINE> ] <NEWLINE> assert len ( original_trackers ) == 3 <NEWLINE> assert len ( training_trackers ) <= 33 <NEWLINE> <DEDENT> <DEDENT>
for tracker , states , actions in zip ( augmented_trackers , all_states_augmented , all_actions_augmented ) : <NEWLINE> <INDENT> recalled = trained_policy . recall ( states , tracker , default_domain ) <NEWLINE> assert recalled == 0 <NEWLINE> <DEDENT>
model_path = get_model ( model ) <NEWLINE> <INDENT> core_path , nlu_path = get_model_subdirectories ( model_path ) <NEWLINE> _endpoints = AvailableEndpoints . read_endpoints ( endpoints ) <NEWLINE> <DEDENT>
_agent = Agent . load ( model_path , interpreter = _interpreter ) <NEWLINE>
_agent = Agent . load ( model_path , interpreter = _interpreter ) <NEWLINE> <NL> <INDENT> kwargs = minimal_kwargs ( kwargs , rasa . core . test , [ <STRING> , <STRING> ] ) <NEWLINE> <DEDENT>
with TempDirectoryPath ( tempfile . mkdtemp ( ) ) as train_path : <NEWLINE> <INDENT> await train ( <NEWLINE> <INDENT> domain , <NEWLINE> file_importer , <NEWLINE> train_path , <NEWLINE> policy_config = policy_config , <NEWLINE> exclusion_percentage = percentage , <NEWLINE> kwargs = kwargs , <NEWLINE> dump_stories = dump_stories , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if <STRING> in self . component_config : <NEWLINE> <INDENT> self . use_cls_token = self . component_config [ <STRING> ] <NEWLINE> else : <NEWLINE> self . use_cls_token = True <NEWLINE> <DEDENT>
assert len ( report . keys ( ) ) == 8 <NEWLINE> <INDENT> assert report [ <STRING> ] == a_results <NEWLINE> assert report [ <STRING> ] == e_results <NEWLINE> <DEDENT>
return tf . gather ( tiled , idxs , batch_dims = 1 ) <NEWLINE>
if self . pooling_operation == <STRING> : <NEWLINE> <INDENT> return np . mean ( non_zero_features , axis = 0 , keepdims = True ) <NEWLINE> elif self . pooling_operation == <STRING> : <NEWLINE> return np . max ( features , axis = 0 , keepdims = True ) <NEWLINE> else : <NEWLINE> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if self . pooling_operation == <STRING> : <NEWLINE> <INDENT> return np . mean ( non_zero_features , axis = 0 , keepdims = True ) <NEWLINE> elif self . pooling_operation == <STRING> : <NEWLINE> return np . max ( non_zero_features , axis = 0 , keepdims = True ) <NEWLINE> else : <NEWLINE> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for idx in range ( 1 , len ( entities ) ) : <NEWLINE> <INDENT> if entities [ idx ] [ <STRING> ] == entities [ idx - 1 ] [ <STRING> ] : <NEWLINE> <INDENT> if entity_indices and entity_indices [ - 1 ] [ - 1 ] == idx - 1 : <NEWLINE> <INDENT> entity_indices [ - 1 ] . append ( idx ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> entity_indices . append ( [ idx - 1 , idx ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if best_model_epoch >= 0 : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> if self . model_summary_file is not None : <NEWLINE> self . _write_model_summary ( ) <NEWLINE> <DEDENT>
val_results = self . _get_metric_results ( prefix = <STRING> ) <NEWLINE> <INDENT> if self . _does_model_improve ( val_results ) : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> best_model_epoch = epochs <NEWLINE> self . save ( self . best_model_file , overwrite = True ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> fh_last = open ( last_path , <STRING> ) <NEWLINE> last_checked = pickle . load ( fh_last ) <NEWLINE> if last_checked < time . time ( ) - 24 * 60 * 60 : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pickle . dump ( time . time ( ) , fh_last ) <NEWLINE> <DEDENT> <DEDENT> except IOError : <NEWLINE> <INDENT> fh_last = open ( last_path , <STRING> ) <NEWLINE> pickle . dump ( time . time ( ) , fh_last ) <NEWLINE> <DEDENT> except UnpicklingError : <NEWLINE> <INDENT> pickle . dump ( time . time ( ) , fh_last ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> <COMMENT> <NL> <DEDENT> finally : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> fh_last . close ( ) <COMMENT> <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
dir_path = con . directory <NEWLINE>
return variant_copy <NEWLINE>
if isinstance ( video_id , str ) : <NEWLINE> <INDENT> captions = _get_captions ( video_id , lang_code = lang_code , parser = parser , ** kwargs ) <NEWLINE> else : <NEWLINE> captions = [ ] <NEWLINE> for v_id in video_id : <NEWLINE> <INDENT> captions . append ( _get_captions ( v_id , lang_code = lang_code , parser = parser , ** kwargs ) ) <NEWLINE> return captions <NEWLINE> <DEDENT> <DEDENT>
with bz2 . BZ2File ( bad_pkg_path , <STRING> ) as f : <NEWLINE> <INDENT> f . write ( <STRING> . encode ( ) ) <NEWLINE> assert bad_pkg_name in os . listdir ( bad_pkg_root ) <NEWLINE> conda_mirror . _validate_packages ( repodata , bad_pkg_root ) <NEWLINE> assert bad_pkg_name not in os . listdir ( bad_pkg_root ) <NEWLINE> <DEDENT>
def daterange ( start , end , step = datetime . timedelta ( 1 ) ) : <NEWLINE> <INDENT> curr = start <NEWLINE> while curr <= end : <NEWLINE> <INDENT> yield curr <NEWLINE> curr += step <NEWLINE> <DEDENT> <DEDENT>
__REQUIRED_INIT_KWARGS = { AMQP_URL , MODEL_EXCHANGE } <NEWLINE> <INDENT> __OPTIONAL_INIT_KWARGS = set ( ) <NEWLINE> __ALLOWED_INIT_KWARGS = __REQUIRED_INIT_KWARGS | __OPTIONAL_INIT_KWARGS <NEWLINE> <DEDENT>
def add_plugin ( self , target , ** kw ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> from kframe . base . plugin import Plugin <NEWLINE> if not issubclass ( target , Plugin ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( str ( target ) ) ) <NEWLINE> <DEDENT> self . plugin_t [ kw . get ( <STRING> , target . name ) ] = { <NEWLINE> <INDENT> <STRING> : target , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else True , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else False , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else ( ) , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else { } , <NEWLINE> <STRING> : kw [ <STRING> ] if <STRING> in kw else [ ] , <NEWLINE> <DEDENT> } <NEWLINE> return self <NEWLINE> <DEDENT>
def update ( self , * args , ** kwargs ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cfg = { } <NEWLINE> for arg in args : <NEWLINE> <INDENT> if not isinstance ( arg , dict ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> cfg . update ( arg ) <NEWLINE> <DEDENT> cfg . update ( kwargs ) <NEWLINE> if <STRING> in cfg : <NEWLINE> <INDENT> cfg [ <STRING> ] = self . _convert_shedule ( cfg [ <STRING> ] ) <NEWLINE> <DEDENT> self . cfg . update ( cfg ) <NEWLINE> <DEDENT>
def update_version ( self , version ) : <NEWLINE> <INDENT> version = self . versioning ( version ) <NEWLINE> content = self . get_pkg_init ( ) <NEWLINE> if <STRING> not in content and not content . startswith ( <STRING> ) : <NEWLINE> <INDENT> raise IOError ( <STRING> ) <NEWLINE> <DEDENT> lines = content . splitlines ( ) <NEWLINE> for i , line in enumerate ( lines ) : <NEWLINE> <INDENT> if line . startswith ( <STRING> ) : <NEWLINE> <INDENT> line = line . split ( <STRING> , 1 ) [ 1 ] . strip ( ) <NEWLINE> line [ 1 ] = version . encode ( <STRING> ) <NEWLINE> lines [ i ] = <STRING> . join ( line ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> with open ( self . pkg_init , <STRING> ) as f : <NEWLINE> <INDENT> f . write ( <STRING> . join ( lines ) ) <NEWLINE> <DEDENT> return version <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def _get_effective_entry_date ( effective_entry_date ) : <NEWLINE> <INDENT> _date = datetime . datetime . today ( ) <NEWLINE> _date += datetime . timedelta ( days = effective_entry_date ) <NEWLINE> while _date . isoweekday ( ) in WEEKEND : <NEWLINE> <INDENT> _date += datetime . timedelta ( days = 1 ) <NEWLINE> <DEDENT> return _date . strftime ( day_format_string ) <NEWLINE> <DEDENT> <DEDENT>
if rating_obj and rating == 0 : <NEWLINE> <INDENT> return rating_obj . clear ( ) <NEWLINE> <DEDENT>
if self . segment_mode : <NEWLINE> <INDENT> return self . data [ index ] , img_tensor , self . labels [ index ] <NEWLINE> <DEDENT>
thr = thr_map [ ... , None ] [ ... , None ] <NEWLINE> <INDENT> segmented = ( prob_map > thr . byte ( ) ) <NEWLINE> <DEDENT>
if gen_images : <NEWLINE> <INDENT> img = segmented_img . clone ( ) . cpu ( ) . numpy ( ) <NEWLINE> img_score . add_array ( img , img_obj . ground_truth ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> IMG . fromarray ( np . array ( img , dtype = np . uint8 ) ) . save ( <NEWLINE> <INDENT> os . path . join ( self . log_dir , img_obj . file_name . split ( <STRING> ) [ 0 ] + <STRING> ) ) <NEWLINE> else : <NEWLINE> <DEDENT> img_score . add_tensor ( segmented_img , gt ) <NEWLINE> eval_score += img_score . get_prf1a ( ) [ 2 ] <NEWLINE> <DEDENT>
if gen_images : <NEWLINE> <INDENT> map_img = map_img . cpu ( ) . numpy ( ) <NEWLINE> predicted_img = predicted_img . cpu ( ) . numpy ( ) <NEWLINE> img_score . add_array ( predicted_img , img_obj . ground_truth ) <NEWLINE> IMG . fromarray ( np . array ( predicted_img , dtype = np . uint8 ) ) . save ( <NEWLINE> <INDENT> os . path . join ( self . log_dir , <STRING> + img_obj . file_name . split ( <STRING> ) [ 0 ] + <STRING> ) ) <NEWLINE> <DEDENT> IMG . fromarray ( np . array ( map_img , dtype = np . uint8 ) ) . save ( <NEWLINE> <INDENT> os . path . join ( self . log_dir , img_obj . file_name . split ( <STRING> ) [ 0 ] + <STRING> ) ) <NEWLINE> else : <NEWLINE> <DEDENT> img_score . add_tensor ( predicted_img , gt ) <NEWLINE> eval_score += img_score . get_prf1a ( ) [ 2 ] <NEWLINE> <DEDENT>
def get_oneSigmaNoise ( self ) : <NEWLINE> <INDENT> return ( np . exp ( self . get_oneHourNoiseLnsigma ( ) ) / <NEWLINE> <INDENT> np . sqrt ( self . integration / 60. ) ) <NEWLINE> <DEDENT> <DEDENT>
beta_condmean_guess = self . regcoef_summarizer . extrapolate_beta_condmean ( gshrink , lshrink ) <NEWLINE> <INDENT> loglik_hessian_matvec = model . get_hessian_matvec_operator ( beta_condmean_guess ) <NEWLINE> precond_scale = self . compute_preconditioning_scale ( gshrink , lshrink ) <NEWLINE> precond_prior_prec = np . concatenate ( ( <NEWLINE> <INDENT> ( self . prior_sd_for_unshrunk / precond_scale [ self . n_unshrunk ] ) ** - 2 , <NEWLINE> np . ones ( len ( lshrink ) ) <NEWLINE> <DEDENT> ) ) <NEWLINE> precond_hessian_matvec = lambda beta : precond_prior_prec * beta - precond_scale * loglik_hessian_matvec ( precond_scale * beta ) <NEWLINE> precond_hessian_op = sp . sparse . linalg . LinearOperator ( <NEWLINE> <INDENT> ( X . shape [ 1 ] , X . shape [ 1 ] ) , precond_hessian_matvec <NEWLINE> <DEDENT> ) <NEWLINE> eigval = sp . sparse . linalg . eigsh ( <NEWLINE> <INDENT> precond_hessian_op , k = 1 , return_eigenvectors = False ) <NEWLINE> <DEDENT> max_curvature = eigval [ 0 ] <NEWLINE> <DEDENT>
if model == <STRING> : <NEWLINE> <INDENT> self . model = LinearModel ( outcome , X ) <NEWLINE> elif model == <STRING> : <NEWLINE> n_success , n_trial = outcome <NEWLINE> self . model = LogisticModel ( n_success , n_trial , X ) <NEWLINE> elif model == <STRING> : <NEWLINE> self . model = CoxModel ( event_time , censoring_time , X ) <NEWLINE> else : <NEWLINE> raise NotImplementedError ( ) <NEWLINE> <DEDENT>
MIN_TIME = - ( 16 << 56 ) <NEWLINE> <INDENT> MAX_TIME = 48 << 56 <NEWLINE> MAX_POINTWIDTH = 63 <NEWLINE> <DEDENT>
if source_url is not None : <NEWLINE> <INDENT> source_domain = urlparse ( source_url ) . netloc <NEWLINE> proper_url = urljoin ( source_url , url ) <NEWLINE> proper_url = redirect_back ( proper_url , source_domain ) <NEWLINE> proper_url = remove_args ( proper_url ) <NEWLINE> else : <NEWLINE> proper_url = remove_args ( url ) <NEWLINE> <DEDENT>
if <STRING> in response : <NEWLINE> <INDENT> raise OnelyaAPIError ( method , response , data ) <NEWLINE> return response <NEWLINE> <DEDENT>
def sanitize_redirect ( host , redirect_to ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if not redirect_to or not isinstance ( redirect_to , six . string_types ) or getattr ( redirect_to , <STRING> , None ) and not isinstance ( redirect_to . decode ( ) , six . string_types ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def partial_pipeline_data ( strategy , user , * args , ** kwargs ) : <NEWLINE> <INDENT> partial = strategy . session_get ( <STRING> , None ) <NEWLINE> if partial : <NEWLINE> <INDENT> idx , backend , xargs , xkwargs = strategy . partial_from_session ( partial ) <NEWLINE> kwargs = kwargs . copy ( ) <NEWLINE> kwargs . setdefault ( <STRING> , user ) <NEWLINE> kwargs . setdefault ( <STRING> , strategy . request ) <NEWLINE> kwargs . update ( xkwargs ) <NEWLINE> return idx , backend , xargs , kwargs <NEWLINE> <DEDENT> <DEDENT>
if isinstance ( exception , SocialAuthBaseException ) : <NEWLINE> <INDENT> backend_name = request . backend . name <NEWLINE> message = self . get_message ( request , exception ) <NEWLINE> url = self . get_redirect_uri ( request , exception ) <NEWLINE> try : <NEWLINE> <INDENT> messages . error ( request , message , <NEWLINE> <INDENT> extra_tags = <STRING> + backend_name ) <NEWLINE> <DEDENT> <DEDENT> except MessageFailure : <NEWLINE> <INDENT> url += ( <STRING> in url and <STRING> or <STRING> ) + <STRING> . format ( urlquote ( message ) , <NEWLINE> <INDENT> backend_name ) <NEWLINE> <DEDENT> <DEDENT> return redirect ( url ) <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def create ( settings : dict = None , base_settings : dict = None ) -> dict : <NEWLINE> <INDENT> <STRING> <NEWLINE> settings = settings or { } <NEWLINE> if base_settings : <NEWLINE> <INDENT> settings = update_dict_recur ( base_settings , settings ) <NEWLINE> <DEDENT> for key , value in os . environ . items ( ) : <NEWLINE> <INDENT> if <STRING> not in key : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> current_settings = settings <NEWLINE> parts = [ <NEWLINE> <INDENT> part . lower ( ) <NEWLINE> for part in key . replace ( <STRING> , <STRING> ) . split ( <STRING> ) <NEWLINE> <DEDENT> ] <NEWLINE> last_index = len ( parts ) - 1 <NEWLINE> for index , part in enumerate ( parts ) : <NEWLINE> <INDENT> if index == last_index : <NEWLINE> <INDENT> current_settings [ part ] = _convert_value ( value ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> current_settings = current_settings . setdefault ( part , { } ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return settings <NEWLINE> <DEDENT> <DEDENT>
if attribute_map : <NEWLINE> <INDENT> if hasattr ( parents [ 0 ] , <STRING> ) : <NEWLINE> <INDENT> full_attribute_map = dict ( parents [ 0 ] . attribute_map ) <NEWLINE> full_attribute_map . update ( attribute_map ) <NEWLINE> attribute_map = full_attribute_map <NEWLINE> <DEDENT> class_dict [ <STRING> ] = attribute_map <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> target_layer . update ( update_params = filtered_payload , token = token ) <NEWLINE> <DEDENT>
def html_snippet ( obj ) : <NEWLINE> <INDENT> loc = IGeolocation ( obj ) . coords <NEWLINE> if not loc : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> lat , lon = loc <NEWLINE> <DEDENT> content = IListing ( obj ) . summary <NEWLINE> if lat > 0 : <NEWLINE> <INDENT> hemi = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> hemi = <STRING> <NEWLINE> <DEDENT> return <STRING> % ( hemi , lat , lon , content ) <NEWLINE> <DEDENT>
if prop . skin != 0 and prop . skin < len ( mdl . skins ) : <NEWLINE> <COMMENT> <NL> <INDENT> swap_skins = dict ( zip ( <NEWLINE> <INDENT> mdl . skins [ 0 ] , <NEWLINE> mdl . skins [ prop . skin ] <NEWLINE> <DEDENT> ) ) <NEWLINE> for tri in child_ref . triangles : <NEWLINE> <INDENT> tri . mat = swap_skins . get ( tri . mat , tri . mat ) <NEWLINE> <DEDENT> <DEDENT>
if self . map . by_target [ base_name ] : <NEWLINE> <COMMENT> <NL> <INDENT> for i in itertools . count ( start = 1 ) : <NEWLINE> <INDENT> name = base_name + str ( i ) <NEWLINE> if not self . map . by_target [ name ] : <NEWLINE> <INDENT> self [ <STRING> ] = name <NEWLINE> break <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> self [ <STRING> ] = base_name <NEWLINE> <DEDENT>
run_transformations ( vmf , fsys , packlist , bsp_file , game_info , studiomdl_loc ) <NEWLINE>
def evalPolynomialDerivative ( poly , u , der = 1 ) : <NEWLINE> <INDENT> z , x = poly <NEWLINE> f = np . poly1d ( z ) <NEWLINE> f2 = np . polyder ( f , m = der ) <NEWLINE> tck , dummy = interpolate . splprep ( [ x . tolist ( ) , x . tolist ( ) ] , s = 0 , k = 1 ) <NEWLINE> xU = np . array ( interpolate . splev ( u , tck ) [ 1 ] ) <NEWLINE> out = f2 ( xU ) <NEWLINE> p = np . array ( [ np . ones ( ( xU . shape [ 0 ] , ) ) , out ] ) . T <NEWLINE> return p <NEWLINE> <DEDENT>
def datahandler ( line ) : <NEWLINE> <INDENT> global n <NEWLINE> if n >= clmax : <NEWLINE> <INDENT> raise StopIteration <NEWLINE> <DEDENT> <DEDENT>
modt = time . localtime ( os . path . getmtime ( path ) ) <NEWLINE> <INDENT> mods = time . strftime ( <STRING> , modt ) <NEWLINE> return self . format % { <NEWLINE> <INDENT> <STRING> : mods , <NEWLINE> <STRING> : stream , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT>
activities , connectivities = evolve ( initial_conditions , adjacencies , timesteps = 100 , <NEWLINE> <INDENT> activity_rule = lambda n , c , t : ActivityRule . nks_ca_rule ( n , c , 30 ) ) <NEWLINE> <DEDENT>
activities , connectivities = evolve ( initial_conditions , hopfield_net . adjacency_matrix , timesteps = 155 , <NEWLINE> <INDENT> activity_rule = hopfield_net . activity_rule ) <NEWLINE> <DEDENT>
def test_set_from_map_valid_bool ( self ) : <NEWLINE> <INDENT> test_value = True <NEWLINE> new_value = 0 <NEWLINE> test_config = { <NEWLINE> <INDENT> <STRING> : 1 , <NEWLINE> <STRING> : test_value , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : { <NEWLINE> <INDENT> <STRING> : <STRING> <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> } <NEWLINE> path = [ <STRING> ] <NEWLINE> set_by_path ( test_config , path , new_value , is_bool = True ) <NEWLINE> value = get_by_path ( test_config , path ) <NEWLINE> assert value == bool ( new_value ) and type ( value ) == bool , <STRING> . format ( <STRING> , bool ( new_value ) , type ( value ) , value ) <NEWLINE> set_by_path ( test_config , path , new_value , is_bool = False ) <NEWLINE> int_value = get_by_path ( test_config , path ) <NEWLINE> assert value == int_value and type ( int_value ) == int , <STRING> . format ( <STRING> , int_value , type ( int_value ) , new_value ) <NEWLINE> <DEDENT>
def move_is_rack_size_or_less ( location_set ) : <NEWLINE> <INDENT> return len ( location_set ) <= config . PLAYER_RACK_SIZE <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>
<COMMENT> <NL> <INDENT> def center ( target , reference = None ) : <NEWLINE> <INDENT> target . update_idletasks ( ) <NEWLINE> if reference is None : <NEWLINE> <INDENT> geometry = get_screen_geometry ( ) <NEWLINE> <DEDENT> elif not isinstance ( reference , ( str , Geometry ) ) : <NEWLINE> <INDENT> reference . update_idletasks ( ) <NEWLINE> geometry = reference . winfo_geometry ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> geometry = reference <NEWLINE> <DEDENT> if isinstance ( geometry , str ) : <NEWLINE> <INDENT> geometry = Geometry ( geometry ) <NEWLINE> <DEDENT> target_geometry = Geometry ( target . winfo_geometry ( ) ) <NEWLINE> target . geometry ( str ( target_geometry . set_to_center ( geometry ) ) ) <NEWLINE> <DEDENT> <DEDENT>
def test_NLPIterations ( state ) : <NEWLINE> <INDENT> reward_func = R . NLPIterations ( ) <NEWLINE> reward_func . reset ( state ) <NEWLINE> assert reward_func . get ( state ) <= 0 <NEWLINE> assert reward_func . get ( state , done = True ) == 0 <NEWLINE> <DEDENT>
def test_NNodes ( model ) : <NEWLINE> <INDENT> reward_func = R . NNodes ( ) <NEWLINE> reward_func . reset ( model ) <NEWLINE> assert reward_func . obtain_reward ( model ) >= 0 <NEWLINE> <DEDENT>
def middleMouseButtonRelease ( self , event : QMouseEvent ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> fake_event = QMouseEvent ( <NEWLINE> <INDENT> event . type ( ) , <NEWLINE> event . localPos ( ) , <NEWLINE> event . screenPos ( ) , <NEWLINE> Qt . LeftButton , <NEWLINE> event . buttons ( ) | - Qt . LeftButton , <NEWLINE> event . modifiers ( ) , <NEWLINE> <DEDENT> ) <NEWLINE> super ( ) . mouseReleaseEvent ( fake_event ) <NEWLINE> self . setDragMode ( QGraphicsView . RubberBandDrag ) <NEWLINE> <DEDENT>
