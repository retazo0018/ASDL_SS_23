def postprocess ( self , content ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> node = content . node <NEWLINE> if len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> return node <NEWLINE> <DEDENT> attributes = AttrList ( node . attributes ) <NEWLINE> if attributes . rlen ( ) and not len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> p = Factory . property ( node . name , node . getText ( ) ) <NEWLINE> return merge ( content . data , p ) <NEWLINE> <DEDENT> if len ( content . data ) : <NEWLINE> <INDENT> return content . data <NEWLINE> <DEDENT> lang = attributes . lang ( ) <NEWLINE> if not len ( node . children ) and content . text is None : <NEWLINE> <INDENT> if self . nillable ( content . data ) or content . node . isnil ( ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return xlstr . string ( <STRING> , lang ) <NEWLINE> <DEDENT> <DEDENT> if isinstance ( content . text , basestring ) : <NEWLINE> <INDENT> return xlstr . string ( content . text , lang ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return content . text <NEWLINE> <DEDENT> <DEDENT>
Definition ( <STRING> , ( list , tuple ) , [ ] ) , <NEWLINE>
def get_characters_per_line ( self , font_width ) : <NEWLINE> <INDENT> return self . BASE_CHARS_PER_LINE // font_width <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for sample in overlap_samples : <NEWLINE> <INDENT> if bsub_flag : <NEWLINE> <INDENT> print ( <STRING> . format ( dt . now ( ) . strftime ( <STRING> ) , sample ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
return False , <STRING> <NEWLINE>
while end <= len ( self . _table_data ) : <NEWLINE> <INDENT> filt_data . extend ( self . _filter_list ( self . _table_data [ start : end ] , mask ) ) <NEWLINE> start = end <NEWLINE> end += row_lenght <NEWLINE> <DEDENT>
def request ( method , path , queryParams = None , content = None ) : <NEWLINE> <INDENT> res = cloudshare . req ( hostname = <STRING> , <NEWLINE> <INDENT> method = method , <NEWLINE> apiId = API_ID , <NEWLINE> apiKey = API_KEY , <NEWLINE> path = path , <NEWLINE> queryParams = queryParams , <NEWLINE> content = content ) <NEWLINE> <DEDENT> if res . status // 100 != 2 : <NEWLINE> <INDENT> raise Exception ( <STRING> . format ( res . status , res . content [ <STRING> ] ) ) <NEWLINE> <DEDENT> return res . content <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if os . path . isfile ( testing_filename ) : <NEWLINE> <INDENT> testing_dataset . load ( testing_filename , args . nb_testing_image ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> input_image_dir = os . path . join ( input_repo , <STRING> ) <NEWLINE> testing_dataset . populate ( preprocessed_testing_path , input_image_dir , <NEWLINE> <INDENT> nb_images = args . nb_testing_image , labelling = False ) <NEWLINE> <DEDENT> testing_dataset . save ( testing_filename ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if split is not None : <NEWLINE> <INDENT> assert 0.0 < split < 1.0 <NEWLINE> <DEDENT> <DEDENT>
token2idx = self . ot_dict . copy ( ) <NEWLINE> <INDENT> if <STRING> in self . corpus_path : <NEWLINE> <INDENT> with open ( file = self . corpus_path , mode = <STRING> , encoding = <STRING> ) as fd : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> term_one = fd . readline ( ) <NEWLINE> if not term_one : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> if term_one not in token2idx : <NEWLINE> <INDENT> token2idx [ term_one ] = len ( token2idx ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> elif os . path . exists ( self . corpus_path ) : <NEWLINE> <INDENT> with open ( file = self . corpus_path , mode = <STRING> , encoding = <STRING> ) as fd : <NEWLINE> <INDENT> terms = fd . readlines ( ) <NEWLINE> for line in terms : <NEWLINE> <INDENT> ques_label = json . loads ( line . strip ( ) ) <NEWLINE> term_one = ques_label [ <STRING> ] <NEWLINE> term_one = <STRING> . join ( term_one ) <NEWLINE> if self . level_type == <STRING> : <NEWLINE> <INDENT> text = list ( term_one . replace ( <STRING> , <STRING> ) . strip ( ) ) <NEWLINE> <DEDENT> elif self . level_type == <STRING> : <NEWLINE> <INDENT> text = macropodus_cut ( term_one ) <NEWLINE> <DEDENT> elif self . level_type == <STRING> : <NEWLINE> <INDENT> text = get_ngrams ( term_one , ns = self . ngram_ns ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> for text_one in text : <NEWLINE> <INDENT> if text_one not in token2idx : <NEWLINE> <INDENT> token2idx [ text_one ] = len ( token2idx ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> self . token2idx = token2idx <NEWLINE> self . idx2token = { } <NEWLINE> for key , value in self . token2idx . items ( ) : <NEWLINE> <INDENT> self . idx2token [ value ] = key <NEWLINE> <DEDENT> <DEDENT>
def convert ( self , case ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return IRCString ( case , self ) <NEWLINE> <DEDENT>
if self . casecmp ( target . nick , basicrfc . nick ) : <NEWLINE> <COMMENT> <NL> <INDENT> isupport = self . get_extension ( <STRING> ) <NEWLINE> <DEDENT>
def inject_line ( self , line ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> assert isinstance ( line , Line ) <NEWLINE> self . recvq . put ( line ) <NEWLINE> <DEDENT>
if self . background is not None : <NEWLINE> <INDENT> bgc = ColoursANSI [ self . background . name ] . value <NEWLINE> ret . append ( str ( bgc . background_16 ) ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> ret . append ( self . fmt_resetbackground ) <NEWLINE> <DEDENT>
if User is not PFUser : <NEWLINE> <INDENT> raise ImproperlyConfigured ( <STRING> ) <NEWLINE> <DEDENT>
self . workers . append ( ( name , worker , i ) ) <NEWLINE>
if pipeline . enable_task ( config [ <STRING> ] , <STRING> ) : <NEWLINE> <INDENT> step = <STRING> . format ( i ) <NEWLINE> table = prefix + <STRING> <NEWLINE> fieldtoplot = [ ] <NEWLINE> fieldtoplot . append ( utils . get_field_id ( msinfo , trans ) [ 0 ] ) <NEWLINE> recipe . add ( <STRING> , step , <NEWLINE> <INDENT> { <NEWLINE> <INDENT> <STRING> : <STRING> . format ( get_dir_path ( pipeline . caltables , pipeline ) , table , <STRING> ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : fieldtoplot , <NEWLINE> <STRING> : corr_indexes [ config [ <STRING> ] [ <STRING> ] . get ( <STRING> ) ] , <NEWLINE> <STRING> : <STRING> . format ( get_dir_path ( pipeline . reports , pipeline ) ) + <STRING> . format ( prefix ) <NEWLINE> <DEDENT> } , <NEWLINE> input = pipeline . input , <NEWLINE> output = pipeline . output , <NEWLINE> label = <STRING> . format ( step , msname ) ) <NEWLINE> <DEDENT> <DEDENT>
if config [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> version = config [ <STRING> ] [ <STRING> ] <NEWLINE> substep = <STRING> . format ( version , target_iter ) <NEWLINE> manflags . restore_cflags ( pipeline , recipe , version , fms , cab_name = substep ) <NEWLINE> available_flagversions = manflags . get_flags ( pipeline , fms ) <NEWLINE> if available_flagversions [ - 1 ] != version : <NEWLINE> <INDENT> substep = <STRING> . format ( version , target_iter ) <NEWLINE> manflags . delete_cflags ( pipeline , recipe , <NEWLINE> <INDENT> available_flagversions [ available_flagversions . index ( version ) + 1 ] , <NEWLINE> fms , cab_name = substep ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
assert re . count == 420 <NEWLINE> <INDENT> assert re . me is True <NEWLINE> test_state . parse_emoji . assert_called_with ( emoji_dict , None ) <NEWLINE> <DEDENT>
for role_id in role_ids : <NEWLINE> <INDENT> role_obj = self . fabric . state_registry . get_role_by_id ( guild_id , role_id ) <NEWLINE> if role_obj is not None : <NEWLINE> <INDENT> role_objs . append ( role_obj ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . logger . warning ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> role_id , <NEWLINE> user_id , <NEWLINE> guild_id , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
if plat != <STRING> : <NEWLINE> <INDENT> if plat == <STRING> : <NEWLINE> <INDENT> supports_color |= os . getenv ( <STRING> , None ) == <STRING> <NEWLINE> supports_color |= <STRING> in os . environ <NEWLINE> supports_color &= is_a_tty <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> supports_color = is_a_tty <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def urlparse ( cls , url ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> _url = deepcopy ( url ) <NEWLINE> if url [ 0 : 5 ] == <STRING> : <NEWLINE> <INDENT> _url = cls . https_to_s3 ( url ) <NEWLINE> <DEDENT> if _url [ 0 : 5 ] != <STRING> : <NEWLINE> <INDENT> raise Exception ( <STRING> % _url ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if isinstance ( corr , Pha ) : <NEWLINE> <INDENT> self . corr = corr <NEWLINE> self . input_corr = True <NEWLINE> <DEDENT> elif corr is None : <NEWLINE> <INDENT> self . input_corr = False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . input_corr = False <NEWLINE> message . error ( <STRING> ) <NEWLINE> return 1 <NEWLINE> <DEDENT> <DEDENT>
def create_real_first_image ( self , path = <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> apcor_str = <STRING> <NEWLINE> with open ( self . get_abs_path ( path ) , <STRING> ) as fh : <NEWLINE> <INDENT> self . first_image = DownloadedFitsImage ( fh . read ( ) , Mock ( ) , apcor_str , in_memory = True ) <NEWLINE> first_reading = self . model . get_current_workunit ( ) . get_sources ( ) [ 0 ] . get_readings ( ) [ 0 ] <NEWLINE> self . model . _on_image_loaded ( first_reading , self . first_image ) <NEWLINE> <DEDENT> <DEDENT>
name = m . pop ( 0 ) <NEWLINE> <INDENT> app , model = name . lower ( ) . split ( <STRING> ) <NEWLINE> if app != instance . _meta . app_label or model != instance . _meta . module_name : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT>
if aggregation_method not in aggregates : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if len ( forces ) == 0 : <NEWLINE> <INDENT> updater ( dt , omegaB , ensemble . x , ensemble . v ) <NEWLINE> else : <NEWLINE> updater ( 0.5 * dt , omegaB , ensemble . x , ensemble . v ) <NEWLINE> f = np . zeros_like ( ensemble . v ) <NEWLINE> for force in forces : <NEWLINE> <INDENT> force . force ( dt , ensemble , f ) <NEWLINE> <DEDENT> ensemble . v += f / m <NEWLINE> updater ( 0.5 * dt , omegaB , ensemble . x , ensemble . v ) <NEWLINE> <DEDENT>
plugins = acl [ <STRING> ] <NEWLINE> <INDENT> authenticators = plugins . listPlugins ( IAuthenticationPlugin ) <NEWLINE> for authenticator_id , auth in authenticators : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> info = auth . authenticateCredentials ( credentials ) <NEWLINE> if info is not None and info [ 0 ] is not None : <NEWLINE> <COMMENT> <NL> <INDENT> return info <NEWLINE> <DEDENT> <DEDENT> except _SWALLOWABLE_PLUGIN_EXCEPTIONS : <NEWLINE> <INDENT> logger . info ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> , <NEWLINE> <STRING> . join ( auth . getPhysicalPath ( ) ) , <NEWLINE> credentials [ <STRING> ] ) <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>
lpca = self . new_task ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> Jplace_PCA , <NEWLINE> containerinfo = highmem_containerinfo , <NEWLINE> path = os . path . join ( <NEWLINE> <INDENT> self . destination_dir , <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) , <NEWLINE> prefix = <STRING> , <NEWLINE> pca = <STRING> <NEWLINE> ) <NEWLINE> lpca . in_refpkg_tgz = refpkg_tgz . out_refpkg_tgz <NEWLINE> lpca . in_seq_map = seq_map . out_file <NEWLINE> lpca . in_jplace = redup_jplace . out_jplace <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> batch_errModels = { } <NEWLINE> for batch , batched_specimens in manifest . batched_specimens ( ) : <NEWLINE> <INDENT> batch_errModels [ batch ] = self . new_task ( <NEWLINE> <INDENT> <STRING> . format ( batch ) , <NEWLINE> DADA2_LearnError , <NEWLINE> containerinfo = midcpu_containerinfo , <NEWLINE> batch = batch , <NEWLINE> tar_reads = False , <NEWLINE> path = os . path . join ( <NEWLINE> <INDENT> self . working_dir , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> batch_errModels [ batch ] . in_reads = [ <NEWLINE> <INDENT> specimen_tasks [ s ] [ <STRING> ] . out_reads <NEWLINE> for s in specimen_tasks <NEWLINE> if s in batched_specimens <NEWLINE> <DEDENT> ] <NEWLINE> for specimen in batched_specimens : <NEWLINE> <INDENT> specimen_tasks [ specimen ] [ <STRING> ] = batch_errModels [ batch ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
placement_db_classified = self . new_task ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> PlacementDB_Classify_SV , <NEWLINE> containerinfo = midcpu_containerinfo , <NEWLINE> ) <NEWLINE> placement_db_classified . in_placement_db = placement_db_w_si . out_placement_db <NEWLINE> placement_db_classified . in_refpkg_tgz = refpkg_tgz . out_refpkg_tgz <NEWLINE> placement_db_classified . in_sv_refpkg_aln_sto = sv_refpkg_aln_sto . out_aln_sto <NEWLINE> placement_db_classified . in_jplace = jplace . out_file <NEWLINE> <DEDENT>
placement_db_classified = self . new_task ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> PlacementDB_Classify_SV , <NEWLINE> containerinfo = highmem_containerinfo , <NEWLINE> ) <NEWLINE> placement_db_classified . in_placement_db = placement_db_w_si . out_placement_db <NEWLINE> placement_db_classified . in_refpkg_tgz = refpkg_tgz . out_refpkg_tgz <NEWLINE> placement_db_classified . in_sv_refpkg_aln_sto = sv_refpkg_aln_sto . out_aln_sto <NEWLINE> placement_db_classified . in_jplace = jplace . out_file <NEWLINE> <DEDENT>
labels = kwargs . get ( <STRING> , False ) <NEWLINE> <INDENT> if labels : <NEWLINE> <INDENT> for ptc in labels : <NEWLINE> <INDENT> if ptc not in self . participants : <NEWLINE> <INDENT> self . add_participant ( ptc ) <NEWLINE> <DEDENT> self . ptcs_update_labels ( labels ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
offset = delta % index . step <NEWLINE> <INDENT> if offset == 0 : <NEWLINE> <INDENT> offset = None <NEWLINE> else : <NEWLINE> <DEDENT> if start is None : <NEWLINE> <INDENT> offset = index . step + ( head . length - 1 ) % ( - index . step ) <NEWLINE> <DEDENT> elif start >= 0 : <NEWLINE> <INDENT> offset = index . step + min ( start , head . length - 1 ) % ( - index . step ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> offset = index . step + ( start + head . length ) % ( - index . step ) <NEWLINE> <DEDENT> <DEDENT>
def invoke ( self , args , app = None , ** kwargs ) : <NEWLINE> <INDENT> if len ( args ) != 0 : <NEWLINE> <INDENT> raise multitool . UsageError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
for collection_cls in collection_classes : <NEWLINE> <INDENT> entities . add ( collection_cls . entity ) <NEWLINE> collection = collection_cls ( storage ) <NEWLINE> self . collections_by_class_name [ collection_cls . __name__ ] = collection <NEWLINE> setattr ( self , collection_cls . plural_name , collection ) <NEWLINE> <DEDENT>
local_coordinates = _np . array ( [ [ 1.0 / 3 ] , [ 1.0 / 3 ] ] ) <NEWLINE> <INDENT> values = _np . zeros ( grid . entity_count ( 0 ) , dtype = <STRING> ) <NEWLINE> for element in grid . entity_iterator ( 0 ) : <NEWLINE> <INDENT> index = element . index <NEWLINE> local_values = np . real ( <NEWLINE> <INDENT> transformation ( obj . evaluate ( index , local_coordinates ) ) <NEWLINE> <DEDENT> ) <NEWLINE> values [ index ] = local_values . flatten ( ) <NEWLINE> <DEDENT> <DEDENT>
nshape_test = dual_to_range . number_of_shape_functions <NEWLINE> <INDENT> nshape_trial = domain . number_of_shape_functions <NEWLINE> <DEDENT>
for trial_element_index in range ( n_trial_elements ) : <NEWLINE> <INDENT> if is_adjacent [ trial_element_index ] : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> trial_element = trial_elements [ trial_element_index ] <NEWLINE> trial_normal = ( <NEWLINE> <INDENT> trial_grid_data . normals [ trial_element ] <NEWLINE> * trial_normal_multipliers [ trial_element ] <NEWLINE> <DEDENT> ) <NEWLINE> normal_prod = _np . dot ( test_normal , trial_normal ) <NEWLINE> curl_product = ( <NEWLINE> <INDENT> test_surface_curls_trans [ i ] <NEWLINE> @ trial_surface_curls [ trial_element_index ] <NEWLINE> <DEDENT> ) <NEWLINE> for test_fun_index in range ( nshape_test ) : <NEWLINE> <INDENT> for trial_fun_index in range ( nshape_trial ) : <NEWLINE> <INDENT> for quad_point_index in range ( n_quad_points ) : <NEWLINE> <INDENT> local_result [ <NEWLINE> <INDENT> trial_element_index , test_fun_index , trial_fun_index <NEWLINE> <DEDENT> ] += tmp [ <NEWLINE> <INDENT> trial_element_index * n_quad_points + quad_point_index <NEWLINE> <DEDENT> ] * ( <NEWLINE> <INDENT> curl_product [ test_fun_index , trial_fun_index ] <NEWLINE> - wavenumber <NEWLINE> * wavenumber <NEWLINE> * local_test_fun_values [ <NEWLINE> <INDENT> 0 , test_fun_index , test_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * local_trial_fun_values [ <NEWLINE> <INDENT> 0 , trial_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * normal_prod <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
for trial_element_index in range ( n_trial_elements ) : <NEWLINE> <INDENT> if is_adjacent [ trial_element_index ] : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> trial_element = trial_elements [ trial_element_index ] <NEWLINE> trial_normal = ( <NEWLINE> <INDENT> trial_grid_data . normals [ trial_element ] <NEWLINE> * trial_normal_multipliers [ trial_element ] <NEWLINE> <DEDENT> ) <NEWLINE> normal_prod = _np . dot ( test_normal , trial_normal ) <NEWLINE> curl_product = ( <NEWLINE> <INDENT> test_surface_curls_trans [ i ] <NEWLINE> @ trial_surface_curls [ trial_element_index ] <NEWLINE> <DEDENT> ) <NEWLINE> for test_fun_index in range ( nshape_test ) : <NEWLINE> <INDENT> for trial_fun_index in range ( nshape_trial ) : <NEWLINE> <INDENT> for quad_point_index in range ( n_quad_points ) : <NEWLINE> <INDENT> local_result [ <NEWLINE> <INDENT> trial_element_index , test_fun_index , trial_fun_index <NEWLINE> <DEDENT> ] += tmp [ <NEWLINE> <INDENT> trial_element_index * n_quad_points + quad_point_index <NEWLINE> <DEDENT> ] * ( <NEWLINE> <INDENT> curl_product [ test_fun_index , trial_fun_index ] <NEWLINE> - wavenumber <NEWLINE> * wavenumber <NEWLINE> * local_test_fun_values [ <NEWLINE> <INDENT> 0 , test_fun_index , test_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * local_trial_fun_values [ <NEWLINE> <INDENT> 0 , trial_fun_index , quad_point_index <NEWLINE> <DEDENT> ] <NEWLINE> * normal_prod <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
x_transformed = self . space . map_to_full_grid @ ( <NEWLINE> <INDENT> self . space . dof_transformation @ x <NEWLINE> ) <NEWLINE> result = implementation ( x_transformed ) <NEWLINE> return result . reshape ( [ kernel_dimension , - 1 ] , order = <STRING> ) <NEWLINE> <DEDENT>
def read_config ( conf_file = <STRING> ) : <NEWLINE> <INDENT> parser = AgaveConfigParser ( ) <NEWLINE> places = [ <STRING> . format ( conf_file ) , <NEWLINE> <INDENT> <STRING> . format ( conf_file ) , <NEWLINE> <STRING> . format ( os . getcwd ( ) , conf_file ) ] <NEWLINE> <DEDENT> place = places [ 0 ] <NEWLINE> for p in places : <NEWLINE> <INDENT> if os . path . exists ( p ) : <NEWLINE> <INDENT> place = p <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> if not parser . parser . read ( place ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> <NEWLINE> <INDENT> . format ( <STRING> . join ( places ) ) ) <NEWLINE> <DEDENT> <DEDENT> return parser <NEWLINE> <DEDENT>
def __call__ ( self , number_of_neighbors , number_of_common_neighbors ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if number_of_common_neighbors > number_of_neighbors : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> neighbors_list = { } <NEWLINE> for element in self . dataset_elements : <NEWLINE> <INDENT> neighbors_list [ element ] = self . calculate_k_nearest_elements ( element , number_of_neighbors ) [ : number_of_neighbors ] <NEWLINE> <DEDENT> for element , neighbors in neighbors_list . items ( ) : <NEWLINE> <INDENT> for other_element , other_neighbors in neighbors_list . items ( ) : <NEWLINE> <INDENT> if element != other_element : <NEWLINE> <COMMENT> <NL> <INDENT> if element in other_neighbors and other_element in neighbors : <NEWLINE> <INDENT> if len ( set ( neighbors ) . intersection ( other_neighbors ) ) >= number_of_common_neighbors : <NEWLINE> <INDENT> self . reconfigure_clusters ( element , other_element ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> result = defaultdict ( list ) <NEWLINE> for element , cluster_nro in self . cluster . items ( ) : <NEWLINE> <INDENT> result [ cluster_nro ] . append ( element ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> unicode_params = [ ] <NEWLINE> for k , v in params : <NEWLINE> <INDENT> if isinstance ( k , str ) : <NEWLINE> <INDENT> k = k . decode ( <STRING> ) <NEWLINE> <DEDENT> if isinstance ( v , str ) : <NEWLINE> <INDENT> if k . startswith ( <STRING> ) : <NEWLINE> <INDENT> v = utils . unescape ( v ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> v = v . decode ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> unicode_params . append ( ( k , v ) ) <NEWLINE> <DEDENT> <DEDENT>
def add_params_to_uri ( uri , params , fragment = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sch , net , path , par , query , fra = urlparse . urlparse ( uri ) <NEWLINE> if fragment : <NEWLINE> <INDENT> fra = add_params_to_qs ( fra , params ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> query = add_params_to_qs ( query , params ) <NEWLINE> <DEDENT> return urlparse . urlunparse ( ( sch , net , path , par , query , fra ) ) <NEWLINE> <DEDENT>
bindata = numpy . zeros ( <NEWLINE> <INDENT> ( T // time_bin_length , ) + data . shape [ 1 : ] , dtype = <STRING> ) <NEWLINE> for index , i in enumerate ( range ( 0 , T - time_bin_length + 1 , <NEWLINE> <INDENT> time_bin_length ) ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> bindata [ index ] = weighted_avg_and_std ( data [ i : i + time_bin_length ] , <NEWLINE> <INDENT> axis = 0 , <NEWLINE> weights = sample_selector [ i : i + <NEWLINE> time_bin_length ] ) [ 0 ] <NEWLINE> <DEDENT> <DEDENT>
def tsg_to_net ( self , node , max_lag ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> row = node // max_lag <NEWLINE> lag = node % max_lag <NEWLINE> return ( row , - lag ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if not isinstance ( self . n_symbs ** dim , int ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if self . n_symbs ** dim * 16. / 8. / 1024. ** 3 > 3. : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if dim * self . n_symbs ** dim > 2 ** 65 : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> % ( self . n_symbs , dim ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if a == b : <NEWLINE>
axo . text ( - .1 , .5 , <STRING> % hi , rotation = <STRING> , <NEWLINE>
def resize_convex ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> pending = None <NEWLINE> c0 = None <NEWLINE> for el in self [ 1 : - 1 ] : <NEWLINE> <INDENT> if not hasattr ( el , <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> c = getattr ( el , <STRING> , 0 ) <NEWLINE> if pending is not None : <NEWLINE> <INDENT> r = max ( el . radius , pending . radius ) <NEWLINE> if c <= 0 : <NEWLINE> <INDENT> el . radius = r <NEWLINE> <DEDENT> if c0 > 0 : <NEWLINE> <INDENT> pending . radius = r <NEWLINE> <DEDENT> pending = None <NEWLINE> if not el . material or el . material . solid : <NEWLINE> <INDENT> pending = el <NEWLINE> <DEDENT> <DEDENT> if not el . material or el . material . solid : <NEWLINE> <INDENT> pending , c0 = el , c <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def main ( ) : <NEWLINE> <INDENT> if len ( sys . argv ) == 3 : <NEWLINE> <INDENT> keep_count = 0 <NEWLINE> <DEDENT> elif len ( sys . argv ) == 4 : <NEWLINE> <INDENT> keep_count = int ( sys . argv [ 3 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> conf_file = sys . argv [ 1 ] <NEWLINE> if sys . argv [ 2 ] [ - 1 ] == <STRING> : <NEWLINE> <INDENT> dump_dir = sys . argv [ 2 ] [ : - 1 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> dump_dir = sys . argv [ 2 ] <NEWLINE> <DEDENT> if not os . path . isfile ( conf_file ) : <NEWLINE> <INDENT> print ( <STRING> % conf_file ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> if not os . path . isdir ( dump_dir ) : <NEWLINE> <INDENT> print ( <STRING> % dump_dir ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> if keep_count < 0 : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> make_dump ( conf_file , dump_dir , keep_count ) <NEWLINE> <DEDENT>
if not self . check_plugin_installed ( plugin_package ) : <NEWLINE> <INDENT> self . pip . install ( plugin_full_name ) <COMMENT> <NEWLINE> <DEDENT>
def convert_all_validator ( self , schema_node , validator ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> converted = None <NEWLINE> if isinstance ( validator , colander . All ) : <NEWLINE> <INDENT> converted = { } <NEWLINE> for v in validator . validators : <NEWLINE> <INDENT> ret = self ( schema_node , v ) <NEWLINE> converted . update ( ret ) <NEWLINE> <DEDENT> <DEDENT> return converted <NEWLINE> <DEDENT>
wlgrid = np . log10 ( 10000 / bindown_wngrid ) <NEWLINE> <INDENT> if args . plot : <NEWLINE> <INDENT> if get_rank ( ) == 0 and nprocs ( ) <= 1 : <NEWLINE> <INDENT> import matplotlib . pyplot as plt <NEWLINE> if args . contrib : <NEWLINE> <INDENT> for name , value in contrib : <NEWLINE> <INDENT> new_value = bindown ( native_grid , value , bindown_wngrid ) <NEWLINE> plt . plot ( wlgrid , new_value , label = name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if solution is not None : <NEWLINE> <INDENT> out . store_dictionary ( solution , group_name = <STRING> ) <NEWLINE> priors = { } <NEWLINE> priors [ <STRING> ] = profiles <NEWLINE> priors [ <STRING> ] = spectrum <NEWLINE> out . store_dictionary ( priors , group_name = <STRING> ) <NEWLINE> else : <NEWLINE> out . store_dictionary ( profiles , group_name = <STRING> ) <NEWLINE> out . store_dictionary ( spectrum , group_name = <STRING> ) <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> from mpi4py import MPI <NEWLINE> except ImportError : <NEWLINE> return 1 <NEWLINE> <DEDENT>
log_start = time <NEWLINE> <INDENT> click . echo ( <STRING> ) <NEWLINE> context . session . add_time_entry ( <NEWLINE> <INDENT> start_time = time , description = message , project = project_obj <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> foundDocuments . append ( GraphDbPackedSegmentTuple ( <NEWLINE> <INDENT> key = subKey , <NEWLINE> packedJson = segmentsByKey [ subKey ] <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT>
def get_matsize ( m ) : <NEWLINE> <INDENT> size = 0 <NEWLINE> depth = len ( m ) <NEWLINE> for i in range ( depth ) : <NEWLINE> <INDENT> layer = m [ i ] <NEWLINE> lngth = len ( layer ) <NEWLINE> for j in range ( lngth ) : <NEWLINE> <INDENT> size = size + 1 <NEWLINE> <DEDENT> <DEDENT> return ( size ) <NEWLINE> <DEDENT>
def select ( loomfile , min_read_count , min_cell_count , layer ) : <NEWLINE> <INDENT> with loompy . connect ( loomfile ) as ds : <NEWLINE> <INDENT> gsurv = ( ds . sparse ( layer = layer ) >= min_read_count ) . sum ( axis = 1 ) >= min_cell_count <NEWLINE> ds . ra . Selected = np . squeeze ( np . asarray ( gsurv ) ) <NEWLINE> LOG . info ( <STRING> % gsurv . sum ( ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT>
with bind . connect ( ) as conn : <NEWLINE> <INDENT> select_files . bind = conn <NEWLINE> select_values . bind = conn <NEWLINE> for in_slice in window_slices ( File . id , size = windowsize , bind = conn ) : <NEWLINE> <INDENT> if log . level <= logging . DEBUG : <NEWLINE> <INDENT> where = literal_compile ( in_slice ( File . id ) ) <NEWLINE> log . debug ( <STRING> , where . string ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def process_view ( self , request , view_func , view_args , view_kwargs ) : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> realm = view_func . _rated_realm <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> realm = settings . REALM_MAP [ request . resolver_match . url_name ] <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> return None <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT>
def _task_gen ( item , ** kwds ) : <NEWLINE> <INDENT> runner . _update_current_test_var ( item , <STRING> ) <NEWLINE> dag = item . _request . getfixturevalue ( <STRING> ) <NEWLINE> ihook = getattr ( item . ihook , <STRING> ) <NEWLINE> task_id = _gen_task_id ( item ) <NEWLINE> task = PythonOperator ( <NEWLINE> <INDENT> task_id = task_id , <NEWLINE> python_callable = lambda : ihook ( item = item , ** kwds ) , <NEWLINE> provide_context = True , <NEWLINE> dag = dag , <NEWLINE> <DEDENT> ) <NEWLINE> dag . set_dependency ( <STRING> , task_id ) <NEWLINE> return task <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> N = rv . shape [ 0 ] <NEWLINE> y = y [ 0 : N , : ] <NEWLINE> y_iv = y_iv [ 0 : N , : ] <NEWLINE> <COMMENT> <NL> ebar = rv - y <NEWLINE> ebar_iv = rv_iv - y_iv <NEWLINE> <COMMENT> <NL> uf = uf [ 0 : N , : ] <NEWLINE> <DEDENT>
@ property <NEWLINE> <INDENT> def root ( self ) : <NEWLINE> <INDENT> return self . ancestors [ - 1 ] <NEWLINE> <DEDENT> <DEDENT>
if not configSpec : <NEWLINE> <INDENT> raise UnfurlError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> % ( action , resource . template . name , resource . template . name , notfoundmsg ) <NEWLINE> <DEDENT> ) <NEWLINE> logger . debug ( <NEWLINE> <STRING> , <NEWLINE> configSpec . name , <NEWLINE> configSpec . inputs , <NEWLINE> resource . name , <NEWLINE> reason or action , <NEWLINE> ) <NEWLINE> return ( configSpec , resource , reason or action ) <NEWLINE> <DEDENT>
def _as_html_tags_eval_item ( item ) : <NEWLINE> <INDENT> if not isinstance ( item , HTMLTagsEvalItem ) : <NEWLINE> <INDENT> return HTMLTagsEvalItem ( item ) <NEWLINE> <DEDENT> return item <NEWLINE> <DEDENT>
def decode ( self , probs , seq_lens ) : <NEWLINE> <COMMENT> <NL> <INDENT> probs = probs . cpu ( ) . float ( ) <NEWLINE> seq_lens = seq_lens . cpu ( ) . int ( ) <NEWLINE> batch_size , max_seq_len = probs . size ( 0 ) , probs . size ( 1 ) <NEWLINE> output = torch . IntTensor ( batch_size , self . _beam_width , max_seq_len ) . cpu ( ) . int ( ) <NEWLINE> timesteps = torch . IntTensor ( batch_size , self . _beam_width , max_seq_len ) . cpu ( ) . int ( ) <NEWLINE> scores = torch . IntTensor ( batch_size , self . _beam_width ) . cpu ( ) . int ( ) <NEWLINE> out_seq_len = torch . IntTensor ( batch_size , self . _beam_width ) . cpu ( ) . int ( ) <NEWLINE> if self . _scorer : <NEWLINE> <INDENT> ctc_decode . paddle_beam_decode_lm ( probs , seq_lens , self . _labels , self . _num_labels , self . _beam_width , <NEWLINE> <INDENT> self . _num_processes , self . _cutoff_prob , self . cutoff_top_n , self . _blank_id , <NEWLINE> self . _scorer , output , timesteps , scores , out_seq_len ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> ctc_decode . paddle_beam_decode ( probs , seq_lens , self . _labels , self . _num_labels , self . _beam_width , self . _num_processes , <NEWLINE> <INDENT> self . _cutoff_prob , self . cutoff_top_n , self . _blank_id , output , timesteps , <NEWLINE> scores , out_seq_len ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if new_weight_sum <= V_bin_max : <NEWLINE> <DEDENT>
return validated <NEWLINE>
def assert_read_available_to ( <NEWLINE> <INDENT> self , <NEWLINE> unauthorised_agent : Agent , <NEWLINE> broadcast_candidate : Union [ ReadProtected , List [ ReadProtected ] ] <NEWLINE> ) -> Agent : <NEWLINE> <STRING> <NEWLINE> if isinstance ( broadcast_candidate , list ) : <NEWLINE> <INDENT> for candidate in broadcast_candidate : <NEWLINE> <INDENT> if not candidate . grants_read_to ( unauthorised_agent ) : <NEWLINE> <INDENT> raise NotAuthorised <NEWLINE> <DEDENT> continue <NEWLINE> <DEDENT> return unauthorised_agent <NEWLINE> <DEDENT> <DEDENT>
if k < fppc : <NEWLINE> <COMMENT> <NL> <INDENT> pairs . sp = np . tile ( label , [ fppc , 2 ] ) <NEWLINE> pairs . start = np . random . choice ( contig_frags , [ fppc , 2 ] ) <NEWLINE> pairs . end = pairs . start + frag_steps <NEWLINE> <DEDENT>
stc . SetText ( FileOperations ( ) . readFile ( filePath = path ) ) <NEWLINE> <INDENT> centerPaneTab . window . addTab ( name = <STRING> + fileName , worksheetPanel = stc ) <NEWLINE> <DEDENT>
for book in deleteBooks : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> FindingBook ( libraryPath = self . GetParent ( ) . GetParent ( ) . libraryPath ) . deleteBook ( book ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> except Exception as e : <NEWLINE> <INDENT> logger . error ( e , exc_info = True ) <NEWLINE> logger . error ( <STRING> , selectedBookIndex , len ( self . _items ) ) <NEWLINE> <COMMENT> <NL> if len ( deleteBooks ) >= 0 : <NEWLINE> <DEDENT> self . updateStatusBar ( text = <STRING> ) <NEWLINE> self . GetParent ( ) . GetParent ( ) . loadingBook ( ) <NEWLINE> self . GetParent ( ) . GetParent ( ) . updatePangnation ( ) <NEWLINE> <DEDENT>
self . Fi = unit_vector * ( self . Fs ( defflection ) - self . Fd ( velocity ) ) <NEWLINE> <INDENT> Ti_e = 2 * G ( self . Pi ) . T * ( self . Ti + Skew ( self . ui ) . T * self . Fi ) <NEWLINE> <DEDENT>
page_count = self . results_count ( ) // 24 <NEWLINE> <INDENT> if self . results_count ( ) % 24 > 0 : <NEWLINE> <INDENT> page_count += 1 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def dummy_blast ( query , subj , minoverlap ) : <NEWLINE> <COMMENT> <NL> <INDENT> bools = [ True for e in query ] <NEWLINE> positions = [ 0 for e in subj ] <NEWLINE> max_positions = [ len ( e ) for e in subj ] <NEWLINE> positions . extend ( max_positions ) <NEWLINE> return bools , positions <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def run_all ( klass , wd , stages ) : <NEWLINE> <INDENT> for s in stages : <NEWLINE> <INDENT> if not check ( stage = s , directory = wd ) : <NEWLINE> <INDENT> Stager ( wd , s ) . run ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if sys . argv [ 1 ] == <STRING> : <NEWLINE> <INDENT> if len ( sys . argv ) <= 2 : <NEWLINE> <INDENT> print ( usage_str ) <NEWLINE> <DEDENT> elif sys . argv [ 2 ] == <STRING> : <NEWLINE> <INDENT> print ( bomail . util . datestr . datestr_str ) <NEWLINE> <DEDENT> elif sys . argv [ 2 ] == <STRING> : <NEWLINE> <INDENT> print ( taghelp_str ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( usage_str ) <NEWLINE> <DEDENT> exit ( 0 ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> def recheck ( self , old_disp_info ) : <NEWLINE> <INDENT> if not self . is_loaded : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> display . draw_loading_screen ( self . gui ) <NEWLINE> new_filenames = search . search_argstr ( self . search_str , self . gui . mail_mgr ) <NEWLINE> new_fileset = set ( new_filenames ) <NEWLINE> self . remove_files ( [ t [ 0 ] for t in self . file_data if t [ 0 ] not in new_fileset ] , old_disp_info ) <NEWLINE> <COMMENT> <NL> self . update_for_change ( new_filenames , old_disp_info ) <NEWLINE> <DEDENT> <DEDENT>
def send ( self , request , ** kwargs ) : <NEWLINE> <INDENT> func = super ( FuturesSession , self ) . send <NEWLINE> if isinstance ( self . executor , ProcessPoolExecutor ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> dumps ( func ) <NEWLINE> <DEDENT> except ( TypeError , PickleError ) : <NEWLINE> <INDENT> raise RuntimeError ( PICKLE_ERROR ) <NEWLINE> <DEDENT> <DEDENT> return self . executor . submit ( func , request , ** kwargs ) <NEWLINE> <DEDENT>
actions . append ( { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : True , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : case . contact . id , <NEWLINE> <STRING> : active_sub_ids <NEWLINE> <DEDENT> } <NEWLINE> } ) <NEWLINE> if len ( active_sub_ids ) > 1 : <NEWLINE> actions . append ( self . get_cancel_action ( active_sub_ids ) ) <NEWLINE> <DEDENT>
def __init__ ( self , param_decls , required = None , ** attrs ) : <NEWLINE> <INDENT> if required is None : <NEWLINE> <INDENT> if attrs . get ( <STRING> ) is not None : <NEWLINE> <INDENT> required = False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> required = attrs . get ( <STRING> , 1 ) > 0 <NEWLINE> <DEDENT> <DEDENT> Parameter . __init__ ( self , param_decls , required = required , ** attrs ) <NEWLINE> <DEDENT>
def callback ( ctx , param , value ) : <NEWLINE> <INDENT> if not value or ctx . resilient_parsing : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> prog = prog_name <NEWLINE> if prog is None : <NEWLINE> <INDENT> prog = ctx . find_root ( ) . info_name <NEWLINE> <DEDENT> ver = version <NEWLINE> if ver is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> import pkg_resources <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for dist in pkg_resources . working_set : <NEWLINE> <INDENT> scripts = dist . get_entry_map ( ) . get ( <STRING> ) or { } <NEWLINE> for script_name , entry_point in iteritems ( scripts ) : <NEWLINE> <INDENT> if entry_point . module_name == module : <NEWLINE> <INDENT> ver = dist . version <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> if ver is None : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> echo ( message % { <NEWLINE> <INDENT> <STRING> : prog , <NEWLINE> <STRING> : ver , <NEWLINE> <DEDENT> } ) <NEWLINE> ctx . exit ( ) <NEWLINE> <DEDENT>
old_env = { } <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> for key , value in iteritems ( env ) : <NEWLINE> <INDENT> old_env [ key ] = os . environ . get ( key ) <NEWLINE> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> yield bytes_output <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> for key , value in iteritems ( old_env ) : <NEWLINE> <INDENT> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> sys . stdout = old_stdout <NEWLINE> sys . stderr = old_stderr <NEWLINE> sys . stdin = old_stdin <NEWLINE> clickpkg . termui . visible_prompt_func = old_visible_prompt_func <NEWLINE> clickpkg . termui . hidden_prompt_func = old_hidden_prompt_func <NEWLINE> clickpkg . termui . _getchar = old__getchar_func <NEWLINE> clickpkg . utils . should_strip_ansi = old_should_strip_ansi <NEWLINE> clickpkg . formatting . FORCED_WIDTH = old_forced_width <NEWLINE> <DEDENT> <DEDENT>
if len ( messages ) > this . lastMessage : <NEWLINE> <INDENT> id = dom . createElement ( <STRING> ) <NEWLINE> dom . setLayoutXSL ( id , this . buildXML ( ) , <STRING> ) <NEWLINE> dom . insertChild ( id , <STRING> ) <NEWLINE> <DEDENT>
def send_request ( url , method , <NEWLINE> <INDENT> data , args , params , headers , cookies , timeout , is_json ) : <NEWLINE> <STRING> <NEWLINE> <COMMENT> <NL> for p in args : <NEWLINE> <INDENT> url = url . replace ( <STRING> + p , str ( args [ p ] ) ) <NEWLINE> <DEDENT> <DEDENT>
security . declareProtected ( <NEWLINE> <INDENT> <STRING> , <STRING> ) <NEWLINE> def digest ( self , * args ) : <NEWLINE> assert len ( args ) > 1 , <STRING> <NEWLINE> challenge = hmac . new ( self . __key , str ( args [ 0 ] ) , hashlib . sha1 ) <NEWLINE> for arg in args [ 1 : ] : <NEWLINE> <INDENT> challenge . update ( str ( arg ) ) <NEWLINE> <DEDENT> return challenge . hexdigest ( ) <NEWLINE> <DEDENT>
def is_request_ready ( self , _request_id : str , _retry : bool = True ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . login ( ) <NEWLINE> url = <STRING> <NEWLINE> req = Request ( url , headers = self . headers ) <NEWLINE> try : <NEWLINE> <INDENT> decoded = json . load ( urlopen ( req ) ) <NEWLINE> if len ( decoded ) >= 1 and <STRING> in decoded [ 0 ] : <NEWLINE> <INDENT> return decoded [ 0 ] [ <STRING> ] <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT> except HTTPError as e : <NEWLINE> <INDENT> if self . _error_handling ( e ) and _retry : <NEWLINE> <INDENT> return self . is_request_ready ( _request_id , _retry = False ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def update ( self , _id , data : dict ) -> dict : <NEWLINE> <INDENT> <STRING> <NEWLINE> file_path = self . file_path ( _id ) <NEWLINE> if not self . exists ( _id ) : <NEWLINE> <INDENT> raise Exception ( <STRING> . format ( file_path ) ) <NEWLINE> <DEDENT> if <STRING> not in data : <NEWLINE> <INDENT> data [ <STRING> ] = _id <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> cur_data = self . fetch ( _id ) <NEWLINE> new_data = DictUtils . merge ( cur_data , data ) <NEWLINE> Yaml . to_file ( file_path = file_path , data = new_data ) <NEWLINE> return new_data <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if rel . many : <NEWLINE> <INDENT> related_bizobj_list = [ ] <NEWLINE> for obj in related_data : <NEWLINE> <INDENT> if isinstance ( obj , rel . target ) : <NEWLINE> <INDENT> related_bizobj_list . append ( obj ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> related_bizobj_list . append ( <NEWLINE> <INDENT> rel . target ( obj ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _aggregate_related ( <NEWLINE> <INDENT> self , <NEWLINE> bizobj : <STRING> , <NEWLINE> to_create : Dict , <NEWLINE> to_update : Dict , <NEWLINE> ) -> None : <NEWLINE> for k , v in bizobj . related . items ( ) : <NEWLINE> <INDENT> rel = bizobj . relationships [ k ] <NEWLINE> if not v : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if rel . many : <NEWLINE> <INDENT> for x in v : <NEWLINE> <INDENT> if x . _id is None : <NEWLINE> <INDENT> to_create [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> elif x . dirty : <NEWLINE> <INDENT> to_update [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> <DEDENT> for x in v : <NEWLINE> <INDENT> self . _aggregate_related ( x , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v_type = v . __class__ <NEWLINE> if v . _id is None : <NEWLINE> <INDENT> to_create [ v_type ] . append ( v ) <NEWLINE> <DEDENT> elif v . dirty : <NEWLINE> <INDENT> to_update [ v_type ] . append ( x ) <NEWLINE> <DEDENT> self . _aggregate_related ( v , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _aggregate_related ( <NEWLINE> <INDENT> self , <NEWLINE> bizobj : <STRING> , <NEWLINE> to_create : Dict , <NEWLINE> to_update : Dict , <NEWLINE> ) -> None : <NEWLINE> for k , v in bizobj . related . items ( ) : <NEWLINE> <INDENT> rel = bizobj . relationships [ k ] <NEWLINE> if not v : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if rel . many : <NEWLINE> <INDENT> for x in v : <NEWLINE> <INDENT> if x . _id is None : <NEWLINE> <INDENT> to_create [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> elif x . dirty : <NEWLINE> <INDENT> to_update [ v . biz_type ] . append ( x ) <NEWLINE> <DEDENT> <DEDENT> for x in v : <NEWLINE> <INDENT> self . _aggregate_related ( x , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v_type = v . __class__ <NEWLINE> if v . _id is None : <NEWLINE> <INDENT> to_create [ v_type ] . append ( v ) <NEWLINE> <DEDENT> elif v . dirty : <NEWLINE> <INDENT> to_update [ v_type ] . append ( v ) <NEWLINE> <DEDENT> self . _aggregate_related ( v , to_create , to_update ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if self . _op == OP_CODE . AND : <NEWLINE> <COMMENT> <NL> <INDENT> if lhs_exc is None : <NEWLINE> <INDENT> rhs_exc = self . _rhs ( context , arguments ) <NEWLINE> if rhs_exc is not None : <NEWLINE> <INDENT> return rhs_exc <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> return lhs_exc <NEWLINE> elif self . _op == OP_CODE . OR : <NEWLINE> <DEDENT> rhs_exc = self . _rhs ( context , arguments ) <NEWLINE> if rhs_exc is not None and lhs_exc is not None : <NEWLINE> <INDENT> return CompositeGuardException ( lhs_exc , rhs_exc ) <NEWLINE> elif self . _op == OP_CODE . NOT : <NEWLINE> <DEDENT> if lhs_exc is None : <NEWLINE> <INDENT> return lhs_exc <NEWLINE> else : <NEWLINE> <DEDENT> return ValueError ( <STRING> ) <NEWLINE> <DEDENT>
new_decorator = type ( self ) ( self . app , * self . args , ** kwargs ) <NEWLINE> <INDENT> new_decorator . _api_object = self . _api_object <NEWLINE> new_decorator . setup_action ( v . target , True ) <NEWLINE> else : <NEWLINE> self . setup_action ( v . __func__ , False ) <NEWLINE> return api_type <NEWLINE> else : <NEWLINE> func = obj <NEWLINE> action = self . setup_action ( func , False ) <NEWLINE> return func <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if date is True : <NEWLINE> <INDENT> p ( star , Fore . LIGHTMAGENTA_EX + str ( item [ <STRING> ] ) . rjust ( 2 ) , mark , text_color + item [ <STRING> ] , tag_text , <NEWLINE> <INDENT> ( Fore . LIGHTBLACK_EX + <STRING> . format ( color + str ( duedate ) + Fore . LIGHTBLACK_EX ) ) if item [ <STRING> ] else <STRING> , <NEWLINE> Fore . LIGHTBLACK_EX + str ( item [ <STRING> ] ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> p ( star , Fore . LIGHTMAGENTA_EX + str ( item [ <STRING> ] ) . rjust ( 2 ) , mark , text_color + item [ <STRING> ] , tag_text , day_text , due_text ) <NEWLINE> print ( ) <NEWLINE> print_footer ( ) <NEWLINE> print_total ( ) <NEWLINE> print ( ) <NEWLINE> <DEDENT> <DEDENT>
MAXFD = 1024 <NEWLINE> <INDENT> fdset = c_int32 * ( MAXFD // 32 ) <NEWLINE> <DEDENT>
def decodeOid ( pdu ) : <NEWLINE> <INDENT> return tuple ( [ pdu . val . objid [ i ] for i in range ( pdu . val_len // sizeof ( u_long ) ) ] ) <NEWLINE> <DEDENT>
def stats ( dir_stats = DIR_STATS ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_links = ( <STRING> + dir_stats + <STRING> + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_links = ( <STRING> + str ( average ( content ) ) ) <NEWLINE> if len ( content ) > 10000 : <NEWLINE> <INDENT> compress_stats ( dir_stats + <STRING> ) <NEWLINE> <DEDENT> <DEDENT> result = stat_links + <STRING> <NEWLINE> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_webpages = <STRING> + dir_stats + <STRING> + <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_webpages = <STRING> + str ( average ( content ) ) <NEWLINE> <DEDENT> result += stat_webpages + <STRING> <NEWLINE> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_dl_index = <STRING> + dir_stats + <STRING> + <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_dl_index = <STRING> + str ( average ( content ) ) <NEWLINE> <DEDENT> result += stat_dl_index + <STRING> <NEWLINE> try : <NEWLINE> <INDENT> with open ( dir_stats + <STRING> , <STRING> ) as myfile : <NEWLINE> <INDENT> content = myfile . read ( ) . split ( ) <NEWLINE> <DEDENT> <DEDENT> except FileNotFoundError : <NEWLINE> <INDENT> stat_up_index = <STRING> + dir_stats + <STRING> + <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stat_up_index = <STRING> + str ( average ( content ) ) <NEWLINE> <DEDENT> result += stat_up_index + <STRING> <NEWLINE> return result <NEWLINE> <DEDENT>
if len ( sys . argv ) > 1 : <NEWLINE> <INDENT> sys . stdin = open ( sys . argv [ 1 ] ) <NEWLINE> while not stdio . isEmpty ( ) : <NEWLINE> <INDENT> input_item = stdio . readString ( ) <NEWLINE> if input_item is not <STRING> : <NEWLINE> <INDENT> queue . enqueue ( input_item ) <NEWLINE> <DEDENT> elif not queue . is_empty ( ) : <NEWLINE> <INDENT> print ( queue . dequeue ( ) ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> . format ( queue . size ( ) ) ) <NEWLINE> <DEDENT>
class ReCaptchaField ( forms . CharField ) : <NEWLINE> <INDENT> def __init__ ( self , attrs = None , * args , ** kwargs ) : <NEWLINE> <INDENT> if os . environ . get ( <STRING> , None ) is None : <NEWLINE> <INDENT> self . _private_key = kwargs . pop ( <STRING> , settings . RECAPTCHA_PRIVATE_KEY ) <NEWLINE> self . _score_threshold = kwargs . pop ( <STRING> , settings . RECAPTCHA_SCORE_THRESHOLD ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def parse_object ( self , json_string ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> json_object = json . loads ( json_string ) <NEWLINE> <DEDENT> except json . JSONDecodeError as json_error : <NEWLINE> <INDENT> LOGGER . warning ( <STRING> ) <NEWLINE> LOGGER . warning ( json_string ) <NEWLINE> LOGGER . warning ( <STRING> , json_error ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>
stmt = list ( ) <NEWLINE> <INDENT> for ln in sql_in : <NEWLINE> <INDENT> cline = ln . strip ( ) <NEWLINE> if cline . startswith ( <STRING> ) or len ( cline ) == 0 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if r . status_code == 204 : <NEWLINE> <INDENT> comments_to_delete += r . json ( ) . get ( <STRING> , list ( ) ) <NEWLINE> deleted_comments += 1 <NEWLINE> except : <NEWLINE> continue <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> lstTriangle = tripy . earclip ( cleanPoly ) <NEWLINE> lstArea = [ ] <NEWLINE> for i in range ( len ( lstTriangle ) ) : <NEWLINE> <INDENT> lstArea . append ( geoAreaOfTriangle ( lstTriangle [ i ] [ 0 ] , lstTriangle [ i ] [ 1 ] , lstTriangle [ i ] [ 2 ] ) ) <NEWLINE> <DEDENT> <DEDENT>
plotting . eigs ( root = rootname , affinity = mdl_obj . affinity_matrix_ , <NEWLINE> <INDENT> n_clusters = n_clusters , <NEWLINE> title = <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if hasattr ( mdl_obj , <STRING> ) : <NEWLINE> _est_name = mdl_obj . __dict__ . get ( <STRING> , <STRING> ) or type ( mdl_obj ) . __name__ <NEWLINE> if _est_name != <STRING> : <NEWLINE> <COMMENT> <NL> plotting . voronoi ( root = rootname , labels = y , data_in = step_in , <NEWLINE> <INDENT> model = voronoi_mdl_obj ) <NEWLINE> elif hasattr ( mdl_obj , <STRING> ) : <NEWLINE> plotting . tree ( root = rootname , data_in = step_in , <NEWLINE> <DEDENT> <DEDENT> labels = y , model = mdl_obj ) <NEWLINE> plotting . dendrogram ( root = rootname , data_in = step_in , <NEWLINE> <INDENT> labels = y , model = mdl_obj ) <NEWLINE> <DEDENT> <DEDENT>
if ch == 27 : <NEWLINE> <INDENT> return EVENT_ESC <NEWLINE> elif ch == - 1 or ch == curses . KEY_RESIZE : <NEWLINE> return EVENT_RESIZE <NEWLINE> elif ch == 10 or ch == curses . KEY_ENTER : <NEWLINE> return EVENT_ENTER <NEWLINE> elif ch == 127 or ch == curses . KEY_BACKSPACE : <NEWLINE> return EVENT_BACKSPACE <NEWLINE> elif ch == curses . KEY_UP : <NEWLINE> return EVENT_UP <NEWLINE> elif ch == curses . KEY_DOWN : <NEWLINE> return EVENT_DOWN <NEWLINE> elif ch == curses . KEY_LEFT : <NEWLINE> return EVENT_LEFT <NEWLINE> elif ch == curses . KEY_RIGHT : <NEWLINE> return EVENT_RIGHT <NEWLINE> elif ch == 3 : <NEWLINE> return EVENT_CTRL_C <NEWLINE> elif 0 <= ch < 256 : <NEWLINE> return chr ( ch ) <NEWLINE> else : <NEWLINE> return EVENT_UNHANDLED <NEWLINE> <DEDENT>
def read ( self , aTableName ) : <NEWLINE> <INDENT> if self . _objStore_flag : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> objPath = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> self . _objStore . get_object ( self . _admin_username , objPath ) <NEWLINE> tableFile = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> self . _decrypt ( tableFile ) <NEWLINE> dataframe = pq . read_table ( tableFile ) . to_pandas ( ) <NEWLINE> os . remove ( tableFile ) <NEWLINE> os . remove ( objPath ) <NEWLINE> return dataframe <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> return ( <STRING> <NEWLINE> <INDENT> . format ( self . _admin_username , objPath ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> tableFile = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> tableFileEnc = ( self . _path + <STRING> + aTableName + <STRING> ) <NEWLINE> self . _decrypt ( tableFile ) <NEWLINE> dataframe = pq . read_table ( tableFile ) . to_pandas ( ) <NEWLINE> os . remove ( tableFile ) <NEWLINE> return dataframe <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> return ( <STRING> <NEWLINE> <INDENT> . format ( self . _path , tableFile ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> if self . op == <STRING> : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 + self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 - self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 * self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = self . r1 / self . r2 <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = math . ceil ( self . r1 ) <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = math . fabs ( self . r1 ) <NEWLINE> <DEDENT> elif self . op == <STRING> : <NEWLINE> <INDENT> self . r1 = math . fmod ( self . r1 , self . r2 ) <NEWLINE> <DEDENT> <DEDENT>
def _write_solocache_group_to_file ( self , data_dict , group_prefix = <STRING> ) : <NEWLINE> <INDENT> if self . _is_master ( ) and group_prefix != <STRING> : <NEWLINE> <INDENT> if group_prefix not in self . _f : <NEWLINE> <INDENT> self . _f . create_group ( group_prefix ) <NEWLINE> <DEDENT> <DEDENT> keys = data_dict . keys ( ) <NEWLINE> keys . sort ( ) <NEWLINE> for k in keys : <NEWLINE> <INDENT> name = group_prefix + k <NEWLINE> if isinstance ( data_dict [ k ] , dict ) : <NEWLINE> <INDENT> self . _write_solocache_group_to_file ( data_dict [ k ] , group_prefix = name + <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ( data , op ) = data_dict [ k ] <NEWLINE> if op is not None : <NEWLINE> <INDENT> if numpy . isscalar ( data ) : <NEWLINE> <INDENT> sendobj = numpy . array ( data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sendobj = data <NEWLINE> <DEDENT> recvobj = numpy . empty_like ( data ) <NEWLINE> log_debug ( logger , self . _log_prefix + <STRING> % ( name ) ) <NEWLINE> self . comm . Reduce ( <NEWLINE> <INDENT> [ sendobj , MPI . DOUBLE ] , <NEWLINE> [ recvobj , MPI . DOUBLE ] , <NEWLINE> op = op , <NEWLINE> root = 0 <NEWLINE> <DEDENT> ) <NEWLINE> data = recvobj <NEWLINE> <DEDENT> if self . _is_master ( ) : <NEWLINE> <INDENT> log_debug ( logger , self . _log_prefix + <STRING> % ( name ) ) <NEWLINE> self . _f [ name ] = data <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def __call__ ( self ) : <NEWLINE> <INDENT> if np . random . uniform ( ) < self . _epsilon : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>
last = 0 if n_steps < self . mdp . horizon and not absorbing else 1 <NEWLINE> <INDENT> sample = self . state . ravel ( ) . tolist ( ) + action . ravel ( ) . tolist ( ) + [ reward ] + next_state . ravel ( ) . tolist ( ) + [ absorbing , last ] <NEWLINE> <DEDENT>
if iterate_over == <STRING> : <NEWLINE> <INDENT> self . logger . info ( <STRING> % ( i + 1 ) ) <NEWLINE> self . logger . info ( self . state ) <NEWLINE> while i < how_many : <NEWLINE> J = 0. <NEWLINE> action_idx = self . agent . draw_action ( self . state , <NEWLINE> <INDENT> self . agent . approximator ) <NEWLINE> <DEDENT> action_value = self . mdp . action_space . get_value ( action_idx ) <NEWLINE> next_state , reward , absorbing , _ = self . mdp . step ( action_value ) <NEWLINE> J += self . mdp . gamma ** n_steps * reward <NEWLINE> n_steps += 1 <NEWLINE> <DEDENT>
def parse_quantity ( str_value ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for i in range ( len ( str_value ) , 0 , - 1 ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> value = float ( str_value [ : i ] ) <NEWLINE> unit = units . Unit ( str_value [ i : ] ) <NEWLINE> return units . Quantity ( value , unit ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> raise ValueError ( <STRING> . format ( str_value ) ) <NEWLINE> <DEDENT>
def clear ( self ) : <NEWLINE> <INDENT> self . ensure_all_bound ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if self . _fill is None : <NEWLINE> <INDENT> self . buffer ( <STRING> ) . zero ( self . command_queue ) <NEWLINE> <DEDENT> <DEDENT>
def render ( self , output_dir , mode , stokes , channel , rel_channel ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> filename = os . path . join ( output_dir , self . fits_filename ( mode , stokes , channel , rel_channel ) ) <NEWLINE> with closing ( fits . open ( filename ) ) as hdulist : <NEWLINE> <INDENT> naxis = int ( hdulist [ 0 ] . header [ <STRING> ] ) <NEWLINE> slices = [ 0 ] * ( naxis - 2 ) <NEWLINE> for i in range ( 3 , naxis + 1 ) : <NEWLINE> <INDENT> axis_type = hdulist [ 0 ] . header [ <STRING> . format ( i ) ] <NEWLINE> if axis_type == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> slices [ i - 3 ] = <STRING> . find ( stokes ) <NEWLINE> <DEDENT> elif axis_type == <STRING> : <NEWLINE> <INDENT> slices [ i - 3 ] = rel_channel <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . _render_thumb ( output_dir , filename , slices , mode , stokes , channel ) <NEWLINE> self . _render_full ( output_dir , filename , slices , mode , stokes , channel ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> cmd_project_config = ProjectConfig ( cmd_name , project_dir ) <NEWLINE> handler = get_handler ( project_dir , module , service ) <NEWLINE> cmd = project_config . get_command ( handler , module , service , workspace ) <NEWLINE> if not cmd : <NEWLINE> <INDENT> raise CwsClientError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def require_relationship ( resource_list , data ) : <NEWLINE> <INDENT> for resource in resource_list : <NEWLINE> <INDENT> if resource not in data : <NEWLINE> <INDENT> raise UnprocessableEntity ( <STRING> . format ( resource ) , <NEWLINE> <INDENT> { <STRING> : <STRING> . format ( resource ) } ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def require_relationship ( resource_list , data ) : <NEWLINE> <INDENT> for resource in resource_list : <NEWLINE> <INDENT> if resource not in data : <NEWLINE> <INDENT> raise UnprocessableEntity ( <STRING> . format ( resource ) , <NEWLINE> <INDENT> { <STRING> : <STRING> . format ( resource ) } ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def render_templates ( target_path , replace_values , file_types ) : <NEWLINE> <INDENT> basedir = os . path . abspath ( target_path ) <NEWLINE> for root , dirnames , files in os . walk ( basedir ) : <NEWLINE> <INDENT> for f in files : <NEWLINE> <INDENT> skip = True <NEWLINE> full_path = os . path . join ( root , f ) <NEWLINE> for ft in file_types : <NEWLINE> <INDENT> if f . endswith ( <STRING> . format ( ft ) ) : <NEWLINE> <INDENT> skip = False <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> if skip : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> env = Environment ( loader = FileSystemLoader ( root ) ) <NEWLINE> template = env . get_template ( f ) <NEWLINE> rendered = template . render ( replace_values ) <NEWLINE> with open ( full_path , <STRING> ) as fh : <NEWLINE> <INDENT> print ( <STRING> . format ( full_path ) ) <NEWLINE> fh . write ( rendered ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def sync ( args ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> import operator <NEWLINE> session = Session ( args ) <NEWLINE> if <STRING> in args [ <STRING> ] : <NEWLINE> <INDENT> targetfeeds = session . list_feeds ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> targetfeeds = [ ] <NEWLINE> for name in args [ <STRING> ] : <NEWLINE> <INDENT> if name not in session . feeds : <NEWLINE> <INDENT> print ( <STRING> <NEWLINE> <INDENT> . format ( name ) , file = sys . stderr , flush = True ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> targetfeeds . append ( name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for target in targetfeeds : <NEWLINE> <INDENT> feed = Feed ( session , target , None ) <NEWLINE> if not feed . wentwrong : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> title = feed . podcast . target . title <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> title = target <NEWLINE> <DEDENT> print ( <STRING> , title , end = <STRING> ) <NEWLINE> currentdate , stop = feed . how_many ( ) <NEWLINE> entrycounter = 0 <NEWLINE> entries_to_download = feed . podcast . entries <NEWLINE> for entry in entries_to_download : <NEWLINE> <INDENT> feed . fix_linkdate ( entry ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> if stop >= len ( entries_to_download ) : <NEWLINE> <INDENT> entries_to_download . sort ( key = operator . attrgetter ( <STRING> ) , <NEWLINE> <INDENT> reverse = False ) <NEWLINE> <DEDENT> <DEDENT> for entry in entries_to_download : <NEWLINE> <INDENT> if entry . linkdate > currentdate : <NEWLINE> <INDENT> downloaded = feed . download_entry ( entry ) <NEWLINE> entrycounter += downloaded <NEWLINE> <DEDENT> if entrycounter >= stop : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> msg = <STRING> . join ( [ <STRING> , feed , <NEWLINE> <INDENT> <STRING> ] ) <NEWLINE> <DEDENT> print ( msg , file = sys . stderr , flush = True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not request . user . has_perm ( <STRING> ) and not request . user == instance . comment . user : <NEWLINE> <INDENT> raise Http404 <NEWLINE> <DEDENT>
def __call__ ( self , action_set , payoff , situation ) : <NEWLINE> <INDENT> matching_degrees = [ <NEWLINE> <INDENT> classifier . calc_matching_degree ( self . _rule_repr , situation ) <NEWLINE> for classifier in action_set <NEWLINE> <DEDENT> ] <NEWLINE> total_matching_degrees = sum ( matching_degrees ) <NEWLINE> assert total_matching_degrees > 0.0 <NEWLINE> for ( classifier , matching_degree ) in zip ( action_set , matching_degrees ) : <NEWLINE> <INDENT> credit_weight = ( matching_degree / total_matching_degrees ) <NEWLINE> self . _update_experience ( classifier , credit_weight ) <NEWLINE> payoff_diff = payoff - classifier . get_prediction ( situation ) <NEWLINE> self . _update_weight_vec ( classifier , payoff_diff , situation , <NEWLINE> <INDENT> credit_weight ) <NEWLINE> <DEDENT> self . _update_prediction_error ( classifier , payoff_diff , <NEWLINE> <INDENT> credit_weight ) <NEWLINE> <DEDENT> self . _update_action_set_size ( classifier , action_set ) <NEWLINE> <DEDENT> <DEDENT>
@ click . command ( ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , is_flag = True , help = <STRING> ) <NEWLINE> @ click . argument ( <STRING> ) <NEWLINE> @ click . pass_context <NEWLINE> def update ( context , answered_out , case_id ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if answered_out : <NEWLINE> <INDENT> hk_db = apps . hk . connect ( context . obj ) <NEWLINE> lims_api = apps . lims . connect ( context . obj ) <NEWLINE> log . debug ( <STRING> ) <NEWLINE> hk_case = apps . hk . api . case ( case_id ) <NEWLINE> log . debug ( <STRING> ) <NEWLINE> delivery_dates = [ ] <NEWLINE> hk_run = hk_case . current <NEWLINE> for hk_sample in hk_run . samples : <NEWLINE> <INDENT> log . debug ( <STRING> ) <NEWLINE> delivery_date = lims_api . is_delivered ( hk_sample . lims_id ) <NEWLINE> if delivery_date is None : <NEWLINE> <INDENT> log . warn ( <STRING> , hk_sample . lims_id ) <NEWLINE> context . abort ( ) <NEWLINE> <DEDENT> delivery_dates . append ( delivery_date ) <NEWLINE> <DEDENT> latest_date = sorted ( delivery_dates ) [ - 1 ] <NEWLINE> log . debug ( <STRING> ) <NEWLINE> hk_run . answeredout_at = datetime . combine ( latest_date , datetime . min . time ( ) ) <NEWLINE> hk_db . commit ( ) <NEWLINE> log . info ( <STRING> , case_id ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if len ( is_externals ) == 1 : <NEWLINE> <INDENT> data = dict ( is_external = is_externals . pop ( ) ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> data = dict ( is_external = False ) <NEWLINE> <DEDENT>
for family_obj in families : <NEWLINE> <INDENT> LOG . debug ( <STRING> ) <NEWLINE> row = [ <NEWLINE> <INDENT> family_obj . internal_id , <NEWLINE> family_obj . name , <NEWLINE> family_obj . customer . internal_id , <NEWLINE> family_obj . priority_human , <NEWLINE> <STRING> . join ( family_obj . panels ) , <NEWLINE> family_obj . action or <STRING> , <NEWLINE> <DEDENT> ] <NEWLINE> click . echo ( tabulate ( [ row ] , headers = FAMILY_HEADERS , tablefmt = <STRING> ) ) <NEWLINE> if samples : <NEWLINE> <INDENT> sample_ids = [ link_obj . sample . internal_id for link_obj in family_obj . links ] <NEWLINE> context . invoke ( sample , sample_ids = sample_ids , families = False ) <NEWLINE> <DEDENT> <DEDENT>
def maximum_flowcells_ondisk ( self , max_flowcells : int = 700 ) -> bool : <NEWLINE> <INDENT> <STRING> <NEWLINE> ondisk_flowcells = self . status . flowcells ( status = <STRING> ) . count ( ) <NEWLINE> LOG . debug ( <STRING> ) <NEWLINE> return ondisk_flowcells > max_flowcells <NEWLINE> <DEDENT>
def manage_addComment ( self , author , body , url = <STRING> , email = <STRING> , date = None , bitakora_cpt = <STRING> , random_cpt = <STRING> , captcha_zz = 0 , REQUEST = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> from utils import checkCaptchaValue , isCommentSpam <NEWLINE> if captcha_zz : <NEWLINE> <INDENT> if not checkCaptchaValue ( random_cpt , bitakora_cpt ) : <NEWLINE> <INDENT> if REQUEST is not None : <NEWLINE> <INDENT> return REQUEST . RESPONSE . redirect ( self . absolute_url ( ) + <STRING> % ( self . gettext ( <STRING> ) , url_quote ( body . encode ( <STRING> ) ) , url_quote ( author . encode ( <STRING> ) ) , url_quote ( email . encode ( <STRING> ) ) , url_quote ( url . encode ( <STRING> ) ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if event == <STRING> : <NEWLINE> <INDENT> minions |= set ( data [ <STRING> ] ) <NEWLINE> elif event == <STRING> : <NEWLINE> minions . discard ( data [ <STRING> ] ) <NEWLINE> <DEDENT>
response_verification = self . _response . verify ( <NEWLINE> <INDENT> response , <NEWLINE> ValueContext ( origin_datetime = execution . starts ) , <NEWLINE> ) <NEWLINE> return CaseResult ( <NEWLINE> label = self . _label , <NEWLINE> execution = execution , <NEWLINE> response = response_verification , <NEWLINE> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if interpolate : <NEWLINE> <INDENT> ser = Series ( arr ) . interpolate ( limit = window_size ) <NEWLINE> y = array ( ser ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> y = array ( arr ) <NEWLINE> <DEDENT> <DEDENT>
def _is_wc_root ( root , info ) : <NEWLINE> <INDENT> root = os . path . normpath ( os . path . abspath ( root ) ) <NEWLINE> if os . path . normcase ( root ) == os . path . normcase ( info . get ( <STRING> , <STRING> ) ) : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> elif info : <NEWLINE> <INDENT> p = os . path . dirname ( root ) <NEWLINE> return ( p == root <NEWLINE> <INDENT> or not os . path . isdir ( os . path . join ( p , <STRING> ) ) <NEWLINE> or _info ( p ) . get ( <STRING> ) != info [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>
while <STRING> in string and <STRING> in string : <NEWLINE> <INDENT> tags = string . split ( <STRING> , 1 ) [ 1 ] . split ( <STRING> , 1 ) [ 0 ] . split ( ) <NEWLINE> if tags [ 0 ] [ 0 ] == <STRING> : <NEWLINE> <INDENT> tags [ 0 ] = tags [ 0 ] [ 1 : ] <NEWLINE> if tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> colorStack . pop ( ) <NEWLINE> <DEDENT> elif tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> bgcolorStack . pop ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> formatStack . reverse ( ) <NEWLINE> formatStack . remove ( formatTable [ tags [ 0 ] ] ) <NEWLINE> formatStack . reverse ( ) <NEWLINE> <DEDENT> if len ( colorStack ) is 0 or len ( bgcolorStack ) is 0 or len ( formatStack ) is 0 : <NEWLINE> <INDENT> raise SyntaxError ( <STRING> % tags [ 0 ] ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> if len ( tags ) is 1 : <NEWLINE> <INDENT> tags [ 1 ] = <STRING> <NEWLINE> <DEDENT> colorStack . append ( colorTable [ tags [ 1 ] ] ) <NEWLINE> <DEDENT> elif tags [ 0 ] == <STRING> : <NEWLINE> <INDENT> if len ( tags ) is 1 : <NEWLINE> <INDENT> tags [ 1 ] = <STRING> <NEWLINE> <DEDENT> bgcolorStack . append ( colorTable [ tags [ 1 ] ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> formatStack . append ( formatTable [ tags [ 0 ] ] ) <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> raise SyntaxError ( <STRING> % ( tags [ 0 ] , tags [ 1 ] ) ) <NEWLINE> <DEDENT> <DEDENT> newString = string . split ( <STRING> , 1 ) [ 0 ] <NEWLINE> newString += <STRING> % ( <NEWLINE> <INDENT> formatStack [ - 1 ] , <NEWLINE> colorStack [ - 1 ] + 30 , <NEWLINE> bgcolorStack [ - 1 ] + 40 <NEWLINE> ) <NEWLINE> <DEDENT> newString += string . split ( <STRING> , 1 ) [ 1 ] <NEWLINE> string = newString <NEWLINE> return string <NEWLINE> <DEDENT>
@ feature ( <STRING> ) <NEWLINE> <INDENT> def feature_command ( tgen ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cmd_dir = tgen . make_node ( tgen . worch . command_dir ) <NEWLINE> cmd_node = cmd_dir . make_node ( tgen . worch . command_cmd ) <NEWLINE> cmd_target = map ( tgen . make_node , tgen . to_list ( tgen . worch . command_target ) ) <NEWLINE> cmd_rule = <STRING> <NEWLINE> tgen . step ( <STRING> , <NEWLINE> <INDENT> rule = tgen . worch . format ( cmd_rule ) , <NEWLINE> source = cmd_node , <NEWLINE> target = cmd_target , <NEWLINE> cwd = cmd_dir . abspath ( ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if ss > 0 and correct : <NEWLINE> <INDENT> correction = correction_function ( max ( ig1 . mut , ig2 . mut ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> ss += correction <NEWLINE> <COMMENT> <NL> return max ( ss , 0 ) <NEWLINE> <DEDENT>
if ss > 0 and correct : <NEWLINE> <INDENT> correction = correction_function ( max ( ig1 . mut , ig2 . mut ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> ss *= ( 1. - correction ) <NEWLINE> <COMMENT> <NL> return max ( ss , 0 ) <NEWLINE> <DEDENT>
if not connect_only : <NEWLINE> <INDENT> inspection = reflection . Inspector . from_engine ( self . engine ) <NEWLINE> views = inspection . get_view_names ( schema = schema ) <NEWLINE> tables = inspection . get_table_names ( schema = schema ) <NEWLINE> <DEDENT>
def print_result ( peps , print_peps , threshold ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> scores = list ( peps . values ( ) ) <NEWLINE> scores_in_tens = [ int ( round ( i , - 1 ) ) if i <= threshold else - 1 for i in scores ] <COMMENT> <NEWLINE> freq_scores = { x : scores_in_tens . count ( x ) for x in scores_in_tens } <COMMENT> <NEWLINE> <DEDENT>
def import_patch ( self , patch_name , new_name = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if new_name : <NEWLINE> <INDENT> dir_name = os . path . dirname ( new_name ) <NEWLINE> name = os . path . basename ( new_name ) <NEWLINE> dest_dir = self . quilt_patches + Directory ( dir_name ) <NEWLINE> dest_dir . create ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> name = os . path . basename ( patch_name ) <NEWLINE> dest_dir = self . quilt_patches <NEWLINE> <DEDENT> <DEDENT>
applied = self . db . applied_patches ( ) <NEWLINE> <INDENT> for patch in applied : <NEWLINE> <INDENT> if patch in patches : <NEWLINE> <INDENT> patches . remove ( patch ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def test_a_verifier_retrieves_previous_failing_examples_from_the_database ( ) : <NEWLINE> <INDENT> database = ExampleDatabase ( ) <NEWLINE> verifier = Verifier ( settings = hs . Settings ( database = database ) ) <NEWLINE> verifier . falsify ( lambda x : x < 11 , int ) <NEWLINE> called = [ ] <NEWLINE> <DEDENT>
def incorporate_new_buffer ( self , buffer ) : <NEWLINE> <INDENT> if ( <NEWLINE> <INDENT> self . settings . timeout > 0 and <NEWLINE> time . time ( ) >= self . start_time + self . settings . timeout <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> raise RunIsComplete ( ) <NEWLINE> <DEDENT> self . examples_considered += 1 <NEWLINE> if ( <NEWLINE> <INDENT> buffer [ : self . last_data . index ] >= <NEWLINE> self . last_data . buffer [ : self . last_data . index ] <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> data = TestData . for_buffer ( buffer [ : self . last_data . index ] ) <NEWLINE> self . test_function ( data ) <NEWLINE> data . freeze ( ) <NEWLINE> if data . status >= self . last_data . status : <NEWLINE> <INDENT> debug_report ( <STRING> % ( <NEWLINE> <INDENT> data . index , <NEWLINE> list ( data . buffer [ : data . index ] ) , data . status , <NEWLINE> data . output . decode ( <STRING> ) , <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> if data . status >= Status . VALID : <NEWLINE> <INDENT> self . valid_examples += 1 <NEWLINE> <DEDENT> if self . consider_new_test_data ( data ) : <NEWLINE> <INDENT> if self . last_data . status == Status . INTERESTING : <NEWLINE> <INDENT> self . shrinks += 1 <NEWLINE> self . last_data = data <NEWLINE> if self . shrinks >= self . settings . max_shrinks : <NEWLINE> <INDENT> raise RunIsComplete ( ) <NEWLINE> <DEDENT> <DEDENT> self . last_data = data <NEWLINE> self . changed += 1 <NEWLINE> return True <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
def __ifThenElse ( self , element , conditions ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> ifClause = element . arg1 . arg1 <NEWLINE> thenClause = element . arg1 . arg2 <NEWLINE> elseClause = element . arg2 <NEWLINE> <COMMENT> <NL> newConditions = self . __makeConditionRouter ( ifClause ) <NEWLINE> <COMMENT> <NL> currentConditions = self . __appendCondition ( conditions , newConditions ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> self . __generic ( thenClause , deepcopy ( currentConditions ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> self . __invertCondition ( newConditions ) <NEWLINE> self . __generic ( elseClause , deepcopy ( currentConditions ) ) <NEWLINE> <DEDENT>
def testFail ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> fit = Fit ( ) <NEWLINE> skill1 = Skill ( Type ( 56 ) ) <NEWLINE> fit . items . append ( skill1 ) <NEWLINE> skill2 = Skill ( Type ( 56 ) ) <NEWLINE> fit . items . append ( skill2 ) <NEWLINE> restrictionError1 = fit . getRestrictionError ( skill1 , Restriction . skillUniqueness ) <NEWLINE> self . assertIsNotNone ( restrictionError1 ) <NEWLINE> self . assertEqual ( restrictionError1 . skill , 56 ) <NEWLINE> restrictionError2 = fit . getRestrictionError ( skill2 , Restriction . skillUniqueness ) <NEWLINE> self . assertIsNotNone ( restrictionError2 ) <NEWLINE> self . assertEqual ( restrictionError2 . skill , 56 ) <NEWLINE> fit . items . remove ( skill1 ) <NEWLINE> fit . items . remove ( skill2 ) <NEWLINE> self . assertBuffersEmpty ( fit ) <NEWLINE> <DEDENT>
def registerHolder ( self , holder ) : <NEWLINE> <COMMENT> <NL> <INDENT> slotIndex = holder . item . attributes . get ( self . __slotIndexAttr ) <NEWLINE> if slotIndex is None : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> self . __slottedHolders . addData ( slotIndex , holder ) <NEWLINE> <DEDENT>
def validate ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> stats = getattr ( self . _fit . stats , self . __statName ) <NEWLINE> totalUse = stats . used <NEWLINE> <COMMENT> <NL> output = stats . output or 0 <NEWLINE> <COMMENT> <NL> if totalUse <= output : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> taintedHolders = { } <NEWLINE> for holder in self . __resourceUsers : <NEWLINE> <INDENT> resourceUse = holder . attributes [ self . __usageAttr ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if resourceUse <= 0 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> taintedHolders [ holder ] = ResourceErrorData ( output = output , <NEWLINE> <INDENT> totalUse = totalUse , <NEWLINE> holderUse = resourceUse ) <NEWLINE> <DEDENT> <DEDENT> raise RegisterValidationError ( taintedHolders ) <NEWLINE> <DEDENT>
got_request_exception . connect ( record , app ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> with app . test_request_context ( <STRING> ) : <NEWLINE> <INDENT> api . handle_error ( exception ) <NEWLINE> self . assertEquals ( len ( recorded ) , 1 ) <NEWLINE> self . assertTrue ( exception is recorded [ 0 ] ) <NEWLINE> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> got_request_exception . disconnect ( record , app ) <NEWLINE> <DEDENT> <DEDENT>
class MinifiedManifestStaticFilesStorage ( ManifestStaticFilesStorage ) : <NEWLINE> <INDENT> compressors = { } <NEWLINE> gzip = False <NEWLINE> def __init__ ( self , * args , ** kwargs ) : <NEWLINE> <INDENT> super ( MinifiedManifestStaticFilesStorage , self ) . __init__ ( * args , ** kwargs ) <NEWLINE> try : <NEWLINE> <INDENT> for ext , function in MINIFIED_COMPRESSORS . iteritems ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> regexp = re . compile ( ext ) <NEWLINE> if hasattr ( function , <STRING> ) : <NEWLINE> <INDENT> self . compressors [ regexp ] = function <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . compressors [ regexp ] = import_string ( function ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise MinifiedStorageException ( <STRING> % e ) <NEWLINE> <DEDENT> <DEDENT> def _save ( self , hashed_name , content_file ) : <NEWLINE> <INDENT> content = content_file . read ( ) <NEWLINE> try : <NEWLINE> <INDENT> for regexp , comp_function in self . compressors . iteritems ( ) : <NEWLINE> <INDENT> if regexp . search ( hashed_name ) : <NEWLINE> <INDENT> content = comp_function ( content ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise MinifiedStorageException ( <STRING> % ( hashed_name , e , ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> saved_name = super ( MinifiedManifestStaticFilesStorage , self ) . _save ( hashed_name , ContentFile ( content ) ) <NEWLINE> if MINIFIED_GZIP : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> content = zlib_compress ( content ) <NEWLINE> super ( MinifiedManifestStaticFilesStorage , self ) . _save ( <STRING> % saved_name , ContentFile ( content ) ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise MinifiedStorageException ( <STRING> % ( hashed_name , e , ) ) <NEWLINE> <DEDENT> <DEDENT> return saved_name <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> good_indices = np . where ( np . logical_and ( data_array [ <STRING> ] [ : - 1 ] == 0 , np . diff ( data_array [ <STRING> ] ) == 0 ) ) [ 0 ] <NEWLINE> best_index = good_indices [ good_indices . shape [ 0 ] // 2 ] <NEWLINE> best_delay_setting = data_array [ <STRING> ] [ best_index ] <NEWLINE> logging . info ( <STRING> , best_delay_setting ) <NEWLINE> <DEDENT>
label = str ( self . height ) <NEWLINE> <INDENT> text_width , text_height = draw . textsize ( label , font = font ) <NEWLINE> x = 0.65 * leng - 0.5 * text_width <NEWLINE> y = 0.55 * leng - 0.5 * text_height <NEWLINE> draw . rectangle ( ( ( x + 0.1 * text_width , y + 0.1 * text_height ) , <NEWLINE> <INDENT> ( x + 0.9 * text_width , y + 0.9 * text_height ) ) , fill = white ) <NEWLINE> <DEDENT> draw . text ( ( x , y ) , label , fill = black , font = font ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def strike_number ( self , target ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> game = self . legion . player . game <NEWLINE> map1 = game . battlemap <NEWLINE> hex1 = map1 . hexes [ self . hexlabel ] <NEWLINE> hex2 = map1 . hexes [ target . hexlabel ] <NEWLINE> skill1 = self . skill <NEWLINE> skill2 = target . skill <NEWLINE> if target in self . engaged_enemies : <NEWLINE> <INDENT> hexside = hex1 . neighbor_to_hexside ( hex2 ) <NEWLINE> border = hex1 . borders [ hexside ] <NEWLINE> border2 = hex1 . opposite_border ( hexside ) <NEWLINE> if hex1 . terrain == <STRING> and not self . is_native ( hex1 . terrain ) : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> elif border == <STRING> : <NEWLINE> <INDENT> skill1 += 1 <NEWLINE> <DEDENT> elif border2 == <STRING> and not self . is_native ( border2 ) : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> elif border2 == <STRING> : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> if ( not self . magicmissile and map1 . range ( self . hexlabel , <NEWLINE> <INDENT> target . hexlabel ) >= 4 ) : <NEWLINE> <INDENT> skill1 -= 1 <NEWLINE> <DEDENT> <DEDENT> if not self . magicmissile and not self . is_native ( <STRING> ) : <NEWLINE> <INDENT> skill1 -= map1 . count_bramble_hexes ( self . hexlabel , <NEWLINE> <INDENT> target . hexlabel , game ) <NEWLINE> <DEDENT> <DEDENT> if not self . magicmissile : <NEWLINE> <INDENT> skill1 -= map1 . count_walls ( self . hexlabel , target . hexlabel , <NEWLINE> <INDENT> game ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> strike_number = 4 - skill1 + skill2 <NEWLINE> if target in self . engaged_enemies : <NEWLINE> <INDENT> if ( hex2 . terrain == <STRING> and not self . is_native ( hex2 . terrain ) <NEWLINE> <INDENT> and target . is_native ( hex2 . terrain ) ) : <NEWLINE> <INDENT> strike_number += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if ( hex2 . terrain == <STRING> and target . is_native ( hex2 . terrain ) <NEWLINE> <INDENT> and not self . is_native ( hex2 . terrain ) ) : <NEWLINE> <INDENT> strike_number += 1 <NEWLINE> <DEDENT> <DEDENT> elif ( hex2 . terrain == <STRING> and target . is_native ( <NEWLINE> <INDENT> hex2 . terrain ) ) : <NEWLINE> <INDENT> strike_number += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> strike_number = min ( strike_number , 6 ) <NEWLINE> return strike_number <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def number_of_dice ( self , target ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> map1 = self . legion . player . game . battlemap <NEWLINE> hex1 = map1 . hexes [ self . hexlabel ] <NEWLINE> hex2 = map1 . hexes [ target . hexlabel ] <NEWLINE> if target in self . engaged_enemies : <NEWLINE> <INDENT> dice = self . power <NEWLINE> if hex1 . terrain == <STRING> and self . is_native ( hex1 . terrain ) : <NEWLINE> <INDENT> dice += 2 <NEWLINE> <DEDENT> hexside = hex1 . neighbor_to_hexside ( hex2 ) <NEWLINE> border = hex1 . borders [ hexside ] <NEWLINE> if border == <STRING> and self . is_native ( border ) : <NEWLINE> <INDENT> dice += 1 <NEWLINE> <DEDENT> elif border == <STRING> and self . is_native ( border ) : <NEWLINE> <INDENT> dice += 2 <NEWLINE> <DEDENT> border2 = hex1 . opposite_border ( hexside ) <NEWLINE> if border2 == <STRING> and not self . is_native ( border2 ) : <NEWLINE> <INDENT> dice -= 1 <NEWLINE> <DEDENT> <DEDENT> elif target in self . rangestrike_targets : <NEWLINE> <INDENT> dice = int ( self . power / 2 ) <NEWLINE> if hex1 . terrain == <STRING> and self . is_native ( hex1 . terrain ) : <NEWLINE> <INDENT> dice += 2 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> dice = 0 <NEWLINE> <DEDENT> return dice <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if creature . name != <STRING> or game . battle_turn >= 4 : <NEWLINE> <INDENT> score += HIT_BONUS * max_mean_hits <NEWLINE> score += KILL_BONUS * probable_kill <NEWLINE> <DEDENT> score -= DAMAGE_PENALTY * total_mean_damage_taken <NEWLINE> score -= DEATH_PENALTY * probable_death <NEWLINE> <DEDENT>
def init_border_overlays ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> myboxsize = [ int ( round ( 0.97 * mag ) ) for mag in self . bboxsize ] <NEWLINE> self . border_surface_x = int ( round ( self . center [ 0 ] - myboxsize [ 0 ] / 2. ) ) <NEWLINE> self . border_surface_y = int ( round ( self . center [ 1 ] - myboxsize [ 1 ] / 2. ) ) <NEWLINE> for hexside , border in enumerate ( self . battlehex . borders ) : <NEWLINE> <INDENT> border_surface = None <NEWLINE> overlay_filename = <STRING> % border <NEWLINE> image_path = os . path . join ( IMAGE_DIR , overlay_filename ) <NEWLINE> if os . path . exists ( image_path ) : <NEWLINE> <INDENT> hexsides = self . battlehex . hexsides_with_border ( border ) <NEWLINE> hexsides_str = <STRING> . join ( map ( str , sorted ( hexsides ) ) ) <NEWLINE> border_filename = <STRING> % ( border , hexsides_str ) <NEWLINE> border_path = os . path . join ( IMAGE_DIR , border_filename ) <NEWLINE> if not os . path . exists ( border_path ) : <NEWLINE> <INDENT> sliceborder . slice_border_image ( image_path , border_path , <NEWLINE> <INDENT> hexsides ) <NEWLINE> <DEDENT> <DEDENT> input_surface = cairo . ImageSurface . create_from_png ( border_path ) <NEWLINE> input_width = input_surface . get_width ( ) <NEWLINE> input_height = input_surface . get_height ( ) <NEWLINE> output_width = myboxsize [ 0 ] <NEWLINE> output_height = myboxsize [ 1 ] <NEWLINE> border_surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , <NEWLINE> <INDENT> output_width , output_height ) <NEWLINE> <DEDENT> ctx = cairo . Context ( border_surface ) <NEWLINE> ctx . scale ( float ( output_width ) / input_width , <NEWLINE> <INDENT> float ( output_height ) / input_height ) <NEWLINE> <DEDENT> ctx . move_to ( 0 , 0 ) <NEWLINE> ctx . set_source_surface ( input_surface ) <NEWLINE> ctx . paint ( ) <NEWLINE> <DEDENT> self . border_surfaces . append ( border_surface ) <NEWLINE> <DEDENT> <DEDENT>
with open ( conf_path , <STRING> ) as infile : <NEWLINE> <INDENT> dataset_conf = json . load ( infile ) <NEWLINE> func = getattr ( self , dataset_conf [ <STRING> ] ) <NEWLINE> dataset = func ( ) <NEWLINE> dataset . name = name <NEWLINE> dataset . load ( dataset_conf , dataset_path ) <NEWLINE> <DEDENT>
def interpolate_range ( self , begin , end , interpolation_mode = None ) : <NEWLINE> <INDENT> positions = [ [ i , self . get_position ( i ) ] for i in range ( begin , end + 1 ) if self . get_position ( i ) is not None ] <NEWLINE> if len ( positions ) >= 2 : <NEWLINE> <INDENT> positions = interpolate_positions ( positions , begin , end , interpolation_mode ) <NEWLINE> for frame , pos in positions : self . set_position ( frame , pos [ 0 ] , pos [ 1 ] ) <NEWLINE> self . _tmp_points = [ ] <NEWLINE> <DEDENT> <DEDENT>
for obj_dir in objects_dirs : <NEWLINE> <INDENT> name = os . path . basename ( obj_dir ) <NEWLINE> conf_path = os . path . join ( obj_dir , <STRING> ) <NEWLINE> with open ( conf_path , <STRING> ) as infile : <NEWLINE> <INDENT> dataset_conf = json . load ( infile ) <NEWLINE> func = getattr ( self , dataset_conf [ <STRING> ] ) <NEWLINE> dataset = func ( ) <NEWLINE> dataset . load ( dataset_conf , obj_dir ) <NEWLINE> dataset . name = name <NEWLINE> <DEDENT> <DEDENT>
class Terminal ( blessings . Terminal ) : <NEWLINE> <INDENT> def draw_block ( self , block , x , y ) : <NEWLINE> <INDENT> for y , line in enumerate ( block , start = y ) : <NEWLINE> <INDENT> self . stream . write ( self . move ( y , x ) ) <NEWLINE> self . stream . write ( line ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if isinstance ( variadic_arg , OrderedDict ) : <NEWLINE> <INDENT> variadic_arg = variadic_arg . items ( ) <NEWLINE> <DEDENT>
if ret_data . status_code != 200 : <NEWLINE> <INDENT> if ret_dic is not None and <STRING> in ret_dic and ret_dic [ <STRING> ] == - 4001 : <NEWLINE> <INDENT> logger . error ( <STRING> , self . _url ( path ) , req_data , ret_dic ) <NEWLINE> return None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise APIError ( ret_data . status_code , ret_dic ) <NEWLINE> else : <NEWLINE> <DEDENT> return ret_dic <NEWLINE> <DEDENT>
b = sps . add_parser ( <STRING> ) <NEWLINE> <INDENT> b . add_argument ( <STRING> , <STRING> ) <NEWLINE> b . add_aggregate ( <STRING> , lambda args : args . bbb . upper ( ) ) <NEWLINE> <DEDENT>
n = num <NEWLINE> <INDENT> while 0 < n : <NEWLINE> <INDENT> new_profs = sched . random_deviation_profiles ( <NEWLINE> <INDENT> min ( n , chunk_size ) , mix ) . reshape ( ( - 1 , mix . size ) ) <NEWLINE> <DEDENT> n -= chunk_size <NEWLINE> new_futures = [ asyncio . ensure_future ( sched . sample_payoffs ( prof ) ) <NEWLINE> <INDENT> for prof in new_profs ] <NEWLINE> <DEDENT> await update ( ) <NEWLINE> futures = new_futures <NEWLINE> <DEDENT> await update ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> job_tree ( job_file_list , tier_list ) <NEWLINE> <DEDENT>
if len ( set_list ) == 0 or len ( target_element ) > 1 : <NEWLINE> <COMMENT> <NL> <INDENT> return CandidatePolicy ( target_dict = param_dict ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> matches = set . intersection ( * set_list ) <NEWLINE> <DEDENT>
def report_error ( code , message , event = None , oid = None ) : <NEWLINE> <INDENT> payload = { <NEWLINE> <INDENT> <STRING> : { <STRING> : code , <STRING> : message } <NEWLINE> <DEDENT> } <NEWLINE> if event : <NEWLINE> <INDENT> payload [ <STRING> ] = event <NEWLINE> <DEDENT> if oid : <NEWLINE> <INDENT> payload [ <STRING> ] = oid <NEWLINE> <DEDENT> write ( payload ) <NEWLINE> <DEDENT>
return cls . from_file ( filepath = os . path . join ( dir_ , settingsfile ) ) <NEWLINE>
<COMMENT> <NL> <INDENT> fragSize = int ( self . fld ( size = 1 ) ) <NEWLINE> if fragSize <= readTempSize : <NEWLINE> <INDENT> tryCnt += 1 <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> self . _widget_path = <STRING> % ( <NEWLINE> <INDENT> parentpath , safe_class_name , next ( counts [ safe_class_name ] ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if predictor != 1 : <NEWLINE> <INDENT> columns = decodeParms [ <STRING> ] <NEWLINE> <COMMENT> <NL> if predictor >= 10 and predictor <= 15 : <NEWLINE> <INDENT> output = StringIO ( ) <NEWLINE> <COMMENT> <NL> rowlength = columns + 1 <NEWLINE> assert len ( data ) % rowlength == 0 <NEWLINE> prev_rowdata = ( 0 , ) * rowlength <NEWLINE> for row in range ( len ( data ) // rowlength ) : <NEWLINE> <INDENT> rowdata = [ ord ( x ) for x in data [ ( row * rowlength ) : ( ( row + 1 ) * rowlength ) ] ] <NEWLINE> filterByte = rowdata [ 0 ] <NEWLINE> if filterByte == 0 : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> elif filterByte == 1 : <NEWLINE> <INDENT> for i in range ( 2 , rowlength ) : <NEWLINE> <INDENT> rowdata [ i ] = ( rowdata [ i ] + rowdata [ i - 1 ] ) % 256 <NEWLINE> <DEDENT> <DEDENT> elif filterByte == 2 : <NEWLINE> <INDENT> for i in range ( 1 , rowlength ) : <NEWLINE> <INDENT> rowdata [ i ] = ( rowdata [ i ] + prev_rowdata [ i ] ) % 256 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> raise PdfReadError ( <STRING> % filterByte ) <NEWLINE> <DEDENT> prev_rowdata = rowdata <NEWLINE> output . write ( <STRING> . join ( [ chr ( x ) for x in rowdata [ 1 : ] ] ) ) <NEWLINE> <DEDENT> data = output . getvalue ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> raise PdfReadError ( <STRING> % predictor ) <NEWLINE> <DEDENT> <DEDENT> return data <NEWLINE> decode = staticmethod ( decode ) <NEWLINE> <DEDENT>
def _build_batch_arrays ( self , batch_size ) : <NEWLINE> <INDENT> if batch_size > self . num_of_states : <NEWLINE> <INDENT> batch_size = self . num_of_states <NEWLINE> <DEDENT> if self . num_of_states % batch_size != 0 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> self . batch_size = batch_size <NEWLINE> self . num_of_batch_until_full_cycle = self . num_of_states // self . batch_size <NEWLINE> self . batch_complex_local_energies = np . zeros ( ( self . batch_size , ) , dtype = np . complex128 ) <NEWLINE> self . batch_naive_complex_local_energies = np . zeros ( ( self . batch_size , ) , dtype = np . complex128 ) <NEWLINE> <DEDENT>
if name in craftables : <NEWLINE> <INDENT> items , item = craftables [ name ] <NEWLINE> else : <NEWLINE> for k in craftables : <NEWLINE> <INDENT> if k . startswith ( name ) or k . endswith ( name ) or ( k in name ) : <NEWLINE> <INDENT> items , item = craftables [ k ] <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _find_item ( item , items ) : <NEWLINE> <INDENT> for i in items : <NEWLINE> <INDENT> if ( i . name == item . name ) and isinstance ( i , item . __class__ ) : <NEWLINE> <INDENT> return i <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
iterator = gpsdio . filter ( filter_expr , src ) if filter_expr else src <NEWLINE> <INDENT> for msg in gpsdio . sort ( iterator , sort_field ) if sort_field else iterator : <NEWLINE> <INDENT> dst . write ( msg ) <NEWLINE> <DEDENT> <DEDENT>
if iteration >= total : <NEWLINE> <INDENT> sys . stdout . write ( <STRING> ) <NEWLINE> sys . stdout . flush ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT>
logger . debug ( <STRING> % values ) <NEWLINE> <INDENT> code , data = await self . _sql . update ( info , values ) <NEWLINE> if code == RETCODE . SUCCESS : <NEWLINE> <INDENT> await async_call ( self . after_update , values ) <NEWLINE> <DEDENT> self . finish ( code , data ) <NEWLINE> <DEDENT>
def request_role ( role = None ) : <NEWLINE> <INDENT> def _ ( func ) : <NEWLINE> <INDENT> async def __ ( view : AbstractSQLView , * args , ** kwargs ) : <NEWLINE> <INDENT> if role != view . current_request_role : <NEWLINE> <INDENT> return view . finish ( RETCODE . INVALID_ROLE ) <NEWLINE> <DEDENT> return await func ( view , * args , ** kwargs ) <NEWLINE> <DEDENT> return __ <NEWLINE> <DEDENT> return _ <NEWLINE> <DEDENT>
def _inside_skip ( soup_elem ) : <NEWLINE> <INDENT> parent = soup_elem . parent <NEWLINE> while parent is not None : <NEWLINE> <INDENT> if parent . name in skip_elements : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> parent = parent . parent <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if compare_models : <NEWLINE> <INDENT> choose_box_and_violin_plots ( names , <NEWLINE> <INDENT> scoring , <NEWLINE> compare_models , <NEWLINE> results , <NEWLINE> is_continuous ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if ROC : <NEWLINE> <INDENT> if not is_continuous : <NEWLINE> <INDENT> timeit ( plot_rocs , models , df_X , y ) <NEWLINE> plt . show ( ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> return names , results , fit_models , pipeline , df_X <NEWLINE> <DEDENT>
def postprocess ( self , content ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> node = content . node <NEWLINE> if len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> return node <NEWLINE> <DEDENT> attributes = AttrList ( node . attributes ) <NEWLINE> if attributes . rlen ( ) and not len ( node . children ) and node . hasText ( ) : <NEWLINE> <INDENT> p = Factory . property ( node . name , node . getText ( ) ) <NEWLINE> return merge ( content . data , p ) <NEWLINE> <DEDENT> if len ( content . data ) : <NEWLINE> <INDENT> return content . data <NEWLINE> <DEDENT> lang = attributes . lang ( ) <NEWLINE> if not len ( node . children ) and content . text is None : <NEWLINE> <INDENT> if self . nillable ( content . data ) or content . node . isnil ( ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return xlstr . string ( <STRING> , lang ) <NEWLINE> <DEDENT> <DEDENT> if isinstance ( content . text , basestring ) : <NEWLINE> <INDENT> return xlstr . string ( content . text , lang ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return content . text <NEWLINE> <DEDENT> <DEDENT>
Definition ( <STRING> , ( list , tuple ) , [ ] ) , <NEWLINE>
ctx : BringDynamicContextTing = self . _tingistry_obj . create_ting ( <COMMENT> <NEWLINE> <INDENT> <STRING> , <STRING> <NEWLINE> ) <NEWLINE> indexes = [ _path ] <NEWLINE> ctx . input . set_values ( <COMMENT> <NEWLINE> ting_dict = { <STRING> : indexes } <NEWLINE> ) <NEWLINE> <DEDENT>
pyinstaller = { <STRING> : [ x . __name__ for x in _hi ] } <NEWLINE> <INDENT> if os . name == <STRING> : <NEWLINE> <INDENT> import pkgutil <NEWLINE> import jinxed . terminfo <NEWLINE> <DEDENT> <DEDENT>
return _all_values <NEWLINE>
_allowed_strings = [ ] <NEWLINE> <INDENT> for _arg , _aliases in _allowed . items ( ) : <NEWLINE> <INDENT> if not _aliases : <NEWLINE> <INDENT> a = _arg <NEWLINE> <DEDENT> elif len ( _aliases ) == 1 : <NEWLINE> <INDENT> a = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> a = <STRING> <NEWLINE> <DEDENT> _allowed_strings . append ( a ) <NEWLINE> <DEDENT> arg_table . add_row ( <STRING> , _allowed_strings [ 0 ] ) <NEWLINE> if limit_allowed and len ( _allowed_strings ) > limit_allowed : <NEWLINE> <INDENT> _allowed_strings = _allowed_strings [ 0 : limit_allowed ] + [ <STRING> , <STRING> ] <NEWLINE> <DEDENT> for a in _allowed_strings [ 1 : ] : <NEWLINE> <INDENT> arg_table . add_row ( <STRING> , a ) <NEWLINE> <DEDENT> <DEDENT>
install_args = { } <NEWLINE> <INDENT> if target : <NEWLINE> <INDENT> install_args [ <STRING> ] = target <NEWLINE> <DEDENT> if target_config : <NEWLINE> <INDENT> install_args [ <STRING> ] = target_config <NEWLINE> <DEDENT> <DEDENT>
text = self . text <NEWLINE> <INDENT> chunks = re . split ( <STRING> , text ) <NEWLINE> text_chunks = [ ] <NEWLINE> for index , chunk in enumerate ( chunks ) : <NEWLINE> <INDENT> if not index % 2 : <NEWLINE> <INDENT> text_chunks . append ( chunk ) <NEWLINE> <DEDENT> <DEDENT> for hit in sorted ( hits , key = lambda chunk : chunk [ 1 ] , reverse = True ) : <NEWLINE> <INDENT> hit_start , hit_end = hit <NEWLINE> placed = 0 <NEWLINE> for index , chunk in enumerate ( chunks ) : <NEWLINE> <INDENT> if placed == 2 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if index % 2 : <NEWLINE> <COMMENT> <NL> <INDENT> continue <NEWLINE> <DEDENT> chunk_start = len ( <STRING> . join ( text_chunks [ 0 : index // 2 ] ) ) <NEWLINE> chunk_end = chunk_start + len ( chunk ) <NEWLINE> if hit_start >= chunk_start and hit_start < chunk_end : <NEWLINE> <INDENT> chunk = chunk [ : hit_start - chunk_start ] + tags [ 0 ] + chunk [ hit_start - chunk_start : ] <NEWLINE> if hit_end <= chunk_end : <NEWLINE> <INDENT> hit_end += len ( tags [ 0 ] ) <NEWLINE> chunk_end += len ( tags [ 0 ] ) <NEWLINE> <DEDENT> placed = 1 <NEWLINE> <DEDENT> if hit_end > chunk_start and hit_end <= chunk_end : <NEWLINE> <INDENT> chunk = chunk [ : hit_end - chunk_start ] + tags [ 1 ] + chunk [ hit_end - chunk_start : ] <NEWLINE> placed = 2 <NEWLINE> <DEDENT> chunks [ index ] = chunk <NEWLINE> <DEDENT> if placed == 1 : <NEWLINE> <INDENT> chunks [ - 1 ] = chunks [ - 1 ] + tags [ 1 ] <NEWLINE> <DEDENT> <DEDENT> result = [ ] <NEWLINE> for index , chunk in enumerate ( chunks ) : <NEWLINE> <INDENT> if index % 2 : <NEWLINE> <COMMENT> <NL> <INDENT> result . append ( <STRING> % chunk ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> result . append ( chunk ) <NEWLINE> <DEDENT> <DEDENT> self . text = <STRING> . join ( result ) <NEWLINE> return self . text <NEWLINE> <DEDENT>
with open ( filepath ) as file : <NEWLINE> <INDENT> if hasattr ( root , <STRING> ) : <NEWLINE> <INDENT> errors = root . doc . errors <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> errors = None <NEWLINE> <DEDENT> result = build ( file , filepath , errors ) <NEWLINE> return Holder ( result ) <NEWLINE> <DEDENT>
try : <NEWLINE> <COMMENT> <NL> <INDENT> latest = user . password_history . latest ( <STRING> ) <NEWLINE> except PasswordHistory . DoesNotExist : <NEWLINE> return True <NEWLINE> <DEDENT>
for i , subjects in enumerate ( data_splits ) : <NEWLINE> <INDENT> for subject in tqdm ( subjects ) : <NEWLINE> <INDENT> print ( subject ) <NEWLINE> spath = { } <NEWLINE> subject_path = os . path . join ( dataset_path , subject ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , subject + <STRING> ) <NEWLINE> spath [ <STRING> ] = os . path . join ( subject_path , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if r . status_code == 204 : <NEWLINE> <INDENT> comments_to_delete += r . json ( ) . get ( <STRING> , list ( ) ) <NEWLINE> deleted_comments += 1 <NEWLINE> except : <NEWLINE> continue <NEWLINE> <DEDENT>
sys . path . insert ( 0 , <STRING> ) <NEWLINE>
elif isinstance ( input_ , string_types ) : <NEWLINE> <INDENT> for sep in [ <STRING> , <STRING> , <STRING> ] : <NEWLINE> <INDENT> if len ( input_ . split ( sep ) ) in range ( 2 , 3 ) : <NEWLINE> <INDENT> t = input_ . split ( sep ) <NEWLINE> <DEDENT> <DEDENT> if isinstance ( t , string_types ) : <NEWLINE> <INDENT> raise Exception ( <STRING> % input_ ) <NEWLINE> <DEDENT> <DEDENT>
def is_relative ( self , url ) : <NEWLINE> <INDENT> domain = self . url_splitter . get_domain ( url ) <NEWLINE> return len ( domain ) == 0 <NEWLINE> <DEDENT>
elif len ( gname ) >= 2 : <NEWLINE> <INDENT> p0_name += <STRING> % gname [ 1 ] <NEWLINE> <DEDENT>
def read_bam_file ( bamfile , chrnames_bam , max_NM = 0 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> read_objects = { } <NEWLINE> for read in bamfile . fetch ( ) : <NEWLINE> <INDENT> if not read . is_unmapped : <NEWLINE> <INDENT> nm_num = get_NM_number ( read . tags ) <NEWLINE> if nm_num > max_NM : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> alt_list = get_XA_mapping ( read . tags , max_NM ) <NEWLINE> min_pos = read . pos <NEWLINE> min_is_rev = read . is_reverse <NEWLINE> for al in alt_list : <NEWLINE> <INDENT> apos = int ( al [ 1 ] [ 1 : ] ) <NEWLINE> <COMMENT> <NL> if int ( al [ 3 ] ) > nm_num : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if apos < min_pos : <NEWLINE> <INDENT> min_pos = apos <NEWLINE> min_is_rev = al [ 1 ] [ 0 ] == <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> tags = read . tags <NEWLINE> if read . pos != min_pos : <NEWLINE> <INDENT> for xt in tags : <NEWLINE> <INDENT> if xt [ 0 ] == <STRING> : <NEWLINE> <INDENT> xaval = xt [ 1 ] <NEWLINE> tags . remove ( xt ) <NEWLINE> strs = <STRING> <NEWLINE> if read . is_reverse : <NEWLINE> <INDENT> strs = <STRING> <NEWLINE> <DEDENT> tags . append ( ( <STRING> , <STRING> % ( <NEWLINE> <INDENT> chrnames_bam [ read . tid ] , <NEWLINE> strs , read . pos , <NEWLINE> read . cigarstring , nm_num ) + xaval ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> read . tags = tags <NEWLINE> <DEDENT> read . pos = min_pos <NEWLINE> read . is_reverse = min_is_rev <NEWLINE> read_objects [ read . qname ] = read <NEWLINE> <DEDENT> <DEDENT> return read_objects <NEWLINE> <DEDENT>
updater = Updater ( token = sys . argv [ 1 ] ) <NEWLINE>
if header_after_slash is None : <NEWLINE> <INDENT> final_header = header_code <NEWLINE> else : <NEWLINE> if len ( header_before_slash ) != 0 : <NEWLINE> <INDENT> final_header = str ( header_code ) + <STRING> + str ( header_after_slash ) <NEWLINE> <DEDENT> elif len ( header_before_slash ) == 0 : <NEWLINE> <INDENT> final_header = <STRING> + str ( decsCodes_list_dict ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( header , <STRING> , header_before_slash , <STRING> , header_after_slash ) <NEWLINE> <DEDENT> <DEDENT>
def wrapper ( cls , request , * args , ** kwargs ) : <NEWLINE> <INDENT> paramap = dict ( kwargs ) <NEWLINE> paramap . setdefault ( <NEWLINE> <INDENT> <STRING> , kwargs . get ( <STRING> , None ) <NEWLINE> <DEDENT> ) <COMMENT> <NEWLINE> data = data_method ( request ) <NEWLINE> paramap . update ( { x : y for x , y in data . items ( ) } ) <NEWLINE> result = cls . result_class ( ) <COMMENT> <NEWLINE> for item in itemset : <NEWLINE> <INDENT> name , v , required , msg , replace = [ <NEWLINE> <INDENT> item [ x ] <NEWLINE> for x in [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> value = None <COMMENT> <NEWLINE> para = paramap . get ( name ) <NEWLINE> if required and para in ( None , <STRING> ) : <COMMENT> <NEWLINE> <INDENT> result . error ( name , <STRING> ) <NEWLINE> <DEDENT> if para is not None : <NEWLINE> <INDENT> if para : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> value = v ( para ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> if settings . DEBUG : <NEWLINE> <INDENT> from traceback import print_exc <NEWLINE> print_exc ( ) <NEWLINE> <DEDENT> <DEDENT> msg = v . msg or msg <NEWLINE> if v . status == 403 : <COMMENT> <NEWLINE> <INDENT> return result . perm ( name , msg ) ( status = v . status ) <NEWLINE> <DEDENT> if value in ( None , False ) : <NEWLINE> <INDENT> result . error ( name , msg ) <NEWLINE> <DEDENT> <DEDENT> kwargs . update ( { <NEWLINE> <INDENT> replace or name : value or para <NEWLINE> <DEDENT> } ) <COMMENT> <NEWLINE> <DEDENT> <DEDENT> if not result : <NEWLINE> <INDENT> return result ( status = 400 ) <NEWLINE> <DEDENT> return func ( cls , request , * args , ** kwargs ) <NEWLINE> <DEDENT>
noise = salt_pepper_noise ( noise_shape [ 1 ] , batch_size , density , salt_value , pepper_value , seed ) <NEWLINE>
if scale_first : <NEWLINE> <INDENT> path_rev = _scaled_path ( path_rev , scaling_path , flip_paths ) <NEWLINE> <DEDENT>
