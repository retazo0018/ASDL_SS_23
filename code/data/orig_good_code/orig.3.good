self . easy . product_cach [ product . id ] = product <NEWLINE> <INDENT> self . products . buffer [ i + offset ] = product <NEWLINE> <DEDENT>
n = abs ( n ) <NEWLINE> <INDENT> if n == 1 : <NEWLINE> <INDENT> return [ 1 ] <NEWLINE> <DEDENT> factor = [ ] <NEWLINE> for i in prime : <NEWLINE> <INDENT> if i ** 2 > n : <NEWLINE> <INDENT> factor . append ( n ) <NEWLINE> break <NEWLINE> <DEDENT> while not n % i : <NEWLINE> <INDENT> factor . append ( i ) <NEWLINE> n //= i <NEWLINE> <DEDENT> if n == 1 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> return factor <NEWLINE> <DEDENT>
async def handle_user_message ( app , ws , msg ) : <NEWLINE> <INDENT> if isinstance ( msg , api . SetDocumentMessage ) : <NEWLINE> <INDENT> assert app [ <STRING> ] == State . idle <NEWLINE> try : <NEWLINE> <INDENT> app [ <STRING> ] = State . processing <NEWLINE> notify_state ( app ) <NEWLINE> job = await plotting . process_upload_background ( app , msg . document ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> notify_error ( app , ws , str ( e ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> app [ <STRING> ] = job . document <NEWLINE> app [ <STRING> ] = job <NEWLINE> app [ <STRING> ] = job . duration ( ) . total_seconds ( ) <NEWLINE> notify_new_document ( app , exclude_client = ws ) <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> app [ <STRING> ] = State . idle <NEWLINE> notify_state ( app ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> async for raw_msg in ws : <NEWLINE> <INDENT> if raw_msg . tp == aiohttp . MsgType . text : <NEWLINE> <INDENT> msg = api . Message . deserialize ( raw_msg . data ) <NEWLINE> log . info ( <STRING> , msg ) <NEWLINE> await handle_user_message ( app , ws , msg ) <NEWLINE> <DEDENT> elif raw_msg . tp == aiohttp . MsgType . closed : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> elif raw_msg . tp == aiohttp . MsgType . error : <NEWLINE> <INDENT> log . info ( <STRING> , raw_msg ) <NEWLINE> break <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . error ( <STRING> , <NEWLINE> <INDENT> raw_msg . tp ) <NEWLINE> finally : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> log . info ( <STRING> ) <NEWLINE> clients . remove ( ws ) <NEWLINE> <DEDENT>
def process_image ( elm , registry , container ) : <NEWLINE> <INDENT> img = Image ( ) <NEWLINE> container . add_content ( img ) <NEWLINE> for subitem in elm . items ( ) : <NEWLINE> <INDENT> if subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . alt = subitem [ 1 ] <NEWLINE> <DEDENT> elif subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . witdh = subitem [ 1 ] <NEWLINE> <DEDENT> elif subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . height = subitem [ 1 ] <NEWLINE> <DEDENT> elif subitem [ 0 ] == <STRING> : <NEWLINE> <INDENT> img . uri = subitem [ 1 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def test_gsheets_add_rows_to_active_sheet_sets_add_rows_time ( gsheets_handler_no_thread ) : <NEWLINE> <INDENT> gsheets_handler_no_thread . _add_rows_time = 99 <NEWLINE> assert isinstance ( gsheets_handler_no_thread . _add_rows_to_active_sheet ( [ ] ) , unittest . mock . Mock ) <NEWLINE> assert gsheets_handler_no_thread . _add_rows_time < 99 and gsheets_handler_no_thread . _add_rows_time >= 0 <NEWLINE> <DEDENT>
def add ( self , fee , result ) : <NEWLINE> <INDENT> if fee . id not in self . applications : <NEWLINE> <INDENT> self . applications [ fee . id ] = { <NEWLINE> <INDENT> <STRING> : fee , <NEWLINE> <STRING> : result , <NEWLINE> <STRING> : fee . name , <NEWLINE> <STRING> : fee . description , <NEWLINE> <STRING> : 0 , <NEWLINE> <STRING> : D ( <STRING> ) } <NEWLINE> <DEDENT> <DEDENT> self . applications [ fee . id ] [ <STRING> ] += result . fee <NEWLINE> self . applications [ fee . id ] [ <STRING> ] += 1 <NEWLINE> <DEDENT>
<STRING> <NEWLINE> <INDENT> assert ( len ( nodes . columns ) >= 1 ) <NEWLINE> if len ( nodes . columns ) == 1 : <NEWLINE> <INDENT> nodes [ <STRING> ] = np . zeros ( len ( nodes ) , np . bool_ ) <NEWLINE> <DEDENT> <DEDENT>
<STRING> <NEWLINE> <INDENT> timer . tic ( <STRING> ) <NEWLINE> starts = backend . zeros ( len ( X ) + 1 , dtype = np . uint32 ) <NEWLINE> if nodal is True : <NEWLINE> <INDENT> sizes = np . array ( [ len ( g . nodes ) for g in X ] , dtype = np . uint32 ) <NEWLINE> np . cumsum ( sizes , out = starts [ 1 : ] ) <NEWLINE> output_length = int ( starts [ - 1 ] ) <NEWLINE> <DEDENT> elif nodal is False : <NEWLINE> <INDENT> starts [ : ] = np . arange ( len ( X ) + 1 ) <NEWLINE> output_length = len ( X ) <NEWLINE> <DEDENT> elif nodal == <STRING> : <NEWLINE> <INDENT> sizes = np . array ( [ len ( g . nodes ) for g in X ] , dtype = np . uint32 ) <NEWLINE> np . cumsum ( sizes ** 2 , out = starts [ 1 : ] ) <NEWLINE> output_length = int ( starts [ - 1 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ( ValueError ( <STRING> % nodal ) ) <NEWLINE> <DEDENT> if traits . eval_gradient is True : <NEWLINE> <INDENT> output_shape = ( output_length , 1 + self . n_dims ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> output_shape = ( output_length , 1 ) <NEWLINE> <DEDENT> output = backend . empty ( int ( np . prod ( output_shape ) ) , np . float32 ) <NEWLINE> timer . toc ( <STRING> ) <NEWLINE> <DEDENT>
class SpectralApprox ( FactorApprox ) : <NEWLINE> <INDENT> def __init__ ( self , X , rcut = 0 , acut = 0 ) : <NEWLINE> <INDENT> if isinstance ( X , np . ndarray ) : <NEWLINE> <INDENT> U , S , _ = np . linalg . svd ( X , full_matrices = False ) <NEWLINE> mask = ( S >= S . max ( ) * rcut ) & ( S >= acut ) <NEWLINE> self . U = U [ : , mask ] <NEWLINE> self . S = S [ mask ] <NEWLINE> <DEDENT> elif isinstance ( X , tuple ) and len ( X ) == 2 : <NEWLINE> <INDENT> self . U , self . S = X <NEWLINE> <DEDENT> self . _lhs = self . U * self . S <NEWLINE> <DEDENT> <DEDENT>
S = uctypes . struct ( uctypes . addressof ( data ) , desc , uctypes . LITTLE_ENDIAN ) <NEWLINE>
S = uctypes . struct ( uctypes . addressof ( data ) , desc , uctypes . LITTLE_ENDIAN ) <NEWLINE>
S = uctypes . struct ( uctypes . addressof ( data ) , desc , uctypes . NATIVE ) <NEWLINE>
S = uctypes . struct ( uctypes . addressof ( buf ) , desc , uctypes . LITTLE_ENDIAN ) <NEWLINE>
def stop ( self ) -> None : <NEWLINE> <INDENT> with self . _stop_lock : <NEWLINE> <INDENT> self . _do_stop = True <NEWLINE> <DEDENT> <DEDENT>
self . __pt1000_adc = pt1000_conf . adc ( MCP342X . GAIN_4 , MCP342X . RATE_15 ) if pt1000 else None <NEWLINE>
hasNoMissingJobs = reissueMissingJobs ( updatedJobFiles , jobBatcher , batchSystem , childJobFileToParentJob , childCounts , config ) <NEWLINE> <INDENT> if hasNoMissingJobs : <NEWLINE> <INDENT> timeSinceJobsLastRescued = time . time ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> timeSinceJobsLastRescued += 60 <COMMENT> <NEWLINE> <DEDENT> logger . info ( <STRING> ) <NEWLINE> <DEDENT>
def writeGlobalFile ( self , localFileName ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return self . jobStore . writeFile ( self . job . jobStoreID , localFileName ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> self . predecessorNumber = predecessorNumber <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> self . predecessorsFinished = predecessorsFinished or set ( ) <NEWLINE> <DEDENT>
def setLogFile ( self , logFile , jobStore ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . logJobStoreFileID is not None : <NEWLINE> <INDENT> self . clearLogFile ( jobStore ) <NEWLINE> <DEDENT> self . logJobStoreFileID = jobStore . writeFile ( logFile , self . jobStoreID ) <NEWLINE> assert self . logJobStoreFileID is not None <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if jobWrapper . remainingRetryCount != self . _defaultTryCount ( ) : <NEWLINE> <INDENT> jobWrapper . remainingRetryCount = self . _defaultTryCount ( ) <NEWLINE> changed = True <NEWLINE> <DEDENT> <DEDENT>
DATA_TEST_CDEC_PARSE = [ <NEWLINE> <COMMENT> <NL> <INDENT> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 1 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 2 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , default = 1 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 1 , 2 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , tuple ( <STRING> ) , 2 , 3 ) ) , <NEWLINE> ( <STRING> , dict_cdec_parse ( <STRING> , <STRING> , default = 10 ) ) , <NEWLINE> ( cmem ( DmyCDT , <STRING> ) , dict_cdec_parse ( DmyCDT , <STRING> , valtype = <STRING> ) ) , <NEWLINE> ] <NEWLINE> <DEDENT>
if os . path . isdir ( runName ) : <NEWLINE> <COMMENT> <NL> <INDENT> if errnum > 0 : <NEWLINE> <INDENT> shutil . rmtree ( rdir ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> os . system ( <STRING> % ( runName , rdir ) ) <NEWLINE> shutil . rmtree ( runName ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> if errnum == 0 : <NEWLINE> <INDENT> files = os . listdir ( <STRING> ) <NEWLINE> for f in files : <NEWLINE> <INDENT> if os . path . getmtime ( f ) > self . start_time : <NEWLINE> <INDENT> shutil . copy2 ( f , rdir ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _get_next_task ( self , cores ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if cores >= self . sweep_cores and self . sweeps : <NEWLINE> <INDENT> swp : ParamsTask = self . sweeps . pop ( ) <NEWLINE> del self . params_to_sweeps_ind [ swp . params ] <NEWLINE> self . _len -= 1 <NEWLINE> self . in_progress . add ( swp . params ) <NEWLINE> return swp . as_task ( <NEWLINE> <INDENT> self . module , self . hparams , self . folder , <NEWLINE> self . params_to_id [ swp . params ] , <NEWLINE> 0 , self . sweep_cores , <NEWLINE> self . listeners , <NEWLINE> self . params_to_ntrials [ swp . params ] <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
if self . _orientation == gtk . ORIENTATION_HORIZONTAL : <NEWLINE> <INDENT> width += w <NEWLINE> height = max ( height , h ) <NEWLINE> <COMMENT> <NL> item . min_size = w <NEWLINE> else : <NEWLINE> width = max ( width , w ) <NEWLINE> height += h <NEWLINE> <COMMENT> <NL> item . min_size = h <NEWLINE> <DEDENT>
def clean_phrases ( phrases ) : <NEWLINE> <INDENT> for phrase in phrases . values ( ) : <NEWLINE> <INDENT> phrase [ <STRING> ] = <STRING> . join ( phrase [ <STRING> ] ) . replace ( <STRING> , <STRING> ) <NEWLINE> phrase [ <STRING> ] = ( len ( phrase [ <STRING> ] ) / phrase [ <STRING> ] ) + bool ( len ( phrase [ <STRING> ] ) % phrase [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT>
self . dictionary = gensim . corpora . Dictionary ( data_words ) <NEWLINE> <INDENT> corpus = self . preprocess_texts ( docs ) <NEWLINE> saved_model = self . main_data_path / <STRING> / str ( <STRING> + str ( str ( self . _training_path ) ) + str ( self . numb_topics ) ) <NEWLINE> saved_model_fname = str ( hash ( saved_model ) ) + <STRING> <NEWLINE> if os . path . exists ( saved_model_fname ) : <NEWLINE> <COMMENT> <NL> <INDENT> lda_model = gensim . models . ldamodel . LdaModel . load ( os . path . abspath ( saved_model_fname ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> lda_model = gensim . models . ldamodel . LdaModel ( corpus = corpus , <NEWLINE> <INDENT> id2word = self . dictionary , <NEWLINE> num_topics = self . numb_topics , <NEWLINE> random_state = 100 , <NEWLINE> update_every = 1 , <NEWLINE> chunksize = 100 , <NEWLINE> passes = 10 , <NEWLINE> alpha = <STRING> , <NEWLINE> per_word_topics = True ) <NEWLINE> <COMMENT> <NL> <DEDENT> lda_model . save ( os . path . abspath ( saved_model_fname ) ) <NEWLINE> <DEDENT> <DEDENT>
return tuple ( format_sharded_filename ( base_name , shards , i ) for i in range ( shards ) ) <NEWLINE>
shuffle_stats_report = self . _sensitivity_stats ( self . shuffled_sensitivity ) <NEWLINE> <INDENT> missing_stats_report = self . _sensitivity_stats ( self . missing_sensitivity ) <NEWLINE> vulnerability_report = self . _vulnerability_report ( <NEWLINE> <INDENT> shuffled_sensitivity = self . shuffled_sensitivity , <NEWLINE> missing_sensitivity = self . missing_sensitivity , <NEWLINE> shuffled_sensitivity_stats = shuffle_stats_report ) <NEWLINE> <DEDENT> <DEDENT>
def start_and_commit ( self , container , cmd ) : <NEWLINE> <INDENT> client = self . project . client <NEWLINE> logger . info ( <STRING> ) <NEWLINE> client . put_archive ( container , <STRING> , self . archive . getfile ( ) ) <NEWLINE> logger . info ( <STRING> ) <NEWLINE> client . start ( container ) <NEWLINE> for line in client . logs ( container , stream = True ) : <NEWLINE> <INDENT> logger . info ( line . decode ( ) . rstrip ( ) ) <NEWLINE> <DEDENT> result = client . wait ( container ) <NEWLINE> if result == 0 : <NEWLINE> <COMMENT> <NL> <INDENT> repository , tag = self . image_name , None <NEWLINE> if <STRING> in repository : <NEWLINE> <INDENT> repository , tag = repository . split ( <STRING> ) <NEWLINE> <DEDENT> conf = client . create_container_config ( self . image_name , cmd , working_dir = APP_PATH ) <NEWLINE> client . commit ( container , repository = repository , tag = tag , conf = conf ) <NEWLINE> <DEDENT> client . remove_container ( container ) <NEWLINE> return result <NEWLINE> <DEDENT>
def remove_bidi_marks ( string , positions_to_this = None , position_from_this_list = None , embedding_levels = None ) : <NEWLINE> <INDENT> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> c_string = str_to_chars ( string ) <NEWLINE> if positions_to_this != None : <NEWLINE> <INDENT> c_positions_to_this = seq_to_ct ( positions_to_this , FRIBIDI . StrIndex ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> c_positions_to_this = None <NEWLINE> <COMMENT> <NL> <DEDENT> if position_from_this_list != None : <NEWLINE> <INDENT> c_position_from_this_list = seq_to_ct ( position_from_this_list , FRIBIDI . StrIndex ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> c_position_from_this_list = None <NEWLINE> <COMMENT> <NL> <DEDENT> if embedding_levels != None : <NEWLINE> <INDENT> c_embedding_levels = seq_to_ct ( embedding_levels , FRIBIDI . StrIndex ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> c_embedding_levels = None <NEWLINE> <COMMENT> <NL> <DEDENT> new_str_len = fribidi . fribidi_remove_bidi_marks ( c_string , len ( string ) , c_positions_to_this , c_position_from_this_list , c_embedding_levels ) <NEWLINE> if new_str_len < 0 : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> result = [ chars_to_str ( c_string [ : new_str_len ] ) , None , None , None ] <NEWLINE> if positions_to_this != None : <NEWLINE> <INDENT> result [ 1 ] = tuple ( c_positions_to_this ) <NEWLINE> <COMMENT> <NL> <DEDENT> if position_from_this_list != None : <NEWLINE> <INDENT> result [ 2 ] = tuple ( c_position_from_this_list ) <NEWLINE> <COMMENT> <NL> <DEDENT> if embedding_levels != None : <NEWLINE> <INDENT> result [ 3 ] = tuple ( c_embedding_levels ) <NEWLINE> <COMMENT> <NL> <DEDENT> return tuple ( result ) <NEWLINE> <COMMENT> <NL> <DEDENT>
def between ( min , max , iterable ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if min < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if max < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if min > max : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def __get_safe_conn ( self , retry_count ) : <NEWLINE> <INDENT> self . current_size += 1 <NEWLINE> c = self . unuse_list . pop ( ) <NEWLINE> if self . ping_check : <NEWLINE> <INDENT> now = int ( time ( ) ) <NEWLINE> timeout = now <NEWLINE> if isinstance ( self . ping_check , int ) : <NEWLINE> <INDENT> timeout = timeout - self . ping_check <NEWLINE> <DEDENT> if not hasattr ( c , <STRING> ) : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> if c . __ping_check_timestamp < timeout : <NEWLINE> <INDENT> c . __ping_check_timestamp = now <NEWLINE> c . ping ( ) <NEWLINE> <DEDENT> <DEDENT> except : <NEWLINE> <INDENT> self . current_size -= 1 <NEWLINE> if retry_count < 10 : c = self . __get_conn ( retry_count + 1 ) <NEWLINE> <DEDENT> <DEDENT> if c : self . inuse_list . add ( c ) <NEWLINE> return c <NEWLINE> <DEDENT>
def _set_registers ( self , registers ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> buf = ( ctypes . c_int32 * self . register_count ) ( ) <NEWLINE> buf [ : ] = registers <NEWLINE> self . _vba . set_registers ( buf ) <NEWLINE> <DEDENT>
reason = e [ SCWEFEA ] [ <STRING> ] <NEWLINE>
yield instr ( arg ) <NEWLINE>
@ pytest . fixture <NEWLINE> <INDENT> def local_android_download ( request , monkeypatch , tmpdir ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for lang in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> dictfile = tmpdir / ( <STRING> % lang ) <NEWLINE> src_path = os . path . join ( <NEWLINE> <INDENT> os . path . dirname ( __file__ ) , <STRING> % lang ) <NEWLINE> <DEDENT> dictfile . write ( base64 . b64encode ( open ( src_path , <STRING> ) . read ( ) ) ) <NEWLINE> <DEDENT> fake_base_url = <STRING> % str ( tmpdir ) <NEWLINE> monkeypatch . setattr ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> fake_base_url ) <NEWLINE> <DEDENT> return tmpdir <NEWLINE> <DEDENT> <DEDENT>
end_time = time . time ( ) <NEWLINE> <INDENT> self . log . debug ( <STRING> , len ( message ) , end_time - start_time ) <NEWLINE> return self . _update ( m , commit = commit , waitFlush = waitFlush , waitSearcher = waitSearcher ) <NEWLINE> <DEDENT>
def get_kind_ids ( self , txn , kind ) : <NEWLINE> <INDENT> ENTITY_COUNTER = METADATA_VERTEX_COUNTER if kind == KIND_VERTEX else METADATA_EDGE_COUNTER <NEWLINE> METADATA_ID_LIST_PREFIX = METADATA_VERTEX_ID_LIST_PREFIX if kind == KIND_VERTEX else METADATA_EDGE_ID_LIST_PREFIX <NEWLINE> limit = int ( self . _graph . _get ( None , ENTITY_COUNTER ) ) // CHUNK_SIZE <NEWLINE> keys = [ build_key ( METADATA_ID_LIST_PREFIX , i ) for i in range ( 0 , limit + 1 ) ] <NEWLINE> list_entity_ids = self . _graph . _bulk_get_lst ( txn , keys ) <NEWLINE> for entity_ids in list_entity_ids : <NEWLINE> <INDENT> if entity_ids != UNDEFINED : <NEWLINE> <INDENT> for entity_id in entity_ids : <NEWLINE> <INDENT> yield entity_id <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
return my_conversation <NEWLINE>
check_connection_callback ( my_connection , prev_status ) <NEWLINE> <INDENT> except : <NEWLINE> <INDENT> raise <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> if initialize_vcx : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> shutdown ( False ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> raise <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> utg_lib_obj = dict ( ) <COMMENT> <NEWLINE> utg_lib_obj [ <STRING> ] = node . pn <NEWLINE> utg_lib_obj [ <STRING> ] = a <NEWLINE> utg_lib_obj [ <STRING> ] = <STRING> <NEWLINE> utg_lib_obj [ <STRING> ] = int ( c ) <NEWLINE> utg_lib_obj [ <STRING> ] = node . weight <NEWLINE> <DEDENT>
def which_org_has ( repo ) : <NEWLINE> <INDENT> orgs = Orgs ( ) <NEWLINE> for org_info in orgs . get_all_organisations ( ) : <NEWLINE> <INDENT> org_repo = Repo ( org_info [ <STRING> ] ) <NEWLINE> for arepo in org_repo . get_all_repos ( ) : <NEWLINE> <INDENT> if repo == arepo [ <STRING> ] : <NEWLINE> <INDENT> return org_info [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>
num_frames = len ( buf ) / in_channels <NEWLINE> <INDENT> output = np . zeros ( num_frames * out_channels , dtype = np . float32 ) <NEWLINE> if in_channels < out_channels : <NEWLINE> <INDENT> in_channel = 0 <NEWLINE> for out_channel in range ( out_channels ) : <NEWLINE> <INDENT> output [ out_channel : : out_channels ] += buf [ in_channel : : in_channels ] <NEWLINE> in_channel = ( in_channel + 1 ) % in_channels <NEWLINE> <DEDENT> <DEDENT> elif out_channels > in_channels : <NEWLINE> <INDENT> out_channel = 0 <NEWLINE> for in_channel in range ( in_channels ) : <NEWLINE> <INDENT> output [ out_channel : : out_channels ] += buf [ in_channel : : in_channels ] <NEWLINE> out_channel = ( out_channel + 1 ) % out_channels <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if type ( other ) . __name__ == <STRING> : <NEWLINE> <INDENT> added = [ False for other_fex in other . expansion_list ] <NEWLINE> for self_fex in self . expansion_list : <NEWLINE> <INDENT> fex = copy . deepcopy ( self_fex ) <NEWLINE> for i , other_fex in enumerate ( other . expansion_list ) : <NEWLINE> <INDENT> if ( not added [ i ] ) and self_fex . compatible ( other_fex ) : <NEWLINE> <INDENT> fex = fex + other_fex <NEWLINE> added [ i ] = True <NEWLINE> <DEDENT> <DEDENT> pfe_sum . expansion_list . append ( fex ) <NEWLINE> <DEDENT> for i , other_fex in enumerate ( other . expansion_list ) : <NEWLINE> <INDENT> if not added [ i ] : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( other_fex ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> added = False <NEWLINE> for self_fex in self . expansion_list : <NEWLINE> <INDENT> fex = copy . deepcopy ( self_fex ) <NEWLINE> if ( not added ) and fex . compatible ( other ) : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( fex + other ) <NEWLINE> added = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( fex ) <NEWLINE> <DEDENT> <DEDENT> if not added : <NEWLINE> <INDENT> pfe_sum . expansion_list . append ( other ) <NEWLINE> <DEDENT> <DEDENT>
def render ( self , orcname : str , fname : str , <NEWLINE> <INDENT> sr : int = 48000 , ksmps : int = 1 ) -> str : <NEWLINE> sconame = self . to_file ( fname ) <NEWLINE> outname = fname + <STRING> <NEWLINE> call ( [ <STRING> , <NEWLINE> <STRING> + str ( sr ) , <NEWLINE> <STRING> + str ( sr / ksmps ) , <NEWLINE> <STRING> + fname + <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> + outname , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> orcname , <NEWLINE> sconame ] ) <NEWLINE> return outname <NEWLINE> <DEDENT>
self . assertEqual ( mock_adapter1 . device , <STRING> ) <NEWLINE> <INDENT> self . assertEqual ( mock_adapter2 . device , <STRING> ) <NEWLINE> <DEDENT>
exposed_members = members if members else self . _public_members ( ) <NEWLINE> <INDENT> exclude = list ( exclude or [ ] ) <NEWLINE> if exclude_inherited : <NEWLINE> <INDENT> for base in inspect . getmro ( type ( obj ) ) [ 1 : ] : <NEWLINE> <INDENT> exclude += dir ( base ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
cfg . action ( discr , introspectables = ( intr , ) ) <NEWLINE> <INDENT> log . info ( <STRING> % ( layer , path ) ) <NEWLINE> <DEDENT>
def get_final_info ( self ) : <NEWLINE> <INDENT> response = self . session . get ( <NEWLINE> <INDENT> self . info_url , headers = self . headers ) <NEWLINE> <DEDENT> info = json . loads ( response . text ) <NEWLINE> status = info [ <STRING> ] [ <STRING> ] <NEWLINE> if status != <STRING> : <NEWLINE> <INDENT> self . sendmail ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NotDoneError ( status ) <NEWLINE> <DEDENT> <DEDENT>
for ratingTr in appRatingTable : <NEWLINE> <INDENT> inputs = ratingTr . xpath ( <STRING> ) <NEWLINE> if len ( inputs ) < 2 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> appRating = { <STRING> : inputs [ 0 ] . attrib [ <STRING> ] , <STRING> : [ ] } <NEWLINE> for inpt in inputs : <NEWLINE> <INDENT> appRating [ <STRING> ] . append ( inpt . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> appRatings . append ( appRating ) <NEWLINE> <DEDENT>
def scale ( self , image : Image , position : geometry . Point , <NEWLINE> <INDENT> width : int , height : int ) : <NEWLINE> width , height = self . calculate_resolution ( image , width , height ) <NEWLINE> image_width , image_height = image . width , image . height <NEWLINE> if image_width > image_height : <NEWLINE> image_height = int ( image_height * width / image_width ) <NEWLINE> image_width = width <NEWLINE> else : <NEWLINE> image_width = int ( image_width * height / image_height ) <NEWLINE> image_height = height <NEWLINE> offset_x = self . get_offset ( position . x , width , image_width ) <NEWLINE> offset_y = self . get_offset ( position . y , height , image_height ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if ( not debug ) : <NEWLINE> <INDENT> if ( type_out == <STRING> ) : <NEWLINE> <INDENT> nClasses = len ( self . classes [ id_out ] ) <NEWLINE> y = np_utils . to_categorical ( y , nClasses ) . astype ( np . uint8 ) <NEWLINE> <DEDENT> elif ( type_out == <STRING> ) : <NEWLINE> <INDENT> y = np . array ( y ) . astype ( np . uint8 ) <NEWLINE> <DEDENT> elif ( type_out == <STRING> ) : <NEWLINE> <INDENT> y = self . loadText ( y , self . vocabulary [ id_out ] , self . max_text_len [ id_out ] , self . text_offset [ id_out ] ) <NEWLINE> <COMMENT> <NL> y_aux = np . zeros ( list ( y . shape ) + [ self . n_classes_text [ id_out ] ] ) . astype ( np . uint8 ) <NEWLINE> for idx in range ( y . shape [ 0 ] ) : <NEWLINE> <INDENT> y_aux [ idx ] = np_utils . to_categorical ( y [ idx ] , self . n_classes_text [ id_out ] ) . astype ( np . uint8 ) <NEWLINE> <DEDENT> y = y_aux <NEWLINE> <DEDENT> <DEDENT> Y . append ( y ) <NEWLINE> <DEDENT>
scheduler = Scheduler ( ) <NEWLINE> <INDENT> if not self . without_checks : <NEWLINE> <INDENT> for name , server in servers . items ( ) : <NEWLINE> <INDENT> scheduler . register ( server , name ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def save ( self ) : <NEWLINE> <INDENT> with open ( self . location , <STRING> ) as fle : <NEWLINE> <INDENT> return json . dump ( self . data , fle ) <NEWLINE> <DEDENT> <DEDENT>
def tuples ( * args ) : <NEWLINE> <INDENT> res = ( ) <NEWLINE> for arg in args : <NEWLINE> <INDENT> res += tuple ( arg ) if is_list ( arg ) else ( arg , ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> upload_file_job = model . upload_forecast ( forecast_csv_file , timezero_date ) <NEWLINE> busy_poll_upload_file_job ( upload_file_job ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> @ pytest . fixture ( scope = <STRING> ) <NEWLINE> def k_sweep ( hpar , par ) : <NEWLINE> <INDENT> par_sweep = deepcopy ( par ) <NEWLINE> par_sweep [ <STRING> ] [ <STRING> ] = True <NEWLINE> k_sweep = pm . KineticsRun ( hpar , ** par_sweep ) <NEWLINE> k_sweep . solveode ( ) <NEWLINE> return k_sweep <NEWLINE> <DEDENT> <DEDENT>
if i == 0 or next in <STRING> or s [ i - 1 ] not in <STRING> : <NEWLINE>
agent . create_database ( db_config [ <STRING> ] ) <NEWLINE> <INDENT> agent . create_retention_policy ( <STRING> % db_config [ <STRING> ] , <NEWLINE> <INDENT> db_config [ <STRING> ] , <NEWLINE> db_config [ <STRING> ] , <NEWLINE> db_config [ <STRING> ] ) <NEWLINE> <DEDENT> logger . info ( <STRING> , db_config [ <STRING> ] ) <NEWLINE> except : <NEWLINE> pass <NEWLINE> <DEDENT>
def get_sender_organization_id ( from_email , to_email , allowed_senders , config ) : <NEWLINE> <INDENT> if from_email and to_email : <NEWLINE> <INDENT> from_email = from_email . lower ( ) . strip ( ) <NEWLINE> to_email = to_email . lower ( ) . strip ( ) <NEWLINE> for row in allowed_senders : <NEWLINE> <INDENT> if is_email_match ( from_email , row [ <STRING> ] ) and is_email_match ( to_email , row [ <STRING> ] ) : <NEWLINE> <INDENT> return row [ <STRING> ] <NEWLINE> <DEDENT> <DEDENT> default_sender_to_address = config . get ( <STRING> ) <NEWLINE> default_sender_organization_id = config . get ( <STRING> ) <NEWLINE> if default_sender_to_address and default_sender_organization_id : <NEWLINE> <INDENT> default_sender_to_address = default_sender_to_address . lower ( ) . strip ( ) <NEWLINE> if is_email_match ( to_email , default_sender_to_address ) : <NEWLINE> <INDENT> return default_sender_organization_id <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return None <NEWLINE> <DEDENT>
if self . save_data : <NEWLINE> <INDENT> if self . average : <NEWLINE> <INDENT> self . avg_buffer [ irep , : ] = response <NEWLINE> if irep == self . nreps - 1 : <NEWLINE> <INDENT> avg_response = self . avg_buffer . mean ( axis = 0 ) <NEWLINE> self . datafile . append ( self . current_dataset_name , avg_response ) <NEWLINE> self . avg_buffer = np . zeros_like ( self . avg_buffer ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . datafile . append ( self . current_dataset_name , response ) <NEWLINE> <DEDENT> <DEDENT>
pytest . raises ( TypeError , WSignalCallbackProto ) <NEWLINE> <INDENT> pytest . raises ( NotImplementedError , WSignalCallbackProto . __call__ , None , S ( ) , <STRING> , 1 ) <NEWLINE> <DEDENT>
class ClassificationError ( LossFunction ) : <NEWLINE> <INDENT> @ output_loss <NEWLINE> def loss ( self , output , targets ) : <NEWLINE> <INDENT> return ( np . argmax ( output , axis = - 1 ) != <NEWLINE> <INDENT> np . argmax ( np . nan_to_num ( targets ) , axis = - 1 ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not date_step : <NEWLINE> <INDENT> return self . _methods ( method . lower ( ) ) ( ** kwargs ) <NEWLINE> <DEDENT>
def char_at ( text , i ) : <NEWLINE> <INDENT> if len ( text ) - 1 < i : <NEWLINE> <INDENT> return - 1 <NEWLINE> <DEDENT> return ord ( text [ i ] ) <NEWLINE> <DEDENT>
data_copy = data . copy ( ) <NEWLINE> <INDENT> for struct_nm in xrsdefs . structure_names : <NEWLINE> <INDENT> print ( <STRING> + struct_nm + <STRING> ) <NEWLINE> model_id = struct_nm + <STRING> <NEWLINE> model = Classifier ( model_id , None ) <NEWLINE> labels = [ struct_nm in sys_cls for sys_cls in all_sys_cls ] <NEWLINE> data_copy . loc [ : , model_id ] = labels <NEWLINE> if <STRING> in classification_models . keys ( ) and model_id in classification_models [ <STRING> ] and classification_models [ <STRING> ] [ model_id ] . trained : <NEWLINE> <INDENT> old_pars = classification_models [ <STRING> ] [ model_id ] . model . get_params ( ) <NEWLINE> model . model . set_params ( alpha = old_pars [ <STRING> ] , l1_ratio = old_pars [ <STRING> ] ) <NEWLINE> <DEDENT> model . train ( data_copy , hyper_parameters_search = hyper_parameters_search ) <NEWLINE> if model . trained : <NEWLINE> <INDENT> f1_score = model . cross_valid_results [ <STRING> ] <NEWLINE> acc = model . cross_valid_results [ <STRING> ] <NEWLINE> print ( <STRING> . format ( f1_score , acc ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> . format ( model_id , model . default_val ) ) <NEWLINE> <DEDENT> cls_models [ <STRING> ] [ model_id ] = model <NEWLINE> <DEDENT> <DEDENT>
def _train_models ( self , display ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> dataset_dir = self . _vars [ <STRING> ] [ <STRING> ] . get ( ) <NEWLINE> output_dir = self . _vars [ <STRING> ] [ <STRING> ] . get ( ) <NEWLINE> model_config_path = os . path . join ( output_dir , <STRING> ) <NEWLINE> self . _print_to_listbox ( display , <STRING> . format ( dataset_dir ) ) <NEWLINE> df , idx_df = read_local_dataset ( dataset_dir , downsampling_distance = 1. , <NEWLINE> <INDENT> message_callback = partial ( self . _print_to_listbox , display ) ) <NEWLINE> <DEDENT> self . _print_to_listbox ( display , <STRING> ) <NEWLINE> self . _print_to_listbox ( display , <STRING> ) <NEWLINE> self . _print_to_listbox ( display , <STRING> . format ( model_config_path ) ) <NEWLINE> reg_mods , cls_mods = train_from_dataframe ( df , <NEWLINE> <INDENT> train_hyperparameters = False , select_features = False , <NEWLINE> output_dir = output_dir , model_config_path = model_config_path , <NEWLINE> message_callback = partial ( self . _print_to_listbox , display ) <NEWLINE> ) <NEWLINE> <DEDENT> self . _print_to_listbox ( display , <STRING> ) <NEWLINE> <DEDENT>
def generate_rand_from_pdf ( pdf , x_grid , N ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> cdf = np . cumsum ( pdf ) <NEWLINE> cdf = cdf / cdf [ - 1 ] <NEWLINE> values = np . random . rand ( N ) <NEWLINE> value_bins = np . searchsorted ( cdf , values ) <NEWLINE> random_from_cdf , nz = x_grid [ value_bins ] , pdf [ value_bins ] <NEWLINE> return random_from_cdf , nz <NEWLINE> <DEDENT>
str_targets = collections . defaultdict ( list ) <NEWLINE> <INDENT> for target_type , targets in targets_dict . iteritems ( ) : <NEWLINE> <INDENT> for target in targets : <NEWLINE> <INDENT> str_targets [ target_type ] . append ( target . unexpanded_id ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if gene_mean is None : <NEWLINE> <INDENT> gene_mean = E . mean ( 0 ) <NEWLINE> if gene_stdev is None : <NEWLINE> gene_stdev = np . sqrt ( sparse_var ( E ) ) <NEWLINE> return sparse_multiply ( ( E - gene_mean ) . T , 1 / gene_stdev ) . T <NEWLINE> <DEDENT>
count = 0 <NEWLINE> <INDENT> for result in sr . paginate ( StackName = name ) : <NEWLINE> <INDENT> done = ( 1 for x in result [ <STRING> ] <NEWLINE> <INDENT> if <STRING> in x [ <STRING> ] ) <NEWLINE> <DEDENT> count += sum ( done ) <NEWLINE> <DEDENT> if count : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if ( count - current_resources ) > 0 : <NEWLINE> <INDENT> progress . update ( count - current_resources ) <NEWLINE> <DEDENT> <DEDENT> current_resources = count <NEWLINE> progress . close ( ) <NEWLINE> <DEDENT>
def extract_relationships ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for obj in self . classes ( ) : <NEWLINE> <INDENT> node = obj . node <NEWLINE> obj . attrs = self . get_attrs ( node ) <NEWLINE> obj . methods = self . get_methods ( node ) <NEWLINE> <COMMENT> <NL> if is_interface ( node ) : <NEWLINE> <INDENT> obj . shape = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> obj . shape = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> for par_node in node . ancestors ( recurs = False ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> par_obj = self . object_from_node ( par_node ) <NEWLINE> self . add_relationship ( obj , par_obj , <STRING> ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for impl_node in node . implements : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> impl_obj = self . object_from_node ( impl_node ) <NEWLINE> self . add_relationship ( obj , impl_obj , <STRING> ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> for name , values in node . instance_attrs_type . items ( ) : <NEWLINE> <INDENT> for value in values : <NEWLINE> <INDENT> if value is astng . YES : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if isinstance ( value , astng . Instance ) : <NEWLINE> <INDENT> value = value . _proxied <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> ass_obj = self . object_from_node ( value ) <NEWLINE> self . add_relationship ( ass_obj , obj , <STRING> , name ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def leave_function ( self , node ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> not_consumed = self . _to_consume . pop ( ) [ 0 ] <NEWLINE> self . _vars . pop ( 0 ) <NEWLINE> <COMMENT> <NL> if is_error ( node ) : <NEWLINE> <INDENT> return <NEWLINE> <COMMENT> <NL> <DEDENT> is_method = node . is_method ( ) <NEWLINE> klass = node . parent . frame ( ) <NEWLINE> if is_method and ( klass . type == <STRING> or node . is_abstract ( ) ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> authorized_rgx = self . config . dummy_variables_rgx <NEWLINE> overridden = marker = [ ] <NEWLINE> argnames = node . argnames ( ) <NEWLINE> for name , stmts in not_consumed . iteritems ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> if authorized_rgx . match ( name ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> stmt = stmts [ 0 ] <NEWLINE> if isinstance ( stmt , astng . Global ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> if name in argnames : <NEWLINE> <INDENT> if is_method : <NEWLINE> <COMMENT> <NL> <INDENT> if node . type != <STRING> and name == argnames [ 0 ] : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> if overridden is marker : <NEWLINE> <INDENT> overridden = overridden_method ( klass , node . name ) <NEWLINE> <DEDENT> if overridden is not None and name in overridden . argnames ( ) : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if node . name . startswith ( <STRING> ) or node . name . endswith ( <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> self . add_message ( <STRING> , args = name , node = stmt ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . add_message ( <STRING> , args = name , node = stmt ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _check_redefinition ( self , redeftype , node ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> defined_self = node . parent . frame ( ) [ node . name ] <NEWLINE> if defined_self is not node and not astroid . are_exclusive ( node , defined_self ) : <NEWLINE> <INDENT> dummy_variables_rgx = lint_utils . get_global_option ( <NEWLINE> <INDENT> self , <STRING> , default = None ) <NEWLINE> <DEDENT> if dummy_variables_rgx and dummy_variables_rgx . match ( node . name ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> self . add_message ( <STRING> , node = node , <NEWLINE> <INDENT> args = ( redeftype , defined_self . fromlineno ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not isinstance ( error_type , tuple ) : <NEWLINE> <INDENT> error_type = ( error_type , ) <COMMENT> <NEWLINE> expected_errors = { stringify_error ( error ) for error in error_type } <COMMENT> <NEWLINE> if not handler . type : <NEWLINE> return False <NEWLINE> return handler . catch ( expected_errors ) <NEWLINE> <DEDENT>
if gain > best_split . gain and gain > context . min_gain_to_split : <NEWLINE> <INDENT> best_split . gain = gain <NEWLINE> best_split . feature_idx = feature_idx <NEWLINE> best_split . bin_idx = bin_idx <NEWLINE> best_split . gradient_left = gradient_left <NEWLINE> best_split . hessian_left = hessian_left <NEWLINE> best_split . n_samples_left = n_samples_left <NEWLINE> best_split . gradient_right = gradient_right <NEWLINE> best_split . hessian_right = hessian_right <NEWLINE> best_split . n_samples_right = n_samples_right <NEWLINE> <DEDENT>
i = i32 ( tos . value ) . value <NEWLINE> <INDENT> j = i32 ( lr . value ) . value <NEWLINE> D ( <STRING> , i , j , type ( i ) , type ( j ) ) <NEWLINE> i //= j <NEWLINE> D ( <STRING> , i , type ( i ) ) <NEWLINE> tos . value = i <NEWLINE> <DEDENT>
pages = self . memory . alloc_pages ( segment = 0x00 , count = align ( PAGE_SIZE , self . hdt . size ( ) ) // PAGE_SIZE ) <NEWLINE> <INDENT> self . memory . update_pages_flags ( pages [ 0 ] . index , len ( pages ) , <STRING> , True ) <NEWLINE> self . hdt_address = pages [ 0 ] . base_address <NEWLINE> <DEDENT>
for i in range ( 0 , 256 // CPR ) : <NEWLINE> <INDENT> s = [ ] <NEWLINE> t = [ ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for g in list ( groups . values ( ) ) : <NEWLINE> <INDENT> marked |= g - used <NEWLINE> <DEDENT> <DEDENT>
def create ( self , name , blob_name , label = None , container_name = None ) : <NEWLINE> <INDENT> if not container_name : <NEWLINE> <INDENT> container_name = self . account . storage_container ( ) <NEWLINE> <DEDENT> if not label : <NEWLINE> <INDENT> label = name <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> storage = BlobService ( self . account_name , self . account_key ) <NEWLINE> blob_properties = storage . get_blob_properties ( <NEWLINE> <INDENT> container_name , blob_name <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise AzureBlobServicePropertyError ( <NEWLINE> <INDENT> <STRING> % ( blob_name , container_name ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> media_link = storage . make_blob_url ( container_name , blob_name ) <NEWLINE> service = ServiceManagementService ( <NEWLINE> <INDENT> self . publishsettings . subscription_id , <NEWLINE> self . cert_file . name <NEWLINE> <DEDENT> ) <NEWLINE> service_call = service . add_os_image ( <NEWLINE> <INDENT> label , media_link , name , <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_os_image_result = service . get_operation_status ( <NEWLINE> <INDENT> service_call . request_id <NEWLINE> <DEDENT> ) <NEWLINE> status = add_os_image_result . status <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise AzureOsImageCreateError ( <NEWLINE> <INDENT> <STRING> % ( type ( e ) . __name__ , format ( e ) ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return status <NEWLINE> <DEDENT>
sonarsValue = lbot . getSonars ( ) <NEWLINE> <INDENT> if min ( sonarsValue ) < threshold : <NEWLINE> <INDENT> if verbose : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT> if verbose : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
def str2hex ( text ) : <NEWLINE> <INDENT> if sys . version_info [ 0 ] >= 3 : <NEWLINE> <INDENT> return text . encode ( <STRING> ) . hex ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return str ( binascii . hexlify ( bytes ( text ) ) ) <NEWLINE> <DEDENT> <DEDENT>
def look_front ( lbot ) : <NEWLINE> <INDENT> lbot . setJointAngle ( 0 , <STRING> ) <NEWLINE> <DEDENT>
def setAngleCamera ( lbot , angle ) : <NEWLINE> <INDENT> lbot . setJointAngle ( angle , <STRING> ) <NEWLINE> <DEDENT>
def _get_confidence_interval ( bootstrap_dist , stat_val , alpha , is_pivotal ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> val = _np . percentile ( bootstrap_dist , 50 ) <NEWLINE> bootstrap_dist = [ i * ( stat_val / val ) for i in bootstrap_dist ] <COMMENT> <NEWLINE> if is_pivotal : <NEWLINE> <INDENT> low = 2 * stat_val - _np . percentile ( bootstrap_dist , 100 * ( 1 - alpha / 2. ) ) <NEWLINE> val = stat_val <NEWLINE> high = 2 * stat_val - _np . percentile ( bootstrap_dist , 100 * ( alpha / 2. ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> low = _np . percentile ( bootstrap_dist , 100 * ( ( alpha / 2 ) ) ) <NEWLINE> val = _np . percentile ( bootstrap_dist , 50 ) <NEWLINE> high = _np . percentile ( bootstrap_dist , 100 * ( 1 - ( alpha / 2 ) ) ) <NEWLINE> <DEDENT> return BootstrapResults ( low , val , high ) <NEWLINE> <DEDENT>
class Article : <NEWLINE> <INDENT> def __init__ ( <NEWLINE> <INDENT> self , <NEWLINE> title : Optional [ str ] , <NEWLINE> authors : List [ str ] , <NEWLINE> year : Optional [ int ] , <NEWLINE> journal : Optional [ str ] , <NEWLINE> volume : Optional [ str ] = None , <NEWLINE> issue : Optional [ str ] = None , <NEWLINE> page : Optional [ str ] = None , <NEWLINE> doi : Optional [ str ] = None , <NEWLINE> references : Optional [ List [ str ] ] = None , <NEWLINE> keywords : Optional [ List [ str ] ] = None , <NEWLINE> sources : Optional [ Set [ str ] ] = None , <NEWLINE> extra : Optional [ Mapping ] = None , <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> self . title : Optional [ str ] = title <NEWLINE> self . authors : List [ str ] = authors <NEWLINE> self . keywords : List [ str ] = keywords or [ ] <NEWLINE> self . year : Optional [ int ] = year <NEWLINE> self . journal : Optional [ str ] = journal <NEWLINE> self . volume : Optional [ str ] = volume <NEWLINE> self . issue : Optional [ str ] = issue <NEWLINE> self . page : Optional [ str ] = page <NEWLINE> self . doi : Optional [ str ] = doi <NEWLINE> self . references : List [ str ] = references or [ ] <NEWLINE> self . sources : Set [ str ] = sources or set ( ) <NEWLINE> self . extra : Mapping [ str , Any ] = extra or { } <NEWLINE> <DEDENT> <DEDENT>
if not self . active : <NEWLINE> <INDENT> if num_voiced >= 4 : <NEWLINE> <INDENT> sys . stdout . write ( <STRING> ) <NEWLINE> self . active = True <NEWLINE> break <NEWLINE> <DEDENT> elif len ( self . history ) == self . history . maxlen and sum ( self . history ) == 0 : <NEWLINE> <INDENT> sys . stdout . write ( <STRING> ) <NEWLINE> for _ in range ( self . history . maxlen // 2 ) : <NEWLINE> <INDENT> self . history . popleft ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def american_put_exercise_barrier ( mdl , strike ) : <NEWLINE> <INDENT> exercises = [ ] <NEWLINE> payoff = put_payoff ( strike ) <NEWLINE> for cnt , s , ex , opt in mdl . evaluate_american_exercisable_iter ( payoff ) : <NEWLINE> <INDENT> ex_idx = ex >= cnt <NEWLINE> ex_spots = s [ ex_idx ] <NEWLINE> exercises . append ( ex_spots . max ( ) if ex_idx . any ( ) else None ) <NEWLINE> <DEDENT> exercises . reverse ( ) <NEWLINE> return np . array ( exercises ) <NEWLINE> <DEDENT>
attribs_to_remove = [ ] <COMMENT> <NEWLINE> <INDENT> for nsAttrib , val in el . attrib . items ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> attr , ns = strip_prefix ( nsAttrib , el ) <NEWLINE> log . note ( <STRING> % ( <NEWLINE> <INDENT> <STRING> * ( depth * indent ) , attr , val , ns ) ) <NEWLINE> <DEDENT> if ns is not None and ns not in wp . xmlns_urls : <NEWLINE> <INDENT> log . error ( <STRING> . <NEWLINE> <INDENT> format ( element , ns ) , where = el ) <NEWLINE> <DEDENT> attribs_to_remove . append ( nsAttrib ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if not options . no_abnf : <NEWLINE> <INDENT> checker = AbnfChecker ( config ) <NEWLINE> <DEDENT> <DEDENT>
destinatarios = [ ] <NEWLINE> <INDENT> for composicao_comissao in context . zsql . composicao_comissao_obter_zsql ( cod_comissao = comissao . cod_comissao , cod_periodo_comp = periodo . cod_periodo_comp ) : <NEWLINE> <INDENT> if composicao_comissao . dat_desligamento == None or composicao_comissao . dat_desligamento >= DateTime ( ) : <NEWLINE> <INDENT> for destinatario in context . zsql . autor_obter_zsql ( cod_parlamentar = composicao_comissao . cod_parlamentar ) : <NEWLINE> <INDENT> dic = { } <NEWLINE> dic [ <STRING> ] = destinatario . end_email <NEWLINE> if dic [ <STRING> ] != None : <NEWLINE> <INDENT> destinatarios . append ( dic ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def parse_info ( self , data , parse_type ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> res = None <NEWLINE> if parse_type == <STRING> : <NEWLINE> <INDENT> res = [ { <NEWLINE> <INDENT> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : d [ <STRING> ] <NEWLINE> <DEDENT> } for d in data [ <STRING> ] ] <NEWLINE> <DEDENT> elif parse_type == <STRING> : <NEWLINE> <INDENT> tracks = data [ <STRING> ] [ <STRING> ] <NEWLINE> res = [ { <NEWLINE> <INDENT> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : d [ <STRING> ] , <NEWLINE> <STRING> : <STRING> . join ( map ( lambda a : a [ <STRING> ] , d [ <STRING> ] ) ) <NEWLINE> <DEDENT> } for d in tracks ] <NEWLINE> <DEDENT> elif parse_type == <STRING> : <NEWLINE> <INDENT> if <STRING> in data : <NEWLINE> <INDENT> res = { <NEWLINE> <INDENT> <STRING> : data [ <STRING> ] [ <STRING> ] <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> elif <STRING> in data : <NEWLINE> <INDENT> res = { <NEWLINE> <INDENT> <STRING> : <STRING> <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> <DEDENT> return res <NEWLINE> <DEDENT>
def _equalize ( self , template , image ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> template_h , template_w = template . shape [ 0 : 2 ] <NEWLINE> image_h , image_w = image . shape [ 0 : 2 ] <NEWLINE> if ( template_h <= image_h ) and ( template_w <= image_w ) : <NEWLINE> <INDENT> eq_template = template . copy ( ) <COMMENT> <NEWLINE> <COMMENT> <NL> h_scale = float ( template_h ) / image_h <NEWLINE> w_scale = float ( template_w ) / image_w <NEWLINE> scale = max ( h_scale , w_scale ) <COMMENT> <NEWLINE> scaled_h = int ( round ( scale * image_h ) ) <NEWLINE> scaled_w = int ( round ( scale * image_w ) ) <NEWLINE> eq_image = cv2 . resize ( image , ( scaled_w , scaled_h ) , <NEWLINE> <INDENT> interpolation = cv2 . INTER_AREA ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> eq_image = image . copy ( ) <COMMENT> <NEWLINE> <COMMENT> <NL> h_scale = float ( image_h ) / template_h <NEWLINE> w_scale = float ( image_w ) / template_w <NEWLINE> scale = min ( h_scale , w_scale ) <COMMENT> <NEWLINE> scaled_h = int ( round ( scale * template_h ) ) <NEWLINE> scaled_w = int ( round ( scale * template_w ) ) <NEWLINE> eq_template = cv2 . resize ( template , ( scaled_w , scaled_h ) , <NEWLINE> <INDENT> interpolation = cv2 . INTER_AREA ) <NEWLINE> <DEDENT> <DEDENT> return eq_template , eq_image <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> monitor = pig . Grid ( self , meta ) <NEWLINE> self . monitor = monitor <NEWLINE> layout . addWidget ( monitor ) <NEWLINE> <DEDENT>
pwidth = int ( width / size ) <NEWLINE> <INDENT> pheight = int ( height / size ) <NEWLINE> <DEDENT>
key = game . label . lower ( ) <NEWLINE> <INDENT> kgame , label = self . winners [ key ] <NEWLINE> wteam = group . winner ( ) <NEWLINE> setattr ( kgame , label , wteam ) <NEWLINE> if group . is_finished ( ) : <NEWLINE> <INDENT> wteam . games . append ( kgame ) <NEWLINE> <DEDENT> <DEDENT>
xx = self . xx * width <NEWLINE> <INDENT> yy = self . yy * height <NEWLINE> <DEDENT>
for ix , ( r , g , b ) in enumerate ( value ) : <NEWLINE> <INDENT> self . red [ ix ] = r <NEWLINE> self . green [ ix ] = g <NEWLINE> self . blue [ ix ] = b <NEWLINE> <DEDENT>
def fetch_access_token ( self ) : <NEWLINE> <INDENT> url = <STRING> <NEWLINE> key = <STRING> <NEWLINE> res = self . cache . get ( key ) <NEWLINE> if res and res [ <STRING> ] > time . time ( ) : <NEWLINE> <INDENT> return res [ <STRING> ] <NEWLINE> <DEDENT> resp = requests . get ( <NEWLINE> <INDENT> url , <NEWLINE> params = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : self . api_key , <NEWLINE> <STRING> : self . secret_key <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> ) <NEWLINE> jsn = resp . json ( ) <NEWLINE> access_token = jsn [ <STRING> ] <NEWLINE> expires_in = jsn [ <STRING> ] <NEWLINE> self . cache [ key ] = { <NEWLINE> <INDENT> <STRING> : time . time ( ) + expires_in - 20 , <NEWLINE> <STRING> : access_token <NEWLINE> <DEDENT> } <NEWLINE> return access_token <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def zip_parser ( url = None , survey = None ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> _today = datetime . datetime . today ( ) . strftime ( <STRING> ) <NEWLINE> survey_lower = survey . lower ( ) <NEWLINE> path = <STRING> + str ( _today ) + str ( survey_lower ) + <STRING> <COMMENT> <NEWLINE> file = survey + <STRING> <NEWLINE> <DEDENT> <DEDENT>
if ConfigInstall . OS . on_windows or ConfigInstall . OS . on_osx : <NEWLINE> <INDENT> formatter = <STRING> <NEWLINE> else : <NEWLINE> formatter = <STRING> <NEWLINE> logging_config [ <STRING> ] [ <STRING> ] [ <STRING> ] = formatter <NEWLINE> <DEDENT>
def filter_unknown_bases ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . passed = self . stats [ self . stats [ <STRING> ] <= self . max_n_count ] <NEWLINE> self . _criteria_dict [ <STRING> ] [ <STRING> ] = self . stats . index [ <NEWLINE> <INDENT> self . stats [ <STRING> ] > self . max_n_count ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT>
@ click . command ( help = help_text ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , type = int , default = 200 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> , ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , is_flag = True ) <NEWLINE> @ click . option ( <STRING> , is_flag = True , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . argument ( <STRING> , type = click . Path ( exists = True , file_okay = False ) ) <NEWLINE> def cli ( filter_level , max_unknowns , c_deviations , s_deviations , m_deviations , <NEWLINE> <INDENT> dry_run , species , path ) : <NEWLINE> if species : <NEWLINE> from genbankqc import Species <NEWLINE> try : <NEWLINE> <INDENT> s = Species ( path , max_unknowns , c_deviations , s_deviations , <NEWLINE> <INDENT> m_deviations ) <NEWLINE> <DEDENT> s . qc ( ) <NEWLINE> print ( <STRING> , s . species ) <NEWLINE> print ( s ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> print ( <STRING> , s . species ) <NEWLINE> traceback . print_exc ( ) <NEWLINE> else : <NEWLINE> <DEDENT> from genbankqc import Genbank <NEWLINE> genbank = Genbank ( path ) <NEWLINE> genbank . qc ( ) <NEWLINE> <DEDENT> <DEDENT>
@ pytest . fixture ( ) <NEWLINE> <INDENT> def biosample ( ) : <NEWLINE> <INDENT> temp = Path ( tempfile . mkdtemp ( ) ) <NEWLINE> biosample = metadata . BioSample ( temp , <STRING> , sample = 100 ) <NEWLINE> yield biosample <NEWLINE> shutil . rmtree ( temp ) <NEWLINE> <DEDENT> <DEDENT>
def _calculate_proposals ( self ) : <NEWLINE> <INDENT> self . interface . _check_project ( ) <NEWLINE> resource = self . interface . resource <NEWLINE> maxfixes = self . env . get ( <STRING> ) <NEWLINE> proposals = codeassist . code_assist ( <NEWLINE> <INDENT> self . interface . project , self . source , self . offset , <NEWLINE> resource , maxfixes = maxfixes ) <NEWLINE> <DEDENT> proposals = codeassist . sorted_proposals ( proposals ) <NEWLINE> if self . autoimport is not None : <NEWLINE> <INDENT> if self . starting . strip ( ) and <STRING> not in self . expression : <NEWLINE> <INDENT> import_assists = self . autoimport . import_assist ( self . starting ) <NEWLINE> for assist in import_assists : <NEWLINE> <INDENT> p = codeassist . CompletionProposal ( <STRING> . join ( assist ) , <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> proposals . append ( p ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return proposals <NEWLINE> <DEDENT>
assert all ( t1 <= t2 for t1 , t2 in zip ( time_points , time_points [ 1 : ] ) ) , time_points <NEWLINE> <INDENT> epoch = time_points [ 0 ] <NEWLINE> <DEDENT>
@ pytest . fixture <NEWLINE> <INDENT> def app ( session ) : <NEWLINE> <INDENT> return HttpAPI ( [ model ] , session . bind ) <NEWLINE> <DEDENT> <DEDENT>
def _set_job ( cls , job_hash , status , session ) : <NEWLINE> <INDENT> key = cls . _build_jobs_key ( ) <NEWLINE> session . redis_bind . hset ( key , job_hash , json . dumps ( status ) ) <NEWLINE> if session . redis_bind . ttl ( key ) < 0 : <NEWLINE> <INDENT> session . redis_bind . expire ( key , 7 * 24 * 60 * 60 ) <NEWLINE> <DEDENT> <DEDENT>
def __eq__ ( self , other ) : <NEWLINE> <INDENT> if not isinstance ( other , Individual ) : <NEWLINE> <INDENT> return NotImplemented <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> return self . score == other . score <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> return NotImplemented <NEWLINE> <DEDENT> <DEDENT>
chronRates . loc [ pdpind , <STRING> ] += pdpchrg <NEWLINE>
for slot_data in packet . get_slots ( ) : <NEWLINE> <INDENT> StreamIO . write_short ( stream , slot_data . get_id ( ) ) <NEWLINE> if not slot_data . is_empty ( ) : <NEWLINE> <INDENT> StreamIO . write_byte ( stream , slot_data . get_count ( ) ) <NEWLINE> StreamIO . write_short ( stream , slot_data . get_damage ( ) ) <NEWLINE> NBTSerializer . write ( stream , slot_data . get_tag ( ) ) <NEWLINE> <DEDENT> <DEDENT>
@ preprocess . command ( ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , default = [ <STRING> ] , multiple = True , help = <STRING> , type = click . Path ( exists = False ) , show_default = True ) <NEWLINE> @ click . option ( <STRING> , <STRING> , default = <STRING> , help = <STRING> , type = click . Path ( exists = False ) , show_default = True ) <NEWLINE> @ click . option ( <STRING> , <STRING> , default = <STRING> , help = <STRING> , type = click . Path ( exists = False ) , show_default = True ) <NEWLINE> @ click . option ( <STRING> , <STRING> , default = [ ] , multiple = True , help = <STRING> , show_default = True ) <NEWLINE> def combine_methylation_arrays ( input_pkls , optional_input_pkl_dir , output_pkl , exclude ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> os . makedirs ( output_pkl [ : output_pkl . rfind ( <STRING> ) ] , exist_ok = True ) <NEWLINE> if optional_input_pkl_dir : <NEWLINE> <INDENT> input_pkls = glob . glob ( os . path . join ( optional_input_pkl_dir , <STRING> , <STRING> ) ) <NEWLINE> if exclude : <NEWLINE> <INDENT> input_pkls = ( np . array ( input_pkls ) [ ~ np . isin ( np . vectorize ( lambda x : x . split ( <STRING> ) [ - 2 ] ) ( input_pkls ) , np . array ( exclude ) ) ] ) . tolist ( ) <NEWLINE> <DEDENT> <DEDENT> if len ( input_pkls ) > 1 : <NEWLINE> <INDENT> base_methyl_array = MethylationArray ( * extract_pheno_beta_df_from_pickle_dict ( pickle . load ( open ( input_pkls [ 0 ] , <STRING> ) ) , <STRING> ) ) <NEWLINE> methyl_arrays_generator = ( MethylationArray ( * extract_pheno_beta_df_from_pickle_dict ( pickle . load ( open ( input_pkl , <STRING> ) ) , <STRING> ) ) for input_pkl in input_pkls [ 1 : ] ) <NEWLINE> list_methyl_arrays = MethylationArrays ( [ base_methyl_array ] ) <NEWLINE> combined_methyl_array = list_methyl_arrays . combine ( methyl_arrays_generator ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> combined_methyl_array = MethylationArray ( * extract_pheno_beta_df_from_pickle_dict ( pickle . load ( open ( input_pkls [ 0 ] , <STRING> ) ) , <STRING> ) ) <NEWLINE> <DEDENT> combined_methyl_array . write_pickle ( output_pkl ) <NEWLINE> <DEDENT> <DEDENT>
@ then ( <STRING> ) <NEWLINE> <INDENT> def then_categories_number_format_is_value ( context , value ) : <NEWLINE> <INDENT> expected_value = value <NEWLINE> number_format = context . categories . number_format <NEWLINE> assert number_format == expected_value , <STRING> % number_format <NEWLINE> <DEDENT> <DEDENT>
def validate_array ( schema , data ) : <NEWLINE> <INDENT> if schema . get ( <STRING> ) != <STRING> or not schema . get ( <STRING> ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> col_map = { <STRING> : <STRING> , <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> } <NEWLINE> <DEDENT> col_fmt = schema . get ( <STRING> , <STRING> ) <NEWLINE> delimiter = col_map . get ( col_fmt ) <NEWLINE> if not delimiter : <NEWLINE> <INDENT> logger . error ( <STRING> , col_fmt ) <NEWLINE> return <NEWLINE> <DEDENT> if col_fmt == <STRING> : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT> subschema = schema . get ( <STRING> ) <NEWLINE> items = data . split ( delimiter ) <NEWLINE> for subval in items : <NEWLINE> <INDENT> converted_value , error = validate_type ( subschema , subval , schema [ <STRING> ] , schema [ <STRING> ] ) <NEWLINE> if error : <NEWLINE> <INDENT> return error <NEWLINE> <COMMENT> <NL> <DEDENT> for func in VALIDATORS : <NEWLINE> <INDENT> error = func ( subschema , converted_value ) <NEWLINE> if error : <NEWLINE> <INDENT> return error <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if not destination : <NEWLINE> <INDENT> print ( <STRING> . format ( search_for ) ) <NEWLINE> return instructions , user_projects <NEWLINE> for project in projects : <NEWLINE> instructions . append ( ( project , destination ) ) <NEWLINE> if user : <NEWLINE> names = None <NEWLINE> if user [ <STRING> ] != <STRING> : <NEWLINE> names = user [ <STRING> ] <NEWLINE> user_projects = glc . user_projects ( conn_src , names = names , statistics = False ) <NEWLINE> return instructions , user_projects <NEWLINE> <DEDENT>
gl_src = glc . connect ( src_server . url , src_server . auth_token , ssl_verify = src_server . ssl_verify ) <NEWLINE> <INDENT> gl_dst = glc . connect ( dst_server . url , dst_server . auth_token , ssl_verify = dst_server . ssl_verify ) <NEWLINE> <DEDENT>
def test_hiding_internal_options ( ) : <NEWLINE> <INDENT> with MockIO ( <STRING> ) as mockio : <NEWLINE> <INDENT> CliBuilder ( hide_internal = True ) . run ( ) <NEWLINE> assert <STRING> not in mockio . output ( ) <NEWLINE> assert <STRING> not in mockio . output ( ) <NEWLINE> <DEDENT> with MockIO ( <STRING> ) as mockio : <NEWLINE> <INDENT> CliBuilder ( hide_internal = False ) . run ( ) <NEWLINE> assert <STRING> in mockio . output ( ) <NEWLINE> assert <STRING> in mockio . output ( ) <NEWLINE> <DEDENT> <DEDENT>
def add_node ( <NEWLINE> <INDENT> self , id_ , key = None , text = <STRING> , shape = None , <NEWLINE> height = 10.0 , width = 10.0 , x = 0.0 , y = 0.0 , <NEWLINE> fill_color = <STRING> , transparent = False , <NEWLINE> border_color = <STRING> , border_type = <STRING> , border_width = 1.0 , <NEWLINE> geometry = None , label = None , fill = None , border = None , <NEWLINE> ** label_kwargs <NEWLINE> ) : <NEWLINE> if geometry is None : <NEWLINE> <INDENT> geometry = Geometry ( width , height , x , y ) <NEWLINE> <DEDENT> if fill is None : <NEWLINE> <INDENT> fill = Fill ( fill_color , transparent ) <NEWLINE> <DEDENT> if border is None : <NEWLINE> <INDENT> border = Style ( border_color , border_type , border_width ) <NEWLINE> <DEDENT> if label is None : <NEWLINE> <INDENT> label = Label ( ** label_kwargs ) <NEWLINE> <DEDENT> self . nodes [ id_ ] = Node ( id_ , key , text , shape , label , geometry , fill , border ) <NEWLINE> <DEDENT>
rawcof = ( sigcpd ) ** 2 / ( ( pds1 ) * ( pds2 ) ) <NEWLINE>
if input : <NEWLINE> <INDENT> plugin , text = self . brain . query ( input ) <NEWLINE> if plugin and text : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> plugin . handle ( text , self . mic ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> self . _logger . error ( <STRING> , <NEWLINE> <INDENT> exc_info = True ) <NEWLINE> <DEDENT> self . mic . say ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . _logger . debug ( <STRING> + <NEWLINE> <INDENT> <STRING> , text , <NEWLINE> plugin . info . name ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . mic . say ( <STRING> ) <NEWLINE> <DEDENT>
def url ( self , url = None , username = None , password = None , sha1 = None , ignoreUrlErrors = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if url is not None : <NEWLINE> <INDENT> self . _url = url <COMMENT> <NEWLINE> <DEDENT> if username is not None : <NEWLINE> <INDENT> self . _urluser = username <COMMENT> <NEWLINE> <DEDENT> if password is not None : <NEWLINE> <INDENT> self . _urlpassword = password <COMMENT> <NEWLINE> <DEDENT> if sha1 is not None : <NEWLINE> <INDENT> self . _urlsha1 = sha1 <COMMENT> <NEWLINE> <DEDENT> if ignoreUrlErrors is not None : <NEWLINE> <INDENT> self . _ignoreErrors = ignoreUrlErrors <NEWLINE> <DEDENT> if url is None and username is None and password is None and sha1 is None and ignoreUrlErrors is None : <NEWLINE> <INDENT> return self . _url <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self <NEWLINE> <DEDENT> <DEDENT>
if pyautoguiAvailable : <NEWLINE> <INDENT> def clickGraphic ( imagepath , delay = 10 , confidence = 1 ) : <NEWLINE> <INDENT> elemToClick = None <NEWLINE> numTries = 1 <NEWLINE> print ( <STRING> . format ( imagepath ) ) <NEWLINE> while ( elemToClick is None ) and ( numTries < delay ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> elemToClick = pyautogui . locateOnScreen ( imagepath , confidence ) <NEWLINE> <DEDENT> except Exception as exp : <NEWLINE> <INDENT> if isinstance ( exp , pyautogui . pyscreeze . ImageNotFoundException ) : <NEWLINE> <INDENT> print ( <STRING> . format ( numTries , imagepath ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( exp ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> numTries += 1 <NEWLINE> time . sleep ( 1 ) <NEWLINE> <DEDENT> <DEDENT> if elemToClick is None : <NEWLINE> <INDENT> print ( <STRING> . format ( imagepath ) ) <NEWLINE> return False <NEWLINE> <DEDENT> time . sleep ( 1 ) <NEWLINE> pyautogui . click ( pyautogui . center ( elemToClick ) ) <NEWLINE> print ( <STRING> . format ( elemToClick ) ) <NEWLINE> time . sleep ( 1 ) <NEWLINE> return True <NEWLINE> else : <NEWLINE> <DEDENT> def clickGraphic ( imagepath , delay = 10 , confidence = 1 ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> return False <NEWLINE> <DEDENT> <DEDENT>
class UserPlanQuerySet ( models . QuerySet ) : <NEWLINE> <INDENT> def expires_in ( self , days = 7 ) : <NEWLINE> <INDENT> threshold = now ( ) + timedelta ( days = days ) <NEWLINE> return self . filter ( expiration = threshold . date ( ) ) <NEWLINE> <DEDENT> <DEDENT>
fregions = prefix + <STRING> <NEWLINE> <INDENT> with open ( fregions , <STRING> ) as fh : <NEWLINE> <INDENT> list ( peaks . peaks ( prefix + <STRING> , - 1 , threshold , seed , <NEWLINE> <INDENT> dist , fh , operator . le ) ) <NEWLINE> <DEDENT> <DEDENT> n_regions = sum ( 1 for _ in open ( fregions ) ) <NEWLINE> print >> sys . stderr , <STRING> % ( fregions , n_regions ) <NEWLINE> <DEDENT>
def bm25_idf ( N , df ) : <NEWLINE> <INDENT> assert ( N >= df ) <NEWLINE> return log ( ( N - df + 0.5 ) / ( df + 0.5 ) ) <NEWLINE> <DEDENT>
with tf . variable_scope ( scope , <STRING> ) : <NEWLINE> <INDENT> with tf . variable_scope ( <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> ( self . embedding_output , self . embedding_table ) = embedding_lookup ( <NEWLINE> <INDENT> input_ids = input_ids , <NEWLINE> vocab_size = config . vocab_size , <NEWLINE> embedding_size = config . hidden_size , <NEWLINE> initializer_range = config . initializer_range , <NEWLINE> word_embedding_name = <STRING> , <NEWLINE> use_one_hot_embeddings = use_one_hot_embeddings ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
args = parser . parse_args ( ) <NEWLINE> <INDENT> param_str = <STRING> . join ( [ <STRING> % ( k , v ) for k , v in sorted ( vars ( args ) . items ( ) ) ] ) <NEWLINE> print ( <STRING> . format ( <STRING> . join ( [ x for x in sys . argv ] ) , param_str ) ) <NEWLINE> return args <NEWLINE> <DEDENT>
def input_fn_builder ( self , worker ) : <NEWLINE> <INDENT> def gen ( ) : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> if self . result : <NEWLINE> <INDENT> num_result = len ( self . result ) <NEWLINE> worker . send_multipart ( [ ident , <STRING> , pickle . dumps ( self . result ) ] ) <NEWLINE> self . result . clear ( ) <NEWLINE> time_used = time . clock ( ) - start <NEWLINE> logger . info ( <STRING> % <NEWLINE> <INDENT> ( num_result , ident , time_used , int ( num_result / time_used ) ) ) <NEWLINE> <DEDENT> <DEDENT> ident , empty , msg = worker . recv_multipart ( ) <NEWLINE> start = time . clock ( ) <NEWLINE> msg = pickle . loads ( msg ) <NEWLINE> if self . is_valid_input ( msg ) : <NEWLINE> <INDENT> tmp_f = list ( convert_lst_to_features ( msg , self . max_seq_len , self . tokenizer ) ) <NEWLINE> yield { <NEWLINE> <INDENT> <STRING> : [ f . input_ids for f in tmp_f ] , <NEWLINE> <STRING> : [ f . input_mask for f in tmp_f ] , <NEWLINE> <STRING> : [ f . input_type_ids for f in tmp_f ] <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . warning ( <STRING> % self . id ) <NEWLINE> worker . send_multipart ( [ ident , <STRING> , pickle . dumps ( None ) ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
f = os . path . join ( os . path . dirname ( __file__ ) , <STRING> ) <NEWLINE> <INDENT> lexicon . model = Model . load ( f , lexicon ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> inputs_coeffs = tuple ( modwt_level_nd ( x , level , axes ) for x in inputs ) <NEWLINE> bin_args = tuple ( modwt_level_nd ( x , level , axes , approx_only = True ) for x in bin_args ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> for python_version_minor in range ( <NEWLINE> <INDENT> python_version_min_parts [ 1 ] , python_version_minor_max ) : <NEWLINE> classifiers_sane . append ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> PYTHON_VERSION_MAJOR , python_version_minor , ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
return updated_node . with_changes ( <NEWLINE> <INDENT> operator = cst . Is ( <NEWLINE> <INDENT> whitespace_after = original_node . operator . whitespace_after , <NEWLINE> whitespace_before = original_node . operator . whitespace_before , <NEWLINE> <DEDENT> ) <NEWLINE> ) <NEWLINE> <DEDENT>
return updated_node . with_changes ( value = changed_tuple ) <NEWLINE>
def get_payment_status ( self , d = None ) : <NEWLINE> <INDENT> if d is None : <NEWLINE> <INDENT> d = date . today ( ) <NEWLINE> <DEDENT> explanation = <STRING> . join ( <NEWLINE> <INDENT> discount . explanation . strip ( ) <NEWLINE> for discount in self . all_discounts <NEWLINE> if discount . accounted . date ( ) <= d and discount . explanation . strip ( ) <NEWLINE> <DEDENT> ) <NEWLINE> return PaymentStatus ( <NEWLINE> <INDENT> price = self . price if self . approved and self . approved . date ( ) <= d else 0 , <NEWLINE> discount = self . get_discounted ( d ) , <NEWLINE> explanation = explanation , <NEWLINE> paid = self . get_paid ( d ) , <NEWLINE> current_date = d , <NEWLINE> due_from = self . subject . event . due_from , <NEWLINE> due_date = self . subject . event . due_date , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> parameter_type_str = ( <NEWLINE> <INDENT> <STRING> <NEWLINE> if p_type == ParameterType . AVERAGE_TIMESERIES <NEWLINE> else <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> res . _meta . loc [ grp . index ] = res . _meta . loc [ grp . index ] . assign ( <NEWLINE> <INDENT> parameter_type = parameter_type_str <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
def _parse_text ( self , term , pos ) : <NEWLINE> <INDENT> end = pos + len ( term ) <NEWLINE> part = self . source [ pos : end ] <NEWLINE> yield ParseResult ( term , end ) if part == term else ParseFailure <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def indirect_paths ( g , path_lim , aggregation , criterion ) : <NEWLINE> <INDENT> if path_lim == 1 : <COMMENT> <NEWLINE> <INDENT> return g <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> if path_lim % 2 == 0 : <NEWLINE> <INDENT> return indirect_paths ( compute_path ( g , g , aggregation , criterion ) , path_lim // 2 , type , criterion ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return compute_path ( indirect_paths ( g , path_lim - 1 , aggregation , criterion ) , g , type , criterion ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> y_pred = np . asarray ( np . argmax ( q_pred , axis = 1 ) , dtype = int ) <NEWLINE> labels = y_pred . astype ( <STRING> ) <NEWLINE> labels = pd . Categorical ( values = labels , categories = natsorted ( np . unique ( y_pred ) . astype ( <STRING> ) ) ) <NEWLINE> <DEDENT>
target_ds = None <NEWLINE> <INDENT> for r in Clone . __call__ ( <NEWLINE> <INDENT> source = clone_src , <NEWLINE> path = path , <NEWLINE> dataset = dataset , <NEWLINE> description = description , <NEWLINE> reckless = reckless , <NEWLINE> alt_sources = alt_sources , <NEWLINE> result_filter = None , <NEWLINE> result_renderer = <STRING> , <NEWLINE> on_failure = <STRING> ) : <NEWLINE> if r . get ( <STRING> , None ) == <STRING> and r . get ( <STRING> , None ) == <STRING> : <NEWLINE> target_ds = Dataset ( r [ <STRING> ] ) <NEWLINE> yield r <NEWLINE> <DEDENT> <DEDENT>
if refseq != None : <NEWLINE> <INDENT> seqlen = len ( refseq ) <NEWLINE> else : <NEWLINE> seqlen = reads . end . max ( ) <NEWLINE> f = None <NEWLINE> reads = reads [ reads . reads >= cutoff ] <NEWLINE> if by is not None : <NEWLINE> reads = reads . sort_values ( by , ascending = False ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if ref_genome != <STRING> : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> allreads = utils . combine_aligned_reads ( path , files , ref_genome ) <NEWLINE> new , cl = novel . find_mirnas ( allreads , cow_fasta ) <NEWLINE> new . to_csv ( os . path . join ( out , <STRING> ) , index = False ) <NEWLINE> novel . create_report ( new , cl , species , filename = os . path . join ( out , <STRING> ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
headers = self . headers ( encoded_data ) <NEWLINE> <INDENT> if headers : <NEWLINE> <INDENT> rq_kwargs [ <STRING> ] = headers <NEWLINE> <DEDENT> body = encoded_data <NEWLINE> if body : <NEWLINE> <INDENT> rq_kwargs [ <STRING> ] = encoded_data <NEWLINE> <DEDENT> <DEDENT>
def _fom ( self , data1 : Data , data2 : Data ) -> float : <NEWLINE> <INDENT> clusters1 = set ( data1 . df [ <STRING> ] . unique ( ) ) <NEWLINE> clusters2 = set ( data2 . df [ <STRING> ] . unique ( ) ) <NEWLINE> if not clusters1 == clusters2 : <NEWLINE> <INDENT> return np . nan <NEWLINE> <DEDENT> clusters = clusters1 <NEWLINE> cluster2bpoint = { } <NEWLINE> for cluster in clusters : <NEWLINE> <INDENT> bpoints1 = data1 . df [ <NEWLINE> <INDENT> ( data1 . df [ <STRING> ] == cluster ) & data1 . df [ <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> bpoints2 = data2 . df [ <NEWLINE> <INDENT> ( data2 . df [ <STRING> ] == cluster ) & data2 . df [ <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> msg = <STRING> <NEWLINE> if len ( bpoints1 ) != 1 : <NEWLINE> <INDENT> raise ValueError ( msg . format ( len ( bpoints1 ) , 1 ) ) <NEWLINE> <DEDENT> if len ( bpoints2 ) != 1 : <NEWLINE> <INDENT> raise ValueError ( msg . format ( len ( bpoints2 ) , 2 ) ) <NEWLINE> <DEDENT> bpoint1 = bpoints1 . iloc [ 0 ] [ data1 . par_cols ] <NEWLINE> bpoint2 = bpoints2 . iloc [ 0 ] [ data2 . par_cols ] <NEWLINE> cluster2bpoint [ cluster ] = ( bpoint1 , bpoint2 ) <NEWLINE> <DEDENT> return self . _fom2 ( cluster2bpoint ) <NEWLINE> <DEDENT>
hf_kw = dict ( color = light_color ) <NEWLINE> <INDENT> hf_kw . update ( hist_fill_kwargs ) <NEWLINE> <DEDENT>
def check_dependencies ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if self . required and self . start_sequence == 0 : <NEWLINE> <INDENT> self . logger . warn ( <STRING> ) <NEWLINE> self . required = False <NEWLINE> <COMMENT> <NL> <DEDENT> if not self . addresses : <NEWLINE> <INDENT> self . addresses = [ <STRING> ] <NEWLINE> self . logger . warn ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for container in input : <NEWLINE> <INDENT> client = LXCContainer ( container ) <NEWLINE> result . append ( client . create ( ) ) <NEWLINE> <DEDENT> return response . reply ( result , message = <STRING> . format ( container . get ( <STRING> ) ) ) <NEWLINE> except ValueError as ex : <NEWLINE> return response . reply ( message = ex . __str__ ( ) , status = 403 ) <NEWLINE> <DEDENT>
class Par2File ( object ) : <NEWLINE> <INDENT> def __init__ ( self , obj_or_path ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . path = None <NEWLINE> if isinstance ( obj_or_path , basestring ) : <NEWLINE> <INDENT> with open ( obj_or_path ) as f : <NEWLINE> <INDENT> self . contents = f . read ( ) <NEWLINE> self . path = obj_or_path <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . contents = obj_or_path . read ( ) <NEWLINE> if getattr ( obj_or_path , <STRING> , None ) : <NEWLINE> <INDENT> self . path = obj_or_path . name <NEWLINE> <DEDENT> <DEDENT> self . packets = self . read_packets ( ) <NEWLINE> <DEDENT> <DEDENT>
return render ( request , <STRING> , { <NEWLINE> <INDENT> <STRING> : json_encode ( user_options ) , <NEWLINE> } ) <NEWLINE> <DEDENT>
def check ( ** kwargs ) : <NEWLINE> <INDENT> result = SimpleNamespace ( ok = False , time = 0 , size = None , err = None ) <NEWLINE> try : <NEWLINE> <INDENT> t = os . path . getmtime ( kwargs [ <STRING> ] ) <NEWLINE> size = os . path . getsize ( kwargs [ <STRING> ] ) <NEWLINE> <DEDENT> except : <NEWLINE> <INDENT> return result <NEWLINE> <DEDENT> result . time = t <NEWLINE> result . size = size <NEWLINE> if <STRING> in kwargs : <NEWLINE> <INDENT> result . ok = size >= kwargs . get ( <STRING> ) <NEWLINE> if not result . ok : <NEWLINE> <INDENT> result . err = <STRING> <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> result . ok = True <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
row = 1 <NEWLINE> <INDENT> for ( msgid , message ) in messages : <NEWLINE> <INDENT> column = 0 <NEWLINE> sheet . write ( row , 0 , msgid ) <NEWLINE> column += 1 <NEWLINE> if <STRING> in options . comments : <NEWLINE> <INDENT> o = [ ] <NEWLINE> for ( entry , lineno ) in msg . occurrences : <NEWLINE> <INDENT> if lineno : <NEWLINE> <INDENT> o . append ( <STRING> % ( entry , lineno ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> o . append ( entry ) <NEWLINE> <DEDENT> <DEDENT> sheet . write ( row , column , <STRING> . join ( o ) ) <NEWLINE> column += 1 <NEWLINE> <DEDENT> if <STRING> in options . comments : <NEWLINE> <INDENT> sheet . write ( row , column , msg . comment ) <NEWLINE> column += 1 <NEWLINE> <DEDENT> if <STRING> in options . comments : <NEWLINE> <INDENT> sheet . write ( row , column , msg . tcomment ) <NEWLINE> column += 1 <NEWLINE> <DEDENT> for ( i , cat ) in enumerate ( catalogs ) : <NEWLINE> <INDENT> cat = cat [ 1 ] <NEWLINE> msg = cat . find ( msgid ) <NEWLINE> if msg is not None : <NEWLINE> <INDENT> if <STRING> in msg . flags : <NEWLINE> <INDENT> sheet . write ( row , column , msg . msgstr , italic_style ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sheet . write ( row , column , msg . msgstr ) <NEWLINE> <DEDENT> <DEDENT> column += 1 <NEWLINE> <DEDENT> row += 1 <NEWLINE> <DEDENT> <DEDENT>
while lo <= hi : <NEWLINE> <INDENT> mid = lo + ( hi - lo ) // 2 <NEWLINE> if less ( x , a [ mid ] ) : <NEWLINE> <INDENT> hi = mid - 1 <NEWLINE> <DEDENT> elif less ( a [ mid ] , x ) : <NEWLINE> <INDENT> lo = mid + 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return mid <NEWLINE> <DEDENT> <DEDENT>
with self . assertRaises ( ImmutablePropertyException ) : <NEWLINE> <INDENT> refetched_entity . name = <STRING> <NEWLINE> <DEDENT>
for k in targets : <NEWLINE> <INDENT> item = targets [ k ] <NEWLINE> data = item [ <STRING> ] <NEWLINE> if not allgenes : <NEWLINE> <INDENT> if not <STRING> in item : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> hvg = item [ <STRING> ] [ <STRING> ] <NEWLINE> data = data [ data . index . isin ( hvg ) ] <NEWLINE> <DEDENT> elif verbose : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> <DEDENT> if scale : <NEWLINE> <INDENT> d_scaled = sklearn_scale ( <NEWLINE> <INDENT> data . transpose ( ) , <COMMENT> <NEWLINE> axis = 0 , <COMMENT> <NEWLINE> with_mean = True , <COMMENT> <NEWLINE> with_std = True ) <COMMENT> <NEWLINE> <DEDENT> d_scaled = pd . DataFrame ( d_scaled . transpose ( ) , index = data . index ) <NEWLINE> <DEDENT> if verbose : <NEWLINE> <INDENT> v = ( method , k , data . shape [ 0 ] , data . shape [ 1 ] ) <NEWLINE> print ( <STRING> % v ) <NEWLINE> <DEDENT> if method == <STRING> : <NEWLINE> <INDENT> comp , contr = irlb ( data , ncomp , seed ) <NEWLINE> <DEDENT> elif method == <STRING> : <NEWLINE> <INDENT> comp , contr = svd ( data , ncomp ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> comp . index = data . columns <NEWLINE> obj . norm_data [ k ] [ <STRING> ] [ <STRING> ] = { <STRING> : comp , <NEWLINE> <INDENT> <STRING> : contr , <NEWLINE> <STRING> : method } <NEWLINE> <DEDENT> obj . set_assay ( sys . _getframe ( ) . f_code . co_name , method ) <NEWLINE> <DEDENT>
def verify_unit_interval ( value ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not ( value >= 0 and value <= 1 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> return value <NEWLINE> <DEDENT>
def transform ( self , df , n_feats = 10 ) : <NEWLINE> <INDENT> assert df . shape [ 1 ] >= n_feats <NEWLINE> step = self . find_set_of_size ( n_feats ) <NEWLINE> return df [ self . features_at_step ( step ) ] <NEWLINE> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def hue_gump ( cls , bitmap , hue , partial_hue ) : <NEWLINE> <INDENT> copy = bitmap . copy ( ) <NEWLINE> if hue : <NEWLINE> <INDENT> hue = ( hue & 0x3FFF ) - 1 <NEWLINE> return Hues . HUES [ hue ] . apply_to ( copy , only_grey_pixels = partial_hue ) <NEWLINE> <DEDENT> return copy <NEWLINE> <DEDENT> <DEDENT>
if ( board_height <= 0 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> else : <NEWLINE> self . board_height = board_height <NEWLINE> <DEDENT>
if ( board_height <= 0 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> else : <NEWLINE> self . board_height = board_height <NEWLINE> <DEDENT>
args = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : to_b64 ( long2a ( p ) ) , <NEWLINE> <STRING> : to_b64 ( long2a ( g ) ) , <NEWLINE> <STRING> : to_b64 ( long2a ( pow ( g , priv_key , p ) ) ) , <NEWLINE> } <NEWLINE> <DEDENT>
def make_app ( redis_connection_obj , port , host_url , host_name , datadir ) : <NEWLINE> <INDENT> app . register_blueprint ( rpcblueprint , url_prefix = <STRING> ) <NEWLINE> app . port = port <NEWLINE> if gevent : <NEWLINE> <INDENT> redis . connection . socket = gevent . socket <NEWLINE> <DEDENT> rpcblueprint . r = redis . StrictRedis ( host = redis_connection_obj [ <STRING> ] , <NEWLINE> <INDENT> port = redis_connection_obj [ <STRING> ] , <NEWLINE> db = redis_connection_obj [ <STRING> ] ) <NEWLINE> <DEDENT> rpcblueprint . task_queue = TaskQueue ( rpcblueprint . r ) <NEWLINE> server_manager = Servers ( rpcblueprint . r ) <NEWLINE> settings . setup_server ( rpcblueprint . r , datadir , host_url , host_name , <NEWLINE> <INDENT> Catalog ( rpcblueprint . r , datadir , host_name ) , <NEWLINE> server_manager <NEWLINE> <DEDENT> ) <NEWLINE> rpcblueprint . heartbeat_thread = HeartbeatThread ( ) <NEWLINE> return app <NEWLINE> <DEDENT>
def run ( redis_connection , node_url , node_name , queue , datadir ) : <NEWLINE> <INDENT> if node_name is None : <NEWLINE> <INDENT> node_name = node_url <NEWLINE> <DEDENT> redis_connection_obj = parse_redis_connection ( redis_connection ) <NEWLINE> r = redis . StrictRedis ( host = redis_connection_obj [ <STRING> ] , <NEWLINE> <INDENT> port = redis_connection_obj [ <STRING> ] , <NEWLINE> db = redis_connection_obj [ <STRING> ] ) <NEWLINE> <DEDENT> server_manager = Servers ( r ) <NEWLINE> settings . setup_server ( r , datadir , node_url , node_name , <NEWLINE> <INDENT> Catalog ( r , datadir , node_name ) , <NEWLINE> server_manager <NEWLINE> <DEDENT> ) <NEWLINE> if queue is None : <NEWLINE> <INDENT> queue = [ <STRING> ] <NEWLINE> <DEDENT> with Connection ( r ) : <NEWLINE> <INDENT> queues = [ ] <NEWLINE> node_queue = KitchenSinkRedisQueue ( node_url ) <NEWLINE> queues . append ( node_queue ) <NEWLINE> for q in queue : <NEWLINE> <INDENT> if <STRING> in q : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> queues . append ( KitchenSinkRedisQueue ( q ) ) <NEWLINE> queues . append ( KitchenSinkRedisQueue ( <STRING> % ( q , node_name ) ) ) <NEWLINE> w = KitchenSinkWorker ( queues , default_result_ttl = 86400 ) <NEWLINE> <DEDENT> <DEDENT> w . work ( burst = False ) <NEWLINE> <DEDENT>
if x_type == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> if start_time is None : <NEWLINE> <INDENT> start_time = 0 <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <DEDENT> if start_time is None : <NEWLINE> <INDENT> start_time = x_values [ 0 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> assert start_time <= x_values [ 0 ] , <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> x_values = _np . round ( ( x_values - start_time ) * sampling_freq , 10 ) . astype ( int ) <NEWLINE> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def from_dict ( cls , message_dict : typing . Dict [ str , typing . Any ] ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> message_dict = cls . only_fields ( message_dict ) <NEWLINE> intent_dict = message_dict . pop ( <STRING> , { } ) <NEWLINE> slot_dicts = message_dict . pop ( <STRING> , [ ] ) <NEWLINE> message = NluIntent ( <COMMENT> <NEWLINE> <INDENT> ** message_dict , intent = Intent ( ** intent_dict ) <NEWLINE> <DEDENT> ) <NEWLINE> message . slots = [ Slot . from_dict ( s ) for s in slot_dicts ] <NEWLINE> <DEDENT> <DEDENT>
cdl = cdl . resample ( freq ) . agg ( { <STRING> : <STRING> , <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> } ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> cdl = cdl . loc [ <NEWLINE> pytz . timezone ( NY ) . localize ( dtbegin ) : <NEWLINE> pytz . timezone ( NY ) . localize ( dtend ) <NEWLINE> ] . dropna ( subset = [ <STRING> ] ) <NEWLINE> records = cdl . reset_index ( ) . to_dict ( <STRING> ) <NEWLINE> for r in records : <NEWLINE> r [ <STRING> ] = r [ <STRING> ] <NEWLINE> q . put ( r ) <NEWLINE> q . put ( { } ) <COMMENT> <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if reqCookies and not <STRING> in reqHeaders : <NEWLINE> <INDENT> header = <STRING> . join ( [ <STRING> . format ( key , value ) for key , value in reqCookies . items ( ) ] ) <NEWLINE> reqHeaders [ <STRING> ] = header <NEWLINE> <DEDENT> <DEDENT>
logger . debug ( <STRING> , url ) <NEWLINE> <INDENT> if req_params : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_params ) <NEWLINE> <DEDENT> if req_headers : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_headers ) <NEWLINE> <DEDENT> if req_cookies : <NEWLINE> <INDENT> logger . debug ( <STRING> , req_cookies ) <NEWLINE> <DEDENT> if json : <NEWLINE> <INDENT> logger . debug ( <STRING> , json ) <NEWLINE> <DEDENT> if data : <NEWLINE> <INDENT> logger . debug ( <STRING> , data ) <NEWLINE> <DEDENT> <DEDENT>
def print_position ( lines , line_no ) : <NEWLINE> <INDENT> lines = [ l . strip ( <STRING> ) . split ( <STRING> ) for l in lines ] <NEWLINE> word = lines [ 0 ] [ 0 ] <NEWLINE> position = [ word ] <NEWLINE> for i , line in enumerate ( lines ) : <NEWLINE> <INDENT> assert line [ 0 ] == word , ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> position . extend ( line [ 1 : ] ) <NEWLINE> <DEDENT> print ( <STRING> . join ( position ) ) <NEWLINE> <DEDENT>
def zero_prefix_int ( num ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> strnum = str ( num ) <NEWLINE> if len ( strnum ) == 1 : <NEWLINE> <INDENT> return <STRING> + strnum <NEWLINE> <DEDENT> return strnum <NEWLINE> <DEDENT>
def chunks ( l , n ) : <NEWLINE> <INDENT> big_list = [ ] <NEWLINE> n = max ( 1 , n ) <NEWLINE> step = int ( len ( l ) / n ) <NEWLINE> for i in range ( 0 , len ( l ) , step ) : <NEWLINE> <INDENT> big_list . append ( l [ i : i + step ] ) <NEWLINE> <DEDENT> return big_list <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if gene_name not in lnc_seq : <NEWLINE> <INDENT> lnc_seq [ gene_name ] = str ( record . seq ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if len ( lnc_seq [ gene_name ] ) < len ( str ( record . seq ) ) : <NEWLINE> <INDENT> lnc_seq [ gene_name ] = str ( record . seq ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
go_terms_parents = annotation . map ( <NEWLINE> <INDENT> lambda x : list ( { term for term in x if term in leaf_terms } ) if isinstance ( x , list ) else None ) <NEWLINE> return go_terms_parents <NEWLINE> <DEDENT>
go_terms_parents = annotation . map ( <NEWLINE> <INDENT> lambda x : list ( { term for term in x if term in leaf_terms } ) if isinstance ( x , list ) else None ) <NEWLINE> return go_terms_parents <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def _set_cache_for ( self , name , value ) : <NEWLINE> <INDENT> self . get . _cache_set ( name , value ) <NEWLINE> <DEDENT> <DEDENT>
if not dataapi . data . exists ( name ) : <NEWLINE> <INDENT> dataapi . data . set ( type_name , name , value ) <NEWLINE> <DEDENT>
return x [ max_index ] <NEWLINE>
<COMMENT> <NL> <INDENT> self . assertEqual ( solver . update_after_step . call_count , 1 ) <NEWLINE> pd . testing . assert_series_equal ( full_voltages , solver . update_after_step . call_args [ 0 ] [ 0 ] ) <NEWLINE> <DEDENT>
def update ( self ) : <NEWLINE> <INDENT> super ( VLibrasVideoViewlet , self ) . update ( ) <NEWLINE> self . youtube_url = get_video_url ( self . context ) <NEWLINE> self . is_ready = self . youtube_url is not None <NEWLINE> self . enabled = self . is_ready or not api . user . is_anonymous ( ) <NEWLINE> if self . is_ready : <NEWLINE> <INDENT> self . klass = <STRING> <NEWLINE> <DEDENT> <DEDENT>
def sanitise ( s , debug_name ) : <NEWLINE> <INDENT> if not isinstance ( s , basestring ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> . format ( <NEWLINE> <INDENT> debug_name , type ( s ) ) ) <NEWLINE> <DEDENT> <DEDENT> elif not regex . match ( s ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> debug_name , s , regex_str ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if isinstance ( s , unicode ) : <NEWLINE> <INDENT> s = s . encode ( <STRING> ) <NEWLINE> <DEDENT> s = escape ( s ) <NEWLINE> if max_len is None : <NEWLINE> <INDENT> if len ( s ) < min_len : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> debug_name , min_len ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if not min_len <= len ( s ) <= max_len : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( <NEWLINE> <INDENT> debug_name , min_len , max_len ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return s <NEWLINE> return sanitise <NEWLINE> <DEDENT> <DEDENT>
self . _refresh_resources ( False ) <NEWLINE> <INDENT> return self . _store . get_proxy ( combined_filter_opts , self . _blacklist ) <NEWLINE> <DEDENT>
if content_type_to_sync : <NEWLINE> <INDENT> log . info ( <STRING> % content_type_to_sync ) <NEWLINE> try : <NEWLINE> <INDENT> content_type = content_registry [ content_type_to_sync ] <NEWLINE> sync_content_type ( content_type_to_sync ) <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> log . error ( <STRING> % content_type_to_sync ) <NEWLINE> else : <NEWLINE> <DEDENT> for content_type in content_registry . keys ( ) : <NEWLINE> <INDENT> sync_content_type ( content_type ) <NEWLINE> <DEDENT> <DEDENT>
def channel_sort ( self , queryset , channel = channel , profile = None , basekey = None , flavor = None , limit = 0 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> channel_results = self . _channel ( queryset , channel = channel , profile = profile , basekey = basekey , flavor = flavor , limit = limit , include_endorsements = True ) <NEWLINE> remainder_results = queryset . exclude ( pk__in = [ item . pk for item in channel_results ] ) <NEWLINE> final_results = channel_results + [ ContentItemWrapper ( item , 0 ) for item in remainder_results ] <NEWLINE> final_results . sort ( cmp = lambda x , y : cmp ( y . rlevel , x . rlevel ) ) <NEWLINE> return final_results <NEWLINE> <DEDENT>
scope = get_scope_at ( project , fixed_source , lineno , filename , ast_nodes ) <NEWLINE> <INDENT> if not ctx : <NEWLINE> <INDENT> names = get_scope_names ( scope , lineno ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> obj = infer ( ctx , scope , lineno ) <NEWLINE> names = [ obj . get_names ( ) ] <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> <DEDENT> names = ( project . get_possible_imports ( ctx , filename ) , ) <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> names = ( project . get_possible_imports ( ctx , filename ) , ) <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> names = ( <NEWLINE> <INDENT> project . get_module ( ctx , filename ) . get_names ( ) , <NEWLINE> project . get_possible_imports ( ctx , filename ) ) <NEWLINE> elif ctx_type == <STRING> : <NEWLINE> <DEDENT> return [ ] <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> func = s . eval ( func , False ) <NEWLINE> except : <NEWLINE> continue <NEWLINE> <DEDENT>
while length > 0 : <NEWLINE> <INDENT> if maxpoints > length : maxpoints = length <NEWLINE> data = self . _h5 [ startpos : startpos + maxpoints ] <NEWLINE> if isinstance ( self . clock , UniformClock ) : <NEWLINE> <INDENT> yield DataSegment ( self . clock [ startpos ] , UniformTimeSeries ( data , self . clock . rate ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> yield DataSegment ( 0 , TimeSeries ( data , self . clock [ startpos : startpos + maxpoints ] ) ) <NEWLINE> <DEDENT> startpos += len ( data ) <NEWLINE> length -= len ( data ) <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> if filename . endswith ( <STRING> ) or ispickle == True : <NEWLINE> <INDENT> dict = eval ( filedata ) <NEWLINE> try : <NEWLINE> <INDENT> pickle . dump ( dict , filename ) <NEWLINE> <DEDENT> except pickle . PickleError as perr : <NEWLINE> <INDENT> print ( <STRING> + str ( perr ) ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> with open ( filename , <STRING> ) as fileToWrite : <NEWLINE> <INDENT> print ( filedata , file = fileToWrite ) <NEWLINE> except IOError as ioerr : <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> + str ( ioerr ) ) <NEWLINE> return <NEWLINE> <DEDENT>
def __init__ ( self , doctype , src , dst , options = None , settings = None ) : <NEWLINE> <INDENT> self . doctype = doctype <NEWLINE> self . src = src <NEWLINE> self . dst = dst <NEWLINE> self . options = options or { } <NEWLINE> self . settings = settings or { } <NEWLINE> self . logfile = pathjoin ( self . dst , <STRING> ) <NEWLINE> self . status = None <NEWLINE> self . exc_info = None <NEWLINE> self . _build_func = self . _get_build_func ( ) <NEWLINE> self . _log = None <NEWLINE> <DEDENT>
if <STRING> in dir ( ) : <NEWLINE> <INDENT> pdf_file = fitz . open ( file ) <NEWLINE> while i < len ( pdf_file ) : <NEWLINE> <INDENT> text += pdf_file [ i ] . getText ( <STRING> ) <NEWLINE> i += 1 <NEWLINE> else : <NEWLINE> <DEDENT> pdf_file = open ( file , <STRING> ) <NEWLINE> pdf_reader = PyPDF2 . PdfFileReader ( pdf_file ) <NEWLINE> while i < pdf_reader . numPages : <NEWLINE> <INDENT> payload = pdf_reader . getPage ( i ) . extractText ( ) . replace ( <STRING> , <STRING> ) <NEWLINE> text += payload . encode ( <STRING> , <STRING> ) . decode ( <STRING> ) <NEWLINE> i += 1 <NEWLINE> return text <NEWLINE> <DEDENT> <DEDENT>
try : <NEWLINE> <INDENT> scandir_it = scandir ( dirpath ) <NEWLINE> except OSError as error : <NEWLINE> onerror ( error ) <NEWLINE> return <NEWLINE> <DEDENT>
def put_unconfirmed ( self , tx ) : <NEWLINE> <INDENT> assert tx . height is None , <STRING> . format ( tx ) <NEWLINE> if tx . type in ( C . TX_POW_REWARD , C . TX_POS_REWARD ) : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> elif tx . hash in self . unconfirmed : <NEWLINE> <INDENT> logging . debug ( <STRING> . format ( tx ) ) <NEWLINE> return <NEWLINE> <DEDENT> self . unconfirmed [ tx . hash ] = tx <NEWLINE> if tx . hash in self . chained_tx : <NEWLINE> <INDENT> logging . debug ( <STRING> . format ( tx ) ) <NEWLINE> return <NEWLINE> <DEDENT> user_account . affect_new_tx ( tx ) <NEWLINE> <COMMENT> <NL> if P . NEW_CHAIN_INFO_QUE : <NEWLINE> <INDENT> P . NEW_CHAIN_INFO_QUE . put_nowait ( ( <STRING> , tx . getinfo ( ) ) ) <NEWLINE> <DEDENT> <DEDENT>
def contract_signature_check ( extra_tx : TX , v : Validator , include_block : Block ) : <NEWLINE> <INDENT> signed_cks = get_signed_cks ( extra_tx ) <NEWLINE> accept_cks = signed_cks & set ( v . validators ) <NEWLINE> reject_cks = signed_cks - set ( v . validators ) <NEWLINE> if len ( reject_cks ) > 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( reject_cks ) ) <NEWLINE> <DEDENT> elif include_block : <NEWLINE> <COMMENT> <NL> <INDENT> if len ( accept_cks ) >= v . require : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> <NEWLINE> <INDENT> . format ( signed_cks , accept_cks , v . require ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> original_tx = tx_builder . get_tx ( txhash = extra_tx . hash ) <NEWLINE> if original_tx is None : <NEWLINE> <COMMENT> <NL> <INDENT> if 0 < v . require and len ( accept_cks ) == 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( signed_cks ) ) <NEWLINE> <DEDENT> if len ( accept_cks ) > v . require : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( accept_cks , v . require ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> if original_tx . height is not None : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> . format ( original_tx . height ) ) <NEWLINE> <DEDENT> if v . require == 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> ) <NEWLINE> <DEDENT> original_cks = get_signed_cks ( original_tx ) <NEWLINE> accept_new_cks = ( signed_cks - original_cks ) & set ( v . validators ) <NEWLINE> if len ( accept_new_cks ) == 0 : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> <NEWLINE> <INDENT> . format ( signed_cks , original_cks , set ( v . validators ) ) ) <NEWLINE> <DEDENT> <DEDENT> if len ( accept_new_cks ) + len ( original_cks ) > v . require : <NEWLINE> <INDENT> raise BlockChainError ( <STRING> <NEWLINE> <INDENT> . format ( accept_new_cks , original_cks , v . require ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def validator_fill_iter ( v : Validator , best_block = None , best_chain = None ) : <NEWLINE> <COMMENT> <NL> <INDENT> v_iter = builder . db . read_validator_iter ( c_address = v . c_address , start_idx = v . db_index ) <NEWLINE> for index , address , flag , txhash , sig_diff in v_iter : <NEWLINE> <INDENT> yield index , flag , address , sig_diff , txhash <NEWLINE> <COMMENT> <NL> <DEDENT> if best_chain : <NEWLINE> <INDENT> _best_chain = None <NEWLINE> <DEDENT> elif best_block and best_block == builder . best_block : <NEWLINE> <INDENT> _best_chain = builder . best_chain <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> dummy , _best_chain = builder . get_best_chain ( best_block = best_block ) <NEWLINE> <DEDENT> for block in reversed ( best_chain or _best_chain ) : <NEWLINE> <INDENT> for tx in block . txs : <NEWLINE> <INDENT> if tx . type != C . TX_VALIDATOR_EDIT : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> c_address , address , flag , sig_diff = decode ( tx . message ) <NEWLINE> if c_address != v . c_address : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> index = block . height * 0xffffffff + block . txs . index ( tx ) <NEWLINE> yield index , flag , address , sig_diff , tx . hash <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if best_block is None : <NEWLINE> <INDENT> for tx in sorted ( tx_builder . unconfirmed . values ( ) , key = lambda x : x . create_time ) : <NEWLINE> <INDENT> if tx . type != C . TX_VALIDATOR_EDIT : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> c_address , address , flag , sig_diff = decode ( tx . message ) <NEWLINE> if c_address != v . c_address : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if len ( tx . signature ) < v . require : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> yield None , flag , address , sig_diff , tx . hash <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
async def new_address ( request ) : <NEWLINE> <INDENT> with closing ( create_db ( V . DB_ACCOUNT_PATH ) ) as db : <NEWLINE> <INDENT> cur = db . cursor ( ) <NEWLINE> user_name = request . query . get ( <STRING> , C . account2name [ C . ANT_UNKNOWN ] ) <NEWLINE> user_id = read_name2user ( user_name , cur ) <NEWLINE> address = create_new_user_keypair ( user_id , cur ) <NEWLINE> db . commit ( ) <NEWLINE> if user_id == C . ANT_CONTRACT : <NEWLINE> <INDENT> address = convert_address ( address , V . BLOCK_CONTRACT_PREFIX ) <NEWLINE> <DEDENT> <DEDENT> return web_base . json_res ( { <STRING> : user_name , <STRING> : user_id , <STRING> : address } ) <NEWLINE> <DEDENT>
def check_already_started ( ) : <NEWLINE> <INDENT> assert V . DB_HOME_DIR is not None <NEWLINE> <COMMENT> <NL> pid_path = os . path . join ( V . DB_HOME_DIR , <STRING> ) <NEWLINE> if os . path . exists ( pid_path ) : <NEWLINE> <INDENT> with open ( pid_path , mode = <STRING> ) as fp : <NEWLINE> <INDENT> pid = int ( fp . read ( ) ) <NEWLINE> <DEDENT> if psutil . pid_exists ( pid ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( pid ) ) <NEWLINE> <DEDENT> <DEDENT> new_pid = os . getpid ( ) <NEWLINE> with open ( pid_path , mode = <STRING> ) as fp : <NEWLINE> <INDENT> fp . write ( str ( new_pid ) ) <NEWLINE> <DEDENT> log . info ( <STRING> . format ( new_pid ) ) <NEWLINE> <DEDENT>
if keystroke_history and keystroke_history [ - 1 ] == <STRING> : <NEWLINE>
@ contextmanager <NEWLINE> <INDENT> def timer ( name = <STRING> ) : <NEWLINE> <INDENT> start = time ( ) <NEWLINE> try : <NEWLINE> <INDENT> yield <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> duration = time ( ) - start <NEWLINE> if not name : <NEWLINE> <INDENT> logging . warning ( <STRING> , duration ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logging . warning ( <STRING> , name , duration ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _discover_run ( self , packages , missing = True ) : <NEWLINE> <COMMENT> <NL> <INDENT> def formatPathPrint ( path , line = None ) : <NEWLINE> <INDENT> if not line : <NEWLINE> <INDENT> line = 1 <NEWLINE> <DEDENT> path = os . path . realpath ( path ) <NEWLINE> return <STRING> % ( path , line ) <NEWLINE> <DEDENT> total , failed = [ ] , [ ] <NEWLINE> s = SmokeTestDiscover ( ) <NEWLINE> for pkg_pth in packages : <NEWLINE> <INDENT> pkg = importlib . import_module ( self . _path_to_modstr ( pkg_pth ) ) <NEWLINE> <COMMENT> <NL> t , f = s . discover_run ( pkg ) <NEWLINE> total += t <NEWLINE> failed += f <NEWLINE> <COMMENT> <NL> if missing : <NEWLINE> <INDENT> for m in s . get_missing ( pkg ) : <NEWLINE> <INDENT> pth = m . __file__ <NEWLINE> if pth . endswith ( <STRING> ) : <NEWLINE> <INDENT> pth = pth [ : - 1 ] <NEWLINE> <DEDENT> s . log ( <STRING> % m ) <NEWLINE> s . log ( formatPathPrint ( pth ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> return total , failed <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for step in steps : <NEWLINE> <COMMENT> <NL> <INDENT> if <STRING> not in steps and step == <STRING> : <NEWLINE> <INDENT> job_name = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> else : <NEWLINE> <INDENT> job_name = step + <STRING> <NEWLINE> <DEDENT> pipeline . write ( self . get_template ( slurm , prev_step , job_name , step ) ) <NEWLINE> if <STRING> in step or step not in self . softwares : <NEWLINE> <INDENT> prev_step = self . task_names [ step ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> ax_groupdelay = ax_power . twinx ( ) <NEWLINE> <COMMENT> <NL> ax_groupdelay . plot ( omega / 2 / np . pi , <NEWLINE> <INDENT> - np . gradient ( np . unwrap ( <NEWLINE> <INDENT> np . angle ( transferfunc ) ) ) / np . gradient ( omega ) * 1000 , <NEWLINE> <DEDENT> <STRING> ) <NEWLINE> <DEDENT> ax_power . set_title ( <STRING> ) <NEWLINE> ax_power . set_ylabel ( <STRING> , color = <STRING> ) <NEWLINE> <COMMENT> <NL> ax_groupdelay . set_ylabel ( <STRING> , color = <STRING> ) <NEWLINE> ax_power . set_xlabel ( <STRING> ) <NEWLINE> ax_power . set_xscale ( <STRING> ) <NEWLINE> ax_power . set_xlim ( [ 20 , 20000 ] ) <NEWLINE> ax_power . xaxis . set_major_formatter ( <NEWLINE> <INDENT> matplotlib . ticker . FormatStrFormatter ( <STRING> ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> ax_groupdelay . set_xscale ( <STRING> ) <NEWLINE> ax_groupdelay . xaxis . set_major_formatter ( <NEWLINE> <INDENT> matplotlib . ticker . FormatStrFormatter ( <STRING> ) ) <NEWLINE> <DEDENT> ax_power . grid ( True , which = <STRING> , color = <STRING> , ls = <STRING> ) <NEWLINE> <COMMENT> <NL> ax_groupdelay . grid ( True , which = <STRING> , color = <STRING> , ls = <STRING> ) <NEWLINE> for tlabel in ax_power . get_yticklabels ( ) : <NEWLINE> <INDENT> tlabel . set_color ( <STRING> ) <NEWLINE> <DEDENT> for tlabel in ax_groupdelay . get_yticklabels ( ) : <NEWLINE> <INDENT> tlabel . set_color ( <STRING> ) <NEWLINE> <DEDENT> align_y_axis ( ax_power , ax_groupdelay , 1 , .1 ) <NEWLINE> <COMMENT> <NL> plotwidget . draw ( ) <NEWLINE> <DEDENT>
def load_dataset ( self ) : <NEWLINE> <INDENT> if ( self . _source is not None ) : <NEWLINE> <INDENT> self . _data_set = self . _source . retrieve_dataset ( ) <NEWLINE> self . _is_load_information = True <NEWLINE> <DEDENT> <DEDENT>
