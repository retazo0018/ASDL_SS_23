dgt = asyncdgt . auto_connect ( loop , port_globs ) <NEWLINE>
fields = data . get ( <STRING> , list ( ) ) <NEWLINE> <INDENT> for ( field ) in fields : <NEWLINE> <INDENT> field = Field . from_dict ( field ) <NEWLINE> fragment . add_field ( field ) <NEWLINE> <DEDENT> <DEDENT>
return topics <NEWLINE>
if is_url ( input_value ) : <NEWLINE> <INDENT> detected_type = <STRING> <NEWLINE> new_actions . append ( set_input_type ( detected_type ) ) <NEWLINE> new_actions . append ( add_task ( FETCH_HTTP_NODE , url = input_value ) ) <NEWLINE> new_actions . append ( set_validation_subject ( input_value ) ) <NEWLINE> elif input_is_json ( input_value ) : <NEWLINE> id_url = find_id_in_jsonld ( input_value , options . get ( <STRING> , jsonld_use_cache ) ) <NEWLINE> if is_url ( id_url ) : <NEWLINE> <INDENT> detected_type = <STRING> <NEWLINE> new_actions . append ( store_input ( id_url ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> detected_type = <STRING> <NEWLINE> <DEDENT> new_actions . append ( set_input_type ( detected_type ) ) <NEWLINE> if detected_type == <STRING> : <NEWLINE> <INDENT> new_actions . append ( add_task ( FETCH_HTTP_NODE , url = id_url ) ) <NEWLINE> new_actions . append ( set_validation_subject ( id_url ) ) <NEWLINE> elif input_is_jws ( input_value ) : <NEWLINE> <DEDENT> detected_type = <STRING> <NEWLINE> new_actions . append ( set_input_type ( detected_type ) ) <NEWLINE> new_actions . append ( add_task ( PROCESS_JWS_INPUT , data = input_value ) ) <NEWLINE> else : <NEWLINE> raise NotImplementedError ( <STRING> ) <NEWLINE> <DEDENT>
def collision_free ( object1 , object2 , car_scale_factor , pedestrian_scale_factor ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> object1_vertices , x , y , radius = get_bounding_box ( object1 , car_scale_factor , pedestrian_scale_factor ) <NEWLINE> object2_vertices , x2 , y2 , radius2 = get_bounding_box ( object2 , car_scale_factor , pedestrian_scale_factor ) <NEWLINE> <DEDENT>
self . set_title ( folder , utils . _remove_ext ( obj ) ) <NEWLINE>
poss = [ ] <NEWLINE> <INDENT> height , pos = left_peak_height_pos ( mmr_size ) <NEWLINE> poss . append ( pos ) <NEWLINE> while height > 0 : <NEWLINE> <INDENT> height , pos = get_right_peak ( height , pos , mmr_size ) <NEWLINE> if height >= 0 : <NEWLINE> <INDENT> poss . append ( pos ) <NEWLINE> <DEDENT> <DEDENT> return poss <NEWLINE> <DEDENT>
def random_interval ( ) : <NEWLINE> <INDENT> now = datetime . now ( ) <NEWLINE> start = now - timedelta ( days = random . randint ( 0 , 365 * 3 ) ) <NEWLINE> end = start + timedelta ( days = random . randint ( 1 , 10 ) ) <NEWLINE> return start . isoformat ( ) , end . isoformat ( ) <NEWLINE> <DEDENT>
logs . append ( log ) <NEWLINE>
data_files [ location ] [ parameter ] = csvFile_path <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> data_files [ location ] [ parameter ] = None <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> with rasterio . open ( dst ) as f : <NEWLINE> <INDENT> geometry = util . bbox2poly ( f . bounds . left , f . bounds . bottom , f . bounds . right , f . bounds . top , as_shapely = True ) <NEWLINE> <DEDENT> update_metadata ( feature , quest_metadata = { <STRING> : geometry . to_wkt ( ) } ) <NEWLINE> <DEDENT>
snap = options . get ( <STRING> ) <NEWLINE> <INDENT> if snap is not None : <NEWLINE> <INDENT> if snap . lower ( ) == <STRING> : <NEWLINE> <INDENT> proj_points = snap_points_max_flow_acc ( flow_accumulation , proj_points , options . get ( <STRING> ) ) <NEWLINE> <DEDENT> if snap . lower ( ) == <STRING> : <NEWLINE> <INDENT> stream_threshold_pct = options . get ( <STRING> ) <NEWLINE> stream_threshold_abs = options . get ( <STRING> ) <NEWLINE> proj_points = snap_points_jenson ( flow_accumulation , proj_points , <NEWLINE> <INDENT> stream_threshold_pct = stream_threshold_pct , stream_threshold_abs = stream_threshold_abs ) <NEWLINE> <DEDENT> <DEDENT> if p . is_latlong ( ) : <NEWLINE> <INDENT> snapped_points = [ src . xy ( * point ) for point in proj_points ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> snapped_points = [ src . xy ( * p ( * point , inverse = True ) ) for point in proj_points ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
collection_path = os . path . join ( project_path , dataset_metadata [ <STRING> ] ) <NEWLINE>
def readReg ( self , addr ) : <NEWLINE> <INDENT> return self . spi . xfer ( [ addr & 0x7F , 0 ] ) <NEWLINE> <DEDENT>
model_store = resource . ModelStore ( complete_conf ) <NEWLINE> <INDENT> model_store . dump_trained_model ( complete_conf , model , metrics ) <NEWLINE> <DEDENT>
def is_valid ( data , explain = False ) : <NEWLINE> <INDENT> if not explain : <NEWLINE> <INDENT> return predicate ( data ) <NEWLINE> <DEDENT> elems = list ( data ) <NEWLINE> valid , explanation = predicate ( elems , explain = True ) <NEWLINE> return ( True , explanation ) if valid else ( False , { <NEWLINE> <INDENT> elems [ i ] : value for i , value in explanation . items ( ) <NEWLINE> <DEDENT> } ) <NEWLINE> return is_if ( is_set , is_valid , else_valid = False ) <NEWLINE> <DEDENT>
pool = Pool ( 50 ) <NEWLINE> <INDENT> i = 0 <NEWLINE> with ProgressBar ( max_value = len ( self . dataframe ) ) as p : <NEWLINE> <INDENT> for _ in pool . imap_unordered ( save , self . iter ( ) ) : <NEWLINE> <INDENT> i += 1 <NEWLINE> if i % 10 == 0 : <NEWLINE> <INDENT> p . update ( i ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if self . parent is not None : <NEWLINE> <INDENT> for k in self . parent . keys ( ) : <NEWLINE> <INDENT> if k not in _keys_set : <NEWLINE> <INDENT> _keys_set . add ( k ) <NEWLINE> yield k <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not ( path . exists ( ) or path . suffix ) : <NEWLINE> <INDENT> for ext in Node . _strip_exts : <NEWLINE> <INDENT> new_path = path . with_suffix ( ext ) <NEWLINE> if new_path . exists ( ) : <NEWLINE> <INDENT> return str ( new_path ) <NEWLINE> elif path . is_dir ( ) : <NEWLINE> <DEDENT> <DEDENT> new_path = pathlib . Path ( path , Node . _index_file ) <NEWLINE> if new_path . exists ( ) : <NEWLINE> <INDENT> return str ( new_path ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> recruit = t_count <NEWLINE> <DEDENT>
fe = fg . add_entry ( ) <NEWLINE> <INDENT> fe . author ( name = page_id , email = <STRING> % page_id ) <NEWLINE> fe . id ( post_id ) <NEWLINE> fe . link ( href = post_url , rel = <STRING> ) <NEWLINE> fe . published ( timestamp ) <NEWLINE> fe . updated ( timestamp ) <NEWLINE> fe . title ( <STRING> . join ( txt . split ( <STRING> ) [ : 15 ] ) ) <NEWLINE> fe . content ( entry_content , type = <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if reason and node . metadata and reason . extras and set ( reason . extras ) - node . extras : <NEWLINE> <INDENT> metadata_to_apply = node . metadata <NEWLINE> <DEDENT> <DEDENT>
def _validate_oid ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> if not isinstance ( getattr ( self , <STRING> ) , uuid . UUID ) : <NEWLINE> <INDENT> return <STRING> . format ( type ( getattr ( self , <STRING> ) ) , getattr ( self , <STRING> ) ) <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> value = __next__ ( self ) <NEWLINE> finally : <NEWLINE> self . _started_iteration = True <NEWLINE> bound_method = __next__ . __get__ ( self , self . __class__ ) <NEWLINE> self . __next__ = bound_method <COMMENT> <NEWLINE> return value <NEWLINE> <DEDENT>
if not self . dry_run : <NEWLINE> <INDENT> if not os . path . isfile ( self . imagefile ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> . format ( self . imagefile ) ) <NEWLINE> <DEDENT> <DEDENT>
if epoch_start_time > epoch_end_time : <NEWLINE> <INDENT> raise Exception ( INVALID_DATES ) <NEWLINE> <DEDENT>
def act_cz ( self , control , target ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> control = 1 << control <NEWLINE> target = 1 << target <NEWLINE> for i in xrange ( self . d ) : <NEWLINE> <INDENT> if ( i & control ) and ( i & target ) : <NEWLINE> <INDENT> self . state [ i , 0 ] *= - 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
ci = self . get_connection_info ( a , b ) <NEWLINE> <INDENT> if ci [ <STRING> ] and not clifford . is_diagonal ( self . node [ a ] [ <STRING> ] ) : <NEWLINE> <INDENT> debug ( <STRING> ) <NEWLINE> self . remove_vop ( a , b ) <NEWLINE> <DEDENT> <DEDENT>
if mode == <STRING> : <NEWLINE> <INDENT> chunk_info = data [ <STRING> ] <NEWLINE> self . id_ = chunk_info [ <STRING> ] <NEWLINE> self . head = chunk_info [ <STRING> ] <NEWLINE> self . dep = chunk_info [ <STRING> ] <NEWLINE> self . chunk_head = chunk_info [ <STRING> ] <NEWLINE> self . chunk_func = chunk_info [ <STRING> ] <NEWLINE> self . links = [ <NEWLINE> <INDENT> Reshape ( mode = <STRING> , data = link ) <NEWLINE> for link in chunk_info [ <STRING> ] <NEWLINE> <DEDENT> ] <NEWLINE> self . predicate = chunk_info [ <STRING> ] if <STRING> in chunk_info else [ ] <NEWLINE> <DEDENT>
return build_dataframe ( raw_data , parsed_aggregateby , groupby ) <NEWLINE>
def formfield ( self , ** kwargs ) : <NEWLINE> <INDENT> defaults = { <NEWLINE> <INDENT> <STRING> : self . max_length , <NEWLINE> <STRING> : self . min_length , <NEWLINE> <DEDENT> } <NEWLINE> defaults . update ( kwargs ) <NEWLINE> return super ( RandomStringFieldBase , self ) . formfield ( ** defaults ) <NEWLINE> <DEDENT>
for x in etree . iter ( ) : <NEWLINE> <INDENT> if x . tag == <STRING> : <NEWLINE> <INDENT> self . name = x . attrib [ <STRING> ] <NEWLINE> <DEDENT> if x . tag == <STRING> : <NEWLINE> <INDENT> if x . attrib [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> self . return_signature += x . attrib [ <STRING> ] <NEWLINE> <DEDENT> elif x . attrib [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> self . arg_signatures . append ( x . attrib [ <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> if flags_obj . no_gpu or not tf . test . is_gpu_available ( ) : <NEWLINE> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if flags_obj . data_format is not None : <NEWLINE> <INDENT> data_format = flags_obj . data_format <NEWLINE> <DEDENT> print ( <STRING> % ( device , data_format ) ) <NEWLINE> <DEDENT>
policy_loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = memory . actions , <NEWLINE> <INDENT> logits = logits ) <NEWLINE> policy_loss *= tf . stop_gradient ( advantage ) <NEWLINE> policy_loss -= 0.01 * entropy <NEWLINE> total_loss = tf . reduce_mean ( ( 0.5 * value_loss + policy_loss ) ) <NEWLINE> return total_loss <NEWLINE> <DEDENT>
for f in w . required_fields : <NEWLINE> <INDENT> f_name = f . replace ( <STRING> , <STRING> ) <NEWLINE> setattr ( SingleForm , f_name , StringField ( f ) ) <NEWLINE> <DEDENT>
def login ( request ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> manipulator = AuthenticationForm ( request ) <NEWLINE> redirect_to = request . REQUEST . get ( REDIRECT_FIELD_NAME , <STRING> ) <NEWLINE> if request . POST : <NEWLINE> <INDENT> errors = manipulator . get_validation_errors ( request . POST ) <NEWLINE> if not errors : <NEWLINE> <COMMENT> <NL> <INDENT> if not redirect_to or <STRING> in redirect_to or <STRING> in redirect_to : <NEWLINE> <INDENT> redirect_to = <STRING> <NEWLINE> <DEDENT> request . session [ users . SESSION_KEY ] = manipulator . get_user_id ( ) <NEWLINE> return HttpResponseRedirect ( redirect_to ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> errors = { } <NEWLINE> <DEDENT> response = HttpResponse ( ) <NEWLINE> request . session . set_test_cookie ( ) <NEWLINE> t = template_loader . get_template ( <STRING> ) <NEWLINE> c = Context ( request , { <NEWLINE> <INDENT> <STRING> : formfields . FormWrapper ( manipulator , request . POST , errors ) , <NEWLINE> REDIRECT_FIELD_NAME : redirect_to , <NEWLINE> <STRING> : sites . get_current ( ) . name , <NEWLINE> <DEDENT> } ) <NEWLINE> response . write ( t . render ( c ) ) <NEWLINE> return response <NEWLINE> <DEDENT>
def test_cache_page_old_style ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def my_view ( request ) : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> my_view_cached = cache_page ( my_view , 123 ) <NEWLINE> self . assertEqual ( my_view_cached ( HttpRequest ( ) ) , <STRING> ) <NEWLINE> <DEDENT>
def visit_binary_expr ( self , expr ) : <NEWLINE> <INDENT> left = expr . left . accept ( self ) <NEWLINE> right = expr . right . accept ( self ) <NEWLINE> if left is expr . left and right is expr . right : <NEWLINE> <INDENT> return expr <NEWLINE> <DEDENT> return Make . binary_op ( left , expr . op , right ) <NEWLINE> <DEDENT>
def request ( self , verb , method , ** kwargs ) : <NEWLINE> <INDENT> verb = verb . upper ( ) <NEWLINE> request_kwargs = { } <NEWLINE> if verb == <STRING> : <NEWLINE> <INDENT> request_kwargs [ <STRING> ] = kwargs <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> request_kwargs [ <STRING> ] = kwargs <NEWLINE> <DEDENT> url = self . config [ <STRING> ] + method <NEWLINE> logger . debug ( <STRING> % ( verb , url ) ) <NEWLINE> r = self . requester . request ( verb , url , ** request_kwargs ) <NEWLINE> if r . status_code != 200 : <NEWLINE> <INDENT> raise APIError ( r . status_code ) <NEWLINE> <DEDENT> return r . json ( ) <NEWLINE> <DEDENT>
def _step ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> keys_raw , self . data = next ( self . it , ( <STRING> , <STRING> ) ) <NEWLINE> keys = keylib . KeyList . from_raw ( keys_raw , self . prefix ) <NEWLINE> self . keys = keys <NEWLINE> return keys is not None <NEWLINE> <DEDENT>
def _encode ( self , s ) : <NEWLINE> <INDENT> return keylib . packs ( s , self . prefix ) <NEWLINE> <DEDENT>
writer = self . null <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> if not key in self . map : <NEWLINE> <INDENT> writer = Subject ( ) <NEWLINE> self . map [ key ] = writer <NEWLINE> fireNewMapEntry = True <NEWLINE> except Exception as e : <NEWLINE> <DEDENT> <DEDENT> self . onError ( e ) <NEWLINE> return <NEWLINE> <DEDENT>
validator = ProjectValidator ( ) <NEWLINE> <INDENT> data = validator . deserialize ( data ) <NEWLINE> <DEDENT>
@ blueprint . route ( <STRING> , methods = [ <STRING> , <STRING> ] ) <NEWLINE> <INDENT> def update ( slug , name ) : <NEWLINE> <INDENT> project = object_or_404 ( Project . by_slug ( slug ) ) <NEWLINE> authz . require ( authz . project_manage ( project ) ) <NEWLINE> schema = object_or_404 ( Schema . by_name ( project , name ) ) <NEWLINE> data = request_data ( { <STRING> : project } ) <NEWLINE> schema = schemata . save ( data , schema = schema ) <NEWLINE> db . session . commit ( ) <NEWLINE> return jsonify ( schemata . to_rest ( schema ) ) <NEWLINE> <DEDENT> <DEDENT>
if self . _forces is None : <NEWLINE> <COMMENT> <NL> <INDENT> if self . parallel > 0 : <NEWLINE> <INDENT> with Pool ( processes = self . parallel ) as pool : <NEWLINE> <INDENT> image_number = len ( self . images ) <NEWLINE> par_images = pool . map ( self . par_calc , range ( image_number ) ) <NEWLINE> <STRING> <NEWLINE> self . images = par_images <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> [ image . calc_energy_and_forces ( ) for image in self . images ] <NEWLINE> <DEDENT> <DEDENT>
def track_root ( self , atoms , coords ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . store_wfo_data ( atoms , coords ) <NEWLINE> <COMMENT> <NL> old_root = self . root <NEWLINE> if self . calc_counter > 1 : <NEWLINE> <INDENT> last_two_coords = self . wfow . last_two_coords <NEWLINE> self . root = self . wfow . track ( old_root = self . root ) <NEWLINE> if self . root != old_root : <NEWLINE> <INDENT> self . log ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_geoms ( xyz_fns , idpp = False , between = 0 , dump = False , multiple_geoms = False ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if isinstance ( xyz_fns , str ) and xyz_fns . endswith ( <STRING> ) : <NEWLINE> <INDENT> geoms = [ geom_from_xyz_file ( xyz_fns ) , ] <NEWLINE> <COMMENT> <NL> <DEDENT> elif len ( xyz_fns ) == 1 and xyz_fns [ 0 ] . endswith ( <STRING> ) : <NEWLINE> <INDENT> geoms = geoms_from_trj ( xyz_fns [ 0 ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> elif isinstance ( xyz_fns , str ) and xyz_fns . endswith ( <STRING> ) : <NEWLINE> <INDENT> geoms = geoms_from_trj ( xyz_fns ) <NEWLINE> <DEDENT> elif not multiple_geoms : <NEWLINE> <INDENT> geoms = geoms_from_trj ( xyz_fns [ 0 ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> else : <NEWLINE> <INDENT> geoms = [ geom_from_xyz_file ( fn ) for fn in xyz_fns ] <NEWLINE> <DEDENT> <DEDENT>
ax2 . plot ( rms_forces , ** ax_kwargs ) <NEWLINE> <INDENT> ax2 . set_title ( <STRING> ) <NEWLINE> ax2 . set_xlabel ( <STRING> ) <NEWLINE> ax2 . set_ylabel ( <STRING> ) <NEWLINE> <DEDENT>
inp = make_input ( ** inp_kwargs ) <NEWLINE> <INDENT> inp_fn = <STRING> <NEWLINE> with open ( inp_fn , <STRING> ) as handle : <NEWLINE> <INDENT> handle . write ( inp ) <NEWLINE> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if cs_alpha : <NEWLINE> <INDENT> base_img = cb . copy ( ) <NEWLINE> base_img . paste ( img , mask = cs_alpha ) <NEWLINE> img = base_img <NEWLINE> <DEDENT> <DEDENT>
src_lists = [ ] <NEWLINE> <INDENT> for s in arguments : <NEWLINE> <INDENT> key , value = s . split ( <STRING> ) <NEWLINE> gl = re . match ( <STRING> , value ) <NEWLINE> if <STRING> in value : <NEWLINE> <INDENT> possible_values = value . split ( <STRING> ) <NEWLINE> <DEDENT> elif <STRING> in value : <NEWLINE> <INDENT> possible_values = range ( * [ int ( v ) for v in value . split ( <STRING> ) ] ) <NEWLINE> <DEDENT> elif gl : <NEWLINE> <INDENT> possible_values = list ( glob . glob ( gl [ 1 ] , recursive = True ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> possible_values = [ value ] <NEWLINE> <DEDENT> src_lists . append ( [ <STRING> . format ( key , val ) for val in possible_values ] ) <NEWLINE> <DEDENT> <DEDENT>
data = np . append ( dop , po4 , axis = 0 ) <NEWLINE> <INDENT> util . io . save_npy ( data , npy_file , make_read_only = True , create_path_if_not_exists = True ) <NEWLINE> <DEDENT>
assert po4_var . units == measurements . po4 . wod . constants . PO4_UNIT <NEWLINE> <INDENT> po4 = po4_var . data <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> cm = matplotlib . pyplot . cm . winter_r <NEWLINE> util . plot . data ( file , data , land_value = 0 , power_limit = 10 , colormap = cm ) <NEWLINE> <DEDENT>
def _plot_map ( data , lsm , file , layer = None , v_min = None , v_max = None , use_log_scale = False , colorbar_kwargs = { <STRING> : 0.021 , <STRING> : 0.05 , <STRING> : 20 , <STRING> : <STRING> } ) : <NEWLINE> <INDENT> data = lsm . insert_index_values_in_map ( data , no_data_value = np . inf ) <NEWLINE> if layer is not None : <NEWLINE> <INDENT> data = data [ : , : , : , layer ] <NEWLINE> data = data . reshape ( data . shape + ( 1 , ) ) <NEWLINE> <DEDENT> file = _prepare_filename ( file , lsm ) <NEWLINE> util . plot . data ( file , data , no_data_value = np . inf , v_min = v_min , v_max = v_max , use_log_scale = use_log_scale , contours = False , colorbar = True , power_limit = 0 , colorbar_kwargs = colorbar_kwargs ) <NEWLINE> <DEDENT>
def push_to_list ( self , queue , instance , trim = 500 , redis_conn = None , bump = True , site = None ) : <NEWLINE> <INDENT> backend = RedisBackend ( conn = redis_conn ) <NEWLINE> key = self . get_key ( queue , site = site ) <NEWLINE> current_list = backend . get_ids ( key ) <NEWLINE> known_length = len ( current_list ) + 1 <NEWLINE> if bump : <NEWLINE> <INDENT> if instance . pk in current_list : <NEWLINE> <INDENT> backend . remove ( key , instance . pk ) <NEWLINE> known_length -= 1 <NEWLINE> <DEDENT> <DEDENT> backend . add ( key , instance . pk ) <NEWLINE> if trim and known_length > trim : <NEWLINE> <INDENT> backend . trim ( key , trim ) <NEWLINE> <DEDENT> <DEDENT>
relations [ field_name ] = getattr ( self , method_name , None ) <NEWLINE> <INDENT> return relations <NEWLINE> <DEDENT>
def _cmp_value_to_nodes ( base_op , first , second ) : <NEWLINE> <INDENT> node_values = set ( [ number ( node ) for node in second ] ) <NEWLINE> first = number ( first ) <NEWLINE> verbose_print ( <STRING> . format ( len ( node_values ) , first ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> unique_ids_query = prepare_unique_query ( session , section ) <NEWLINE> for unique_id in unique_ids_query : <NEWLINE> <INDENT> db_contour_unique = session . query ( Contour ) . get ( unique_id ) <NEWLINE> unique_reconstruct_contour = section . contours [ db_contour_unique . index ] <NEWLINE> unique_dict = prepare_contour_dict_for_frontend ( <NEWLINE> <INDENT> unique_reconstruct_contour , <NEWLINE> unique_id , <NEWLINE> section , <NEWLINE> keep = True <NEWLINE> <DEDENT> ) <NEWLINE> section_matches [ <STRING> ] . append ( unique_dict ) <NEWLINE> <DEDENT> return section_matches <NEWLINE> <DEDENT>
def onverbalization ( e ) : <NEWLINE> <INDENT> for t in e : <NEWLINE> <INDENT> text = pr2 . knowledge [ <STRING> % t ] [ 0 ] <NEWLINE> logger . warning ( <STRING> % text ) <NEWLINE> pr2 . say ( text ) <NEWLINE> <DEDENT> <DEDENT>
def add_metric ( tensor , name = None ) : <NEWLINE> <INDENT> return tf . add_to_collection ( <NEWLINE> <INDENT> METRICS , <NEWLINE> tensor if name is None else util . rename ( tensor , name ) ) <NEWLINE> <DEDENT> <DEDENT>
return ( predictions , <NEWLINE> <INDENT> loss + l2_regularization_loss ( regularization_scale ) , <NEWLINE> train . minimize ( loss ) , <NEWLINE> _evaluate ( predicted_labels , true_label ) ) <NEWLINE> <DEDENT>
def atstart ( self , received ) : <NEWLINE> <INDENT> file = <STRING> . format ( __file__ , <STRING> ) <NEWLINE> if not hasattr ( self , <STRING> ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> self . mlogger . debug ( <STRING> . format ( file ) ) <NEWLINE> self . file = open ( file , <STRING> ) <NEWLINE> <DEDENT>
if len ( fields ) < 0 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT>
def dependencies_iterator ( xs ) : <NEWLINE> <INDENT> if hasattr ( xs , <STRING> ) : <NEWLINE> <INDENT> for k , v in xs . items ( ) : <NEWLINE> <INDENT> yield k , v <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> for e in xs : <NEWLINE> <INDENT> yield e , None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
generating = Generating ( config ) <NEWLINE> <INDENT> for c in params [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> if <STRING> not in params [ c ] : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> . format ( c ) ) <NEWLINE> sys . stderr . flush ( ) <NEWLINE> skip = <STRING> != sys . stdin . readline ( ) . strip ( ) . lower ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> skip = params [ c ] [ <STRING> ] <NEWLINE> <DEDENT> if skip : <NEWLINE> <INDENT> sys . stderr . write ( <STRING> . format ( c ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> generating . generate ( params [ c ] , args . dst ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> setup_path = os . path . join ( path , <STRING> ) <NEWLINE> if not os . path . exists ( setup_path ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> with open ( setup_path , <STRING> ) as f : <NEWLINE> <INDENT> new_setup = f . read ( ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> old_version = re . search ( <STRING> , new_setup ) . group ( 0 ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> old_version = re . sub ( <STRING> , <STRING> , old_version ) <NEWLINE> old_version = re . sub ( <STRING> , <STRING> , old_version ) <NEWLINE> old_version = re . sub ( <STRING> , <STRING> , old_version ) <NEWLINE> if version < old_version : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( version , old_version ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
return_key_values = _get_versions ( ) <NEWLINE> <INDENT> return_key_values [ <STRING> ] = default_keys_values [ <STRING> ] <NEWLINE> return return_key_values <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for key , value in dart . prior_kwargs . items ( ) : <NEWLINE> <INDENT> if key == <STRING> : kick_sigma = value <NEWLINE> if key == <STRING> : M1_alpha = value <NEWLINE> if key == <STRING> : M1_min = value <NEWLINE> if key == <STRING> : M1_max = value <NEWLINE> if key == <STRING> : M2_min = value <NEWLINE> if key == <STRING> : a_min = value <NEWLINE> if key == <STRING> : a_max = value <NEWLINE> if key == <STRING> : t_min = value <NEWLINE> if key == <STRING> : t_max = value <NEWLINE> if key == <STRING> : mass_function = value <NEWLINE> <DEDENT> <DEDENT>
if ntemps != 1 and ntemps is not None : <NEWLINE> <INDENT> if ln_likelihood_function is None : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sys . exit ( - 1 ) <NEWLINE> <DEDENT> <DEDENT>
shutil . copyfile ( os . path . join ( builddir , SO_NAME ) , <NEWLINE> <INDENT> os . path . join ( build_lib , <STRING> , SO_NAME ) ) <NEWLINE> shutil . copyfile ( os . path . join ( source_dir , HEADER_NAME ) , <NEWLINE> os . path . join ( build_lib , <STRING> , os . path . basename ( HEADER_NAME ) ) ) <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT>
def __city_and_province ( self ) : <NEWLINE> <INDENT> if self . city . isNotEmpty ( ) and self . province . isNotEmpty ( ) : <NEWLINE> <INDENT> if not self . city . isBlong ( self . province . name ) : <NEWLINE> <INDENT> if self . city . precision >= self . province . precision : <NEWLINE> <INDENT> self . province . name = self . city . belong <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . city . reset ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif self . city . isNotEmpty ( ) and self . province . isEmpty ( ) : <NEWLINE> <INDENT> self . province . name = self . city . belong <NEWLINE> <DEDENT> <DEDENT>
if status_code in range ( 200 , 299 ) : <NEWLINE> <INDENT> return response <NEWLINE> <DEDENT>
if verbose : <NEWLINE> <INDENT> print ( <STRING> . format ( t2 - t1 ) ) <NEWLINE> <DEDENT>
x0 = self . _initialize_params_triangulation ( <NEWLINE> <INDENT> p3ds_intp , constraints , constraints_weak ) <NEWLINE> <DEDENT>
def _get_single_mod ( self , modno , mod_slices ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> mod_data = self . _data [ modno ] <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> if modno >= self . _nmodules : <NEWLINE> <INDENT> raise IndexError ( modno ) <NEWLINE> <DEDENT> mod_data = np . full ( self . _mod_shape , self . _fillvalue , self . dtype ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , host , port , verify , cert_path , pkey_path , pkey_passphrase = <STRING> ) : <NEWLINE> <INDENT> self . host = host <NEWLINE> self . port = port <NEWLINE> try : <NEWLINE> <INDENT> self . pkey = crypto . load_privatekey ( crypto . FILETYPE_PEM , open ( pkey_path , <STRING> ) . read ( ) , pkey_passphrase ) <NEWLINE> <DEDENT> except IOError : <NEWLINE> <INDENT> raise eStreamerKeyError ( <STRING> . format ( pkey_path ) ) <NEWLINE> <DEDENT> except crypto . Error : <NEWLINE> <INDENT> raise eStreamerKeyError ( <STRING> . format ( pkey_path ) ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> self . cert = crypto . load_certificate ( crypto . FILETYPE_PEM , open ( cert_path , <STRING> ) . read ( ) ) <NEWLINE> <DEDENT> except IOError : <NEWLINE> <INDENT> raise eStreamerCertError ( <STRING> . format ( cert_path ) ) <NEWLINE> <DEDENT> except crypto . Error : <NEWLINE> <INDENT> raise eStreamerCertError ( <STRING> . format ( cert_path ) ) <NEWLINE> <DEDENT> self . verify = verify <NEWLINE> self . ctx = None <NEWLINE> self . sock = None <NEWLINE> self . _bytes = None <NEWLINE> <DEDENT>
if sequentialData : <NEWLINE> <INDENT> dataDimensions = list ( map ( lambda x : <STRING> % ( x ) , dataDimensions ) ) <NEWLINE> <DEDENT>
if not self . model . onModelLoad ( loadedState [ <STRING> ] ) : <NEWLINE> <INDENT> loaded = loadedState [ <STRING> ] <NEWLINE> current = self . model . onModelSave ( ) <NEWLINE> Str = <STRING> % ( loaded , current ) <NEWLINE> for key in set ( list ( loaded . keys ( ) ) + list ( current . keys ( ) ) ) : <NEWLINE> <INDENT> if not key in current : <NEWLINE> <INDENT> Str += <STRING> % ( key ) <NEWLINE> continue <NEWLINE> <DEDENT> if not key in loaded : <NEWLINE> <INDENT> Str += <STRING> % ( key ) <NEWLINE> continue <NEWLINE> <DEDENT> if current [ key ] != loaded [ key ] : <NEWLINE> <INDENT> Str += <STRING> % ( key , current [ key ] , key , loaded [ key ] ) <NEWLINE> <DEDENT> <DEDENT> raise Exception ( Str ) <NEWLINE> <DEDENT>
def __debug ( self , i , facet0 , facet1 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if facet0 == facet1 == None : <NEWLINE> <INDENT> debug ( <STRING> % i ) <NEWLINE> <DEDENT> elif facet1 == None : <NEWLINE> <INDENT> debug ( <STRING> % ( i , facet0 ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> debug ( <STRING> % ( i , facet0 , facet1 ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if abs ( a0 ) < epsilon : continue <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> common_cell = form_datas [ 0 ] . cell <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> code += [ format [ <STRING> ] ( geometric_dimension , topological_dimension ) ] <NEWLINE> code += [ <STRING> , format [ <STRING> ] ( element_cell_domain , geometric_dimension ) ] <NEWLINE> <DEDENT>
def generate_xi_from_x_snippets ( cell , restriction ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> gd = cell . geometric_dimension ( ) <NEWLINE> td = cell . topological_dimension ( ) <NEWLINE> name_A = <STRING> % restriction <NEWLINE> name_x = <STRING> % restriction <NEWLINE> name_y = <STRING> % restriction <NEWLINE> name_z = <STRING> % restriction <NEWLINE> return generate_z_Axmy_snippets ( name_z , name_A , name_x , name_y , td , gd ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> inner = [ f_mul ( [ inv_jacobian_column [ j ] , basis_col [ j ] ] ) for j in range ( tdim ) ] <NEWLINE> value = f_group ( f_add ( inner ) ) <NEWLINE> name = f_component ( f_derivatives + _p , f_matrix_index ( i , f_r , f_num_derivs ( _t ) ) ) <NEWLINE> lines += [ f_assign ( name , value ) ] <NEWLINE> elif mapping == <STRING> : <NEWLINE> lines += [ <STRING> , f_comment ( <STRING> ) ] <NEWLINE> lines += [ f_const_double ( f_tmp ( i ) , <NEWLINE> <INDENT> f_component ( f_derivatives , <NEWLINE> <INDENT> f_matrix_index ( i , f_r , f_num_derivs ( _t ) ) ) ) <NEWLINE> for i in range ( num_components ) ] <NEWLINE> basis_col = [ f_tmp ( j ) for j in range ( num_components ) ] <NEWLINE> for p in range ( num_components ) : <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> i = p // tdim <NEWLINE> l = p % tdim <NEWLINE> <COMMENT> <NL> value = f_group ( f_inner ( <NEWLINE> <INDENT> [ f_inner ( [ f_transform ( <STRING> , j , i , tdim , gdim , None ) <NEWLINE> <INDENT> for j in range ( tdim ) ] , <NEWLINE> [ basis_col [ j * tdim + k ] for j in range ( tdim ) ] ) <NEWLINE> for k in range ( tdim ) ] , <NEWLINE> <DEDENT> [ f_transform ( <STRING> , k , l , tdim , gdim , None ) <NEWLINE> <INDENT> for k in range ( tdim ) ] ) ) <NEWLINE> <DEDENT> <DEDENT> name = f_component ( f_derivatives + _p , f_matrix_index ( p , f_r , f_num_derivs ( _t ) ) ) <NEWLINE> lines += [ f_assign ( name , value ) ] <NEWLINE> else : <NEWLINE> error ( <STRING> % mapping ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if num_points is None : <NEWLINE> <INDENT> weight = L . LiteralFloat ( 1.0 ) <NEWLINE> <DEDENT> elif self . ir [ <STRING> ] in custom_integral_types : <NEWLINE> <INDENT> weights = self . backend . symbols . custom_weights_table ( ) <NEWLINE> weight = weights [ iq ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> weights = self . backend . symbols . weights_table ( num_points ) <NEWLINE> weight = weights [ iq ] <NEWLINE> <DEDENT> <DEDENT>
s = sreg . create_schedule ( sched_type , p , c ) <NEWLINE>
self . train_loader = DataLoader ( train_set , batch_size , sampler = train_sampler , num_workers = 4 ) <NEWLINE> <INDENT> self . val_loader = DataLoader ( test_set , batch_size , sampler = valid_sampler , num_workers = 4 ) <NEWLINE> self . test_loader = DataLoader ( test_set , batch_size , num_workers = 4 ) <NEWLINE> <DEDENT>
def private_encrypt ( self , value , padding = RSA_PKCS1_PADDING ) : <NEWLINE> <INDENT> buf = create_string_buffer ( value , len ( value ) ) <NEWLINE> size = RSA_size ( self . key ) <NEWLINE> output = create_string_buffer ( size ) <NEWLINE> ret = RSA_private_encrypt ( len ( buf ) , buf , output , self . key , padding ) <NEWLINE> if ret <= 0 : <NEWLINE> <INDENT> raise SSLError ( <STRING> ) <NEWLINE> <DEDENT> return output . raw [ : ret ] <NEWLINE> <DEDENT>
variables = getattr ( schema . schema , <STRING> , None ) <NEWLINE> <INDENT> if variables : <NEWLINE> <INDENT> for environment_key , response_key in variables . items ( ) : <NEWLINE> <INDENT> environment_key = camelcase ( environment_key ) <NEWLINE> response_key = camelcase ( response_key ) <NEWLINE> tests . append ( <NEWLINE> <INDENT> <STRING> <NEWLINE> % ( response_key , environment_key , response_key ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def not_inactives_filter ( column ) : <NEWLINE> <INDENT> return and_ ( <NEWLINE> <INDENT> or_ ( column . start_date <= func . now ( ) , column . start_date . is_ ( None ) ) , <NEWLINE> or_ ( column . end_date > func . now ( ) , column . end_date . is_ ( None ) ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> cut_deep = np . inf <NEWLINE> if hasattr ( sitegrp [ <STRING> ] , <STRING> ) : <NEWLINE> <INDENT> cut_deep = float ( sitegrp [ <STRING> ] . cut_deep ) <NEWLINE> <DEDENT> cut_shallow = - np . inf <NEWLINE> if hasattr ( sitegrp [ <STRING> ] , <STRING> ) : <NEWLINE> <INDENT> cut_shallow = float ( sitegrp [ <STRING> ] . cut_shallow ) <NEWLINE> <DEDENT> depth = sitegrp [ <STRING> ] . variables [ <STRING> ] [ : ] <NEWLINE> cutoff_msk = ( depth >= cut_shallow ) & ( depth <= cut_deep ) <NEWLINE> <DEDENT>
assert flank in [ <STRING> , <STRING> ] <NEWLINE> <INDENT> pos = sig <= 0 if flank == <STRING> else sig > 0 <NEWLINE> <DEDENT>
def render ( self , name , value , attrs = None ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if not isinstance ( value , list ) : <NEWLINE> <INDENT> value = self . decompress ( value ) <NEWLINE> <DEDENT> output = [ ] <NEWLINE> final_attrs = self . build_attrs ( attrs ) <NEWLINE> id_ = final_attrs . get ( <STRING> , None ) <NEWLINE> i = - 1 <NEWLINE> for i , widget_value in enumerate ( value ) : <NEWLINE> <INDENT> if id_ : <NEWLINE> <INDENT> final_attrs = dict ( final_attrs , id = <STRING> % ( id_ , i ) ) <NEWLINE> <DEDENT> output . append ( self . subfield . widget . render ( name + <STRING> % i , widget_value , final_attrs ) ) <NEWLINE> <DEDENT> output . append ( self . subfield . widget . render ( name + <STRING> % ( i + 1 ) , None , final_attrs ) ) <NEWLINE> output . append ( HiddenInput ( ) . render ( name + <STRING> , str ( i + 1 ) , { } ) ) <NEWLINE> return mark_safe ( self . format_output ( output ) ) <NEWLINE> <DEDENT>
def private_keys_for_decryption ( self , identifier : ID ) -> Optional [ list ] : <NEWLINE> <INDENT> keys = super ( ) . private_keys_for_decryption ( identifier = identifier ) <NEWLINE> if keys is None : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> key = self . private_key_for_signature ( identifier ) <NEWLINE> if key is not None : <NEWLINE> <INDENT> keys = [ key ] <NEWLINE> <DEDENT> <DEDENT> return keys <NEWLINE> <DEDENT>
if pat_error == _PATError . PAT_SUCCESS : <NEWLINE> <INDENT> return <NEWLINE> elif pat_error == _PATError . PAT_INTERRUPTED_ERROR : <NEWLINE> os . kill ( os . getpid ( ) , signal . SIGINT ) <NEWLINE> elif pat_error == _PATError . PAT_TERMINATED_ERROR : <NEWLINE> os . kill ( os . getpid ( ) , signal . SIGTERM ) <NEWLINE> return <NEWLINE> else : <NEWLINE> raise PATException ( <STRING> . format ( audio_path , _error_to_str ( pat_error ) ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for i in graph . edges_iter ( data = True ) : <NEWLINE> <INDENT> if <STRING> in i [ 2 ] : <NEWLINE> <INDENT> node = i [ 2 ] [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> node = <STRING> <NEWLINE> <DEDENT> if <STRING> in i [ 2 ] : <NEWLINE> <INDENT> msg = i [ 2 ] [ <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> msg = <STRING> <NEWLINE> <COMMENT> <NL> <DEDENT> new_row = [ <NEWLINE> <INDENT> i [ 0 ] , <NEWLINE> i [ 1 ] , <NEWLINE> node , <NEWLINE> msg , <NEWLINE> i [ 2 ] [ <STRING> ] , <NEWLINE> i [ 2 ] [ <STRING> ] , <NEWLINE> i [ 2 ] [ <STRING> ] , <NEWLINE> 1 <NEWLINE> ] <NEWLINE> <COMMENT> <NL> <DEDENT> time_dataframe . loc [ len ( time_dataframe ) ] = new_row <NEWLINE> <DEDENT> <DEDENT>
def _search_other ( self , increment ) : <NEWLINE> <INDENT> original_start = self . option . start <NEWLINE> self . option . start += increment <NEWLINE> papers = self . _search ( ) <NEWLINE> if not papers : <NEWLINE> <INDENT> self . option . start = original_start <NEWLINE> <DEDENT> return papers <NEWLINE> <DEDENT>
def nic_type ( self ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> nic = [ ] <NEWLINE> out = { <STRING> : <STRING> , <STRING> : False , <STRING> : nic , <STRING> : <STRING> } <NEWLINE> try : <NEWLINE> <INDENT> proc = subprocess . Popen ( <STRING> , stdout = subprocess . PIPE , shell = True ) <NEWLINE> ( output , err ) = proc . communicate ( ) <NEWLINE> output = str ( output ) . strip ( ) <NEWLINE> output = output . split ( <STRING> ) <NEWLINE> for o in output : <NEWLINE> <INDENT> did = subprocess . Popen ( <STRING> % o , stdout = subprocess . PIPE , shell = True ) <NEWLINE> ( didout , err ) = did . communicate ( ) <NEWLINE> didout = str ( didout ) . strip ( ) [ - 7 : ] <NEWLINE> <COMMENT> <NL> brand = subprocess . Popen ( <STRING> % didout , stdout = subprocess . PIPE , shell = True ) <NEWLINE> ( brandout , err ) = brand . communicate ( ) <NEWLINE> nic_brand = str ( brandout ) . strip ( ) [ 29 : ] <NEWLINE> if ( nic_brand == <STRING> ) : <NEWLINE> <INDENT> nic_brand = <STRING> <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> speed = open ( <STRING> % o , <STRING> ) <NEWLINE> nic_speed = 0 <NEWLINE> if ( int ( speed . read ( ) . strip ( ) ) > 0 ) : <NEWLINE> <INDENT> nic_speed = int ( speed . read ( ) . strip ( ) ) <NEWLINE> if ( nic_speed == 1000 ) : <NEWLINE> <INDENT> nic . append ( { <STRING> : o , <STRING> : nic_speed , <STRING> : nic_brand , <STRING> : <STRING> } ) <NEWLINE> <DEDENT> elif ( nic_speed >= 10000 ) : <NEWLINE> <INDENT> nic . append ( { <STRING> : o , <STRING> : nic_speed , <STRING> : nic_brand , <STRING> : <STRING> } ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> except Exception as e : <NEWLINE> <INDENT> nic . append ( { <STRING> : o , <STRING> : <STRING> , <STRING> : nic_brand , <STRING> : <STRING> } ) <NEWLINE> <DEDENT> <DEDENT> out = { <STRING> : <STRING> , <STRING> : False , <STRING> : nic , <STRING> : <STRING> } <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> out = { <STRING> : e , <STRING> : False , <STRING> : nic , <STRING> : <STRING> } <NEWLINE> <DEDENT> <DEDENT>
def write ( pattern , f , settings = None ) : <NEWLINE> <INDENT> if settings is None : <NEWLINE> <INDENT> settings = { } <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def _create_instance_from_data ( cls , data ) : <NEWLINE> <INDENT> subcls = cls . _unmodified_cls . _search_subclass ( data [ <STRING> ] ) <NEWLINE> return subcls ( cls . connection_alias , ** data ) <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def from_bytes ( cls , in_data ) : <NEWLINE> <INDENT> version = int . from_bytes ( in_data [ 0 : 2 ] , <STRING> ) <NEWLINE> source_wport = int . from_bytes ( in_data [ 2 : 4 ] , <STRING> ) <NEWLINE> destination_wport = int . from_bytes ( in_data [ 4 : 6 ] , <STRING> ) <NEWLINE> length = int . from_bytes ( in_data [ 6 : 8 ] , <STRING> ) <NEWLINE> body = in_data [ 8 : ] <NEWLINE> body_length = len ( body ) <NEWLINE> if not length == body_length : <NEWLINE> <INDENT> raise ValueError ( ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) ) <NEWLINE> <DEDENT> <DEDENT> return cls ( source_wport , destination_wport , body , version ) <NEWLINE> <DEDENT> <DEDENT>
name = self . default_module_name <NEWLINE> <INDENT> resource_name = <STRING> <NEWLINE> <COMMENT> <NL> split = re . split ( <STRING> , operation . path ) <NEWLINE> for s in split : <NEWLINE> <COMMENT> <NL> <INDENT> pattern = re . compile ( <STRING> ) <NEWLINE> if s and pattern . search ( s ) is None : <NEWLINE> <INDENT> resource_name += s . title ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def is_subpath ( base , path , sep = os . path . sep ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if path . startswith ( base ) : <NEWLINE> <INDENT> trailing = base [ len ( base ) : ] <NEWLINE> return trailing == <STRING> or trailing [ 0 ] == sep <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
xlim1 = ax1 . get_xlim ( ) <NEWLINE> <INDENT> ylim1 = ax1 . get_ylim ( ) <NEWLINE> <DEDENT>
def __init__ ( self , multihash_length : int , digest_length : int ) -> None : <NEWLINE> <INDENT> template = <STRING> <NEWLINE> super ( ) . __init__ ( template . format ( digest_length , multihash_length ) ) <NEWLINE> <DEDENT>
def read_version ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> file_path = os . path . join ( <NEWLINE> <INDENT> os . path . dirname ( __file__ ) , PACKAGE_NAME , <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> regex = re . compile ( <STRING> ) <NEWLINE> with codecs . open ( file_path , encoding = <STRING> ) as fobj : <NEWLINE> <INDENT> for line in fobj : <NEWLINE> <INDENT> mobj = regex . match ( line ) <NEWLINE> if mobj : <NEWLINE> <INDENT> return mobj . group ( 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT>
if ( <NEWLINE> <COMMENT> <NL> <INDENT> type ( value ) == type <NEWLINE> <COMMENT> <NL> or is_dunder ( attr ) <NEWLINE> <COMMENT> <NL> or inspect . isfunction ( value ) <NEWLINE> <COMMENT> <NL> or attr in ( <STRING> , <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> or attr in fields <NEWLINE> <COMMENT> <NL> or isinstance ( value , Field ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , property ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , RELATED_DESCRIPTORS ) <NEWLINE> ) : <NEWLINE> return False <NEWLINE> else : <NEWLINE> return True <NEWLINE> <DEDENT>
return results_sorted <NEWLINE>
suites = collect_suites ( repo_directory ) <NEWLINE>
suites = collect_suites ( repo_directory ) <NEWLINE>
def _get ( name , default = None , compat = None ) : <NEWLINE> <INDENT> compat = compat or [ ] <NEWLINE> if default is None : <NEWLINE> <INDENT> default = _DEFAULTS . get ( name ) <NEWLINE> <DEDENT> compat = [ name ] + compat <NEWLINE> for i , alias in enumerate ( compat ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> value = getattr ( settings , alias ) <NEWLINE> i > 0 and warnings . warn ( DeprecationWarning ( _DEPRECATION_FMT % ( <NEWLINE> <INDENT> alias , name ) ) ) <NEWLINE> <DEDENT> return value <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> return default <NEWLINE> <DEDENT>
def route ( self , options , task , args = ( ) , kwargs = { } ) : <NEWLINE> <COMMENT> <NL> <INDENT> options = self . expand_destination ( options ) <NEWLINE> if self . routes : <NEWLINE> <INDENT> route = self . lookup_route ( task , args , kwargs ) <NEWLINE> if route : <NEWLINE> <COMMENT> <NL> <INDENT> return merge ( self . expand_destination ( route ) , options ) <NEWLINE> <DEDENT> <DEDENT> return options <NEWLINE> <DEDENT>
if self . state != RUN or self . started != len ( parent . steps ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . state = TERMINATE <NEWLINE> self . shutdown_complete . set ( ) <NEWLINE> return <NEWLINE> self . close ( parent ) <NEWLINE> self . state = CLOSE <NEWLINE> self . restart ( parent , <STRING> if terminate else <STRING> , what ) <NEWLINE> <DEDENT>
def test_stop ( self ) : <NEWLINE> <INDENT> c = Mock ( ) <NEWLINE> tasks = Tasks ( c ) <NEWLINE> self . assertIsNone ( c . task_consumer ) <NEWLINE> self . assertIsNone ( c . qos ) <NEWLINE> self . assertEqual ( c . initial_prefetch_count , 2 ) <NEWLINE> <DEDENT>
self . host = uhost or config . get ( <STRING> , self . host ) <NEWLINE> <INDENT> self . port = int ( uport or config . get ( <STRING> , self . port ) ) <NEWLINE> self . bucket_name = ubucket or config . get ( <STRING> , self . bucket_name ) <NEWLINE> self . protocol = protocol or config . get ( <STRING> , self . protocol ) <NEWLINE> <DEDENT>
def config_from_envvar ( self , variable_name , silent = False , force = False ) : <NEWLINE> <INDENT> module_name = os . environ . get ( variable_name ) <NEWLINE> if not module_name : <NEWLINE> <INDENT> if silent : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> raise ImproperlyConfigured ( ERR_ENVVAR_NOT_SET . format ( variable_name ) ) <NEWLINE> <DEDENT> return self . config_from_object ( module_name , silent = silent , force = force ) <NEWLINE> <DEDENT>
attrs = { <NEWLINE> <INDENT> attr_name : ( prepare_attr ( attr ) if prepare_attr else attr ) <NEWLINE> for attr_name , attr in items ( attrs ) <NEWLINE> } <NEWLINE> module = sys . modules [ fqdn ] = type ( modname , ( base , ) , cls_attrs ) ( name ) <NEWLINE> module . __dict__ . update ( attrs ) <NEWLINE> return module <NEWLINE> <DEDENT>
def on_timeout ( self , soft , timeout ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> task_ready ( self ) <NEWLINE> if soft : <NEWLINE> <INDENT> warn ( <STRING> , <NEWLINE> <INDENT> timeout , self . name , self . id ) <NEWLINE> <DEDENT> exc = SoftTimeLimitExceeded ( soft ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> error ( <STRING> , <NEWLINE> <INDENT> timeout , self . name , self . id ) <NEWLINE> <DEDENT> exc = TimeLimitExceeded ( timeout ) <NEWLINE> <DEDENT> <DEDENT>
if carbon_sources is not None : <NEWLINE> <INDENT> for cs in carbon_sources : <NEWLINE> <INDENT> add_metabolite_exchange ( model , cs ) <NEWLINE> <DEDENT> <DEDENT>
bw . wire . TristateBuffer ( switch , input_1 , output ) <NEWLINE>
self . carry_in = carry_in <NEWLINE> <INDENT> self . a = a <NEWLINE> self . b = b <NEWLINE> self . carry_out = carry_out <NEWLINE> self . sum = sum <NEWLINE> <DEDENT>
if self . oauth_error == None and self . oauth_token != None or self . state != None : <NEWLINE>
with tempfile . TemporaryDirectory ( ) as temp_dir : <NEWLINE> <INDENT> file = pathlib . Path ( temp_dir ) / <STRING> <NEWLINE> sxs . horizons . xor_multishuffle_bzip2 . save ( horizons_spec , file ) <NEWLINE> with pytest . raises ( ValueError ) : <NEWLINE> <INDENT> horizons_error = sxs . horizons . spec_horizons_h5 . load ( file ) <NEWLINE> <DEDENT> horizons_xmb = sxs . horizons . xor_multishuffle_bzip2 . load ( file ) <NEWLINE> <DEDENT>
return wellbarofixed , drift <NEWLINE>
def get_modules ( THIRD_PARTY , INTERNAL , PROJ_PATH , <NEWLINE> <INDENT> SO_SUFFIX , source_for_module_with_pyinit ) : <NEWLINE> <STRING> <NEWLINE> PYSQLITE2 = INTERNAL + <STRING> <NEWLINE> APSW = INTERNAL + <STRING> <NEWLINE> PYSQLITE = THIRD_PARTY + <STRING> <NEWLINE> APSW_TP = THIRD_PARTY + <STRING> <NEWLINE> SQLITE3 = THIRD_PARTY + <STRING> <NEWLINE> ICU_UNIX = SQLITE3 + <STRING> <NEWLINE> ICU_WIN32 = SQLITE3 + <STRING> <NEWLINE> includes = [ os . path . relpath ( SQLITE3 , PROJ_PATH ) ] <NEWLINE> libraries = [ os . path . relpath ( SQLITE3 , PROJ_PATH ) ] <NEWLINE> link_args = [ <STRING> ] <NEWLINE> if sys . platform == <STRING> : <NEWLINE> libraries . append ( ICU_WIN32 ) <NEWLINE> includes . append ( ICU_WIN32 ) <NEWLINE> link_args . append ( <STRING> + ICU_WIN32 ) <NEWLINE> else : <NEWLINE> libraries . append ( ICU_UNIX ) <NEWLINE> includes . append ( ICU_UNIX ) <NEWLINE> link_args . append ( <STRING> + ICU_UNIX ) <NEWLINE> <DEDENT>
if state != 0 : <NEWLINE> <INDENT> if grepy ( <STRING> , outfile ) : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise RuntimeError ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> . format ( <STRING> . join ( command ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> submission_info = SUBMISSION_FILE . format ( name = subreddit , content = <STRING> ) <NEWLINE> curses . endwin ( ) <NEWLINE> submission_text = open_editor ( submission_info ) <NEWLINE> curses . doupdate ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for item in stack [ : : - 1 ] : <NEWLINE> <INDENT> if not item : <NEWLINE> <INDENT> stack . pop ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
source = ColumnDataSource ( data = dict ( <NEWLINE> <INDENT> x = params . values , <NEWLINE> y = np . arange ( 1 , len ( params . index ) + 1 ) , <NEWLINE> factor_names = params . index . values , <NEWLINE> bar_colours = bar_colours , <NEWLINE> bar_signs = bar_signs , <NEWLINE> full_names = full_names , <NEWLINE> original_magnitude_with_sign = beta_str , <NEWLINE> alias_strings = alias_strings , <NEWLINE> ) ) <NEWLINE> TOOLTIPS = [ <NEWLINE> ( <STRING> , <STRING> ) , <NEWLINE> ( <STRING> , <STRING> ) , <NEWLINE> ( <STRING> , <STRING> ) , <NEWLINE> ] <NEWLINE> if len ( alias_strings ) != 0 : <NEWLINE> TOOLTIPS . append ( ( <STRING> , <STRING> ) , ) <NEWLINE> <DEDENT>
next_character_position = found + len ( old_approach ) + 1 <NEWLINE> <INDENT> if next_character_position >= len ( line ) : <NEWLINE> <INDENT> return found <NEWLINE> <DEDENT> <DEDENT>
def __deepcopy__ ( self , memo ) : <NEWLINE> <INDENT> snippet = PMXSnippet ( self . namespace , self . hash ) <NEWLINE> memo [ <STRING> ] = deepcopy ( self . snippet , memo ) <NEWLINE> snippet . bundle = self . bundle <NEWLINE> return snippet <NEWLINE> <DEDENT>
def setFilterNamespace ( self , namespace ) : <NEWLINE> <INDENT> if not namespace : <NEWLINE> <INDENT> self . namespacesFilter = [ <STRING> , <STRING> ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . namespacesFilter = namespace . split ( ) <NEWLINE> <DEDENT> self . setFilterRegExp ( <STRING> ) <NEWLINE> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def flyweightScopeFactory ( cls , scopeStack ) : <NEWLINE> <INDENT> scopeHash = hash ( scopeStack ) <NEWLINE> if scopeHash not in cls . SCOPES : <NEWLINE> <INDENT> scopeName = <STRING> . join ( scopeStack ) <NEWLINE> cls . SCOPES [ scopeHash ] = CodeEditorScope ( <NEWLINE> <INDENT> name = scopeName , <NEWLINE> path = scopeStack , <NEWLINE> settings = cls . application . supportManager . getPreferenceSettings ( scopeStack ) , <NEWLINE> group = PMXSyntax . findGroup ( scopeStack [ : : - 1 ] ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return scopeHash <NEWLINE> <DEDENT> <DEDENT>
def tokenAtPosition ( self , pos ) : <NEWLINE> <INDENT> for token in self . __tokens [ : : - 1 ] : <NEWLINE> <INDENT> if token . start <= pos < token . end : <NEWLINE> <INDENT> return token <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def blockRevision ( self , block ) : <NEWLINE> <INDENT> return _revision ( self . scope_name , block . text ( ) + <STRING> , block . previous ( ) . userState ( ) ) <NEWLINE> <DEDENT>
trajectory_table , _ = import_trajectory_table ( trajectories_filename ) <NEWLINE> <INDENT> genotype_table = calculate_genotypes . workflow ( trajectory_table , options = goptions ) <NEWLINE> <COMMENT> <NL> <DEDENT>
def waitForChromeToClose ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . proc . returncode is None and not self . chromeClosed : <NEWLINE> <INDENT> self . chromeClosed = True <NEWLINE> if psutil . pid_exists ( self . proc . pid ) : <NEWLINE> <INDENT> self . proc . terminate ( ) <NEWLINE> self . proc . kill ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
ppath = ctx . obj [ <STRING> ] <NEWLINE> <INDENT> if norm_id is not None : <NEWLINE> <INDENT> normfolder = ppath . basedir / <STRING> . format ( norm_id , folder_suffix ) <NEWLINE> norm = get_normalisation ( fdname = normfolder , name = norm_name ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> import numpy as np <NEWLINE> norm = np . array ( 1 ) <NEWLINE> <DEDENT> <DEDENT>
doc . update ( d ) <NEWLINE> <INDENT> yaml . dump ( doc , <NEWLINE> <INDENT> stream = open ( path , <STRING> ) , <NEWLINE> default_flow_style = False , <NEWLINE> indent = 2 , <NEWLINE> width = 72 ) <NEWLINE> <DEDENT> <DEDENT>
while xs or ys : <NEWLINE> <INDENT> if xs and not ys : <NEWLINE> <INDENT> yield xs . pop ( ) , None <NEWLINE> <DEDENT> elif ys and not xs : <NEWLINE> <INDENT> yield None , ys . pop ( ) <NEWLINE> <DEDENT> elif key ( xs [ - 1 ] ) == key ( ys [ - 1 ] ) : <NEWLINE> <INDENT> yield xs . pop ( ) , ys . pop ( ) <NEWLINE> <DEDENT> elif key ( xs [ - 1 ] ) < key ( ys [ - 1 ] ) : <NEWLINE> <INDENT> yield xs . pop ( ) , None <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> yield None , ys . pop ( ) <NEWLINE> <DEDENT> <DEDENT>
curate . remove_old_genomes ( genbank_mirror , assembly_summary , old_genomes , logger ) <NEWLINE> <INDENT> sync . sync_latest_genomes ( genbank_mirror , assembly_summary , new_genomes , logger ) <NEWLINE> curate . unzip_genbank_mirror ( genbank_mirror ) <NEWLINE> rename . rename ( genbank_mirror , assembly_summary ) <NEWLINE> <DEDENT>
sigmahat = VonMises_std ( self . phi , self . theta ) <NEWLINE> <INDENT> self . suggested_bandwidth = 1.06 * sigmahat * len ( weights ) ** - 0.2 <NEWLINE> <DEDENT>
class ModelServer ( object ) : <NEWLINE> <INDENT> def __init__ ( self , config , source_path , resource_path , model_conf , pipe ) : <NEWLINE> <INDENT> self . _pipe = pipe <NEWLINE> self . _resources = ModelServer . acquire_resources ( config , model_conf , resource_path ) <NEWLINE> self . _model_class = ModelServer . import_model ( model_conf [ <STRING> ] , source_path ) <NEWLINE> self . _model = self . _model_class ( self . _resources , config = model_conf ) <NEWLINE> <DEDENT> <DEDENT>
if not self . app . mount . mountUp : <NEWLINE> <INDENT> return False <NEWLINE> if self . ui . checkRefracNone . isChecked ( ) : <NEWLINE> return False <NEWLINE> if self . ui . checkRefracNoTrack . isChecked ( ) : <NEWLINE> if self . app . mount . obsSite . status == 0 : <NEWLINE> <INDENT> return False <NEWLINE> temp , press = self . app . environment . getFilteredRefracParams ( ) <NEWLINE> if temp is None or press is None : <NEWLINE> <DEDENT> return False <NEWLINE> suc = self . app . mount . obsSite . setRefractionParam ( temperature = temp , <NEWLINE> <INDENT> pressure = press ) <NEWLINE> if not suc : <NEWLINE> <DEDENT> self . app . message . emit ( <STRING> , 2 ) <NEWLINE> return False <NEWLINE> return True <NEWLINE> <DEDENT>
self . data [ key ] = value <NEWLINE>
def isNoneOrEmpty ( obj ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if obj is None : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if isinstance ( obj , list ) : <NEWLINE> <INDENT> return len ( obj ) <= 0 <NEWLINE> <DEDENT> if isinstance ( obj , str ) : <NEWLINE> <INDENT> return len ( obj . replace ( <STRING> , <STRING> ) ) <= 0 <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>
@ cli . command ( ) <NEWLINE> <INDENT> @ click . pass_context <NEWLINE> def upgrade ( ctx ) : <NEWLINE> <INDENT> local_packages = ctx . obj . get ( <STRING> , { } ) . items ( ) <NEWLINE> remote_packages = map ( get_package , map ( operator . itemgetter ( 0 ) , local_packages ) ) <NEWLINE> remote_package_versions = map ( lambda x : x and x [ <STRING> ] , remote_packages ) <NEWLINE> for ( n , lv ) , rv in zip ( local_packages , remote_package_versions ) : <NEWLINE> <INDENT> if rv is None : <NEWLINE> <INDENT> print ( <STRING> . format ( n ) ) <NEWLINE> continue <NEWLINE> <DEDENT> elif lv != rv : <NEWLINE> <INDENT> print ( <STRING> . format ( rv , lv , n ) , end = <STRING> ) <NEWLINE> sys . stdout . flush ( ) <NEWLINE> answer = sys . stdin . readline ( ) . strip ( ) <NEWLINE> if answer in ( <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> print ( <STRING> . format ( n , lv , rv ) ) <NEWLINE> install ( ctx , n ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> . format ( n , lv , rv ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> print ( <STRING> . format ( n ) ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
@ B . dispatch ( LowRank , LowRank ) <NEWLINE> <INDENT> def matmul ( a , b , tr_a = False , tr_b = False ) : <NEWLINE> <INDENT> _assert_composable ( a , b , tr_a = tr_a , tr_b = tr_b ) <NEWLINE> a = _tr ( a , tr_a ) <NEWLINE> b = _tr ( b , tr_b ) <NEWLINE> middle = B . matmul ( a . right , b . left , tr_a = True ) <NEWLINE> rows , cols = B . shape ( middle ) <NEWLINE> if rows > cols : <NEWLINE> <INDENT> return LowRank ( B . matmul ( a . left , middle ) , b . right ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return LowRank ( a . left , B . matmul ( b . right , middle , tr_b = True ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def download ( <NEWLINE> <INDENT> cls , <NEWLINE> id_package_list : Optional [ List [ int ] ] = None , <NEWLINE> guide_list : Optional [ List [ str ] ] = None , <NEWLINE> ivoy_guide_list : Optional [ List [ str ] ] = None , <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> if not any ( [ id_package_list , guide_list , ivoy_guide_list ] ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> json_data = cls . _download_json ( <NEWLINE> <INDENT> id_package_list , guide_list , ivoy_guide_list <NEWLINE> <DEDENT> ) <NEWLINE> resp = cls . _client . post ( cls . _endpoint , json = json_data ) <NEWLINE> return cls ( <NEWLINE> <INDENT> id_package_list = id_package_list , <NEWLINE> guide_list = guide_list , <NEWLINE> ivoy_guide_list = ivoy_guide_list , <NEWLINE> byte_content = resp . content , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
sccoord = np . atleast_2d ( sccoord ) <NEWLINE> <INDENT> if sccoord . shape [ 0 ] == 3 : <NEWLINE> <INDENT> T = np . zeros ( ( 6 , 6 ) ) <NEWLINE> T [ : 3 , : 3 ] = sccoord <NEWLINE> T [ 3 : , 3 : ] = sccoord <NEWLINE> return T <NEWLINE> <DEDENT> <DEDENT>
if datasets [ self . dataset_name ] . get ( self . split_name ) is None : <NEWLINE> <INDENT> logger . error ( <STRING> . format ( self . split_name , self . dataset_name ) ) <NEWLINE> self . dataset_name = None <NEWLINE> return <NEWLINE> <DEDENT>
def orientation_with ( self , point : Point ) -> int : <NEWLINE> <INDENT> return Angle ( self . end , self . start , point ) . orientation <NEWLINE> <DEDENT>
@ given ( strategies . points , strategies . points , strategies . points ) <NEWLINE> <INDENT> def test_basic ( vertex : Point , <NEWLINE> <INDENT> first_ray_point : Point , <NEWLINE> second_ray_point : Point ) -> None : <NEWLINE> angle = Angle ( first_ray_point , vertex , second_ray_point ) <NEWLINE> <DEDENT> <DEDENT>
cnx = dbmgr . get ( db_file ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> with open ( sql_file , <STRING> ) as sf : <NEWLINE> <INDENT> log . debug ( chlogger , { <NEWLINE> <INDENT> <STRING> : __name__ , <NEWLINE> <STRING> : resource_name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : db_file , <NEWLINE> <STRING> : idx , <NEWLINE> <STRING> : sql_file , <NEWLINE> <STRING> : depth , <NEWLINE> <STRING> : str ( dbmgr ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> } ) <NEWLINE> <DEDENT> cnx . executescript ( sf . read ( ) ) <NEWLINE> log . debug ( chlogger , { <NEWLINE> <INDENT> <STRING> : __name__ , <NEWLINE> <STRING> : resource_name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : db_file , <NEWLINE> <STRING> : idx , <NEWLINE> <STRING> : sql_file , <NEWLINE> <STRING> : depth , <NEWLINE> <STRING> : str ( dbmgr ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> } ) <NEWLINE> <DEDENT> <DEDENT> return sql_file_name <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> log . error ( chlogger , { <NEWLINE> <INDENT> <STRING> : __name__ , <NEWLINE> <STRING> : resource_name , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : idx , <NEWLINE> <STRING> : db_file , <NEWLINE> <STRING> : sql_file , <NEWLINE> <STRING> : depth , <NEWLINE> <STRING> : str ( dbmgr ) , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : str ( e ) , <NEWLINE> } ) <NEWLINE> <DEDENT> insert_file ( logger , resource_name , dbmgr , sql_dir , db_dir , sql_file , idx , depth + 1 , max_depth ) <NEWLINE> <DEDENT> <DEDENT>
@ given ( <NEWLINE> <INDENT> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> binary ( ) , <NEWLINE> ) <NEWLINE> def test_build_regular_packet ( iv , <NEWLINE> <INDENT> iv_hash , <NEWLINE> payload_hash , <NEWLINE> handshake_key , <NEWLINE> payload ) : <NEWLINE> <DEDENT> data = join_encode_data ( [ iv , <NEWLINE> <INDENT> iv_hash , <NEWLINE> payload_hash , <NEWLINE> handshake_key , <NEWLINE> payload ] ) <NEWLINE> <DEDENT> if ( len ( iv ) == packets . IV_LEN and <NEWLINE> <INDENT> len ( iv_hash ) == packets . HASH_LEN and <NEWLINE> len ( payload_hash ) == packets . HASH_LEN and <NEWLINE> not len ( handshake_key ) and <NEWLINE> len ( payload ) ) : <NEWLINE> assert isinstance ( packets . build_regular_packet ( data ) , <NEWLINE> <INDENT> packets . RegularPacket ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> with pytest . raises ( errors . MalformedPacketError ) : <NEWLINE> <INDENT> packets . build_regular_packet ( data ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
else : <NEWLINE> <INDENT> pat = re . compile ( text ) <NEWLINE> if pat . search ( state . student_result ) : <NEWLINE> <INDENT> state . do_test ( msg . format ( text ) ) <NEWLINE> <DEDENT> <DEDENT>
def set_states ( self , time ) : <NEWLINE> <INDENT> if time > self . memory_dump . x [ - 1 ] + self . T : <NEWLINE> <INDENT> fit = self . memory [ 0 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> fit = self . memory_dump <NEWLINE> <DEDENT> <DEDENT>
if convergence : <NEWLINE> <INDENT> vc = t_list . apply ( pd . Series . value_counts ) <NEWLINE> vc = vc [ target_id_type ] . dropna ( ) . sort_index ( ) <NEWLINE> count = vc . ix [ target_id ] <COMMENT> <NEWLINE> else : <NEWLINE> vc = s_list . apply ( pd . Series . value_counts ) <NEWLINE> vc = vc [ source_id_type ] . dropna ( ) . sort_index ( ) <NEWLINE> count = vc . ix [ source_id ] <COMMENT> <NEWLINE> <DEDENT>
segpassivewidget = SegregationPassiveWidget ( fir_widget , ctg . root_sec . cell ( ) , other_cells , section_selected , ctg . mechanism_dict , gleak_var = gleak , eleak_var = eleak ) <NEWLINE> <INDENT> ctg . add_widget ( window_index , column_index , segpassivewidget ) <NEWLINE> <DEDENT>
def as_dict ( self ) : <NEWLINE> <INDENT> d = super ( GCECredentials , self ) . as_dict ( ) <NEWLINE> gce_creds = json . loads ( self . credentials ) <NEWLINE> <COMMENT> <NL> gce_creds . update ( d ) <NEWLINE> return gce_creds <NEWLINE> <DEDENT>
line = <STRING> . join ( line_list ) <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if self . settings [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> exp = re . compile ( message . guild . me . display_name , re . IGNORECASE ) <NEWLINE> line = exp . sub ( <STRING> , line ) <NEWLINE> <DEDENT> <DEDENT> except KeyError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT>
performance = performance , <NEWLINE>
def connect ( self , host = None , port = None ) : <NEWLINE> <INDENT> if host is not None : <NEWLINE> <INDENT> self . _host = host <NEWLINE> <DEDENT> <DEDENT>
if cond_val is True : <NEWLINE> <INDENT> comp_insts = [ ] <NEWLINE> last_comp = None <NEWLINE> for comp_time in cond_comp . comp_times : <NEWLINE> <INDENT> comp_time_inst = comp_time . eval ( context , last_comp ) <NEWLINE> comp_insts . append ( comp_time_inst ) <NEWLINE> last_comp = comp_time <NEWLINE> <DEDENT> setattr ( self , <STRING> , comp_insts ) <NEWLINE> break <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> for idx , var_exp in enumerate ( cond_spec ) : <NEWLINE> <INDENT> if type ( var_exp ) is LoopExpression : <NEWLINE> <INDENT> loops . append ( var_exp . exp . resolve ( ) ) <NEWLINE> loops_idx . append ( idx ) <NEWLINE> cond_template . append ( None ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> var_exp_resolved = var_exp . resolve ( ) <NEWLINE> if isinstance ( var_exp_resolved , Sequence ) : <NEWLINE> <INDENT> if has_loops or len ( var_exp_resolved ) < max_len : <NEWLINE> <INDENT> cond_template . append ( cycle ( var_exp_resolved ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cond_template . append ( iter ( var_exp_resolved ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if should_repeat : <NEWLINE> <INDENT> cond_template . append ( repeat ( var_exp ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cond_template . append ( iter ( [ var_exp ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if DEEP_ENTRY_INPUT in metric_args : <NEWLINE> <INDENT> if DEEP_ENTRY_LABEL in metric_args : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , outputs , labels , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , labels ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , outputs , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( inputs , outputs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if DEEP_ENTRY_LABEL in metric_args : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , labels , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , labels ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if DEEP_ENTRY_ADDITIONAL_DATA in metric_args : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs , additional_data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> temp_metric_result = metric_method ( outputs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def __contains__ ( self , point ) : <NEWLINE> <INDENT> return self . low <= point < self . high <NEWLINE> <DEDENT>
if genes is None and isinstance ( X , ( pd . SparseDataFrame , <NEWLINE> <INDENT> sparse . spmatrix ) ) and np . prod ( X . shape ) > 5000 * 20000 : <NEWLINE> warnings . warn ( <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> <NEWLINE> <STRING> . format ( <NEWLINE> X . shape [ 0 ] , X . shape [ 1 ] , <NEWLINE> np . prod ( X . shape ) * 8 / ( 1024 ** 3 ) ) , <NEWLINE> UserWarning ) <NEWLINE> if genes == <STRING> : <NEWLINE> genes = None <NEWLINE> elif genes is not None : <NEWLINE> genes = np . array ( [ genes ] ) . flatten ( ) <NEWLINE> if not issubclass ( genes . dtype . type , numbers . Integral ) : <NEWLINE> <COMMENT> <NL> if not isinstance ( X , pd . DataFrame ) : <NEWLINE> raise ValueError ( <NEWLINE> <STRING> <NEWLINE> <STRING> . format ( type ( X ) . __name__ , <NEWLINE> <INDENT> genes ) ) <NEWLINE> if not np . all ( np . isin ( genes , X . columns ) ) : <NEWLINE> warnings . warn ( <STRING> . format ( <NEWLINE> genes [ ~ np . isin ( genes , X . columns ) ] ) ) <NEWLINE> genes = np . argwhere ( np . isin ( X . columns , genes ) ) . reshape ( - 1 ) <NEWLINE> <DEDENT> <DEDENT>
if getattr ( django_settings , name ) : <NEWLINE> <INDENT> self . assertNotContains ( <NEWLINE> <INDENT> response , <STRING> % getattr ( django_settings , name ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
for j , low in enumerate ( all_splits [ : - 1 ] ) : <NEWLINE> <INDENT> high = all_splits [ j + 1 ] <NEWLINE> pff , tsj , weights , pmap , pixels_sub = do_lc ( tpf , <NEWLINE> <INDENT> ts , ( low , high ) , sub , order , maxiter = 101 , split_times = None , w_init = w_init , random_init = random_init , <NEWLINE> thresh = thresh , minflux = minflux , consensus = consensus , analytic = analytic , sigclip = sigclip , verbose = verbose ) <NEWLINE> <DEDENT> tss . append ( tsj ) <NEWLINE> if low is None : <NEWLINE> <INDENT> cad1 . append ( ts [ <STRING> ] [ 0 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cad1 . append ( ts [ <STRING> ] [ low ] ) <NEWLINE> <DEDENT> if high is None : <NEWLINE> <INDENT> cad2 . append ( ts [ <STRING> ] [ - 1 ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cad2 . append ( ts [ <STRING> ] [ high ] ) <NEWLINE> <DEDENT> sat . append ( pmap [ <STRING> ] ) <NEWLINE> weightmap . append ( pmap [ <STRING> ] ) <NEWLINE> wmap = { <NEWLINE> <STRING> : cad1 , <NEWLINE> <STRING> : cad2 , <NEWLINE> <STRING> : sat , <NEWLINE> <STRING> : weightmap <NEWLINE> } <NEWLINE> ts = stitch ( tss ) <NEWLINE> <DEDENT>
if self . deltax >= 0 : <NEWLINE>
<COMMENT> <NL> <INDENT> match_count = 0 <NEWLINE> pred_match = - 1 * np . ones ( [ pred_boxes . shape [ 0 ] ] ) <NEWLINE> gt_match = - 1 * np . ones ( [ gt_boxes . shape [ 0 ] ] ) <NEWLINE> for i in range ( len ( pred_boxes ) ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> sorted_ixs = np . argsort ( overlaps [ i ] ) [ : : - 1 ] <NEWLINE> <COMMENT> <NL> low_score_idx = np . where ( overlaps [ i , sorted_ixs ] < score_threshold ) [ 0 ] <NEWLINE> if low_score_idx . size > 0 : <NEWLINE> <INDENT> sorted_ixs = sorted_ixs [ : low_score_idx [ 0 ] ] <NEWLINE> <COMMENT> <NL> <DEDENT> for j in sorted_ixs : <NEWLINE> <COMMENT> <NL> <INDENT> if gt_match [ j ] > - 1 : <NEWLINE> <INDENT> continue <NEWLINE> <COMMENT> <NL> <DEDENT> iou = overlaps [ i , j ] <NEWLINE> if iou < iou_threshold : <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> <DEDENT> if pred_class_ids [ i ] == gt_class_ids [ j ] : <NEWLINE> <INDENT> match_count += 1 <NEWLINE> gt_match [ j ] = i <NEWLINE> pred_match [ i ] = j <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def read_xattrs_from_disk ( self , myfile ) : <NEWLINE> <INDENT> id_table = _Xattr_table ( ) <NEWLINE> if self . sBlk . xattr_id_table_start == SQUASHFS_INVALID_BLK : <NEWLINE> <INDENT> return SQUASHFS_INVALID_BLK <NEWLINE> <DEDENT> myfile . seek ( self . offset + self . sBlk . xattr_id_table_start ) <NEWLINE> id_table . read ( myfile ) <NEWLINE> ids = id_table . xattr_ids <NEWLINE> xattr_table_start = id_table . xattr_table_start <NEWLINE> index_bytes = SQUASHFS_XATTR_BLOCK_BYTES ( ids ) <NEWLINE> indexes = SQUASHFS_XATTR_BLOCKS ( ids ) <NEWLINE> index = [ ] <NEWLINE> for r in range ( 0 , indexes ) : <NEWLINE> <INDENT> index . append ( self . makeInteger ( myfile , SQUASHFS_XATTR_BLOCK_BYTES ( 1 ) ) ) <NEWLINE> <DEDENT> bytes = SQUASHFS_XATTR_BYTES ( ids ) <NEWLINE> xattr_ids = { } <NEWLINE> for i in range ( 0 , indexes ) : <NEWLINE> <INDENT> block , next , byte_count = self . read_block ( myfile , index [ i ] ) <NEWLINE> cur_idx = ( i * SQUASHFS_METADATA_SIZE ) / 16 <NEWLINE> ofs = 0 <NEWLINE> while ofs < len ( block ) : <NEWLINE> <INDENT> xattr_id = _Xattr_id ( ) <NEWLINE> xattr_id . fill ( block , ofs ) <NEWLINE> xattr_ids [ cur_idx ] = xattr_id <NEWLINE> cur_idx += 1 <NEWLINE> ofs += 16 <NEWLINE> <DEDENT> <DEDENT> start = xattr_table_start <NEWLINE> end = index [ 0 ] <NEWLINE> xattr_values = { } <NEWLINE> i = 0 <NEWLINE> while start < end : <NEWLINE> <INDENT> self . hash_table [ start ] = ( i * SQUASHFS_METADATA_SIZE ) <NEWLINE> block , start , byte_count = self . read_block ( myfile , start ) <NEWLINE> for i in range ( len ( block ) , SQUASHFS_METADATA_SIZE ) : <NEWLINE> <INDENT> block += <STRING> <NEWLINE> <DEDENT> self . xattrs += block <NEWLINE> i += 1 <NEWLINE> <DEDENT> return ids <NEWLINE> <DEDENT>
def process_batch ( key , batch_info , iv ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> batch_info = pickle . loads ( decrypt ( key , batch_info , iv . decode ( <STRING> ) ) ) <NEWLINE> if valid_batch ( batch_info ) : <NEWLINE> <INDENT> items = serializers . deserialize ( <STRING> , batch_info [ <STRING> ] ) <NEWLINE> success = True <NEWLINE> for item in items : <NEWLINE> <INDENT> item . save ( ) <NEWLINE> if isinstance ( item . object , Version ) : <NEWLINE> <INDENT> version = item . object <NEWLINE> if version . type == VERSION_DELETE : <NEWLINE> <INDENT> if version . object : <NEWLINE> <INDENT> version . object . delete ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> item . object . revert ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> return success <NEWLINE> <DEDENT>
def _clean_class ( self , user_class , string_only = None ) : <NEWLINE> <INDENT> if type ( user_class ) != str : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> if <STRING> not in user_class : <NEWLINE> <INDENT> if self . _vocab and ( user_class in self . _vocab ) : <NEWLINE> <INDENT> user_class = self . _vocab [ user_class ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> user_class = <STRING> + user_class <NEWLINE> <DEDENT> <DEDENT> if string_only : <NEWLINE> <INDENT> return user_class <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return self . _expand_variable ( user_class ) <NEWLINE> <DEDENT> <DEDENT>
def full_build_is_required ( ) : <NEWLINE> <INDENT> full_build = _load_state ( _FULL_BUILD ) <NEWLINE> if not full_build : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>
for item in org_tree : <NEWLINE> <INDENT> if isinstance ( item , tuple ) : <NEWLINE> <INDENT> value = org_tree [ item ] <NEWLINE> if value < 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( value , item ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def available_places_validator ( self , context , request , value ) : <NEWLINE> <INDENT> registration = context . getParentNode ( ) . getParentNode ( ) <NEWLINE> period = registration . get ( request . form . get ( <STRING> ) ) <NEWLINE> if int ( value ) <= period . available_places : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return _ ( <STRING> ) <NEWLINE> <DEDENT>
def test_invalid_password ( self ) : <NEWLINE> <INDENT> connection = Connection ( <NEWLINE> <INDENT> credentials = Credentials ( username = <STRING> , password = <STRING> ) , <NEWLINE> base_url = <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> self . assertRaises ( LoginError , connection . login ) <NEWLINE> <DEDENT>
def terminal ( self , state ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return ( state [ ... , 0 ] < - self . params . x_threshold ) | ( <NEWLINE> <INDENT> state [ ... , 0 ] > self . params . x_threshold <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if args . destroy : <NEWLINE> <INDENT> docker_image_remove ( project . image_id ) <NEWLINE> <DEDENT>
self . logger = get_logger ( ) <NEWLINE> <INDENT> if mail_dir is None or not isinstance ( mail_dir , str ) : <NEWLINE> <INDENT> msg = <STRING> . format ( mail_dir ) <NEWLINE> self . logger . error ( msg ) <NEWLINE> raise SystemExit ( msg ) <NEWLINE> <DEDENT> elif not os . path . isdir ( mail_dir ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> os . mkdir ( mail_dir ) <NEWLINE> <DEDENT> except IOError as io_error : <NEWLINE> <INDENT> self . logger . error ( str ( io_error ) ) <NEWLINE> raise <NEWLINE> <DEDENT> <DEDENT> self . mail_dir = mail_dir <NEWLINE> self . print_messages = print_messages is True <NEWLINE> self . logger . info ( <STRING> . format ( * localaddr ) ) <NEWLINE> self . logger . info ( <STRING> . format ( self . mail_dir ) ) <NEWLINE> <DEDENT>
def wrapper ( ) : <NEWLINE> <INDENT> while True : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> return fn ( ) <NEWLINE> <DEDENT> except AssertionError : <NEWLINE> <INDENT> if time . time ( ) > timeout : <NEWLINE> <INDENT> raise <NEWLINE> return wrapper <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if status_code == 502 : <NEWLINE> <INDENT> print ( <STRING> % locals ( ) ) <NEWLINE> print ( url ) <NEWLINE> self . csvDicts = [ ] <NEWLINE> return self . csvDicts <NEWLINE> <DEDENT>
def getLinks ( self , outwardOnly = False ) : <NEWLINE> <INDENT> if getattr ( self , <STRING> , None ) is None : <NEWLINE> <INDENT> return self . youtrack . getLinks ( self . id , outwardOnly ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return [ l for l in self . links if l . source == self . id or not outwardOnly ] <NEWLINE> <DEDENT> <DEDENT>
def _import_issues ( self , project_id ) : <NEWLINE> <INDENT> limit = 100 <NEWLINE> all_issues = self . _get_issues ( project_id ) <NEWLINE> while True : <NEWLINE> <INDENT> issues = list ( itertools . islice ( all_issues , None , limit ) ) <NEWLINE> if not len ( issues ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> self . _target . importIssues ( project_id , project_id + <STRING> , <NEWLINE> <INDENT> [ self . _to_yt_issue ( issue , project_id ) for issue in issues ] ) <NEWLINE> <DEDENT> for issue in issues : <NEWLINE> <INDENT> issue_id = self . _get_issue_id ( issue ) <NEWLINE> issue_attachments = self . _get_attachments ( issue ) <NEWLINE> yt_issue_id = <STRING> % ( project_id , issue_id ) <NEWLINE> self . _import_attachments ( yt_issue_id , issue_attachments ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
with open ( <STRING> , <STRING> ) as fp : <NEWLINE> <INDENT> json . dump ( obj = files , fp = fp , indent = 4 , sort_keys = True ) <NEWLINE> response . outputs [ <STRING> ] . file = fp . name <NEWLINE> return response <NEWLINE> <DEDENT>
cleaned_img1d = geometry_converter . image_2d_to_1d ( cleaned_img , fits_metadata_dict [ <STRING> ] ) <NEWLINE> <INDENT> hillas_params_2_cleaned_img = get_hillas_parameters ( geom1d , cleaned_img1d , HILLAS_IMPLEMENTATION ) <COMMENT> <NEWLINE> <DEDENT>
p = Page ( b ) <NEWLINE> <INDENT> q = Question ( p , <STRING> , qtype = <STRING> , var = <STRING> ) <NEWLINE> <DEDENT>
def truncate_recent ( max_records ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> t = recent_visitors_table <NEWLINE> q = select ( [ t . c . last_timestamp ] ) . order_by ( t . c . last_timestamp . desc ( ) ) . limit ( 1 ) . offset ( max_records ) <NEWLINE> delete_before = q . scalar ( ) <NEWLINE> if delete_before : <NEWLINE> <INDENT> q = t . delete ( ) . where ( t . c . last_timestamp < delete_before ) <NEWLINE> meta . Session . execute ( q ) <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def get_parse_trade_url ( cls , trade_url ) : <NEWLINE> <INDENT> regex = re . compile ( <STRING> ) <NEWLINE> match = regex . match ( trade_url ) <NEWLINE> if not match : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def is_display_small ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> size = wx . GetDisplaySize ( ) <NEWLINE> if size is not None : <NEWLINE> <INDENT> w , h = size <NEWLINE> return w < 1300 or h < 850 <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT>
def parse_escape_markers ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sequence_start = None <NEWLINE> self . escape_markers = [ ] <NEWLINE> for index , char in enumerate ( self . string ) : <NEWLINE> <COMMENT> <NL> <INDENT> if sequence_start is not None and char in string . letters : <NEWLINE> <INDENT> self . escape_markers . append ( EscapeMarker ( sequence_start , index ) ) <NEWLINE> sequence_start = None <COMMENT> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if isinstance ( stmt , str ) : <NEWLINE> <INDENT> return stmt , tuple ( pos_args ) if pos_args is not None else ( ) <NEWLINE> else : <NEWLINE> compiled = stmt . compile ( dialect = _d ) <NEWLINE> params = compiled . construct_params ( named_args ) <NEWLINE> return compiled . string , tuple ( params [ p ] for p in compiled . positiontup ) <NEWLINE> <DEDENT>
def update ( self , obj , ** kwargs ) : <NEWLINE> <INDENT> for h in self . _pheap : <NEWLINE> <INDENT> if h [ 1 ] == obj : <NEWLINE> <INDENT> self . _pheap . remove ( h ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> heapq . heappush ( self . _pheap , ( kwargs [ <STRING> ] , obj ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> elif isinstance ( field_value , list ) and len ( field_value ) > 0 : <NEWLINE> <INDENT> if isinstance ( field_value [ 0 ] , Model ) : <NEWLINE> <INDENT> model_dict [ serialized_name ] = [ model_converter ( vi ) <NEWLINE> <INDENT> for vi in field_value ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def request ( self , method ) : <NEWLINE> <INDENT> m_type = method . m_type <NEWLINE> auth_ = method . auth <NEWLINE> url = self . __get_url ( method ) <NEWLINE> if m_type == <STRING> : <NEWLINE> <INDENT> assert method . body is None , <STRING> <NEWLINE> if self . proxies is None : <NEWLINE> <INDENT> r = requests . get ( url = url , params = method . params , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . get ( url = url , params = method . params , headers = method . headers , proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> assert method . files is not None , <STRING> <NEWLINE> if self . proxies is not None : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ , <NEWLINE> <INDENT> files = method . files ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ , files = method . files ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . post ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . delete ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . delete ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . patch ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . patch ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif m_type == <STRING> : <NEWLINE> <INDENT> if self . proxies is None : <NEWLINE> <INDENT> r = requests . put ( url = url , params = method . params , data = method . body , headers = method . headers , auth = auth_ ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> r = requests . put ( url = url , params = method . params , data = method . body , headers = method . headers , <NEWLINE> <INDENT> proxies = self . proxies , auth = auth_ ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> % method . m_type ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> if r is None or len ( r . content ) == 0 : <NEWLINE> <INDENT> return method . response_process ( { } ) <NEWLINE> <DEDENT> return method . response_process ( r . json ( ) , r . status_code ) <NEWLINE> <DEDENT> except Exception as e : <NEWLINE> <INDENT> logging . info ( <STRING> ) <NEWLINE> return method . response_process ( { } , r . status_code ) <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def _parse_ymd ( cls , period ) : <NEWLINE> <COMMENT> <NL> <INDENT> def _parse ( p , letter ) : <NEWLINE> <INDENT> if p . find ( letter ) >= 0 : <NEWLINE> <INDENT> s , p = p . split ( letter , 1 ) <NEWLINE> s = s [ 1 : ] if s . startswith ( <STRING> ) else s <NEWLINE> sgn , s = ( - 1 , s [ 1 : ] ) if s . startswith ( <STRING> ) else ( 1 , s ) <NEWLINE> if not s . isdigit ( ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> % ( s , p , cls . __name__ ) ) <NEWLINE> <DEDENT> return sgn * int ( s ) , p <NEWLINE> <DEDENT> return 0 , p <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def is_not_regex_match ( self , pattern : str ) -> StringValidator : <NEWLINE> <INDENT> <STRING> <NEWLINE> if RegexHelper . is_match ( pattern , self . value ) : <NEWLINE> <INDENT> raise ArgumentPatternError ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> self . value , <NEWLINE> self . argument_name , <NEWLINE> pattern <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
def authenticate ( get_realm , realm_param = <STRING> ) : <NEWLINE> <INDENT> def partial ( func ) : <NEWLINE> <INDENT> @ wraps ( func ) <NEWLINE> def f ( self , request , ** kw ) : <NEWLINE> <INDENT> realm = kw [ realm_param ] <NEWLINE> del kw [ realm_param ] <NEWLINE> realm = get_realm ( self , realm ) <NEWLINE> username = None <NEWLINE> password = None <NEWLINE> if request . authorization : <NEWLINE> <INDENT> password = b64decode ( request . authorization [ 1 ] ) <NEWLINE> username , password = password . decode ( <STRING> ) . split ( <STRING> , 1 ) <NEWLINE> <DEDENT> if username is None or not realm . authenticate ( username , password ) : <NEWLINE> <INDENT> raise HTTPUnauthorized ( headers = [ <NEWLINE> <INDENT> ( <STRING> , <NEWLINE> <INDENT> <STRING> . format ( realm . description ) ) , <NEWLINE> <DEDENT> <DEDENT> ] ) <NEWLINE> <DEDENT> return func ( self , request , ** kw ) <NEWLINE> <DEDENT> return f <NEWLINE> <DEDENT> return partial <NEWLINE> <DEDENT>
