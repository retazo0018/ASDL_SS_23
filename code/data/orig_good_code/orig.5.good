l = line . split ( ) <NEWLINE> <INDENT> self . channel = <STRING> <NEWLINE> self . verb = <STRING> <NEWLINE> ind = 0 <NEWLINE> privmsg_index = 0 <NEWLINE> for e in l : <NEWLINE> <INDENT> ind += 1 <NEWLINE> if e == <STRING> : <NEWLINE> <INDENT> privmsg_index = ind <NEWLINE> <DEDENT> if e . startswith ( <STRING> ) : <NEWLINE> <INDENT> self . channel = e <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> for v in l : <NEWLINE> <INDENT> if v in [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] : <NEWLINE> <INDENT> self . verb = v <NEWLINE> break <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if self . verb == <STRING> and not len ( self . channel ) : <NEWLINE> <INDENT> self . is_pm = True <NEWLINE> <DEDENT> for s in self . subscribers : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> s . handle ( self ) <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def cross ( self , other ) : <COMMENT> <NEWLINE> <INDENT> return vector ( self . y * other . z - self . z * other . y , self . z * other . x - self . x * other . z , self . x * other . y - self . y * other . x ) <NEWLINE> <DEDENT>
def load_salts ( f ) : <NEWLINE> <INDENT> if hasattr ( f , <STRING> ) : <NEWLINE> <INDENT> return json . loads ( rot13 ( f . read ( ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( f ) as true_f : <NEWLINE> <INDENT> return load_salts ( true_f ) <NEWLINE> def dump_salts ( f , salts ) : <NEWLINE> <DEDENT> <DEDENT> payload = rot13 ( json . dumps ( salts , sort_keys = True , indent = <STRING> ) ) <NEWLINE> if hasattr ( f , <STRING> ) : <NEWLINE> <INDENT> f . write ( payload ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( f , <STRING> ) as true_f : <NEWLINE> <INDENT> true_f . write ( payload ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _createReservationErrback ( self , error , function_name , uuid ) : <NEWLINE> <INDENT> LOGGER . error ( <STRING> % ( function_name , uuid , error ) ) <NEWLINE> return error <NEWLINE> <DEDENT>
def callimpl ( self ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> noiseflag = self . noiseflagreg . value <NEWLINE> if not self . toneflagreg . value : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . tone ( self . block ) ) <NEWLINE> if not noiseflag : <NEWLINE> <INDENT> self . blockbuf . orbuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> <DEDENT> elif not noiseflag : <NEWLINE> <INDENT> self . blockbuf . copybuf ( self . noise ( self . block ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . blockbuf . fill ( 0 ) <NEWLINE> <DEDENT> <DEDENT>
diff = now - fromTime <NEWLINE> <INDENT> for archive in header [ <STRING> ] : <NEWLINE> <INDENT> if archive [ <STRING> ] >= diff : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def merge ( new_values , default_values ) : <NEWLINE> <INDENT> nd = { } <NEWLINE> for key , value in default_values . items ( ) : <NEWLINE> <INDENT> nv = new_values . get ( key , None ) <NEWLINE> if isinstance ( value , dict ) and isinstance ( nv , dict ) : <NEWLINE> <INDENT> nd [ key ] = merge ( nv , value ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if nv is None : <NEWLINE> <INDENT> nd [ key ] = value <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> nd [ key ] = nv <NEWLINE> <DEDENT> <DEDENT> <DEDENT> for key , value in new_values . items ( ) : <NEWLINE> <INDENT> if key not in default_values : <NEWLINE> <INDENT> nd [ key ] = value <NEWLINE> <DEDENT> <DEDENT> return nd <NEWLINE> <DEDENT>
if may_charge : <NEWLINE> <INDENT> pot = ( min_per_interval / 60.0 ) * power <NEWLINE> max_nrg = max_batt * max_soc <NEWLINE> output_batt [ i ] = min ( input_batt [ i ] + pot , max_nrg ) <NEWLINE> <DEDENT>
if version is not None : <NEWLINE> <INDENT> versions = itertools . ifilter ( lambda k : k [ <STRING> ] == version , <NEWLINE> <INDENT> versions ) <NEWLINE> try : <NEWLINE> <DEDENT> metadata = sorted ( versions , key = lambda x : x [ <STRING> ] ) [ 0 ] <NEWLINE> for url in metadata [ <STRING> ] : <NEWLINE> <INDENT> fname = url . split ( <STRING> ) [ - 1 ] <NEWLINE> try : <NEWLINE> <INDENT> fobj = cStringIO . StringIO ( <NEWLINE> <INDENT> _get_from_repo ( <NEWLINE> <INDENT> repo_scheme , <NEWLINE> repo_url , <NEWLINE> fname , <NEWLINE> stream = True , <NEWLINE> headers = headers , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if version is not None : <NEWLINE> <INDENT> versions = itertools . ifilter ( lambda k : k [ <STRING> ] == version , <NEWLINE> <INDENT> versions ) <NEWLINE> try : <NEWLINE> <DEDENT> metadata = sorted ( versions , key = lambda x : list ( map ( int , x [ <STRING> ] . split ( <STRING> ) ) ) ) [ - 1 ] <NEWLINE> for url in metadata [ <STRING> ] : <NEWLINE> <INDENT> fname = url . split ( <STRING> ) [ - 1 ] <NEWLINE> try : <NEWLINE> <INDENT> fobj = cStringIO . StringIO ( <NEWLINE> <INDENT> _get_from_repo ( <NEWLINE> <INDENT> repo_scheme , <NEWLINE> repo_url , <NEWLINE> url , <NEWLINE> stream = True , <NEWLINE> headers = headers , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if d_line_stream : <NEWLINE> <INDENT> for lines in group_d_lines ( d_line_stream ) : <NEWLINE> <COMMENT> <NL> <INDENT> parts = slices ( lines [ 0 ] , [ 1 , 6 , 1 , 2 , 1 , 6 ] ) <NEWLINE> raw_d_descriptor = parts [ 1 ] <NEWLINE> d_descriptor_code = fxy2int ( raw_d_descriptor ) <NEWLINE> n_elements = int ( parts [ 3 ] ) <NEWLINE> actual_elements = len ( lines ) <NEWLINE> if n_elements != actual_elements : <NEWLINE> <INDENT> raise ValueError ( <STRING> % ( n_elements , actual_elements ) ) <NEWLINE> <DEDENT> constituent_codes = [ ] <NEWLINE> for line in lines : <NEWLINE> <INDENT> l_parts = slices ( line , [ 1 , 6 , 1 , 2 , 1 , 6 ] ) <NEWLINE> constituent_codes . append ( fxy2int ( l_parts [ 5 ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
num_clusters , num_taxa = state . leca_file . load_taxa ( file_name ) <NEWLINE> <INDENT> MCMD . print ( <STRING> . format ( num_taxa , num_clusters ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> _ , y3 , self . x3 = sig . lsim ( self . sys3 , U = uVector , T = self . t , <NEWLINE> <INDENT> X0 = [ self . r_x1 [ self . mirror . c_dp - 1 ] , self . Gen . r_Pm [ self . mirror . c_dp - 1 ] ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> Pmech = y2 - dwVec * self . Dt <COMMENT> <NEWLINE> <DEDENT>
wmclass , minimized = get_wm_class_and_state ( win ) <NEWLINE> <INDENT> dock = disp . intern_atom ( <STRING> ) <NEWLINE> if dock in ewmh . getWmWindowType ( win ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT>
async def __call__ ( self , scope : Scope , receive : Receive , send : Send ) -> None : <NEWLINE> <INDENT> if not self . _debug : <NEWLINE> <INDENT> await self . bugsnag_app ( scope , receive , send ) <NEWLINE> return <NEWLINE> <DEDENT> await self . app ( scope , receive , send ) <NEWLINE> <DEDENT>
if value_type == bool : <NEWLINE> <INDENT> value = value . strip ( ) . lower ( ) == <STRING> <NEWLINE> elif value_type == str : <NEWLINE> if value . startswith ( <STRING> ) : <NEWLINE> <INDENT> value_path = os . path . join ( self . base_path , value [ len ( <STRING> ) : ] ) <NEWLINE> with open ( value_path , <STRING> ) as value_file : <NEWLINE> <INDENT> value = value_file . read ( ) <NEWLINE> <DEDENT> if value_path . lower ( ) . endswith ( <STRING> ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> import pypandoc <NEWLINE> value = pypandoc . convert_text ( value , <STRING> , format = <STRING> ) <NEWLINE> value = value . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> elif value_type == list : <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if value . startswith ( <STRING> ) : <NEWLINE> <INDENT> value_path = os . path . join ( self . base_path , value [ len ( <STRING> ) : ] ) <NEWLINE> with open ( value_path , <STRING> ) as value_file : <NEWLINE> <INDENT> value = value_file . readlines ( ) <NEWLINE> value = filter ( lambda k : bool ( k ) , value ) <NEWLINE> value = list ( map ( lambda k : k . strip ( ) . replace ( <STRING> , <STRING> ) , value ) ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> value = value . split ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
subproxy_sn = subproxy_device . get_serial_number ( ) <NEWLINE> <INDENT> subproxy_identifier = <STRING> . format ( identifier , subproxy_sn ) <NEWLINE> device_identifiers [ subproxy_device ] = subproxy_identifier <NEWLINE> <DEDENT>
if depc . _use_globals : <NEWLINE> <INDENT> reg_preproc = __PREPROC_REGISTER__ [ depc . root ] <NEWLINE> reg_algos = __ALGO_REGISTER__ [ depc . root ] <NEWLINE> print ( <STRING> % len ( reg_preproc ) ) <NEWLINE> for args_ , kwargs_ in reg_preproc : <NEWLINE> <INDENT> depc . _register_prop ( * args_ , ** kwargs_ ) <NEWLINE> <DEDENT> print ( <STRING> % len ( reg_algos ) ) <NEWLINE> for args_ , kwargs_ in reg_algos : <NEWLINE> <INDENT> depc . _register_algo ( * args_ , ** kwargs_ ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> superkey_colnames_list = db . get_table_superkey_colnames ( tablename ) <NEWLINE> try : <NEWLINE> <INDENT> superkey_paramxs_list = [ <NEWLINE> <INDENT> [ column_names_ . index ( str ( superkey ) ) <NEWLINE> <INDENT> for superkey in superkey_colnames ] <NEWLINE> <DEDENT> for superkey_colnames in superkey_colnames_list <NEWLINE> <DEDENT> ] <NEWLINE> <DEDENT> except Exception as ex : <NEWLINE> <INDENT> ut . printex ( ex , keys = [ <STRING> , <STRING> ] ) <NEWLINE> raise <NEWLINE> <DEDENT> if len ( superkey_colnames_list ) > 1 : <NEWLINE> <COMMENT> <NL> <INDENT> primary_superkey = db . get_metadata_val ( <NEWLINE> <INDENT> tablename + <STRING> , eval_ = True , default = None ) <NEWLINE> <DEDENT> if primary_superkey is None : <NEWLINE> <INDENT> raise AssertionError ( <NEWLINE> <INDENT> ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) % ( <NEWLINE> <INDENT> tablename , superkey_colnames_list ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> superkey_index = superkey_colnames_list . index ( primary_superkey ) <NEWLINE> superkey_paramx = superkey_paramxs_list [ superkey_index ] <NEWLINE> superkey_colnames = superkey_colnames_list [ superkey_index ] <NEWLINE> <DEDENT> <DEDENT> elif len ( superkey_colnames_list ) == 1 : <NEWLINE> <INDENT> superkey_paramx = superkey_paramxs_list [ 0 ] <NEWLINE> superkey_colnames = superkey_colnames_list [ 0 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> superkey_paramx = superkey_paramxs_list [ 0 ] <NEWLINE> superkey_colnames = superkey_colnames_list [ 0 ] <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT>
slot_value_node = ElementTree . SubElement ( slot_node , <STRING> , { <STRING> : self . type } ) <NEWLINE> <INDENT> if self . type == <STRING> : <NEWLINE> <INDENT> ElementTree . SubElement ( slot_value_node , <STRING> ) . text = datetime . strftime ( self . value , <STRING> ) <NEWLINE> <DEDENT> elif self . type == <STRING> : <NEWLINE> <INDENT> slot_value_node . text = self . value <NEWLINE> <DEDENT> elif self . type in [ <STRING> , <STRING> ] : <NEWLINE> <INDENT> slot_value_node . text = str ( self . value ) <NEWLINE> <DEDENT> elif type ( self . value ) is list and self . value : <NEWLINE> <INDENT> for sub_slot in self . value : <NEWLINE> <INDENT> slot_value_node . append ( sub_slot . as_xml ) <NEWLINE> <DEDENT> <DEDENT> elif self . type == <STRING> : <NEWLINE> <INDENT> pass <COMMENT> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise NotImplementedError ( <STRING> . format ( self . type ) ) <NEWLINE> <DEDENT> <DEDENT>
def grunt_config ( self , config = None , key = None ) : <NEWLINE> <INDENT> return grunt_conf ( <NEWLINE> <INDENT> config = { } if config is None else config , <NEWLINE> key = key if key is not None else self . grunt_config_key ) <NEWLINE> <DEDENT> <DEDENT>
if offset + length <= len ( self . _leaf . data ) : <NEWLINE> <INDENT> self . _offset += length <NEWLINE> return self . _leaf . data [ offset : offset + length ] <NEWLINE> <DEDENT>
def bootstrap_repl ( which_ns : str ) -> types . ModuleType : <NEWLINE> <INDENT> <STRING> <NEWLINE> repl_ns = runtime . Namespace . get_or_create ( sym . symbol ( REPL_NS ) ) <NEWLINE> ns = runtime . Namespace . get_or_create ( sym . symbol ( which_ns ) ) <NEWLINE> core_ns = runtime . Namespace . get ( sym . symbol ( runtime . CORE_NS ) ) <NEWLINE> assert core_ns is not None <NEWLINE> ns . refer_all ( core_ns ) <NEWLINE> repl_module = importlib . import_module ( REPL_NS ) <NEWLINE> ns . add_alias ( repl_ns , sym . symbol ( REPL_NS ) ) <NEWLINE> ns . refer_all ( repl_ns ) <NEWLINE> return repl_module <NEWLINE> <DEDENT>
foo_ns_sym = sym . symbol ( <STRING> ) <NEWLINE> <INDENT> foo_ns = get_or_create_ns ( foo_ns_sym ) <NEWLINE> ns . add_alias ( foo_ns , sym . symbol ( <STRING> ) ) <NEWLINE> assert sym . symbol ( <NEWLINE> <INDENT> <STRING> , ns = foo_ns_sym . name <NEWLINE> <DEDENT> ) == runtime . resolve_alias ( sym . symbol ( <STRING> , ns = <STRING> ) , ns = ns ) <NEWLINE> <DEDENT>
startDate = FinDate ( 1 , 1 , y ) <NEWLINE> <INDENT> dd = dt . _excelDate - startDate . _excelDate + 1 <NEWLINE> weekday = dt . _weekday <NEWLINE> <DEDENT>
liborCurve = FinIborSingleCurve ( valuationDate , depos , fras , swaps ) <NEWLINE>
<COMMENT> <NL> <INDENT> self . assertEqual ( len ( s . workers ) , 2 ) <NEWLINE> count = 0 <NEWLINE> while proxy1 . status ( ) != <STRING> and count < 10 : <NEWLINE> <INDENT> time . sleep ( 0.1 ) <NEWLINE> count += 1 <NEWLINE> <DEDENT> <DEDENT>
class Task ( TaskModel ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , job , frame , parent_task = None , state = None , <NEWLINE> <INDENT> priority = None , attempts = None , agent = None ) : <NEWLINE> <COMMENT> <NL> if modelfor ( job , TABLE_JOB ) : <NEWLINE> jobid = job . jobid <NEWLINE> if jobid is None : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> elif isinstance ( job , int ) : <NEWLINE> jobid = job <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if attempts is not None : <NEWLINE> <INDENT> self . attempts = attempts <NEWLINE> <DEDENT>
def skip ( should_skip , reason ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def wrapper ( func ) : <NEWLINE> <INDENT> @ wraps ( func ) <NEWLINE> def wrapped_func ( * args , ** kwargs ) : <NEWLINE> <INDENT> if should_skip : <NEWLINE> <INDENT> raise SkipTest ( reason ) <NEWLINE> <DEDENT> return func ( * args , ** kwargs ) <NEWLINE> <DEDENT> return wrapped_func <NEWLINE> <DEDENT> return wrapper <NEWLINE> <DEDENT>
if environment is not None and not isinstance ( environment , dict ) : <NEWLINE> <INDENT> raise TypeError ( <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> jobtype_loader = JobType . load ( request_data ) <NEWLINE> jobtype_loader . addCallback ( loaded_jobtype , assignment_uuid ) <NEWLINE> jobtype_loader . addErrback ( assignment_failed , assignment_uuid ) <NEWLINE> <DEDENT>
def assertGreaterEqual ( self , a , b , msg = None ) : <NEWLINE> <INDENT> if not a >= b : <NEWLINE> <INDENT> self . fail ( <NEWLINE> <INDENT> self . _formatMessage ( <NEWLINE> <INDENT> msg , <STRING> % ( a , b ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if existing_task_ids == new_task_ids : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> request . setResponseCode ( ACCEPTED ) <NEWLINE> request . write ( dumps ( { <STRING> : assignment [ <STRING> ] } ) ) <NEWLINE> request . finish ( ) <NEWLINE> return NOT_DONE_YET <NEWLINE> <COMMENT> <NL> <DEDENT> elif existing_task_ids & new_task_ids : <NEWLINE> <INDENT> logger . error ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> unknown_task_ids = new_task_ids - existing_task_ids <NEWLINE> request . setResponseCode ( CONFLICT ) <NEWLINE> request . write ( dumps ( <NEWLINE> <INDENT> { <STRING> : <STRING> , <NEWLINE> <INDENT> <STRING> : list ( unknown_task_ids ) } ) ) <NEWLINE> <DEDENT> <DEDENT> request . finish ( ) <NEWLINE> return NOT_DONE_YET <NEWLINE> <DEDENT> <DEDENT>
if transitions : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT>
def lexeme ( self , lemma_root_str , lemma_root_syntactic_category = None , lemma_root_secondary_syntactic_category = None ) : <NEWLINE> <INDENT> self . lemma_root_str = lemma_root_str <NEWLINE> self . lemma_root_syntactic_category = lemma_root_syntactic_category <NEWLINE> self . lemma_root_secondary_syntactic_category = lemma_root_secondary_syntactic_category <NEWLINE> <DEDENT>
for sample in samples : <NEWLINE> <INDENT> output . write ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> . format ( samp_reads = amplicon_coverage [ amplicon ] [ sample ] , <NEWLINE> <INDENT> s_perc1 = amplicon_coverage [ amplicon ] [ <STRING> . format ( sample , config [ <STRING> ] ) ] , <NEWLINE> s_perc2 = amplicon_coverage [ amplicon ] [ <STRING> . format ( sample , config [ <STRING> ] ) ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
kids = [ ] <NEWLINE> <INDENT> for workerklass in worker . workers : <NEWLINE> <INDENT> pid = os . fork ( ) <NEWLINE> kids . append ( pid ) <NEWLINE> if pid == 0 : <NEWLINE> <INDENT> sys . exit ( workerklass ( statsd , opts . interval ) . run ( ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def write ( self , * DAT , FC = 16 , ADR = 0 ) : <NEWLINE> <INDENT> if FC < 5 : return ( self . fc ( ) ) <NEWLINE> lADR = ADR & 0x00FF <NEWLINE> mADR = ADR >> 8 <NEWLINE> VAL = <STRING> <NEWLINE> for i in DAT : <NEWLINE> <INDENT> VAL = VAL + pack ( <STRING> , i ) <NEWLINE> <DEDENT> if FC == 5 or FC == 6 : <NEWLINE> <INDENT> VAL = VAL [ 0 : 2 ] <NEWLINE> <DEDENT> if FC == 5 or FC == 15 : <NEWLINE> <INDENT> LEN = len ( VAL ) * 8 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> LEN = len ( VAL ) // 2 <NEWLINE> <DEDENT> lLEN = LEN & 0x00FF <NEWLINE> mLEN = LEN >> 8 <NEWLINE> if self . TID < 255 : <NEWLINE> <INDENT> self . TID = self . TID + 1 <NEWLINE> <DEDENT> else : self . TID = 1 <NEWLINE> if FC == 6 : <NEWLINE> <INDENT> cmd = array ( <STRING> , [ 0 , self . TID , 0 , 0 , 0 , 6 , self . unit , FC , mADR , lADR ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> cmd = array ( <STRING> , [ 0 , self . TID , 0 , 0 , 0 , 7 + len ( VAL ) , self . unit , FC , mADR , lADR , mLEN , lLEN , len ( VAL ) ] ) <NEWLINE> <DEDENT> cmd . extend ( VAL ) <NEWLINE> buffer = array ( <STRING> , [ 0 ] * 20 ) <NEWLINE> print ( <STRING> , cmd ) <NEWLINE> self . sock . send ( cmd ) <NEWLINE> self . sock . recv_into ( buffer ) <NEWLINE> <DEDENT>
def event ( self , ev ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . disabled or not self . visible : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>
matches_in_time_slice = 0 <NEWLINE> <INDENT> try : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> for _ in takewhile ( lambda x : matches_in_time_slice <= matches_per_time_slice , range ( matches_per_time_slice ) ) : <NEWLINE> <INDENT> for tier in Tier : <NEWLINE> <INDENT> for player_id , _ in zip ( players_to_analyze . consume ( tier ) , range ( 10 ) ) : <NEWLINE> <INDENT> match_list = get_match_list ( player_id , begin_time = time_slice . begin , end_time = time_slice . end , ranked_queues = queue . name ) <NEWLINE> for match in match_list . matches : <NEWLINE> <INDENT> match_id = match . matchId <NEWLINE> if not match_id in downloaded_matches_by_tier [ tier ] and match_id > minimum_match_id : <NEWLINE> <INDENT> matches_to_download_by_tier [ tier ] . add ( match_id ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if conf [ <STRING> ] . lower ( ) == LATEST and get_patch_changed ( ) : <NEWLINE> <INDENT> analyzed_players = set ( ) <NEWLINE> downloaded_matches = set ( ) <NEWLINE> <DEDENT> <DEDENT>
def read ( self , size = 1 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not self . hComPort : raise portNotOpenError <NEWLINE> if size > 0 : <NEWLINE> <INDENT> win32 . ResetEvent ( self . _overlappedRead . hEvent ) <NEWLINE> flags = win32 . DWORD ( ) <NEWLINE> comstat = win32 . COMSTAT ( ) <NEWLINE> if not win32 . ClearCommError ( self . hComPort , ctypes . byref ( flags ) , ctypes . byref ( comstat ) ) : <NEWLINE> <INDENT> raise SerialException ( <STRING> ) <NEWLINE> <DEDENT> if self . timeout == 0 : <NEWLINE> <INDENT> n = min ( comstat . cbInQue , size ) <NEWLINE> if n > 0 : <NEWLINE> <INDENT> buf = ctypes . create_string_buffer ( n ) <NEWLINE> rc = win32 . DWORD ( ) <NEWLINE> err = win32 . ReadFile ( self . hComPort , buf , n , ctypes . byref ( rc ) , ctypes . byref ( self . _overlappedRead ) ) <NEWLINE> if not err and win32 . GetLastError ( ) != win32 . ERROR_IO_PENDING : <NEWLINE> <INDENT> raise SerialException ( <STRING> % ctypes . WinError ( ) ) <NEWLINE> <DEDENT> err = win32 . WaitForSingleObject ( self . _overlappedRead . hEvent , win32 . INFINITE ) <NEWLINE> read = buf . raw [ : rc . value ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> read = bytes ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> buf = ctypes . create_string_buffer ( size ) <NEWLINE> rc = win32 . DWORD ( ) <NEWLINE> err = win32 . ReadFile ( self . hComPort , buf , size , ctypes . byref ( rc ) , ctypes . byref ( self . _overlappedRead ) ) <NEWLINE> if not err and win32 . GetLastError ( ) != win32 . ERROR_IO_PENDING : <NEWLINE> <INDENT> raise SerialException ( <STRING> % ctypes . WinError ( ) ) <NEWLINE> <DEDENT> err = win32 . GetOverlappedResult ( self . hComPort , ctypes . byref ( self . _overlappedRead ) , ctypes . byref ( rc ) , True ) <NEWLINE> read = buf . raw [ : rc . value ] <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> read = bytes ( ) <NEWLINE> <DEDENT> return bytes ( read ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if key < vector [ 0 ] : <NEWLINE> <INDENT> return 0 <NEWLINE> <DEDENT> <DEDENT>
dictionary = parser ( d , { <NEWLINE> <INDENT> <STRING> : sys [ <STRING> ] [ <STRING> ] , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : sys [ <STRING> ] [ <STRING> ] , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : <STRING> , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : sys [ <STRING> ] , <NEWLINE> <STRING> : recovered , <NEWLINE> <STRING> : file_hash <NEWLINE> } ) <NEWLINE> <DEDENT>
if self . connection_string is not None : <NEWLINE> <INDENT> if self . adaptor . __name__ == <STRING> : <NEWLINE> <INDENT> self . pool = importlib . import_module ( <STRING> ) . ThreadedConnectionPool ( <NEWLINE> <INDENT> self . minconn or 1 , self . maxconn or 1 , self . connection_string or <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> self . connection = self . pool . getconn ( key = self . poolkey ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . connection = self . adaptor . connect ( self . connection_string or <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
bigside = int ( max ( width_old , height_old ) * 1.5 ) <NEWLINE> <INDENT> background = Image . new ( <STRING> , ( bigside , bigside ) , ( 255 , 255 , 255 , 255 ) ) <NEWLINE> offset = ( 0 , 0 ) <NEWLINE> background . paste ( image , offset ) <NEWLINE> file_name2 = <STRING> <NEWLINE> save_image = os . path . join ( cwd , file_name2 ) <NEWLINE> save_image_in_data = os . path . join ( path_save , file_name2 ) <NEWLINE> <DEDENT>
release = repo . create_git_release ( <NEWLINE> <INDENT> tag = tag , <NEWLINE> name = name , <NEWLINE> message = message , <NEWLINE> target_commitish = target , <NEWLINE> prerelease = prerelease , <NEWLINE> ) <NEWLINE> <DEDENT>
def run_local_GPU ( self , folders_glob ) : <NEWLINE> <INDENT> bash_cmd = <STRING> <NEWLINE> if len ( glob ( folders_glob ) ) > ( self . ngpus - self . gpus_in_use ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( len ( glob ( folders_glob ) ) , self . ngpus - self . gpus_in_use ) ) <NEWLINE> <DEDENT> <DEDENT>
return np . vstack ( [ scores , winner_indices ] ) <NEWLINE>
<COMMENT> <NL> <INDENT> annotator = Annotator ( data , [ task1 , task2 ] ) <NEWLINE> annotator ( data . ids ) <NEWLINE> print ( annotator . annotated ) <NEWLINE> <DEDENT>
def refmac ( self , cycles ) : <NEWLINE> <INDENT> directory = self . job_directory ( <STRING> ) <NEWLINE> use_phases = self . args . unbiased and self . min_rwork > 0.35 <NEWLINE> job = Refmac ( self . args , directory , self . current_xyz , cycles , use_phases ) <NEWLINE> self . jobs [ self . cycle ] . append ( job ) <NEWLINE> self . current_hkl = job . hklout <NEWLINE> self . current_xyz = job . xyzout <NEWLINE> return job <NEWLINE> <DEDENT>
@ property <NEWLINE> <INDENT> def dir_ref ( self ) : <NEWLINE> <INDENT> return <STRING> . format ( self . name , self . version , self . arch ) <NEWLINE> <DEDENT> <DEDENT>
if filler >= checkpoint : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT>
def log ( page , message , level = WARNING ) : <NEWLINE> <INDENT> if level >= THRESHOLD : <NEWLINE> <INDENT> print ( <STRING> . format ( CRITICALITY [ level ] , message , page . url ) ) <NEWLINE> <DEDENT> <DEDENT>
ConditionalContainer ( <NEWLINE> <INDENT> get_hline ( ) , <NEWLINE> filter = ShowDefault ( ) | ShowSymbol ( ) <NEWLINE> ) , <NEWLINE> ConditionalContainer ( <NEWLINE> Window ( <NEWLINE> <INDENT> content = BufferControl ( <NEWLINE> <INDENT> buffer_name = <STRING> , <NEWLINE> lexer = lexer <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) , <NEWLINE> filter = ShowDefault ( ) <NEWLINE> ) , <NEWLINE> ConditionalContainer ( <NEWLINE> get_hline ( ) , <NEWLINE> filter = ShowDefault ( ) & ShowSymbol ( ) <NEWLINE> ) , <NEWLINE> ConditionalContainer ( <NEWLINE> Window ( <NEWLINE> <INDENT> content = BufferControl ( <NEWLINE> <INDENT> buffer_name = <STRING> , <NEWLINE> lexer = examLex <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> ) , <NEWLINE> filter = ShowSymbol ( ) <NEWLINE> ) , <NEWLINE> Window ( <NEWLINE> content = BufferControl ( <NEWLINE> <INDENT> buffer_name = <STRING> , <NEWLINE> lexer = toolbarLex <NEWLINE> <DEDENT> ) , <NEWLINE> ) , <NEWLINE> ] ) , <NEWLINE> filter = ~ IsDone ( ) & RendererHeightIsKnown ( ) <NEWLINE> ) <NEWLINE> <DEDENT>
func_dict [ toggle ] = ( func , key_args ) <NEWLINE>
def __rel_change ( self , new : float ) -> float : <NEWLINE> <INDENT> if self . _likelihoods : <NEWLINE> <INDENT> old = self . _likelihoods [ - 1 ] <NEWLINE> return abs ( ( new - old ) / new ) <NEWLINE> <DEDENT> return inf <NEWLINE> <DEDENT>
def register_middleware ( self , middleware , attach_to = None ) : <NEWLINE> <INDENT> if attach_to == <STRING> : <NEWLINE> <INDENT> self . req_middleware . append ( middleware ) <NEWLINE> <DEDENT> elif attach_to == <STRING> : <NEWLINE> <INDENT> self . res_middleware = [ middleware ] + self . res_middleware <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def join ( self , channel : str ) : <NEWLINE> <INDENT> channels = list ( self . channels ) <NEWLINE> if channel not in channels : <NEWLINE> <INDENT> self . __to_join . append ( ( channel , 0 , time . time ( ) ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . __warning ( <STRING> . format ( channel ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def __send ( self , packet , obfuscate_after = None , ignore_throttle = 0 ) : <NEWLINE> <COMMENT> <NL> <INDENT> if self . __anti_throttle ( ) or not ignore_throttle : <NEWLINE> <COMMENT> <NL> <INDENT> if self . __wait_for_status ( 0 ) : <NEWLINE> <INDENT> self . __socket . send ( packet . encode ( <STRING> ) ) <NEWLINE> self . __event_sent_date . append ( time . time ( ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> if obfuscate_after : <NEWLINE> <INDENT> packet_hidden = <STRING> * ( len ( packet ) - obfuscate_after ) <NEWLINE> packet = packet [ 0 : obfuscate_after ] + packet_hidden <NEWLINE> <COMMENT> <NL> <DEDENT> self . __packet_sent ( packet ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def __send_message ( self ) -> None : <NEWLINE> <COMMENT> <NL> <INDENT> if len ( self . __to_send ) > 0 and self . __wait_for_status ( ) and self . __anti_throttle ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> item = self . __to_send . pop ( 0 ) <NEWLINE> channel = item [ 0 ] <NEWLINE> message = item [ 1 ] <NEWLINE> <COMMENT> <NL> if channel not in self . channels : <NEWLINE> <INDENT> self . __warning ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> packet = <STRING> . format ( channel , message ) <NEWLINE> self . __send ( packet ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
next_task , next_instance = extract ( next ( task_file ) ) , extract ( next ( instance_file ) ) <NEWLINE> <INDENT> while next_task [ JOB_ID ] != <STRING> : <NEWLINE> <INDENT> arrive_at , task_lines , instance_lines = next_task [ ARR_TIME ] , [ next_task [ REST ] ] , [ next_instance [ REST ] ] <NEWLINE> next_task = read_lines ( task_file , next_task [ JOB_ID ] , task_lines ) <NEWLINE> next_instance = read_lines ( instance_file , next_instance [ JOB_ID ] , instance_lines ) <NEWLINE> yield float ( arrive_at ) , { <STRING> : task_lines , <STRING> : instance_lines } <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> if flags_obj . no_gpu or not tf . test . is_gpu_available ( ) : <NEWLINE> <INDENT> ( device , data_format ) = ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if flags_obj . data_format is not None : <NEWLINE> <INDENT> data_format = flags_obj . data_format <NEWLINE> <DEDENT> print ( <STRING> % ( device , data_format ) ) <NEWLINE> <DEDENT>
storage . import_blob ( id_ , open ( tmp_destination , <STRING> ) ) <NEWLINE> <INDENT> os . remove ( tmp_destination ) <NEWLINE> <DEDENT>
if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace_meta . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>
if not keyspace : <NEWLINE> <COMMENT> <NL> <INDENT> self . keyspaces = dict ( ( name , meta ) for name , meta in self . keyspaces . items ( ) <NEWLINE> <INDENT> if name in added_keyspaces ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> try : <NEWLINE> <DEDENT> keyspace_meta = self . keyspaces [ keyspace ] <NEWLINE> except KeyError : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> pass <NEWLINE> if keyspace in cf_def_rows : <NEWLINE> for table_row in cf_def_rows [ keyspace ] : <NEWLINE> <INDENT> table_meta = self . _build_table_metadata ( <NEWLINE> <INDENT> keyspace_meta , table_row , col_def_rows [ keyspace ] ) <NEWLINE> <DEDENT> keyspace_meta . tables [ table_meta . name ] = table_meta <NEWLINE> <DEDENT> <DEDENT>
def populate ( self , cluster , hosts ) : <NEWLINE> <INDENT> self . _live_hosts = set ( hosts ) <NEWLINE> if len ( hosts ) <= 1 : <NEWLINE> <INDENT> self . _position = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . _position = randint ( 0 , len ( hosts ) - 1 ) <NEWLINE> <DEDENT> <DEDENT>
if not issubclass ( klass , cls ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , poly_base . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if not issubclass ( klass , cls ) : <NEWLINE> <INDENT> raise PolyMorphicModelException ( <NEWLINE> <INDENT> <STRING> . format ( klass . __name__ , cls . __name__ ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
def on_read_timeout ( self , query , consistency , required_responses , <NEWLINE> <INDENT> received_responses , data_retrieved , retry_num ) : <NEWLINE> if retry_num != 0 : <NEWLINE> return ( self . RETHROW , None ) <NEWLINE> elif received_responses <= required_responses : <NEWLINE> return self . _pick_consistency ( received_responses ) <NEWLINE> elif not data_retrieved : <NEWLINE> return ( self . RETRY , consistency ) <NEWLINE> else : <NEWLINE> return ( self . RETHROW , None ) <NEWLINE> <DEDENT>
MultipleObjectsReturnedBase = MultipleObjectsReturnedBase or attrs . pop ( <STRING> , BaseModel . MultipleObjectsReturned ) <NEWLINE> <INDENT> attrs [ <STRING> ] = type ( <STRING> , ( MultipleObjectsReturnedBase , ) , { } ) <NEWLINE> <DEDENT>
def parse_2d_maze ( maze_str ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> lines = [ x . strip ( ) for x in maze_str . strip ( ) . split ( <STRING> ) ] <NEWLINE> num_rows = len ( lines ) <NEWLINE> if num_rows == 0 : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> num_cols = len ( lines [ 0 ] ) <NEWLINE> walls = [ ] <NEWLINE> start_pos = None <NEWLINE> end_pos = None <NEWLINE> for row , row_str in enumerate ( lines ) : <NEWLINE> <INDENT> if len ( row_str ) != num_cols : <NEWLINE> <INDENT> raise ValueError ( <STRING> % <NEWLINE> <INDENT> ( num_cols , len ( row ) ) ) <NEWLINE> <DEDENT> <DEDENT> sub_walls = [ ] <NEWLINE> for col , cell_str in enumerate ( row_str ) : <NEWLINE> <INDENT> if cell_str == <STRING> : <NEWLINE> <INDENT> sub_walls . append ( True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sub_walls . append ( False ) <NEWLINE> <DEDENT> if cell_str == <STRING> : <NEWLINE> <INDENT> if start_pos : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> start_pos = ( row , col ) <NEWLINE> <DEDENT> elif cell_str == <STRING> : <NEWLINE> <INDENT> if end_pos : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> end_pos = ( row , col ) <NEWLINE> <DEDENT> <DEDENT> walls . append ( sub_walls ) <NEWLINE> <DEDENT> return Maze ( np . array ( walls ) , start_pos = start_pos , end_pos = end_pos ) <NEWLINE> <DEDENT>
s3object = S3RawIO ( path ) <NEWLINE> <INDENT> assert s3object . _client_kwargs == client_args <NEWLINE> assert s3object . name == path <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if system . relpath ( src ) == system . relpath ( dst ) : <NEWLINE> <INDENT> raise same_file_error ( <NEWLINE> <INDENT> <STRING> % ( src , dst ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> for key , value in tuple ( header . items ( ) ) : <NEWLINE> <INDENT> stat [ <STRING> + key . lower ( ) . replace ( <STRING> , <STRING> ) ] = value <NEWLINE> <DEDENT> <DEDENT>
def test_generate_episode ( self ) : <NEWLINE> <INDENT> task = self . __setup_stub_task ( ) <NEWLINE> policy = GreedyPolicy ( ) <NEWLINE> value_func = self . __setup_stub_value_function ( ) <NEWLINE> episode = generate_episode ( task , policy , value_func ) <NEWLINE> self . eq ( 3 , len ( episode ) ) <NEWLINE> self . eq ( ( 0 , 1 , 1 , 1 ) , episode [ 0 ] ) <NEWLINE> self . eq ( ( 1 , 2 , 3 , 9 ) , episode [ 1 ] ) <NEWLINE> self . eq ( ( 3 , 4 , 7 , 49 ) , episode [ 2 ] ) <NEWLINE> <DEDENT>
def mark_as_active ( pathname ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for id_page in ( <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ) : <NEWLINE> <INDENT> if id_page in pathname : <NEWLINE> <INDENT> document [ id_page ] . class_name = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> document [ id_page ] . class_name = <STRING> <NEWLINE> <DEDENT> <DEDENT> if <STRING> in pathname : <NEWLINE> <INDENT> document [ <STRING> ] . class_name = <STRING> <NEWLINE> <DEDENT> <DEDENT>
def changeTemperature ( self , newTemperature ) : <NEWLINE> <COMMENT> <NL> <INDENT> if not isinstance ( newTemperature , int ) and not isinstance ( newTemperature , float ) : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if newTemperature < 180 : <NEWLINE> <INDENT> newTemperature = newTemperature * 10 <NEWLINE> <DEDENT> if ( newTemperature > 180 and newTemperature < 320 ) : <NEWLINE> <INDENT> self . adjust_temperature = newTemperature <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
related_title = anime_page . find ( <STRING> , text = <STRING> ) <NEWLINE> <INDENT> if related_title : <NEWLINE> <INDENT> related_elt = related_title . parent <NEWLINE> utilities . extract_tags ( related_elt . find_all ( <STRING> ) ) <NEWLINE> related = { } <NEWLINE> for link in related_elt . find_all ( <STRING> ) : <NEWLINE> <INDENT> curr_elt = link . previous_sibling <NEWLINE> if curr_elt is None : <NEWLINE> <COMMENT> <NL> <INDENT> break <NEWLINE> <DEDENT> related_type = None <NEWLINE> while True : <NEWLINE> <INDENT> if not curr_elt : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , related_elt , message = <STRING> ) <NEWLINE> <DEDENT> if isinstance ( curr_elt , bs4 . NavigableString ) : <NEWLINE> <INDENT> type_match = re . match ( <STRING> , curr_elt ) <NEWLINE> if type_match : <NEWLINE> <INDENT> related_type = type_match . group ( <STRING> ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> curr_elt = curr_elt . previous_sibling <NEWLINE> <COMMENT> <NL> <DEDENT> href_parts = link . get ( <STRING> ) . split ( <STRING> ) <NEWLINE> title = link . text <NEWLINE> obj_id = int ( href_parts [ 4 ] ) <NEWLINE> non_title_parts = href_parts [ : 5 ] <NEWLINE> if <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . manga ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> elif <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . anime ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , link , message = <STRING> ) <NEWLINE> <DEDENT> if related_type not in related : <NEWLINE> <INDENT> related [ related_type ] = [ new_obj ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> related [ related_type ] . append ( new_obj ) <NEWLINE> <DEDENT> <DEDENT> anime_info [ <STRING> ] = related <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> anime_info [ <STRING> ] = None <NEWLINE> <DEDENT> return anime_info <NEWLINE> <DEDENT>
def __init__ ( self , session , user_name ) : <NEWLINE> <INDENT> super ( MediaList , self ) . __init__ ( session ) <NEWLINE> self . username = user_name <NEWLINE> if not isinstance ( self . username , unicode ) or len ( self . username ) < 1 : <NEWLINE> <INDENT> raise InvalidMediaListError ( self . username ) <NEWLINE> <DEDENT> self . _list = None <NEWLINE> self . _stats = None <NEWLINE> <DEDENT>
if links or nodes : <NEWLINE> <INDENT> self . undostack . beginMacro ( <STRING> ) <NEWLINE> for link in links : <NEWLINE> <INDENT> self . undostack . push ( type ( link . startIO ) . DeleteLinkCommand ( link . startIO ) ) <NEWLINE> <DEDENT> <DEDENT>
db = str ( db ) if isinstance ( db , int ) else quote ( db . encode ( <STRING> ) ) <NEWLINE> <INDENT> path = <STRING> + db <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if round_num >= 158 : <NEWLINE> <INDENT> staking_bonus_perc = 0.05 <NEWLINE> bonus_nmr_only = df [ <STRING> ] * staking_bonus_perc <NEWLINE> bonus_split = df [ <STRING> ] - df [ <STRING> ] / ( 1 + staking_bonus_perc ) <NEWLINE> if <STRING> in df : <NEWLINE> <INDENT> df [ <STRING> ] = df [ <STRING> ] . astype ( float ) <NEWLINE> <DEDENT> if round_num == 158 : <NEWLINE> <INDENT> df [ <STRING> ] = bonus_nmr_only . where ( df [ <STRING> ] . isna ( ) , bonus_split ) <NEWLINE> df [ <STRING> ] = df [ <STRING> ] - df [ <STRING> ] / ( 1 + staking_bonus_perc ) <NEWLINE> df [ <STRING> ] = df [ <STRING> ] - df [ <STRING> ] <NEWLINE> df [ <STRING> ] -= bonus_nmr_only <NEWLINE> df [ <STRING> ] -= bonus_nmr_only <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> df [ <STRING> ] = bonus_nmr_only <NEWLINE> <DEDENT> if <STRING> in df : <NEWLINE> <INDENT> df [ <STRING> ] -= df [ <STRING> ] . fillna ( 0 ) <NEWLINE> return df <NEWLINE> return None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_comment ( func ) : <COMMENT> <NEWLINE> <INDENT> if not inspect . isfunction ( func ) and not inspect . ismethod ( func ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def parse_docstring ( func ) : <COMMENT> <NEWLINE> <INDENT> <STRING> <NEWLINE> if not inspect . isfunction ( func ) and not inspect . ismethod ( func ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def visit_BinOp ( self , node ) : <NEWLINE> <INDENT> node = self . generic_visit ( node ) <NEWLINE> left = node . left <NEWLINE> right = node . right <NEWLINE> if all ( isinstance ( value , ast . Num ) for value in ( left , right ) ) : <NEWLINE> <INDENT> if type ( node . op ) in self . _operators : <NEWLINE> <INDENT> val = self . _operators [ type ( node . op ) ] ( left . n , right . n ) <NEWLINE> node = ast . copy_location ( ast . Num ( n = val ) , node ) <NEWLINE> <DEDENT> <DEDENT> elif all ( isinstance ( value , ast . Str ) for value in ( left , right ) ) : <NEWLINE> <INDENT> if isinstance ( node . op , ast . Add ) : <NEWLINE> <INDENT> val = left . s + right . s <NEWLINE> node = ast . copy_location ( ast . Str ( s = val ) , node ) <NEWLINE> <DEDENT> <DEDENT> return node <NEWLINE> <DEDENT>
self . __addnode ( ports ) <NEWLINE>
if len ( rank ) == 0 : <COMMENT> <NEWLINE> <INDENT> rank = len ( results [ i ] ) <NEWLINE> else : <NEWLINE> rank = rank [ 0 ] <NEWLINE> all_rr . append ( rank ) <NEWLINE> <DEDENT>
for step in steps : <NEWLINE> <INDENT> learner = SVDPlusPlusLearner ( K = 2 , steps = step , alpha = 0.007 , <NEWLINE> <INDENT> random_state = 42 , verbose = False ) <NEWLINE> <DEDENT> recommender = learner ( data ) <NEWLINE> objectives . append ( <NEWLINE> <INDENT> recommender . compute_objective ( data = data , P = recommender . P , <NEWLINE> <INDENT> Q = recommender . Q , <NEWLINE> Y = recommender . Y , <NEWLINE> bias = recommender . bias , <NEWLINE> beta = learner . beta ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not is_system : <NEWLINE> <INDENT> extension = extensionRegistry . get_extension ( ext_name ) <NEWLINE> module_directory = extension . module_directory <NEWLINE> <COMMENT> <NL> if os . path . exists ( os . path . join ( module_directory , <STRING> ) ) : <NEWLINE> <INDENT> registerDirectory ( <STRING> , module_directory ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> pique . msg ( logfile , <STRING> + ipfile ) <NEWLINE> pique . msg ( logfile , <STRING> + bgfile ) <NEWLINE> pique . msg ( logfile , <STRING> + mapfile ) <NEWLINE> pique . msg ( logfile , <STRING> + str ( alpha ) ) <NEWLINE> pique . msg ( logfile , <STRING> + str ( l_thresh ) ) <NEWLINE> <DEDENT>
def has_failed ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return self . _result is not True <NEWLINE> <DEDENT>
sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) <NEWLINE> <INDENT> start_at = monotonic . monotonic ( ) <NEWLINE> while True : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> sock . connect ( <NEWLINE> <INDENT> ( self . node_definition . host , self . node_definition . port ) ) <NEWLINE> <DEDENT> <DEDENT> except socket . error as e : <NEWLINE> <INDENT> time . sleep ( 0.5 ) <COMMENT> <NEWLINE> if monotonic . monotonic ( ) - start_at > timeout : <NEWLINE> <INDENT> raise ConnectionDead ( ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def span_finished ( self ) : <NEWLINE> <INDENT> if self . processing_span is not None : <NEWLINE> <INDENT> self . processing_span . finish ( ) <NEWLINE> self . processing_span = None <NEWLINE> <DEDENT> <DEDENT>
while True : <NEWLINE> <COMMENT> <NL> <INDENT> if remote_stdout_connected == False and file_transfer_mode == False : <NEWLINE> <COMMENT> <NL> <INDENT> if sender_fifo_q . empty ( ) == False : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> sender_fifo_q = asyncio . Queue ( ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT>
def show_ddt ( request ) : <NEWLINE> <INDENT> if request . user . is_authenticated : <NEWLINE> <INDENT> if request . path in ignored : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT> return False <NEWLINE> <DEDENT>
def show_ddt ( request ) : <NEWLINE> <INDENT> if request . user . is_authenticated : <NEWLINE> <INDENT> if request . path in ignored : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT>
N = np . shape ( data_frame ) [ 1 ] <NEWLINE> <INDENT> for j in range ( N ) : <NEWLINE> <INDENT> for k in range ( N ) : <NEWLINE> <INDENT> if j != k : <NEWLINE> <INDENT> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sns . despine ( ax = pairgrid . axes [ i , k ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
N = np . shape ( data_frame ) [ 1 ] <NEWLINE> <INDENT> for i in range ( N ) : <NEWLINE> <INDENT> for j in range ( N ) : <NEWLINE> <INDENT> for k in range ( N ) : <NEWLINE> <INDENT> if i != k : <NEWLINE> <INDENT> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> pairgrid . axes [ i , k ] . spines [ <STRING> ] . set_visible ( True ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sns . despine ( ax = pairgrid . axes [ i , k ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
y = 0 <NEWLINE> <INDENT> if p . y < 0 : <NEWLINE> <INDENT> y = 0 <NEWLINE> <DEDENT> elif p . y > rect . height : <NEWLINE> <INDENT> y = rect . height <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> y = p . y <NEWLINE> <DEDENT> <DEDENT>
if self . _statistics is not None : <NEWLINE> <INDENT> self . _statistics . output_path = self . _statistics_dir + os . path . sep + <STRING> <NEWLINE> self . _statistics . generate ( data ) <NEWLINE> self . _statistics . save ( ) <NEWLINE> <DEDENT>
def create_real_first_image ( self , path = <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> apcor_str = <STRING> <NEWLINE> with open ( self . get_abs_path ( path ) , <STRING> ) as fh : <NEWLINE> <INDENT> self . first_image = DownloadedFitsImage ( fh . read ( ) , Mock ( ) , apcor_str , in_memory = True ) <NEWLINE> first_reading = self . model . get_current_workunit ( ) . get_sources ( ) [ 0 ] . get_readings ( ) [ 0 ] <NEWLINE> self . model . _on_image_loaded ( first_reading , self . first_image ) <NEWLINE> <DEDENT> <DEDENT>
if method_name in type_def . CANCEL_MAPPING : <NEWLINE> <INDENT> meta . cancellable = True <NEWLINE> <DEDENT>
related_title = anime_page . find ( <STRING> , text = <STRING> ) <NEWLINE> <INDENT> if related_title : <NEWLINE> <INDENT> related_elt = related_title . parent <NEWLINE> utilities . extract_tags ( related_elt . find_all ( <STRING> ) ) <NEWLINE> related = { } <NEWLINE> for link in related_elt . find_all ( <STRING> ) : <NEWLINE> <INDENT> curr_elt = link . previous_sibling <NEWLINE> if curr_elt is None : <NEWLINE> <COMMENT> <NL> <INDENT> break <NEWLINE> <DEDENT> related_type = None <NEWLINE> while True : <NEWLINE> <INDENT> if not curr_elt : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , related_elt , message = <STRING> ) <NEWLINE> <DEDENT> if isinstance ( curr_elt , bs4 . NavigableString ) : <NEWLINE> <INDENT> type_match = re . match ( <STRING> , curr_elt ) <NEWLINE> if type_match : <NEWLINE> <INDENT> related_type = type_match . group ( <STRING> ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> curr_elt = curr_elt . previous_sibling <NEWLINE> <COMMENT> <NL> <DEDENT> href_parts = link . get ( <STRING> ) . split ( <STRING> ) <NEWLINE> title = link . text <NEWLINE> obj_id = int ( href_parts [ 4 ] ) <NEWLINE> non_title_parts = href_parts [ : 5 ] <NEWLINE> if <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . manga ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> elif <STRING> in non_title_parts : <NEWLINE> <INDENT> new_obj = self . session . anime ( obj_id ) . set ( { <STRING> : title } ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise MalformedAnimePageError ( self , link , message = <STRING> ) <NEWLINE> <DEDENT> if related_type not in related : <NEWLINE> <INDENT> related [ related_type ] = [ new_obj ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> related [ related_type ] . append ( new_obj ) <NEWLINE> <DEDENT> <DEDENT> anime_info [ <STRING> ] = related <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> anime_info [ <STRING> ] = None <NEWLINE> <DEDENT> return anime_info <NEWLINE> <DEDENT>
def main ( ) : <NEWLINE> <INDENT> if len ( sys . argv ) < 2 : <NEWLINE> <INDENT> usage ( sys . argv [ 0 ] ) <NEWLINE> sys . exit ( 1 ) <NEWLINE> <DEDENT> duration = int ( sys . argv [ 1 ] ) <NEWLINE> logpath = LOGPATH <NEWLINE> if len ( sys . argv ) >= 3 : <NEWLINE> <INDENT> logpath = sys . argv [ 2 ] <NEWLINE> <DEDENT> loader = TailLoader ( logpath , duration ) <NEWLINE> censor = Censor ( ) <NEWLINE> eater = MailLogEater ( ) <NEWLINE> for rawline in loader . readlines ( ) : <NEWLINE> <INDENT> line = censor . censor ( rawline ) <NEWLINE> eater . eat ( line ) <NEWLINE> <DEDENT> print ( eater ) <NEWLINE> <DEDENT>
shape = ( ) <NEWLINE> <INDENT> for i , ( left_element , right_element ) in enumerate ( zip ( node . left_node . shape , node . right_node . shape ) ) : <NEWLINE> <INDENT> if is_symbolic_element ( left_element ) and is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> conditions . append ( BinaryNode ( MOANodeTypes . EQUAL , ( ) , left_element , right_element ) ) <NEWLINE> shape = shape + ( left_element , ) <NEWLINE> <DEDENT> elif is_symbolic_element ( left_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( right_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . EQUAL , ( ) , left_element , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) ) ) <NEWLINE> shape = shape + ( right_element , ) <NEWLINE> <DEDENT> elif is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . EQUAL , ( ) , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) , right_element ) ) <NEWLINE> shape = shape + ( left_element , ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> if left_element != right_element : <NEWLINE> <INDENT> raise MOAShapeException ( <STRING> ) <NEWLINE> <DEDENT> shape = shape + ( left_element , ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
conditions = [ ] <NEWLINE> <INDENT> for i , ( left_element , right_element ) in enumerate ( zip ( left_symbol_node . value , node . right_node . shape ) ) : <NEWLINE> <INDENT> if is_symbolic_element ( left_element ) and is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> conditions . append ( BinaryNode ( MOANodeTypes . LESSTHAN , ( ) , left_element , right_element ) ) <NEWLINE> <DEDENT> elif is_symbolic_element ( left_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( right_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . LESSTHAN , ( ) , left_element , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) ) ) <NEWLINE> <DEDENT> elif is_symbolic_element ( right_element ) : <COMMENT> <NEWLINE> <INDENT> array_name = generate_unique_array_name ( symbol_table ) <NEWLINE> symbol_table = add_symbol ( symbol_table , array_name , MOANodeTypes . ARRAY , ( ) , ( left_element , ) ) <NEWLINE> conditions . append ( BinaryNode ( MOANodeTypes . LESSTHAN , ( ) , ArrayNode ( MOANodeTypes . ARRAY , ( ) , array_name ) , right_element ) ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> if left_element >= right_element : <NEWLINE> <INDENT> raise MOAShapeException ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def paint_path ( self , graphicstate , stroke , fill , evenodd , path ) : <NEWLINE> <COMMENT> <NL> <INDENT> device_path = [ ] <NEWLINE> for segment in path : <NEWLINE> <INDENT> coords = iter ( segment [ 1 : ] ) <NEWLINE> for x in coords : <COMMENT> <NEWLINE> <INDENT> y = next ( coords ) <COMMENT> <NEWLINE> device_path . append ( <NEWLINE> <INDENT> ( segment [ 0 ] , ) <NEWLINE> + pdfminer . utils . apply_matrix_pt ( self . ctm , ( x , y ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . page . add_shape ( content . Shape ( stroke , fill , evenodd , device_path ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> log_corr_obs_matrices = [ ( log_obs_matrix . T - bias_factors ) . T - bias_factors <NEWLINE> <INDENT> for log_obs_matrix in log_obs_matrices ] <NEWLINE> <DEDENT> <DEDENT>
def run ( parser , args , conn_config ) : <NEWLINE> <INDENT> if args . sample_name is None : <NEWLINE> <INDENT> args . sample_name = os . path . basename ( args . kmer_file ) . split ( <STRING> ) [ 0 ] <NEWLINE> <DEDENT> mc = McDBG ( conn_config = conn_config ) <NEWLINE> try : <NEWLINE> <INDENT> colour = mc . add_sample ( args . sample_name ) <NEWLINE> with open ( args . kmer_file , <STRING> ) as inf : <NEWLINE> <INDENT> kmers = [ ] <NEWLINE> for i , line in enumerate ( inf ) : <NEWLINE> <INDENT> kmer = line . strip ( ) <NEWLINE> kmers . append ( kmer ) <NEWLINE> if i % 100000 == 0 : <NEWLINE> <INDENT> mc . set_kmers ( kmers , colour ) <NEWLINE> kmers = [ ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> mc . set_kmers ( kmers , colour ) <NEWLINE> <DEDENT> <DEDENT>
@ hug . object . cli <NEWLINE> <INDENT> @ hug . object . get ( <STRING> , examples = <STRING> ) <NEWLINE> def search ( self , seq : hug . types . text = None , fasta_file : hug . types . text = None , threshold : hug . types . float_number = 1.0 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not seq or not fasta_file : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT> return search ( seq = seq , <NEWLINE> <INDENT> fasta_file = fasta_file , threshold = threshold , conn_config = CONN_CONFIG ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def build ( bloomfilter_filepaths , samples , graph ) : <NEWLINE> <INDENT> bloomfilters = [ ] <NEWLINE> for f in bloomfilter_filepaths : <NEWLINE> <INDENT> bloomfilters . append ( load_bloomfilter ( f ) ) <NEWLINE> <DEDENT> graph . build ( bloomfilters , samples ) <NEWLINE> return { <STRING> : <STRING> } <NEWLINE> <DEDENT>
def pull ( self , block_size , overlap = 0 , pad = False ) : <NEWLINE> <INDENT> if overlap and overlap > block_size : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , title = <STRING> , can_save = True , <NEWLINE> <INDENT> has_config = True , widget = None ) : <NEWLINE> <COMMENT> <NL> self . TITLE = title <NEWLINE> <COMMENT> <NL> self . CAN_SAVE = can_save <NEWLINE> <COMMENT> <NL> self . HAS_CONFIG = has_config <NEWLINE> <COMMENT> <NL> self . widget = widget <NEWLINE> <DEDENT>
def _poll ( self ) : <NEWLINE> <INDENT> for item in self . coins : <NEWLINE> <INDENT> value = self . wrapper . handle ( dict ( self . config . items ( item ) ) ) <NEWLINE> if value : <NEWLINE> <INDENT> setattr ( self , item , value ) <NEWLINE> self . fields . add ( item ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> params = { } <NEWLINE> for paramstr in params_strings : <NEWLINE> <INDENT> if <STRING> not in paramstr : <NEWLINE> <INDENT> raise ParseError ( <STRING> . format ( paramstr ) ) <NEWLINE> <DEDENT> pname , pvals = paramstr . split ( <STRING> , 1 ) <NEWLINE> params [ pname ] = pvals . split ( <STRING> ) <NEWLINE> <DEDENT> return cls ( name , params , value ) <NEWLINE> <DEDENT>
def get_message ( self , code ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> resource_code = self . map . get ( code ) <NEWLINE> if not resource_code : <NEWLINE> <INDENT> if code != 0 : <NEWLINE> <INDENT> warnings . warn ( <STRING> , DeprecationWarning ) <NEWLINE> <DEDENT> return <STRING> <NEWLINE> <DEDENT> return resource_code . get_message ( ) <NEWLINE> <DEDENT>
def upload_aliyun_oss ( folder ) : <NEWLINE> <INDENT> if not hasattr ( settings , <STRING> ) : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> AccessKeyId = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> AccessKeySecret = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> Endpoint = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> BucketName = settings . ALIYUN_OSS [ <STRING> ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> first_times = [ i [ 0 ] [ 0 ] for i in measurements . values ( ) if i and i [ 0 ] ] <NEWLINE> last_times = [ i [ - 1 ] [ 0 ] for i in measurements . values ( ) if i and i [ - 1 ] ] <NEWLINE> if not ( first_times and last_times ) : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> t_0 = min ( first_times ) <NEWLINE> t_max = max ( last_times ) <NEWLINE> steps = int ( math . ceil ( ( t_max - t_0 ) / self . step ) ) <NEWLINE> <DEDENT>
def disable_log ( self , val = None ) : <NEWLINE> <INDENT> if val is not None and val == self . settings . LOG_SCALE : <NEWLINE> <INDENT> self . settings . LOG_SCALE = not val <NEWLINE> self . update ( ) <NEWLINE> <DEDENT> return not self . settings . LOG_SCALE <NEWLINE> <DEDENT>
yield b , settings <NEWLINE>
parts = [ s . split ( sep ) for s in strings ] <NEWLINE> <INDENT> np = [ p for p in zip ( * parts ) if len ( set ( p ) ) >= 1 ] <NEWLINE> <DEDENT>
if end <= 0 : <NEWLINE> <INDENT> end += results . meta ( <STRING> ) <NEWLINE> <DEDENT>
assert response_result . errors == [ ] <NEWLINE> <INDENT> assert response_result . data == data_json <NEWLINE> <DEDENT>
if tag == 0x01 : <NEWLINE> <INDENT> read_assert_tag ( f , 71 ) <NEWLINE> self . domain_type = read_s32le ( f ) <NEWLINE> elif tag == 0x02 : <NEWLINE> mob_id = mobid . MobID ( ) <NEWLINE> read_assert_tag ( f , 65 ) <NEWLINE> length = read_s32le ( f ) <NEWLINE> assert length == 12 <NEWLINE> mob_id . SMPTELabel = [ read_byte ( f ) for i in range ( 12 ) ] <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . length = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . instanceHigh = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . instanceMid = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 68 ) <NEWLINE> mob_id . instanceLow = read_byte ( f ) <NEWLINE> read_assert_tag ( f , 72 ) <NEWLINE> mob_id . Data1 = read_u32le ( f ) <NEWLINE> read_assert_tag ( f , 70 ) <NEWLINE> mob_id . Data2 = read_u16le ( f ) <NEWLINE> read_assert_tag ( f , 70 ) <NEWLINE> mob_id . Data3 = read_u16le ( f ) <NEWLINE> read_assert_tag ( f , 65 ) <NEWLINE> length = read_s32le ( f ) <NEWLINE> assert length == 8 <NEWLINE> mob_id . Data4 = [ read_byte ( f ) for i in range ( 8 ) ] <NEWLINE> self . mob_id = mob_id <NEWLINE> elif tag == 0x03 : <NEWLINE> read_assert_tag ( f , 76 ) <NEWLINE> self . last_known_volume_utf8 = read_string ( f , <STRING> ) <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> % ( str ( self . class_id ) , tag , tag ) ) <NEWLINE> <DEDENT>
def setup_country ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not hasattr ( self , <STRING> ) or self . exchange is None : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> exch_country = find_country_for_exchange ( self . exchange ) <NEWLINE> if hasattr ( self , <STRING> ) and self . country : <NEWLINE> <INDENT> if self . country == exch_country : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT> self . country = exch_country <NEWLINE> <DEDENT>
p = float ( opts [ <STRING> ] ) <NEWLINE> <INDENT> inp = opts [ <STRING> ] <NEWLINE> out = opts [ <STRING> ] <NEWLINE> ( m , _ ) = probe ( inp ) <NEWLINE> if opts [ <STRING> ] is None : <NEWLINE> <INDENT> if opts [ <STRING> ] is not None : <NEWLINE> <INDENT> S = long ( opts [ <STRING> ] ) <NEWLINE> random . seed ( S ) <NEWLINE> <DEDENT> if m [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> ( m , xs ) = kset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kset . write ( K , sampleR ( p , xs ) , out , m ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ( m , xs ) = kfset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kfset . write ( K , sampleR ( p , xs ) , out , m ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> S = 0 <NEWLINE> if opts [ <STRING> ] is not None : <NEWLINE> <INDENT> S = long ( opts [ <STRING> ] ) <NEWLINE> <DEDENT> if m [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> ( m , xs ) = kset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kset . write ( K , sampleD ( p , S , xs , lambda x : x ) , out , m ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> ( m , xs ) = kfset . read ( inp ) <NEWLINE> K = m [ <STRING> ] <NEWLINE> kfset . write ( K , sampleD ( p , S , xs , lambda x : x [ 0 ] ) , out , m ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_dynamic_routes_instances ( self , viewset , route , dynamic_routes ) : <NEWLINE> <INDENT> dynamic_routes_instances = [ ] <NEWLINE> for httpmethods , methodname , endpoint , is_for_list in dynamic_routes : <NEWLINE> <INDENT> initkwargs = route . initkwargs . copy ( ) <NEWLINE> initkwargs . update ( getattr ( viewset , methodname ) . kwargs ) <NEWLINE> dynamic_routes_instances . append ( Route ( <NEWLINE> <INDENT> url = replace_methodname ( route . url , endpoint ) , <NEWLINE> mapping = dict ( ( httpmethod , methodname ) for httpmethod in httpmethods ) , <NEWLINE> name = replace_methodname ( route . name , endpoint ) , <NEWLINE> initkwargs = initkwargs , <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> return dynamic_routes_instances <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if ( self . _num_days ( self . _today ( ) ) - ts ) >= self . timeout_days : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>
def main_tornado ( ) : <NEWLINE> <INDENT> define ( <STRING> , default = 8888 , help = <STRING> , type = int ) <NEWLINE> loop = IOLoop . instance ( ) <NEWLINE> xcfg = t_options . DynamicPatch ( loop , op ) <NEWLINE> xcfg . add_change_callback ( <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> , <STRING> , callback_handler = on_callback <NEWLINE> <DEDENT> ) <NEWLINE> tornado . options . parse_command_line ( ) <NEWLINE> application = tornado . web . Application ( [ ( <STRING> , MainHandler ) ] ) <NEWLINE> http_server = tornado . httpserver . HTTPServer ( application ) <NEWLINE> http_server . listen ( options . port ) <NEWLINE> xcfg . start ( ) <NEWLINE> <DEDENT>
def load_next ( self ) : <NEWLINE> <INDENT> if self . current_model >= len ( self . unprocessed ) : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> self . current_model += 1 <NEWLINE> filename , frequency = self . unprocessed [ self . current_model ] <NEWLINE> model = self . parser . load ( filename , frequencies = [ frequency ] ) [ 0 ] <NEWLINE> self . load ( model ) <NEWLINE> <DEDENT>
def is_available ( self ) : <NEWLINE> <INDENT> return len ( self . subscribed_topics ) < self . max_topics <NEWLINE> <DEDENT>
if ( not txt . startswith ( <STRING> ) or <NEWLINE> <INDENT> not txt . endswith ( <STRING> ) ) : <NEWLINE> return False <NEWLINE> <DEDENT>
def get_recipe_env ( self , arch = None , with_flags_in_cc = True ) : <NEWLINE> <INDENT> env = super ( CoincurveRecipe , self ) . get_recipe_env ( arch , with_flags_in_cc ) <NEWLINE> <COMMENT> <NL> env [ <STRING> ] = env [ <STRING> ] + <STRING> <NEWLINE> libsecp256k1 = self . get_recipe ( <STRING> , self . ctx ) <NEWLINE> libsecp256k1_dir = libsecp256k1 . get_build_dir ( arch . arch ) <NEWLINE> env [ <STRING> ] += <STRING> + os . path . join ( libsecp256k1_dir , <STRING> ) <NEWLINE> <COMMENT> <NL> if self . ctx . ndk == <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> python_version = self . ctx . python_recipe . version [ 0 : 3 ] <NEWLINE> ndk_dir_python = os . path . join ( self . ctx . ndk_dir , <STRING> , python_version ) <NEWLINE> env [ <STRING> ] += <STRING> . format ( os . path . join ( ndk_dir_python , <STRING> , arch . arch ) ) <NEWLINE> env [ <STRING> ] += <STRING> . format ( python_version ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> env [ <STRING> ] += <STRING> . format ( ndk_dir_python ) <NEWLINE> <DEDENT> env [ <STRING> ] += <STRING> <NEWLINE> return env <NEWLINE> <DEDENT>
if <STRING> in args : <NEWLINE> <INDENT> args . remove ( <STRING> ) <NEWLINE> args . append ( <STRING> ) <NEWLINE> if len ( args ) > 0 : <NEWLINE> if args [ 0 ] == <STRING> : <NEWLINE> <INDENT> prog = args [ 1 ] . strip ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> with open ( args [ 0 ] , <STRING> ) as progfile : <NEWLINE> <INDENT> prog = progfile . read ( ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> prog = sys . stdin . readline ( ) <NEWLINE> <DEDENT>
old_json = replace_underscore ( Path ( electrodes_file . filename ) , <STRING> ) <NEWLINE> <INDENT> new_json = replace_underscore ( tsv_electrodes , <STRING> ) <NEWLINE> copyfile ( old_json , new_json ) <COMMENT> <NEWLINE> <DEDENT>
if PARAMETERS [ <STRING> ] : <NEWLINE> <INDENT> with Pool ( ) as p : <NEWLINE> <INDENT> p . starmap ( save_frequency , args ) <NEWLINE> else : <NEWLINE> <DEDENT> for arg in args : <NEWLINE> <INDENT> save_frequency ( * arg ) <NEWLINE> <DEDENT> <DEDENT>
return out <NEWLINE>
def url ( self , name ) : <NEWLINE> <INDENT> if not os . getenv ( <STRING> , <STRING> ) . startswith ( <STRING> ) : <NEWLINE> <COMMENT> <NL> <INDENT> filename = <STRING> + self . location + <STRING> + name <NEWLINE> key = create_gs_key ( filename ) <NEWLINE> return <STRING> + key + <STRING> <NEWLINE> <DEDENT> return self . base_url + <STRING> + name <NEWLINE> <DEDENT>
def clear ( key , participant_identifier ) : <NEWLINE> <INDENT> try : <NEWLINE> <COMMENT> <NL> <INDENT> cache_key = COUNTER_CACHE_KEY % key <NEWLINE> pipe = r . pipeline ( ) <NEWLINE> freq , _ = pipe . hget ( cache_key , participant_identifier ) . hdel ( cache_key , participant_identifier ) . execute ( ) <NEWLINE> <DEDENT> <DEDENT>
if ( doi is not None ) : <NEWLINE> <INDENT> info = { <STRING> : <STRING> , <STRING> : doi } <NEWLINE> print ( <STRING> + str ( doi ) ) <NEWLINE> self . set_status ( 201 ) <NEWLINE> self . write ( json . dumps ( info ) ) <NEWLINE> store_record ( doi , path_to_file , directory_to_zip , access_token ) <NEWLINE> <COMMENT> <NL> self . finish ( ) <NEWLINE> else : <NEWLINE> self . return_error ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> _rl . InitWindow . argtypes = [ Int , Int , CharPtr ] <NEWLINE> _rl . InitWindow . restype = None <NEWLINE> def init_window ( width : int , height : int , title : AnyStr ) -> None : <NEWLINE> <INDENT> <STRING> <NEWLINE> return _rl . InitWindow ( _int ( width ) , _int ( height ) , _str_in ( title ) ) <NEWLINE> <DEDENT> <DEDENT>
def extract_readgroup_json ( bam_path , logger ) : <NEWLINE> <INDENT> step_dir = os . getcwd ( ) <NEWLINE> bam_file = os . path . basename ( bam_path ) <NEWLINE> bam_name , bam_ext = os . path . splitext ( bam_file ) <NEWLINE> readgroups_json_file = bam_name + <STRING> <NEWLINE> with open ( bam_path ) as f : <NEWLINE> <INDENT> samfile = pysam . AlignmentFile ( f , <STRING> , check_header = True , check_sq = False ) <NEWLINE> readgroup_dict_list = get_readgroup_dict_list ( samfile ) <NEWLINE> <DEDENT> with open ( readgroups_json_file , <STRING> ) as f : <NEWLINE> <INDENT> json . dump ( out_readgroup_dict_list , f , indent = 4 ) <NEWLINE> <DEDENT> return readgroups_json_file <NEWLINE> <DEDENT>
def extract_readgroup_json ( bam_path , logger ) : <NEWLINE> <INDENT> bam_file = os . path . basename ( bam_path ) <NEWLINE> bam_name , bam_ext = os . path . splitext ( bam_file ) <NEWLINE> readgroups_json_file = bam_name + <STRING> <NEWLINE> with open ( bam_path ) as f : <NEWLINE> <INDENT> samfile = pysam . AlignmentFile ( f , <STRING> , check_header = True , check_sq = False ) <NEWLINE> if not samfile . is_bam : <NEWLINE> <INDENT> logger . error ( <STRING> ) <NEWLINE> raise NotABamError <NEWLINE> <DEDENT> samfile_header = samfile . header <NEWLINE> bam_readgroup_dict_list = samfile_header . get ( <STRING> ) <NEWLINE> if not bam_readgroup_dict_list : <NEWLINE> <INDENT> logger . error ( <STRING> . format ( samfile . filename ) ) <NEWLINE> raise NoReadGroupError <NEWLINE> <DEDENT> readgroup_dict_list = get_readgroup_dict_list ( bam_readgroup_dict_list , logger ) <NEWLINE> <DEDENT> with open ( readgroups_json_file , <STRING> ) as f : <NEWLINE> <INDENT> json . dump ( readgroup_dict_list , f , indent = 4 ) <NEWLINE> <DEDENT> return readgroups_json_file <NEWLINE> <DEDENT>
def snakeize_dict ( dict_ ) : <NEWLINE> <INDENT> answer = { } <NEWLINE> for key in dict_ : <NEWLINE> <INDENT> nkey = snakeize_s ( key ) <NEWLINE> answer [ nkey ] = dict_ [ key ] <NEWLINE> <DEDENT> return answer <NEWLINE> <DEDENT>
listener = handlers . QueueListener ( log_queue , handler ) <NEWLINE>
def __call__ ( self , transaction ) : <NEWLINE> <INDENT> valid_transaction = self . validate_transaction ( transaction ) <NEWLINE> message = self . build_evm_message ( valid_transaction ) <NEWLINE> computation = self . build_computation ( message , valid_transaction ) <NEWLINE> finalized_computation = self . finalize_computation ( valid_transaction , computation ) <NEWLINE> return finalized_computation <NEWLINE> <DEDENT>
if self . _index < 0 : <NEWLINE> <INDENT> if self . _feed_page is feedparser . FeedParserDict : <NEWLINE> <INDENT> for link in self . _feed_page . links : <NEWLINE> <INDENT> if link . rel == <STRING> : <NEWLINE> <INDENT> self . _url = link . href <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> search = class_re . search ( line ) <NEWLINE> if search : <NEWLINE> <INDENT> if current_class : <NEWLINE> <INDENT> current_class [ 1 ] = last_closing_bracket_number <NEWLINE> <DEDENT> if current_method : <NEWLINE> <INDENT> current_method [ 1 ] = penultimate_closing_bracket_number <NEWLINE> components . append ( current_method ) <NEWLINE> <DEDENT> current_class = [ line_counter , 0 , search . group ( 2 ) ] <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>
if diff : <NEWLINE> <INDENT> schedule = solution_to_schedule ( solution , events , slots ) <NEWLINE> event_diff = event_schedule_difference ( original_schedule , schedule ) <NEWLINE> logger . debug ( <STRING> ) <NEWLINE> for item in event_diff : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
kwargs = dict ( <NEWLINE> <INDENT> content_type = self . get_content_type ( ) , <NEWLINE> object_id = self . instance . id , <NEWLINE> key = self . field . key , <NEWLINE> ) <NEWLINE> if user : <NEWLINE> kwargs [ <STRING> ] = user <NEWLINE> else : <NEWLINE> kwargs [ <STRING> ] = True <NEWLINE> <DEDENT>
if old_html_table != html_table : <NEWLINE> <INDENT> mail_text = <STRING> <NEWLINE> mail_text += <STRING> <NEWLINE> mail_text += html_table <NEWLINE> mail_text += <STRING> <NEWLINE> self . send_mail ( mail_text ) <NEWLINE> <DEDENT>
self . verbosity = options . verbosity <NEWLINE> <INDENT> if options . quickunit_prefix : <NEWLINE> <INDENT> self . prefixes = options . quickunit_prefix <NEWLINE> if len ( self . prefixes ) == 1 : <NEWLINE> <INDENT> self . prefixes = self . prefixes [ 0 ] . split ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> self . prefixes = [ <STRING> ] <NEWLINE> <DEDENT> self . parent = <STRING> <NEWLINE> <DEDENT>
def get_sql_field ( self , prop : Property , lower : bool = False ) : <NEWLINE> <INDENT> if prop . list is not None : <NEWLINE> <INDENT> jsonb = self . table . lists . c . data [ prop . place ] <NEWLINE> if _is_dtype ( prop , ( String , DateTime , Date ) ) : <NEWLINE> <INDENT> field = jsonb . astext <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> field = sa . cast ( jsonb , JSONB ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> field = self . table . main . c [ prop . name ] <NEWLINE> <DEDENT> <DEDENT>
assert model_a != model_b <NEWLINE>
if input_start <= t < input_start + echo_start : <NEWLINE> <INDENT> x_t += 1.0 <NEWLINE> mask_t = np . zeros ( self . N_out ) <NEWLINE> <DEDENT>
return kompile ( src , raw = raw , filename = full_name ) <NEWLINE> <INDENT> except FileNotFoundError : <NEWLINE> <INDENT> pass <NEWLINE> else : <NEWLINE> <DEDENT> raise TemplateNotFound ( name ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if compare_models : <NEWLINE> <INDENT> choose_box_and_violin_plots ( names , <NEWLINE> <INDENT> scoring , <NEWLINE> compare_models , <NEWLINE> results , <NEWLINE> is_continuous ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> if ROC : <NEWLINE> <INDENT> if not is_continuous : <NEWLINE> <INDENT> timeit ( plot_rocs , models , df_X , y ) <NEWLINE> plt . show ( ) <NEWLINE> <DEDENT> <DEDENT> print ( <STRING> ) <NEWLINE> return names , results , fit_models , pipeline , df_X <NEWLINE> <DEDENT>
def prepare_notification ( self , * , subscribers = None , instance = None , <NEWLINE> <INDENT> loop = None , notify_external = True ) : <NEWLINE> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> self_subscribers = self . subscribers . copy ( ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if ( instance is not None and self . name and <NEWLINE> isinstance ( instance . __class__ , SignalAndHandlerInitMeta ) ) : <NEWLINE> class_handlers = type ( instance ) . _get_class_handlers ( <NEWLINE> self . name , instance ) <NEWLINE> for ch in class_handlers : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> if ch not in self_subscribers : <NEWLINE> self_subscribers . append ( ch ) <NEWLINE> <COMMENT> <NL> if subscribers is not None : <NEWLINE> for el in subscribers : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> if el not in self_subscribers : <NEWLINE> self_subscribers . append ( el ) <NEWLINE> loop = loop or self . loop <NEWLINE> <COMMENT> <NL> if notify_external and self . external_signaller is not None : <NEWLINE> self_subscribers . append ( partial ( self . ext_publish , instance , loop ) ) <NEWLINE> if self . _fnotify is None : <NEWLINE> fnotify = None <NEWLINE> else : <NEWLINE> if instance is None : <NEWLINE> fnotify = self . _fnotify <NEWLINE> else : <NEWLINE> fnotify = types . MethodType ( self . _fnotify , instance ) <NEWLINE> validator = self . _fvalidation <NEWLINE> if validator is not None and instance is not None : <NEWLINE> validator = types . MethodType ( validator , instance ) <NEWLINE> return Executor ( self_subscribers , owner = self , <NEWLINE> concurrent = SignalOptions . EXEC_CONCURRENT in self . flags , <NEWLINE> loop = loop , exec_wrapper = fnotify , <NEWLINE> fvalidation = validator ) <NEWLINE> <DEDENT>
def global_interpreter ( self , version ) : <NEWLINE> <INDENT> version_name = <STRING> % version <NEWLINE> if Path ( self . bin_path / <STRING> ) . exists ( ) : <NEWLINE> <INDENT> remove ( str ( self . bin_path / <STRING> ) ) <NEWLINE> <DEDENT> symlink ( str ( self . bin_path / <STRING> ) , str ( self . lib_path / version_name / <STRING> / <STRING> ) ) <NEWLINE> <DEDENT>
@ register . filter <NEWLINE> <INDENT> def order_links ( links ) : <NEWLINE> <INDENT> links_list = list ( links ) <NEWLINE> ordered_links = [ ] <NEWLINE> while links_list : <NEWLINE> <INDENT> minor = links_list [ 0 ] <NEWLINE> for link in links_list : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if ( link . link_type . ordering < minor . link_type . ordering ) : <NEWLINE> <INDENT> minor = link <NEWLINE> <DEDENT> <DEDENT> except TypeError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> ordered_links . append ( minor ) <NEWLINE> links_list . remove ( minor ) <NEWLINE> <DEDENT> return ordered_links <NEWLINE> <DEDENT> <DEDENT>
for topic in topics : <NEWLINE> <INDENT> if topic . projects . count ( ) : <NEWLINE> <INDENT> topics_list . append ( ( topic . id , topic . name ) ) <NEWLINE> return topics_list <NEWLINE> <DEDENT> <DEDENT>
request = dict ( get_default_request_parameters ( chosen_request_params ) ) <NEWLINE> <INDENT> check_review_timestamp = bool ( <STRING> in request . keys ( ) and request [ <STRING> ] != <STRING> ) <NEWLINE> if check_review_timestamp : <NEWLINE> <INDENT> current_date = datetime . datetime . now ( ) <NEWLINE> num_days = int ( request [ <STRING> ] ) <NEWLINE> date_threshold = current_date - datetime . timedelta ( days = num_days ) <NEWLINE> timestamp_threshold = datetime . datetime . timestamp ( date_threshold ) <NEWLINE> if verbose : <NEWLINE> <INDENT> if request [ <STRING> ] == <STRING> : <NEWLINE> <INDENT> collection_keyword = <STRING> <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> collection_keyword = <STRING> <NEWLINE> <DEDENT> print ( <STRING> . format ( collection_keyword , <NEWLINE> <INDENT> date_threshold ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _update_data_with_deltas ( self , packet_id , deltas ) : <NEWLINE> <INDENT> for delta_id in [ 0 , 1 ] : <NEWLINE> <COMMENT> <NL> <INDENT> sample_id = ( packet_id - 1 ) * 2 + delta_id + 1 <NEWLINE> <COMMENT> <NL> self . _last_eeg_data -= np . array ( deltas [ delta_id ] ) <NEWLINE> self . _update_counts_and_enqueue ( <STRING> , sample_id ) <NEWLINE> <DEDENT> <DEDENT>
def get_fleet ( self ) : <NEWLINE> <INDENT> fleets_list = [ ] <NEWLINE> response = self . session . get ( <STRING> <NEWLINE> <INDENT> . format ( self . server_number , self . server_language ) ) <NEWLINE> <DEDENT> if response . status_code != 302 : <NEWLINE> <INDENT> fleets = response . text . split ( <STRING> ) <NEWLINE> del fleets [ 0 ] <NEWLINE> for fleet in fleets : <NEWLINE> <INDENT> fleet_id = fleet [ 0 : 30 ] . split ( <STRING> ) [ 0 ] <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> fleet_mission = int ( fleet [ marker + 19 : marker + 22 ] . split ( <STRING> ) [ 0 ] ) <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> fleet_arrival = datetime . strptime ( fleet [ marker + 35 : marker + 54 ] , <STRING> ) <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> origin_raw = fleet [ marker : marker + 180 ] <NEWLINE> origin_list = origin_raw . split ( <STRING> ) [ 1 ] . split ( <STRING> ) [ 0 ] . split ( <STRING> ) <NEWLINE> fleet_origin = const . coordinates ( origin_list [ 0 ] , origin_list [ 1 ] , origin_list [ 2 ] ) <NEWLINE> marker = fleet . find ( <STRING> ) <NEWLINE> destination_raw = fleet [ marker : marker + 200 ] <NEWLINE> destination_list = destination_raw . split ( <STRING> ) [ 1 ] . split ( <STRING> ) [ 0 ] . split ( <STRING> ) <NEWLINE> fleet_destination = const . coordinates ( destination_list [ 0 ] , destination_list [ 1 ] , destination_list [ 2 ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not key in self . filter_keys : <NEWLINE> <INDENT> raise ImproperlyConfigured ( <NEWLINE> <INDENT> <STRING> % ( key , self . filter_keys ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
if view not in cls . get_views ( ) : <NEWLINE> <INDENT> raise ImproperlyConfigured ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> % view <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
setattr ( ModelMixin , <NEWLINE> <INDENT> <STRING> % view_class . get_underscored_action_name ( ) , <NEWLINE> _get_url_name ) <NEWLINE> <DEDENT>
def read ( self , max_records = None ) : <NEWLINE> <INDENT> array = [ ] <NEWLINE> dictionary = { } <NEWLINE> index_column = self . index_column_number ( ) <NEWLINE> for ( count , entry ) in enumerate ( self . _tfh ) : <NEWLINE> <INDENT> array . append ( entry ) <NEWLINE> dictionary [ entry [ index_column ] ] = entry <COMMENT> <NEWLINE> if max_records and count > max_records : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def __next__ ( self ) : <NEWLINE> <INDENT> row = next ( self . _consumer ) <NEWLINE> decoded_row = unpackb ( row . value ) <NEWLINE> if self . _end_time : <NEWLINE> <INDENT> self . verbose ( <STRING> ) <NEWLINE> self . verbose ( decoded_row ) <NEWLINE> count = 0 <NEWLINE> while True : <NEWLINE> <INDENT> count += 1 <NEWLINE> decoded_time = decoded_row [ self . _time_column ] <NEWLINE> decoded_time = self . decode_item ( decoded_time ) <NEWLINE> decoded_time = self . parse_time ( decoded_time ) <NEWLINE> if decoded_time >= self . _kafka_end_time : <NEWLINE> <INDENT> self . verbose ( <STRING> + str ( count ) + <STRING> ) <NEWLINE> raise StopIteration ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
results = { <STRING> : conns , <NEWLINE> <INDENT> <STRING> : ports } <NEWLINE> return ( self . _output_key , results ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for i in range ( num_rounds ) : <NEWLINE> <COMMENT> <NL> <INDENT> m = max ( int ( client_num * C ) , 1 ) <NEWLINE> <COMMENT> <NL> S = np . array ( random . sample ( range ( client_num ) , m ) ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if nstart is None : <NEWLINE> <INDENT> import random <NEWLINE> x = dict ( [ ( n , random . random ( ) ) for n in G ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> x = nstart <NEWLINE> <COMMENT> <NL> <DEDENT> s = 1.0 / sum ( x . values ( ) ) <NEWLINE> for k in x : x [ k ] *= s <NEWLINE> nnodes = G . number_of_nodes ( ) <NEWLINE> <COMMENT> <NL> for i in range ( max_iter ) : <NEWLINE> <INDENT> xlast = x <NEWLINE> x = dict . fromkeys ( xlast . keys ( ) , 0 ) <NEWLINE> <COMMENT> <NL> for n in x : <NEWLINE> <INDENT> for nbr in G [ n ] : <NEWLINE> <INDENT> x [ n ] += xlast [ nbr ] * G [ n ] [ nbr ] <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> s = 1.0 / sum ( x . values ( ) ) <NEWLINE> for n in x : x [ n ] *= s <NEWLINE> <COMMENT> <NL> err = sum ( [ abs ( x [ n ] - xlast [ n ] ) for n in x ] ) <NEWLINE> if err < nnodes * tol : <NEWLINE> <INDENT> return x <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
labelGenerator = _gen_node_label ( H ) <NEWLINE>
if nbunch is None : <NEWLINE> <INDENT> nbunch = G . nodes_iter ( ) <NEWLINE> for v in nbunch : <COMMENT> <NEWLINE> if v in explored : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> fringe = [ v ] <COMMENT> <NEWLINE> while fringe : <NEWLINE> <INDENT> w = fringe [ - 1 ] <COMMENT> <NEWLINE> if w in explored : <COMMENT> <NEWLINE> <INDENT> fringe . pop ( ) <NEWLINE> continue <NEWLINE> <DEDENT> seen [ w ] = 1 <COMMENT> <NEWLINE> <COMMENT> <NL> new_nodes = [ ] <NEWLINE> for n in G [ w ] : <NEWLINE> <INDENT> if n not in explored : <NEWLINE> <INDENT> if n in seen : <COMMENT> <NEWLINE> <INDENT> raise nx . NetworkXUnfeasible ( <STRING> ) <NEWLINE> <DEDENT> new_nodes . append ( n ) <NEWLINE> <DEDENT> <DEDENT> if new_nodes : <COMMENT> <NEWLINE> <INDENT> fringe . extend ( new_nodes ) <NEWLINE> <DEDENT> else : <COMMENT> <NEWLINE> <INDENT> explored [ w ] = 1 <NEWLINE> order_explored . insert ( 0 , w ) <COMMENT> <NEWLINE> fringe . pop ( ) <COMMENT> <NEWLINE> return order_explored <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def ambig_binary_crossentropy ( y_true , y_pred ) : <NEWLINE> <INDENT> non_ambig = K . cast ( ( y_true > - 0.5 ) , <STRING> ) <NEWLINE> return K . mean ( K . binary_crossentropy ( y_true , y_pred ) <NEWLINE> <INDENT> * non_ambig , axis = - 1 ) <NEWLINE> <DEDENT> <DEDENT>
def execute_from_command_line ( self , argv = None ) : <NEWLINE> <INDENT> if argv is None : <NEWLINE> <INDENT> argv = sys . argv <NEWLINE> <DEDENT> args = self . parser . parse_args ( argv [ 1 : ] ) <NEWLINE> self . nekumo . gateways = list ( self . parse_gateways ( args ) ) <NEWLINE> self . nekumo . ifaces = list ( self . parse_ifaces ( args ) ) <NEWLINE> if <STRING> not in os . environ : <NEWLINE> <INDENT> loop = asyncio . get_event_loop ( ) <NEWLINE> loop . run_forever ( ) <NEWLINE> <DEDENT> <DEDENT>
if token == <STRING> or tk_end == len ( sql ) : <NEWLINE> <INDENT> sqls . append ( sql [ beg : tk_end ] ) <NEWLINE> beg = tk_end <NEWLINE> level = 0 <NEWLINE> status = <STRING> <NEWLINE> self . cte_dico = { } <NEWLINE> elif token == <STRING> and cte_inline : <NEWLINE> if tk_value . lower ( ) == <STRING> : <NEWLINE> <INDENT> from_lvl [ level ] = True <NEWLINE> <DEDENT> elif from_lvl [ level ] : <NEWLINE> <INDENT> if last_other in ( <STRING> , <STRING> , <STRING> ) and ( <NEWLINE> tk_value in self . cte_dico ) : <NEWLINE> <COMMENT> <NL> <INDENT> bg , en , tknext = tk_end , tk_end , <STRING> <NEWLINE> while en < length and tknext == <STRING> : <NEWLINE> <INDENT> bg , ( en , tknext ) = en , self . get_token ( sql , en ) <NEWLINE> <COMMENT> <NL> <DEDENT> if sql [ bg : en ] . lower ( ) != <STRING> : <NEWLINE> <INDENT> sql2 = ( sql [ : end ] + <STRING> + self . cte_dico [ tk_value ] + <NEWLINE> <INDENT> <STRING> + tk_value + <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> sql2 = ( sql [ : end ] + <STRING> + self . cte_dico [ tk_value ] + <NEWLINE> <INDENT> <STRING> + <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
for table_ref in tables : <NEWLINE> <INDENT> table_sql = table_ref + <STRING> <NEWLINE> df = names_env [ table_ref ] <NEWLINE> df = self . _ensure_data_frame ( df , table_ref ) <NEWLINE> <COMMENT> <NL> pre_q = <STRING> % table_sql <NEWLINE> cur = self . _execute_sql ( pre_q ) <NEWLINE> self . _write_table ( table_sql , df , self . conn ) <NEWLINE> <COMMENT> <NL> for q_single in self . get_sqlsplit ( sql , True ) : <NEWLINE> if q_single . strip ( ) != <STRING> : <NEWLINE> <COMMENT> <NL> <INDENT> self . remove_tmp_tables ( <STRING> ) <NEWLINE> cur = self . _execute_cte ( q_single , env ) <NEWLINE> return cur <NEWLINE> <DEDENT> <DEDENT>
global MASK_PREDICTOR_HANDLER <NEWLINE> <INDENT> with LOCK : <NEWLINE> <INDENT> if MASK_PREDICTOR_HANDLER is None : <NEWLINE> <INDENT> MASK_PREDICTOR_HANDLER = MaskPredictor ( boxSize , deepLearningModel , gpus ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
with MaskPredictor ( boxSize , deepLearningModelFname , gpus = [ 0 ] ) as mp : <NEWLINE> <INDENT> mask = mp . predictMask ( mic ) <NEWLINE> <DEDENT>
def get_current_session ( self ) : <NEWLINE> <INDENT> if self . session [ <STRING> ] : <NEWLINE> <INDENT> if ( <NEWLINE> <INDENT> arrow . get ( self . session [ <STRING> ] ) . shift ( <NEWLINE> <INDENT> seconds = int ( self . config [ <STRING> ] ) <NEWLINE> <DEDENT> ) <NEWLINE> > arrow . utcnow ( ) <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> return self . session <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_history ( self , start_time , end_time ) : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for day in arrow . Arrow . range ( <STRING> , start_time , end_time ) : <NEWLINE> <INDENT> data = self . request ( <NEWLINE> <INDENT> self . DATA_ENDPOINT , <NEWLINE> method = <STRING> , <NEWLINE> params = { <STRING> : day . format ( <STRING> ) } , <NEWLINE> <DEDENT> ) <NEWLINE> result . append ( Day . _from_data ( day , data ) ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> rating_true = rating_true [ [ col_user , col_item , col_rating ] ] <NEWLINE> rating_pred = rating_pred [ [ col_user , col_item , col_prediction ] ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for i in range ( len ( ratios ) ) : <NEWLINE> <INDENT> splits [ i ] [ <STRING> ] = i <NEWLINE> <DEDENT> <DEDENT>
@ classmethod <NEWLINE> <INDENT> def from_string ( cls , s ) : <NEWLINE> <COMMENT> <NL> <INDENT> val = base32hex . b32decode ( s . upper ( ) ) <NEWLINE> value_check = [ 0 <= x < 255 for x in val ] <NEWLINE> <DEDENT> <DEDENT>
if <STRING> in binding : <NEWLINE> <INDENT> if <STRING> not in datum : <NEWLINE> <INDENT> datum [ <STRING> ] = [ ] <NEWLINE> <DEDENT> for sf in aslist ( binding [ <STRING> ] ) : <NEWLINE> <INDENT> if isinstance ( sf , dict ) : <NEWLINE> <INDENT> sfpath = expression . do_eval ( sf , self . job , self . requirements , self . docpath , datum [ <STRING> ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> sfpath = { <STRING> : substitute ( datum [ <STRING> ] , sf ) } <NEWLINE> <DEDENT> if isinstance ( sfpath , list ) : <NEWLINE> <INDENT> datum [ <STRING> ] . extend ( sfpath ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> datum [ <STRING> ] . append ( sfpath ) <NEWLINE> <DEDENT> self . files . append ( sfpath ) <NEWLINE> <DEDENT> <DEDENT>
def _draft2toDraft3dev1 ( doc , loader , baseuri ) : <COMMENT> <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> if isinstance ( doc , dict ) : <NEWLINE> <INDENT> if <STRING> in doc : <NEWLINE> <INDENT> imp = urlparse . urljoin ( baseuri , doc [ <STRING> ] ) <NEWLINE> impLoaded = loader . fetch ( imp ) <NEWLINE> r = None <COMMENT> <NEWLINE> if isinstance ( impLoaded , list ) : <NEWLINE> <INDENT> r = { <STRING> : impLoaded } <NEWLINE> <DEDENT> elif isinstance ( impLoaded , dict ) : <NEWLINE> <INDENT> r = impLoaded <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> r [ <STRING> ] = imp <NEWLINE> _ , frag = urlparse . urldefrag ( imp ) <NEWLINE> if frag : <NEWLINE> <INDENT> frag = <STRING> + frag <NEWLINE> r = findId ( r , frag ) <NEWLINE> <DEDENT> return _draft2toDraft3dev1 ( r , loader , imp ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def _resolve_idmap ( self , document , loader ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> for idmapField in loader . idmap : <NEWLINE> <INDENT> if ( idmapField in document ) : <NEWLINE> <INDENT> idmapFieldValue = document [ idmapField ] <NEWLINE> if ( isinstance ( idmapFieldValue , dict ) <NEWLINE> <INDENT> and <STRING> not in idmapFieldValue <NEWLINE> and <STRING> not in idmapFieldValue ) : <NEWLINE> ls = [ ] <NEWLINE> for k in sorted ( idmapFieldValue . keys ( ) ) : <NEWLINE> val = idmapFieldValue [ k ] <NEWLINE> v = None <COMMENT> <NEWLINE> if not isinstance ( val , dict ) : <NEWLINE> <INDENT> if idmapField in loader . mapPredicate : <NEWLINE> <INDENT> v = { loader . mapPredicate [ idmapField ] : val } <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise validate . ValidationException ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> , k , v ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> v = val <NEWLINE> <DEDENT> v [ loader . idmap [ idmapField ] ] = k <NEWLINE> ls . append ( v ) <NEWLINE> document [ idmapField ] = ls <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
async_result = gevent . event . AsyncResult ( ) <NEWLINE> <INDENT> gevent . spawn ( self . _process_response , request_event , bufchan , <NEWLINE> <INDENT> timeout ) . link ( async_result ) <NEWLINE> <DEDENT> return async_result <NEWLINE> except : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> bufchan . close ( ) <NEWLINE> raise <NEWLINE> <DEDENT>
def get_name ( self , regex , path = None ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if path is None : <NEWLINE> <INDENT> path = self . path <NEWLINE> <DEDENT> return super ( CamImage , self ) . get_name ( regex , path ) <NEWLINE> <DEDENT>
highlight = 0 <NEWLINE> <INDENT> page = 0 <NEWLINE> state = <STRING> <NEWLINE> key = 0 <NEWLINE> try : <NEWLINE> <INDENT> while key != ord ( <STRING> ) : <NEWLINE> <INDENT> if windowsize [ 0 ] > 8 and windowsize [ 1 ] > 30 : <NEWLINE> <INDENT> win_l . clear ( ) <NEWLINE> win_l . border ( 0 ) <NEWLINE> win_r . clear ( ) <NEWLINE> win_r . border ( 0 ) <NEWLINE> win_l . addstr ( windowsize [ 0 ] - 1 , windowsize [ 1 ] // 2 - 9 , <STRING> + str ( page + 1 ) ) <NEWLINE> win_r . addstr ( windowsize [ 0 ] - 4 , windowsize [ 1 ] // 2 - 11 , <STRING> ) <NEWLINE> win_r . addstr ( windowsize [ 0 ] - 3 , windowsize [ 1 ] // 2 - 12 , <STRING> ) <NEWLINE> win_r . addstr ( windowsize [ 0 ] - 2 , windowsize [ 1 ] // 2 - 9 , <STRING> ) <NEWLINE> index = 0 <NEWLINE> if state == <STRING> : <NEWLINE> <INDENT> totalitems = len ( data [ <STRING> ] ) <NEWLINE> currentpage = data [ <STRING> ] [ maxitems * page : maxitems * ( page + 1 ) ] <NEWLINE> for i in currentpage : <NEWLINE> <INDENT> if index < maxitems : <NEWLINE> <INDENT> if index == highlight : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen , curses . A_REVERSE ) <NEWLINE> win_r . addnstr ( 2 , 3 , <STRING> + str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> win_r . addnstr ( 3 , 3 , <STRING> + str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen ) <NEWLINE> <DEDENT> <DEDENT> index += 1 <NEWLINE> <DEDENT> <DEDENT> if state == <STRING> : <NEWLINE> <INDENT> totalitems = len ( data [ <STRING> ] ) <NEWLINE> currentpage = data [ <STRING> ] [ maxitems * page : maxitems * ( page + 1 ) ] <NEWLINE> for i in currentpage : <NEWLINE> <INDENT> if index < maxitems : <NEWLINE> <INDENT> if index == highlight : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen , curses . A_REVERSE ) <NEWLINE> win_r . addnstr ( 2 , 3 , str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> win_r . addnstr ( 4 , 3 , <STRING> + str ( i [ <STRING> ] ) , maxlen ) <NEWLINE> win_r . addstr ( 5 , 3 , <STRING> ) <NEWLINE> status = textwrap . wrap ( str ( i [ <STRING> ] [ <STRING> ] ) , windowsize [ 1 ] // 2 - 6 ) <NEWLINE> l_num = 7 <NEWLINE> for line in status : <NEWLINE> <INDENT> if l_num >= windowsize [ 0 ] - 2 : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> win_r . addstr ( l_num , 4 , line ) <NEWLINE> l_num += 1 <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> win_l . addnstr ( index * 2 + 2 , 2 , str ( i [ <STRING> ] [ <STRING> ] ) , maxlen ) <NEWLINE> <DEDENT> <DEDENT> index += 1 <NEWLINE> <DEDENT> <DEDENT> win_l . refresh ( ) <NEWLINE> win_r . refresh ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> stdscr . clear ( ) <NEWLINE> stdscr . addstr ( 0 , 0 , <STRING> ) <NEWLINE> stdscr . addstr ( 1 , 0 , <STRING> ) <NEWLINE> <DEDENT> key = stdscr . getch ( ) <NEWLINE> if key == curses . KEY_DOWN : <NEWLINE> <INDENT> if highlight + page * maxitems + 1 < totalitems : <NEWLINE> <INDENT> if highlight + 1 == maxitems : <NEWLINE> <INDENT> page += 1 <NEWLINE> highlight = 0 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> highlight += 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> elif key == curses . KEY_UP : <NEWLINE> <INDENT> if highlight == 0 and page > 0 : <NEWLINE> <INDENT> page -= 1 <NEWLINE> highlight = maxitems - 1 <NEWLINE> <DEDENT> elif highlight > 0 : <NEWLINE> <INDENT> highlight -= 1 <NEWLINE> <DEDENT> <DEDENT> elif key == curses . KEY_NPAGE and totalitems > ( page + 1 ) * maxitems : <NEWLINE> <INDENT> highlight = 0 <NEWLINE> page += 1 <NEWLINE> <DEDENT> elif key == curses . KEY_PPAGE and page > 0 : <NEWLINE> <INDENT> highlight = 0 <NEWLINE> page -= 1 <NEWLINE> <DEDENT> elif key == curses . KEY_RIGHT or key == 10 : <NEWLINE> <INDENT> if state == <STRING> : <NEWLINE> <INDENT> curses . nocbreak ( ) ; stdscr . keypad ( 0 ) ; curses . echo ( ) <NEWLINE> curses . endwin ( ) <NEWLINE> print ( <STRING> ) <NEWLINE> subprocess . call ( [ <STRING> , currentpage [ highlight ] [ <STRING> ] [ <STRING> ] , <STRING> ] ) <NEWLINE> stdscr = curses . initscr ( ) <NEWLINE> curses . noecho ( ) <NEWLINE> curses . cbreak ( ) <NEWLINE> stdscr . keypad ( 1 ) <NEWLINE> <DEDENT> elif state == <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> query = [ currentpage [ highlight ] [ <STRING> ] [ <STRING> ] , 0 ] <NEWLINE> data = query_twitch ( query [ 0 ] , query [ 1 ] ) <NEWLINE> state = <STRING> <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> <DEDENT> elif key == curses . KEY_LEFT : <NEWLINE> <INDENT> if state != <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> data = query_twitch ( <STRING> , 0 ) <NEWLINE> state = <STRING> <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> <DEDENT> elif key == ord ( <STRING> ) : <NEWLINE> <INDENT> searchbox = curses . newwin ( 3 , windowsize [ 1 ] - 4 , windowsize [ 0 ] // 2 - 1 , 2 ) <NEWLINE> searchbox . border ( 0 ) <NEWLINE> searchbox . addnstr ( 0 , 3 , <STRING> , windowsize [ 0 ] - 4 ) <NEWLINE> searchbox . refresh ( ) <NEWLINE> curses . echo ( ) <NEWLINE> s = searchbox . getstr ( 1 , 1 , windowsize [ 1 ] - 6 ) <NEWLINE> init_display ( stdscr ) <NEWLINE> query = [ s . decode ( <STRING> ) , 1 ] <NEWLINE> data = query_twitch ( query [ 0 ] , query [ 1 ] ) <NEWLINE> state = <STRING> <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> elif key == ord ( <STRING> ) : <NEWLINE> <INDENT> if state == <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> data = query_twitch ( query [ 0 ] , query [ 1 ] ) <NEWLINE> <DEDENT> elif state == <STRING> : <NEWLINE> <INDENT> init_display ( stdscr ) <NEWLINE> data = query_twitch ( <STRING> , 0 ) <NEWLINE> <DEDENT> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> elif key == curses . KEY_RESIZE : <NEWLINE> <INDENT> windowsize = init_display ( stdscr ) <NEWLINE> highlight = 0 <NEWLINE> page = 0 <NEWLINE> <DEDENT> <DEDENT> <DEDENT> finally : <NEWLINE> <INDENT> curses . nocbreak ( ) ; stdscr . keypad ( 0 ) ; curses . echo ( ) <NEWLINE> curses . endwin ( ) <NEWLINE> print ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def get_properties ( self , items ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . _check_values ( ) <NEWLINE> item_widths = [ len ( item ) for item in items ] <NEWLINE> if not item_widths : <NEWLINE> <INDENT> column_widths , num_lines = [ ] , 0 <NEWLINE> <DEDENT> elif any ( width >= self . max_line_width for width in item_widths ) : <NEWLINE> <INDENT> column_widths , num_lines = [ self . max_line_width ] , len ( item_widths ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> column_widths , num_lines = self . calculate_columns ( item_widths ) <NEWLINE> <DEDENT> return LineProperties ( column_widths , self . spacing , num_lines ) <NEWLINE> <DEDENT>
for attr_name , member in class_state . iteritems ( ) : <NEWLINE> <INDENT> if callable ( member ) : <NEWLINE> <INDENT> deps_used = parse_deps_used ( member ) <NEWLINE> invalid_ports = deps_used . difference ( needs_ports_defined ) . difference ( RESERVED_PORT_NAMES ) <NEWLINE> all_deps_used . update ( deps_used ) <NEWLINE> if invalid_ports : <NEWLINE> <INDENT> raise UnknownPort ( <STRING> . format ( <NEWLINE> <INDENT> class_name , <NEWLINE> attr_name , <NEWLINE> <STRING> . join ( sorted ( invalid_ports ) ) <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> def copy_and_tag ( variable , role , name ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> copy = variable . copy ( ) <NEWLINE> copy . name = <STRING> . format ( <COMMENT> <NEWLINE> <INDENT> brick . name , self . name , name ) <NEWLINE> <DEDENT> annotations = getattr ( copy . tag , <STRING> , [ ] ) + [ brick , call ] <NEWLINE> copy . tag . annotations = annotations <NEWLINE> copy . tag . name = name <COMMENT> <NEWLINE> VariableRole . add_role ( copy , role ) <NEWLINE> return copy <NEWLINE> <DEDENT> <DEDENT>
def check_gaussian ( rng , mean , std , shape ) : <NEWLINE> <INDENT> weights = IsotropicGaussian ( std , mean ) . generate ( rng , shape ) <NEWLINE> assert weights . shape == shape <NEWLINE> assert weights . dtype == theano . config . floatX <NEWLINE> assert_allclose ( weights . mean ( ) , mean , atol = 1e-2 ) <NEWLINE> assert_allclose ( weights . std ( ) , std , atol = 1e-2 ) <NEWLINE> yield check_gaussian , rng , 0 , 1 , ( 500 , 600 ) <NEWLINE> yield check_gaussian , rng , 5 , 3 , ( 600 , 500 ) <NEWLINE> <DEDENT>
params = bricks . get_params ( ) <NEWLINE> <INDENT> for name in params . keys ( ) : <NEWLINE> <INDENT> if name not in param_values : <NEWLINE> <INDENT> logger . error ( <STRING> <NEWLINE> <INDENT> . format ( name ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def compute_step ( self , param , previous_step ) : <NEWLINE> <INDENT> if any ( axis >= previous_step . ndim for axis in self . axes ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( <NEWLINE> <INDENT> self . axes , param , previous_step . ndim ) ) <NEWLINE> <DEDENT> <DEDENT> squares = tensor . sqr ( previous_step ) <NEWLINE> if len ( self . axes ) == 0 : <NEWLINE> <INDENT> norms = l2_norm ( [ previous_step ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> norms = tensor . sqrt ( <NEWLINE> <INDENT> reduce ( lambda t , a : t . sum ( axis = a , keepdims = True ) , <NEWLINE> <INDENT> sorted ( self . axes ) , squares ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> return ( previous_step * tensor . switch ( norms > self . threshold , <NEWLINE> <INDENT> self . threshold / norms , <NEWLINE> 1. ) , ( ) ) <NEWLINE> <DEDENT> <DEDENT>
cleaned_img1d = geometry_converter . image_2d_to_1d ( cleaned_img , fits_metadata_dict [ <STRING> ] ) <NEWLINE> <INDENT> hillas_params_2_cleaned_img = get_hillas_parameters ( geom1d , cleaned_img1d , HILLAS_IMPLEMENTATION ) <COMMENT> <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if len ( filter_thresholds ) != ( self . num_scales - 1 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> . format ( len ( filter_thresholds ) , <NEWLINE> <INDENT> self . num_scales - 1 ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
