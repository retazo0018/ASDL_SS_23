class Inertial ( object ) : <NEWLINE> <INDENT> def __init__ ( self , ixx = 0.0 , ixy = 0.0 , ixz = 0.0 , iyy = 0.0 , iyz = 0.0 , izz = 0.0 , <NEWLINE> <INDENT> mass = 0.0 , origin = None ) : <NEWLINE> self . matrix = { } <NEWLINE> self . matrix [ <STRING> ] = ixx <NEWLINE> self . matrix [ <STRING> ] = ixy <NEWLINE> self . matrix [ <STRING> ] = ixz <NEWLINE> self . matrix [ <STRING> ] = iyy <NEWLINE> self . matrix [ <STRING> ] = iyz <NEWLINE> self . matrix [ <STRING> ] = izz <NEWLINE> self . mass = mass <NEWLINE> self . origin = origin <NEWLINE> <DEDENT> <DEDENT>
def __cal_flag_len ( self ) : <NEWLINE> <INDENT> if Markers . __flag_len <= len ( self . __flag_name ) : <NEWLINE> <INDENT> Markers . __flag_len = len ( self . __flag_name ) + 2 <NEWLINE> <DEDENT> <DEDENT>
def refmac ( self , cycles ) : <NEWLINE> <INDENT> directory = self . job_directory ( <STRING> ) <NEWLINE> use_phases = self . args . unbiased and self . min_rwork > 0.35 <NEWLINE> job = Refmac ( self . args , directory , self . current_xyz , cycles , use_phases ) <NEWLINE> self . jobs [ self . cycle ] . append ( job ) <NEWLINE> self . current_hkl = job . hklout <NEWLINE> self . current_xyz = job . xyzout <NEWLINE> return job <NEWLINE> <DEDENT>
def _parse_address ( address ) : <NEWLINE> <INDENT> if isinstance ( address , tuple ) : <NEWLINE> <INDENT> if <STRING> in address [ 0 ] : <NEWLINE> <INDENT> return _socket . AF_INET6 , address <NEWLINE> <DEDENT> return _socket . AF_INET , address <NEWLINE> <DEDENT> elif isinstance ( address , string_types ) : <NEWLINE> <INDENT> if <STRING> in address : <NEWLINE> <INDENT> host , port = address . rsplit ( <STRING> , 1 ) <NEWLINE> family , host = _extract_family ( host ) <NEWLINE> if host == <STRING> : <NEWLINE> <INDENT> host = <STRING> <NEWLINE> <DEDENT> return family , ( host , int ( port ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return _socket . AF_INET , ( <STRING> , int ( address ) ) <NEWLINE> <DEDENT> <DEDENT> elif isinstance ( address , integer_types ) : <NEWLINE> <INDENT> return _socket . AF_INET , ( <STRING> , int ( address ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise TypeError ( <STRING> % type ( address ) ) <NEWLINE> <DEDENT> <DEDENT>
tweets = self . _filter_timeline ( <NEWLINE> <INDENT> self . _twitter_data . iter_tweets_from ( user_id ) , count , since_id , <NEWLINE> max_id ) <NEWLINE> if exclude_replies : <NEWLINE> tweets = [ tweet for tweet in tweets if tweet . reply_to is None ] <NEWLINE> if include_rts is not None : <NEWLINE> raise NotImplementedError ( <STRING> ) <NEWLINE> return [ <NEWLINE> tweet . to_dict ( <NEWLINE> <INDENT> self . _twitter_data , trim_user = trim_user , <NEWLINE> contributor_details = contributor_details ) <NEWLINE> <DEDENT> for tweet in tweets ] <NEWLINE> <DEDENT>
def has ( self , blockname , ptype ) : <NEWLINE> <INDENT> return self . reader . has_block ( self , blockname , ptype ) <NEWLINE> <DEDENT>
class LocalURI ( SwedishLegalSource ) : <NEWLINE> <INDENT> def infer_triples ( self , d , basefile = None ) : <NEWLINE> <INDENT> super ( LocalURI , self ) . infer_triples ( d , basefile ) <NEWLINE> canonicalminter = ... <NEWLINE> sameas = self . canonicalminter ( d ) <NEWLINE> d . rel ( OWL . sameAs , sameas ) <NEWLINE> <DEDENT> <DEDENT>
self . validate_body ( allbody , basefile ) <COMMENT> <NEWLINE>
def test_unreliable_fontspec ( self ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . _f ( <STRING> ) <NEWLINE> self . _f ( <STRING> ) <NEWLINE> textbox = self . _p ( <STRING> ) <NEWLINE> prevbox = self . _p ( <STRING> ) <NEWLINE> self . assertTrue ( self . gluefunc ( textbox , prevbox , textbox ) ) <NEWLINE> textbox = textbox + prevbox <NEWLINE> nextbox = self . _p ( <STRING> ) <NEWLINE> self . assertTrue ( self . gluefunc ( textbox , nextbox , prevbox ) ) <NEWLINE> textbox = textbox + nextbox <NEWLINE> prevbox = nextbox <NEWLINE> nextbox = self . _p ( <STRING> ) <NEWLINE> self . assertTrue ( self . gluefunc ( textbox , nextbox , prevbox ) ) <NEWLINE> <DEDENT>
if not fetched : <NEWLINE> <INDENT> self . log . error ( <STRING> % url ) <NEWLINE> return False <NEWLINE> <COMMENT> <NL> except requests . exceptions . RequestException as e : <NEWLINE> self . log . error ( <STRING> % ( url , e ) ) <NEWLINE> raise e <NEWLINE> if response . status_code == 304 : <NEWLINE> self . log . debug ( <STRING> % url ) <NEWLINE> return False <COMMENT> <NEWLINE> elif response . status_code >= 400 : <NEWLINE> self . log . error ( <STRING> % url ) <NEWLINE> response . raise_for_status ( ) <NEWLINE> <DEDENT>
if inspect . isclass ( local_getter ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> local_getter = local_getter ( url ) <NEWLINE> <DEDENT>
def __init__ ( self , sep = SCOPE_SEPARATOR ) : <NEWLINE> <INDENT> self . __sep = sep <NEWLINE> super ( ScopeDict , self ) . __init__ ( ) <NEWLINE> <DEDENT>
if self . _chunk_size is not None and len ( data ) > self . _chunk_size : <NEWLINE> <INDENT> logger . info ( <STRING> ) <NEWLINE> chunked_headers = dict ( headers ) <NEWLINE> chunked_headers [ <STRING> ] = <STRING> <NEWLINE> chunked_headers [ <STRING> ] = <STRING> <NEWLINE> chunked_headers [ <STRING> ] = <STRING> <NEWLINE> data_chunks = serve_data_chunks ( data ) <NEWLINE> response = self . _session . post ( <NEWLINE> <INDENT> url = url , <NEWLINE> data = data_chunks , <NEWLINE> headers = chunked_headers <NEWLINE> <DEDENT> ) <NEWLINE> else : <NEWLINE> response = self . _session . post ( url = url , data = data , headers = headers ) <NEWLINE> logger . debug ( <STRING> . format ( response . status_code ) ) <NEWLINE> try : <NEWLINE> response . raise_for_status ( ) <NEWLINE> except requests . exceptions . HTTPError as error : <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> pattern = re . compile ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> match = re . match ( pattern , self . base_url ) <NEWLINE> if match is None : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( self . base_url ) ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> self . protocol = match . group ( <STRING> ) <NEWLINE> self . host = match . group ( <STRING> ) <NEWLINE> port = match . group ( <STRING> ) <NEWLINE> <DEDENT> except AttributeError : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( self . base_url ) ) <NEWLINE> <DEDENT> if port : <NEWLINE> <INDENT> self . port = int ( port ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if self . protocol == <STRING> : <NEWLINE> <INDENT> self . port = 80 <NEWLINE> <DEDENT> elif self . protocol == <STRING> : <NEWLINE> <INDENT> self . port = 443 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> . format ( self . protocol ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT> url_components = urlparse ( url ) <NEWLINE> self . url_prefix = url_components . path <NEWLINE> if headers is not None : <NEWLINE> <INDENT> self . _session . headers . update ( headers ) <NEWLINE> <DEDENT> if proxies is not None : <NEWLINE> <INDENT> self . _session . proxies = proxies <NEWLINE> <DEDENT> if callback is not None : <NEWLINE> <INDENT> self . _session . hooks = { <STRING> : [ callback , ] } <NEWLINE> <DEDENT> if chunk_size is None : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> self . _session . headers . update ( { <STRING> : self . host } ) <NEWLINE> <DEDENT> self . _chunk_size = chunk_size <NEWLINE> self . set_http_retry_params ( ) <NEWLINE> <DEDENT>
def form_invalid ( self , form ) : <NEWLINE> <INDENT> for sub_form in form : <NEWLINE> <INDENT> update_valid_or_invalid_form_fields ( sub_form ) <NEWLINE> for error in sub_form . errors : <NEWLINE> <INDENT> messages . error ( self . request , sub_form . errors [ error ] ) <NEWLINE> <DEDENT> <DEDENT> return self . get_success_url ( ) <NEWLINE> <DEDENT>
content = item . get ( <STRING> , None ) <NEWLINE> <INDENT> if not content : <NEWLINE> <INDENT> utils . fs_link ( item_source , item_target , hard_link = True , forced = forced ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> content_items = [ p for item in content for p in <NEWLINE> <INDENT> glob . glob ( os . path . abspath ( os . path . join ( item_source , item ) ) ) ] <NEWLINE> <DEDENT> for content_item in content_items : <NEWLINE> <INDENT> content_item_name = os . path . basename ( content_item ) <NEWLINE> content_item_target = os . path . abspath ( os . path . join ( item_target , content_item_name ) ) <NEWLINE> utils . fs_link ( content_item , content_item_target , hard_link = True , forced = forced ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def generate_mock ( mocked_module , mock_prototype ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> mock_filename = <STRING> . format ( mocked_module ) <NEWLINE> include_filename = <STRING> . format ( mocked_module ) <NEWLINE> logger . debug ( <STRING> , os . getcwd ( ) ) <NEWLINE> logger . debug ( <STRING> , mock_filename ) <NEWLINE> logger . debug ( <STRING> , include_filename ) <NEWLINE> logger . debug ( <STRING> , mock_prototype ) <NEWLINE> if os . path . exists ( mock_filename ) : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> mock_file = open ( mock_filename , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . debug ( <STRING> ) <NEWLINE> mock_file = open ( mock_filename , <STRING> ) <NEWLINE> write_header ( mock_file , FILE_HEADER , include_filename ) <NEWLINE> <DEDENT> add_mock_function ( mock_file , mock_prototype ) <NEWLINE> mock_file . close ( ) <NEWLINE> <DEDENT>
return list_detail . object_list ( <NEWLINE> <INDENT> request , <NEWLINE> queryset = Post . objects . filter ( tags__name__in = [ tag ] ) , <NEWLINE> paginate_by = blog_settings . BLOG_PAGESIZE , <NEWLINE> page = page , <NEWLINE> extra_context = { <NEWLINE> <INDENT> <STRING> : <STRING> , <NEWLINE> <STRING> : tag . id , <NEWLINE> <STRING> : tag <NEWLINE> <DEDENT> } , <NEWLINE> template_name = <STRING> , <NEWLINE> ** kwargs <NEWLINE> ) <NEWLINE> <DEDENT>
while event_aux <= end_utc : <NEWLINE> <INDENT> result . append ( ( event_aux , event_aux . tzinfo . normalize ( event_aux + event_duration ) , 1 ) ) <NEWLINE> event_aux = add_delta_dst ( event_aux , delta ) <NEWLINE> return result <NEWLINE> <DEDENT>
if destination_ip : <NEWLINE> <INDENT> destination_ip_match = False <NEWLINE> if loose_match and <STRING> in rule . destination : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for object_string in rule . destination : <NEWLINE> <COMMENT> <NL> <INDENT> obj = get_object ( device , dev_group , object_string ) <NEWLINE> <COMMENT> <NL> if obj is False : <NEWLINE> <INDENT> if <STRING> in object_string : <NEWLINE> <INDENT> obj = ipaddress . ip_address ( unicode ( destination_ip ) ) <NEWLINE> destination_range = object_string . split ( <STRING> ) <NEWLINE> destination_lower = ipaddress . ip_address ( unicode ( destination_range [ 0 ] ) ) <NEWLINE> destination_upper = ipaddress . ip_address ( unicode ( destination_range [ 1 ] ) ) <NEWLINE> if destination_lower <= obj <= destination_upper : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if destination_ip == object_string : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if isinstance ( obj , objects . AddressObject ) and addr_in_obj ( destination_ip , obj ) : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> elif isinstance ( obj , objects . AddressGroup ) and obj . static_value : <NEWLINE> <INDENT> for member_string in obj . static_value : <NEWLINE> <INDENT> member = get_object ( device , dev_group , member_string ) <NEWLINE> if addr_in_obj ( destination_ip , member ) : <NEWLINE> <INDENT> destination_ip_match = True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> hitlist . append ( destination_ip_match ) <NEWLINE> <DEDENT>
for sample_idx in range ( len ( y_train_tokenized ) ) : <NEWLINE>
return_key_values = _get_versions ( ) <NEWLINE> <INDENT> return_key_values [ <STRING> ] = default_keys_values [ <STRING> ] <NEWLINE> return return_key_values <NEWLINE> <DEDENT>
def get_web_config_apps ( web_config ) : <NEWLINE> <INDENT> doc = minidom . parse ( web_config ) <NEWLINE> for fcgi in doc . getElementsByTagName ( <STRING> ) : <NEWLINE> <INDENT> for app in fcgi . getElementsByTagName ( <STRING> ) : <NEWLINE> <INDENT> yield dict ( ( key , value ) for key , value in app . attributes . items ( ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
@ csrf_exempt <NEWLINE> <INDENT> def createAccount ( request ) : <NEWLINE> <INDENT> if <STRING> not in request . POST and <STRING> not in request . POST : <NEWLINE> <INDENT> return HttpResponse ( status = 500 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def data_is_valid ( post_data , postback_url = POSTBACK_URL ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> post_str = urlencode ( _values_to_encode ( post_data ) ) <NEWLINE> try : <NEWLINE> <INDENT> response = urllib2 . urlopen ( postback_url , post_str ) . read ( ) <NEWLINE> <DEDENT> except urllib2 . HTTPError : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> if response == <STRING> : <NEWLINE> <INDENT> return True <NEWLINE> <DEDENT> if response == <STRING> : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> display_filters = [ ] <NEWLINE> if <STRING> in request . GET : <NEWLINE> <INDENT> filter_val = request . GET [ <STRING> ] <NEWLINE> <COMMENT> <NL> q = q . query ( collection_label = <STRING> % filter_val ) <NEWLINE> <COMMENT> <NL> unfacet_urlopts = url_params . copy ( ) <NEWLINE> del unfacet_urlopts [ <STRING> ] <NEWLINE> display_filters . append ( ( <STRING> , filter_val , <NEWLINE> <INDENT> unfacet_urlopts . urlencode ( ) ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
tissues = samples . groupby ( <STRING> ) . groups <NEWLINE> <INDENT> norm = normalize . Normalizer ( <STRING> ) <NEWLINE> for t in tissues : <NEWLINE> <INDENT> print ( <STRING> . format ( t ) ) <NEWLINE> <COMMENT> <NL> index = list ( samples [ <STRING> ] . loc [ tissues [ t ] ] . values ) <NEWLINE> tissue_expression = ( expression [ index ] . T ) <NEWLINE> <COMMENT> <NL> tissue_expression . fillna ( 0.0 , inplace = True ) <NEWLINE> <COMMENT> <NL> tissue_expression . columns = convert . clean_ensembl_ids ( tissue_expression . columns ) <NEWLINE> tissue_expression = normalize . deduplicate ( tissue_expression ) <NEWLINE> <COMMENT> <NL> tpm = norm . tpm_from_counts ( tissue_expression ) <NEWLINE> <COMMENT> <NL> mean = pandas . concat ( <NEWLINE> <INDENT> [ mean , pandas . DataFrame ( tpm . mean ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> median = pandas . concat ( <NEWLINE> <INDENT> [ median , pandas . DataFrame ( tpm . median ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> std = pandas . concat ( <NEWLINE> <INDENT> [ std , pandas . DataFrame ( tpm . std ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> lower_quartile = pandas . concat ( <NEWLINE> <INDENT> [ lower_quartile , pandas . DataFrame ( <NEWLINE> <INDENT> tpm . quantile ( q = 0.25 , axis = 0 ) ) . rename ( columns = { 0.25 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> upper_quartile = pandas . concat ( <NEWLINE> <INDENT> [ upper_quartile , pandas . DataFrame ( <NEWLINE> <INDENT> tpm . quantile ( q = 0.75 , axis = 0 ) ) . rename ( columns = { 0.75 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> fraction_zero = pandas . concat ( [ <NEWLINE> <INDENT> fraction_zero , pandas . DataFrame ( <NEWLINE> <INDENT> ( tpm == 0 ) . mean ( ) . astype ( float ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> clr = norm . clr_from_tpm ( tpm , imputer = normalize . impute ) <NEWLINE> mean_clr = pandas . concat ( <NEWLINE> <INDENT> [ mean_clr , pandas . DataFrame ( clr . mean ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> median_clr = pandas . concat ( <NEWLINE> <INDENT> [ median_clr , pandas . DataFrame ( clr . median ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> std_clr = pandas . concat ( <NEWLINE> <INDENT> [ std_clr , pandas . DataFrame ( clr . std ( ) , columns = [ t ] ) ] , axis = 1 ) <NEWLINE> <DEDENT> lower_quartile_clr = pandas . concat ( <NEWLINE> <INDENT> [ lower_quartile_clr , pandas . DataFrame ( <NEWLINE> <INDENT> clr . quantile ( q = 0.25 , axis = 0 ) ) . rename ( columns = { 0.25 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> upper_quartile_clr = pandas . concat ( <NEWLINE> <INDENT> [ upper_quartile_clr , pandas . DataFrame ( <NEWLINE> <INDENT> clr . quantile ( q = 0.75 , axis = 0 ) ) . rename ( columns = { 0.75 : t } ) ] , axis = 1 ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if DatabasePopulator . Tags . POST_OP in kwargs and not is_test : <NEWLINE> <INDENT> fn = kwargs [ DatabasePopulator . Tags . POST_OP ] <NEWLINE> value = self . _database_populator . get_replacement ( <NEWLINE> <INDENT> key , <NEWLINE> modify_before_resaving_fn = fn , <NEWLINE> default_value = default_value , <NEWLINE> <DEDENT> ) <NEWLINE> else : <NEWLINE> value = self . _database_populator . get_replacement ( <NEWLINE> <INDENT> key , <NEWLINE> default_value = default_value , <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
class KademliaProtocol ( RPCProtocol ) : <NEWLINE> <INDENT> def __init__ ( self , sourceNode , storage , ksize ) : <NEWLINE> <INDENT> RPCProtocol . __init__ ( self ) <NEWLINE> self . router = RoutingTable ( self , ksize , sourceNode ) <NEWLINE> self . storage = storage <NEWLINE> self . sourceID = sourceNode . id <NEWLINE> self . log = Logger ( system = self ) <NEWLINE> <DEDENT> <DEDENT>
if self . server . secret_key is None : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> secret_key_name = <STRING> . format ( <NEWLINE> <COMMENT> <NL> <INDENT> name . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT> ) <NEWLINE> secret_key = os . environ . get ( <NEWLINE> <INDENT> secret_key_name , SeaSurf ( ) . _generate_token ( ) <NEWLINE> <DEDENT> ) <NEWLINE> os . environ [ secret_key_name ] = secret_key <NEWLINE> self . server . secret_key = secret_key <NEWLINE> <DEDENT>
generate_classes ( <STRING> , METADATA_PATH ) <NEWLINE> <INDENT> from default_namespace . MyComponent import MyComponent as MyComponent_buildtime <NEWLINE> from default_namespace . A import A as A_buildtime <NEWLINE> <DEDENT>
def check_for_tweets ( self , last_max_id = None ) : <NEWLINE> <INDENT> statuses = [ 0 ] <COMMENT> <NEWLINE> last_min_id = None <NEWLINE> max_id = 0 <NEWLINE> tweets_read = 0 <NEWLINE> click . echo ( click . style ( <NEWLINE> <INDENT> <STRING> . format ( last_max_id ) , fg = <STRING> ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> favourite_counts = [ ] <NEWLINE> retweet_counts = [ ] <NEWLINE> while len ( statuses ) > 0 and ( last_max_id is None or last_min_id is None or last_min_id < last_max_id ) : <NEWLINE> <INDENT> statuses = self . api . GetUserTimeline ( <NEWLINE> <INDENT> include_rts = True , <NEWLINE> exclude_replies = False , <NEWLINE> max_id = last_min_id , <NEWLINE> count = 200 <NEWLINE> <DEDENT> ) <NEWLINE> tweets_read += len ( statuses ) <NEWLINE> for status in statuses : <NEWLINE> <INDENT> max_id = max ( [ status . id , max_id ] ) <NEWLINE> if last_min_id : <NEWLINE> <INDENT> last_min_id = min ( [ status . id - 1 , last_min_id ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> last_min_id = status . id - 1 <NEWLINE> <DEDENT> self . to_be_deleted ( status ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _evaluate_predict_element ( model , triple_index , num_expands , element_type , rank_fn , ranks_list , filtered_ranks_list ) : <NEWLINE> <INDENT> batch = data . expand_triple_to_sets ( kgekit . data . unpack ( triple_index ) , num_expands , element_type ) <NEWLINE> batch = data . convert_triple_tuple_to_torch ( batch ) <NEWLINE> logging . debug ( element_type ) <NEWLINE> logging . debug ( <STRING> + str ( len ( batch ) ) + <STRING> + str ( batch [ 0 ] ) ) <NEWLINE> predicted_batch = model . forward ( batch ) . cpu ( ) <NEWLINE> logging . debug ( <STRING> + str ( len ( predicted_batch ) ) + <STRING> + str ( predicted_batch [ 0 ] ) ) <NEWLINE> rank , filtered_rank = rank_fn ( predicted_batch . data . numpy ( ) , triple_index ) <NEWLINE> logging . debug ( <STRING> + str ( rank ) + <STRING> + str ( filtered_rank ) ) <NEWLINE> ranks_list . append ( rank ) <NEWLINE> filtered_ranks_list . append ( filtered_rank ) <NEWLINE> <DEDENT>
if config . save_per_epoch > 0 and i_epoch % config . save_per_epoch : <NEWLINE> <INDENT> save_checkpoint ( { <NEWLINE> <INDENT> <STRING> : i_epoch , <NEWLINE> <STRING> : model . state_dict ( ) , <NEWLINE> <STRING> : optimizer . state_dict ( ) , <NEWLINE> <DEDENT> } , <NEWLINE> <INDENT> config , <NEWLINE> postfix_num = i_epoch ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if timeout is not None : <NEWLINE> <INDENT> txn . setex ( cache_key , timeout , data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> txn . set ( cache_key , data ) <NEWLINE> <DEDENT> <DEDENT>
def open_fs ( self , fs_url , parse_result , writeable , create , cwd ) : <NEWLINE> <INDENT> bucket_name , _ , dir_path = parse_result . resource . partition ( <STRING> ) <NEWLINE> if not bucket_name : <NEWLINE> <INDENT> raise OpenerError ( <STRING> . format ( fs_url ) ) <NEWLINE> <DEDENT> strict = ( <NEWLINE> <INDENT> parse_result . params [ <STRING> ] == <STRING> <NEWLINE> if <STRING> in parse_result . params <NEWLINE> else False <NEWLINE> <DEDENT> ) <NEWLINE> s3fs = S3FS ( <NEWLINE> <INDENT> bucket_name , <NEWLINE> dir_path = dir_path or <STRING> , <NEWLINE> aws_access_key_id = parse_result . username or None , <NEWLINE> aws_secret_access_key = parse_result . password or None , <NEWLINE> endpoint_url = parse_result . params . get ( <STRING> , None ) , <NEWLINE> acl = parse_result . params . get ( <STRING> , None ) , <NEWLINE> cache_control = parse_result . params . get ( <STRING> , None ) , <NEWLINE> strict = strict , <NEWLINE> <DEDENT> ) <NEWLINE> return s3fs <NEWLINE> <DEDENT>
def callback ( ctx , param , value ) : <NEWLINE> <INDENT> if not value or ctx . resilient_parsing : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> prog = prog_name <NEWLINE> if prog is None : <NEWLINE> <INDENT> prog = ctx . find_root ( ) . info_name <NEWLINE> <DEDENT> ver = version <NEWLINE> if ver is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> import pkg_resources <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for dist in pkg_resources . working_set : <NEWLINE> <INDENT> scripts = dist . get_entry_map ( ) . get ( <STRING> ) or { } <NEWLINE> for script_name , entry_point in iteritems ( scripts ) : <NEWLINE> <INDENT> if entry_point . module_name == module : <NEWLINE> <INDENT> ver = dist . version <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> if ver is None : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> echo ( message % { <NEWLINE> <INDENT> <STRING> : prog , <NEWLINE> <STRING> : ver , <NEWLINE> <DEDENT> } ) <NEWLINE> ctx . exit ( ) <NEWLINE> <DEDENT>
old_env = { } <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> for key , value in iteritems ( env ) : <NEWLINE> <INDENT> old_env [ key ] = os . environ . get ( key ) <NEWLINE> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> yield bytes_output <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> for key , value in iteritems ( old_env ) : <NEWLINE> <INDENT> if value is None : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> del os . environ [ key ] <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> os . environ [ key ] = value <NEWLINE> <DEDENT> <DEDENT> sys . stdout = old_stdout <NEWLINE> sys . stderr = old_stderr <NEWLINE> sys . stdin = old_stdin <NEWLINE> clickpkg . termui . visible_prompt_func = old_visible_prompt_func <NEWLINE> clickpkg . termui . hidden_prompt_func = old_hidden_prompt_func <NEWLINE> clickpkg . termui . _getchar = old__getchar_func <NEWLINE> clickpkg . utils . should_strip_ansi = old_should_strip_ansi <NEWLINE> clickpkg . formatting . FORCED_WIDTH = old_forced_width <NEWLINE> <DEDENT> <DEDENT>
for cookie in cookies : <NEWLINE> <INDENT> loose_cookie = Cookie . to_morsel ( cookie ) <NEWLINE> loose_cookies . append ( ( loose_cookie . key , loose_cookie ) ) <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> content = self . __update_with_locales ( raw_content ) <NEWLINE> except UnicodeError as e : <NEWLINE> result = chardet . detect ( raw_content ) <NEWLINE> if not result or result [ <STRING> ] in [ <STRING> , settings . FILE_CHARSET ] : <NEWLINE> <COMMENT> <NL> <INDENT> raise ImproperlyConfigured ( <NEWLINE> <INDENT> <STRING> % self . path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
odds [ i ] [ <STRING> ] = { <STRING> : yes , <STRING> : no } <NEWLINE>
def __init__ ( self , rest , request ) : <NEWLINE> <INDENT> self . rest = rest <NEWLINE> self . request = request <NEWLINE> service = getUtility ( IIntIds ) <NEWLINE> self . get_id = service . register <NEWLINE> self . get_content = service . getObject <NEWLINE> self . get_metadata = getUtility ( IMetadataService ) . getMetadataValue <NEWLINE> self . get_icon = IIconResolver ( self . request ) . get_content_url <NEWLINE> self . check_permission = getSecurityManager ( ) . checkPermission <NEWLINE> locale = self . request . locale <NEWLINE> formatter = locale . dates . getFormatter ( <STRING> , <STRING> ) <NEWLINE> self . format_date = lambda d : formatter . format ( d . asdatetime ( ) ) <NEWLINE> self . format_author = lambda a : a . userid ( ) <NEWLINE> if getUtility ( IMemberService ) . get_display_usernames ( ) : <NEWLINE> <INDENT> self . format_author = lambda a : a . fullname ( ) <NEWLINE> <DEDENT> <DEDENT>
for immigrant in immigrants : <NEWLINE> <INDENT> _ , idxs = ktournament ( 2 , population ) <NEWLINE> bad_idx = idxs [ 1 ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> text_lower = text . lower ( ) <NEWLINE> <COMMENT> <NL> text_norm = text_lower . translate ( str . maketrans ( string . punctuation , <STRING> * len ( string . punctuation ) ) ) . strip ( ) <NEWLINE> <COMMENT> <NL> text_list = text_norm . split ( <STRING> ) <NEWLINE> <DEDENT>
default_conf = BGCONF <NEWLINE> <INDENT> default_conf [ <STRING> ] [ logger . name ] = { <NEWLINE> <INDENT> <STRING> : logging . DEBUG if debug else logging . INFO <NEWLINE> <DEDENT> } <NEWLINE> if conf is None : <NEWLINE> <INDENT> conf_ = default_conf <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> conf_ = copy . deepcopy ( default_conf ) <NEWLINE> override_dict ( conf_ , conf ) <NEWLINE> <DEDENT> <DEDENT>
def run ( solution , installer , builder = Builder ( ) ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> args = parse_args ( ) <NEWLINE> manager = Manager ( installer , solution ) <NEWLINE> if args . quail_rm : <NEWLINE> <INDENT> shutil . rmtree ( args . quail_rm ) <NEWLINE> <DEDENT> elif args . quail_build and helper . running_from_script ( ) : <NEWLINE> <INDENT> builder . register ( solution ) <NEWLINE> builder . build ( ) <NEWLINE> <DEDENT> elif args . quail_uninstall : <NEWLINE> <INDENT> manager . uninstall ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if manager . is_installed ( ) : <NEWLINE> <INDENT> manager . run ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> manager . install ( ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if width > 250 : <NEWLINE> <INDENT> logger . warn ( <STRING> ) <NEWLINE> return False <NEWLINE> else : <NEWLINE> logger . info ( <STRING> ) <NEWLINE> return True <NEWLINE> except ImportError : <NEWLINE> logger . warn ( <STRING> ) <NEWLINE> return False <NEWLINE> <DEDENT>
result = { } <NEWLINE> <INDENT> for key , value in current . items ( ) : <NEWLINE> <INDENT> if key not in grammar : <NEWLINE> <INDENT> raise ValidationError ( <STRING> % key ) <NEWLINE> <DEDENT> result [ key ] = self . _validate_detail ( value , grammar [ key ] ) <NEWLINE> else : <NEWLINE> <COMMENT> <NL> <DEDENT> if type ( grammar ) != type ( current ) : <NEWLINE> <INDENT> raise ValidationError ( <STRING> <NEWLINE> <INDENT> % ( grammar , current ) ) <NEWLINE> return result <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
for dep in module_dependencies : <NEWLINE> <INDENT> dep_name = dependencies [ dep ] [ _Package . system ( ) ] <NEWLINE> yield <STRING> % ( _Package . install ( ) , dep_name ) <NEWLINE> for cmd in commands : <NEWLINE> yield cmd <NEWLINE> <DEDENT>
def _update ( self , d , values ) : <NEWLINE> <INDENT> if isinstance ( values , list ) : <NEWLINE> <INDENT> for v in values : <NEWLINE> <INDENT> if v . title in d : <NEWLINE> <INDENT> self . handle_duplicate_key ( v . title ) <NEWLINE> <DEDENT> d [ v . title ] = v <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if values . title in d : <NEWLINE> <INDENT> self . handle_duplicate_key ( values . title ) <NEWLINE> <DEDENT> d [ values . title ] = values <NEWLINE> <DEDENT> return values <NEWLINE> <DEDENT>
LOGGER . debug ( <STRING> , base_path ) <NEWLINE> <INDENT> hash_val = calculate_hash ( base_path , hash_algorithm ) <NEWLINE> <DEDENT>
parent_path = <STRING> <NEWLINE> <INDENT> for filepath in glob . glob ( <NEWLINE> <INDENT> <STRING> ) : <NEWLINE> style_path = filepath [ len ( parent_path ) : ] <NEWLINE> LOGGER . debug ( style_path ) <NEWLINE> style_name = os . path . splitext ( os . path . basename ( filepath ) ) [ 0 ] <NEWLINE> payload = { <NEWLINE> <STRING> : { <NEWLINE> <STRING> : style_name , <NEWLINE> <STRING> : filepath <NEWLINE> } <NEWLINE> } <NEWLINE> LOGGER . debug ( payload ) <NEWLINE> <DEDENT> <DEDENT>
class Argument ( object ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def __init__ ( self , name , default = None ) : <NEWLINE> <COMMENT> <NL> <INDENT> self . name = name . replace ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> self . underscored = name . replace ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> self . default = default <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if self . name . startswith ( <STRING> ) : <NEWLINE> <INDENT> self . flag = True <NEWLINE> self . default = False <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . flag = False <NEWLINE> <COMMENT> <NL> <DEDENT> if default == None : <NEWLINE> <INDENT> self . necessary = True <NEWLINE> <COMMENT> <NL> <DEDENT> else : <NEWLINE> <INDENT> self . necessary = False <NEWLINE> <COMMENT> <NL> <DEDENT> self . description = <STRING> <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , reason , response = None ) : <NEWLINE> <INDENT> self . reason = reason <NEWLINE> self . response = response <NEWLINE> Exception . __init__ ( self , reason ) <NEWLINE> <DEDENT>
lookup = DNSRecord ( q = DNSQuestion ( qname , qtype , qclass ) ) <NEWLINE> <INDENT> id = lookup . header . id <NEWLINE> self . peers [ id ] = peer <NEWLINE> self . requests [ id ] = request <NEWLINE> <DEDENT>
outDataset = _copy_dataset_config ( inDataset , outMap = outRas , <NEWLINE> <INDENT> bands = bands ) <NEWLINE> <DEDENT>
if ax is not None : <NEWLINE> <INDENT> old_axes = axes_handler ( ax ) <NEWLINE> if type ( x ) is not list : <NEWLINE> x = [ x ] <NEWLINE> y = [ y ] <NEWLINE> L = len ( x ) <NEWLINE> plot_par = dict_splicer ( plot_par , L , [ len ( i ) for i in x ] ) <NEWLINE> for i in range ( L ) : <NEWLINE> plt . scatter ( x [ i ] , y [ i ] , ** plot_par [ i ] ) <NEWLINE> if clabel is not None : <NEWLINE> cbar = plt . colorbar ( ) <NEWLINE> cbar . set_label ( clabel ) <NEWLINE> if cbar_invert : <NEWLINE> <INDENT> cbar . ax . invert_yaxis ( ) <NEWLINE> if plabel [ 0 ] is not None : <NEWLINE> <DEDENT> plt . legend ( loc = lab_loc ) <NEWLINE> if not multi : <NEWLINE> plot_finalizer ( xlog , ylog , xlim , ylim , title , xlabel , ylabel , xinvert , yinvert ) <NEWLINE> if ax is not None : <NEWLINE> old_axes = axes_handler ( old_axes ) <NEWLINE> <DEDENT>
if ( labels is not None ) : <NEWLINE> <INDENT> if ( ii == npar - 1 ) : <NEWLINE> <INDENT> axes [ ii , jj ] . set_xlabel ( labels [ jj ] ) <NEWLINE> <DEDENT> if ( jj == 0 and ii != 0 ) : <NEWLINE> <INDENT> axes [ ii , jj ] . set_ylabel ( labels [ ii ] ) <NEWLINE> <DEDENT> if ( len ( pair_type ) == 2 and jj == npar - 1 and ii != npar - 1 ) : <NEWLINE> <INDENT> axes [ ii , jj ] . set_ylabel ( labels [ ii ] ) <NEWLINE> axes [ ii , jj ] . yaxis . tick_right ( ) <NEWLINE> axes [ ii , jj ] . yaxis . set_label_position ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
self . slices_ids . append ( actual_slice ) <NEWLINE> <INDENT> yield g , previous_slice <NEWLINE> <DEDENT>
def range_maker ( low , hi , step ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> def counter ( ) : <NEWLINE> <INDENT> nonlocal low <NEWLINE> nonlocal hi <NEWLINE> nonlocal step <NEWLINE> cur = low <NEWLINE> while cur <= hi : <NEWLINE> <INDENT> yield cur <NEWLINE> cur += step <NEWLINE> <DEDENT> <DEDENT> return counter <NEWLINE> <DEDENT>
if url is None : <NEWLINE> <INDENT> self . url = slugify ( name ) <NEWLINE> else : <NEWLINE> self . url = url <NEWLINE> self . name = name <NEWLINE> <DEDENT>
def get_catalog ( self , manifest ) : <NEWLINE> <INDENT> schema_map = self . _get_catalog_schemas ( manifest ) <NEWLINE> if len ( schema_map ) > 1 : <NEWLINE> <INDENT> dbt . exceptions . raise_compiler_error ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> <DEDENT>
with contextlib2 . ExitStack ( ) as tf_record_close_stack : <NEWLINE> <INDENT> output_tfrecords = tf_record_creation_util . open_sharded_output_tfrecords ( <NEWLINE> <INDENT> tf_record_close_stack , <NEWLINE> tfrecord_path , <NEWLINE> total_shards , <NEWLINE> ) <NEWLINE> <DEDENT> for index , group in enumerate ( filename_groups ) : <NEWLINE> <INDENT> tf_example = _create_tf_example ( label_indices , group , images_dir ) <NEWLINE> output_shard_index = index % total_shards <NEWLINE> output_tfrecords [ output_shard_index ] . write ( tf_example . SerializeToString ( ) ) <NEWLINE> <DEDENT> <DEDENT>
if ( year is not None ) and ( month is not None ) : <NEWLINE> <INDENT> _date = datetime . date ( year , month , 1 ) <NEWLINE> else : <NEWLINE> _date = date <NEWLINE> return _manager . get_events ( _date ) <NEWLINE> <DEDENT>
def theme_selector ( theme_dicts : dict ) -> str : <NEWLINE> <INDENT> <STRING> <NEWLINE> os . environ [ <STRING> ] += <STRING> <NEWLINE> selected = iterfzf ( theme_name_iter ( theme_dicts ) ) <NEWLINE> if selected is None : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
UserKey = GithubObject ( <NEWLINE> <INDENT> <STRING> , <NEWLINE> BaseUrl ( lambda obj : <STRING> + str ( obj . id ) ) , <NEWLINE> InternalSimpleAttributes ( <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <DEDENT> ) , <NEWLINE> Editable ( [ ] , [ <STRING> , <STRING> ] ) , <NEWLINE> Deletable ( ) , <NEWLINE> ) <NEWLINE> <DEDENT>
if target_endpoint is None : <NEWLINE> <INDENT> addr = target_fqdn if target_fqdn is not None else map_socket . destination_ip <NEWLINE> target_node = Node ( <NEWLINE> <INDENT> name = addr + <STRING> + str ( map_socket . destination_port ) , <NEWLINE> container_id = target_container . id , <NEWLINE> ignore_sync = True <NEWLINE> <DEDENT> ) <NEWLINE> target_node . save ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if filter_cols : <NEWLINE> <INDENT> filter_values = [ filter_values ] if not isinstance ( filter_values , list ) else filter_values <NEWLINE> filter_cols = [ filter_cols ] if not isinstance ( filter_cols , list ) else filter_cols <NEWLINE> for col in filter_cols : <NEWLINE> <INDENT> ts = ts [ ts [ col ] . isin ( filter_values ) ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
myflow = [ <NEWLINE> <INDENT> foo , <NEWLINE> task . IfTask ( check , [ bar ] , [ baz ] ) , <NEWLINE> task . MapTask ( dig_it , [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <DEDENT> ] ) , <NEWLINE> finish , <NEWLINE> ] <NEWLINE> <DEDENT>
myflow = [ <NEWLINE> <INDENT> foo , <NEWLINE> task . IfTask ( check , [ bar ] , [ baz ] ) , <NEWLINE> task . MapTask ( dig_it , [ <NEWLINE> <INDENT> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <STRING> , <NEWLINE> <DEDENT> ] ) , <NEWLINE> finish , <NEWLINE> ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> account_list = AccountList ( config , loggers , callbacks ) <NEWLINE> accounts = account_list . load ( ) <NEWLINE> <DEDENT>
self . _defineSourceRelation ( self . inputClasses , vol ) <NEWLINE>
def _processMovie ( self , movieId , movieName , movieFolder ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> inMovieName = os . path . join ( movieFolder , movieName ) <NEWLINE> if movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = movieName <NEWLINE> <DEDENT> elif movieName . endswith ( <STRING> ) : <NEWLINE> <INDENT> movieNameAux = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> createLink ( inMovieName , movieNameAux ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> micFnMrc = pwutils . replaceExt ( inMovieName , <STRING> ) <NEWLINE> ImageHandler ( ) . convert ( inMovieName , micFnMrc , DT_FLOAT ) <NEWLINE> movieNameAux = pwutils . replaceExt ( movieName , <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if _showVol is not None : <NEWLINE> <COMMENT> <NL> <INDENT> showVolFileName = os . path . abspath ( <NEWLINE> <INDENT> ImageHandler . removeFileType ( _showVol . getFileName ( ) ) ) <NEWLINE> <DEDENT> f . write ( <STRING> % showVolFileName ) <NEWLINE> if _showVol . hasOrigin ( ) : <NEWLINE> <INDENT> x , y , z = _showVol . getOrigin ( ) . getShifts ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> x , y , z = _showVol . getOrigin ( force = True ) . getShifts ( ) <NEWLINE> <DEDENT> <DEDENT>
label_seq_id = str ( resseq ) <NEWLINE>
label_seq_id = str ( resseq ) <NEWLINE>
@ classmethod <NEWLINE> <INDENT> def createEmptyImage ( cls , fnOut , xDim = 1 , yDim = 1 , zDim = 1 , nDim = 1 , <NEWLINE> <INDENT> dataType = None ) : <NEWLINE> dt = dataType or cls . DT_FLOAT <NEWLINE> xmippLib . createEmptyFile ( fnOut , xDim , yDim , zDim , nDim , dt ) <NEWLINE> <DEDENT> <DEDENT>
generator = load_generator ( prog , directed , gentype ) <NEWLINE> <INDENT> generator . run ( nodes , edges , sr ) <NEWLINE> print ( <STRING> % generator . is_constant ( ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> base_generator = create_generator ( directed , gen_type ) <NEWLINE> if base_generator is None : <NEWLINE> <INDENT> self . error_msg = <STRING> % gen_type <NEWLINE> return False <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> gen = load_generator ( prog , directed , gentype ) <NEWLINE> net = gen . run ( nodes , edges , sr ) <NEWLINE> <DEDENT>
for arg in infiles : <NEWLINE> <INDENT> os . system ( <STRING> + arg + <STRING> + dirname ) <NEWLINE> <DEDENT>
if not path . exists ( DIR + <STRING> + <STRING> ) : <NEWLINE> <INDENT> seedNew = random . randint ( 1 , 1000001 ) <NEWLINE> self . seed = seedNew <NEWLINE> <COMMENT> <NL> <DEDENT>
def test_run ( ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> d = testing . play_data ( ) <NEWLINE> models = [ nx . logistic ( ) , fifty ( ) ] <NEWLINE> splitters = [ nx . TournamentSplitter ( d ) , <NEWLINE> <INDENT> nx . ValidationSplitter ( d ) , <NEWLINE> nx . CheatSplitter ( d ) , <NEWLINE> nx . CVSplitter ( d , kfold = 2 ) , <NEWLINE> nx . SplitSplitter ( d , fit_fraction = 0.5 ) ] <NEWLINE> <DEDENT> for model in models : <NEWLINE> <INDENT> for splitter in splitters : <NEWLINE> <INDENT> nx . run ( model , splitter , tournament = 2 , verbosity = 0 ) <NEWLINE> nx . run ( model , splitter , tournament = <STRING> , verbosity = 0 ) <NEWLINE> p = nx . run ( model , splitter , tournament = None , verbosity = 0 ) <NEWLINE> ok_ ( p . shape [ 1 ] == 5 , <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def calc_metrics_arrays ( y , yhat , columns ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> idx = np . isfinite ( y + yhat ) <NEWLINE> y = y [ idx ] <NEWLINE> yhat = yhat [ idx ] <NEWLINE> metrics = [ ] <NEWLINE> for col in columns : <NEWLINE> <INDENT> if col == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> m = spearmanr ( y , yhat ) . correlation <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> m = np . nan <NEWLINE> <DEDENT> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> m = spearmanr ( y , yhat ) . correlation > CORR_BENCHMARK <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> m = np . nan <NEWLINE> <DEDENT> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> m = np . mean ( ( y - yhat ) ** 2 ) <NEWLINE> <DEDENT> except ValueError : <NEWLINE> <INDENT> m = np . nan <NEWLINE> <DEDENT> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . min ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . max ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . mean ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . std ( ) <NEWLINE> <DEDENT> elif col == <STRING> : <NEWLINE> <INDENT> m = yhat . size <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> . format ( col ) ) <NEWLINE> <DEDENT> metrics . append ( m ) <NEWLINE> <DEDENT> return metrics <NEWLINE> <DEDENT>
unit = unit or self . select_build_worker ( p ) <NEWLINE> <INDENT> if unit is None or not self . can_afford ( building ) : <NEWLINE> <INDENT> return ActionResult . Error <NEWLINE> <DEDENT> return await self . do ( unit . build ( building , p ) ) <NEWLINE> <DEDENT>
if self . storage_format <= storage_format : <NEWLINE> <INDENT> replacement = self . values [ 0 ] <NEWLINE> else : <NEWLINE> replacement = self . values [ 1 ] <NEWLINE> <DEDENT>
for strategy_str in strategy_strs : <NEWLINE> <INDENT> dir = os . path . join ( linnea . config . results_path , args . experiment , <STRING> , strategy_str ) <NEWLINE> if not os . path . exists ( dir ) : <NEWLINE> <INDENT> os . makedirs ( dir ) <NEWLINE> <DEDENT> <DEDENT>
df_code_gen_time = pd . DataFrame ( [ t_end - t_start ] , index = [ name ] , columns = [ <STRING> ] ) <NEWLINE>
return algorithms <NEWLINE>
return session . Session ( auth = remote_auth ) <NEWLINE>
def iter_changes_since ( self , since ) : <NEWLINE> <INDENT> for change in self . news [ <STRING> ] : <NEWLINE> <INDENT> if change [ <STRING> ] > since : <NEWLINE> <INDENT> yield change <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def versions_from_parentdir ( parentdir_prefix , root , verbose = False ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> dirname = os . path . basename ( root ) <NEWLINE> if not dirname . startswith ( parentdir_prefix ) : <NEWLINE> <INDENT> if verbose : <NEWLINE> <INDENT> print ( <NEWLINE> <INDENT> ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) % ( root , dirname , parentdir_prefix ) <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return None <NEWLINE> <DEDENT> ret = dirname [ len ( parentdir_prefix ) : ] <NEWLINE> if ret . find ( <STRING> ) != - 1 : <NEWLINE> <INDENT> ret = ret [ : ret . find ( <STRING> ) ] <NEWLINE> <DEDENT> return { <STRING> : ret , <STRING> : <STRING> } <NEWLINE> <DEDENT>
fp = request . fingerprint ( ) <NEWLINE> <INDENT> if fp in info . downloaded : <NEWLINE> <INDENT> cached = info . downloaded [ fp ] <NEWLINE> defer_result ( cached ) . chainDeferred ( wad ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> info . waiting . setdefault ( fp , [ ] ) . append ( wad ) <NEWLINE> if fp not in info . downloading : <NEWLINE> <INDENT> self . _download ( request , info , fp ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def _itemproc_finished ( self , output , item , spider ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> self . sites [ spider ] . itemproc_size -= 1 <NEWLINE> if isinstance ( output , Failure ) : <NEWLINE> <INDENT> ex = output . value <NEWLINE> if isinstance ( ex , DropItem ) : <NEWLINE> <INDENT> log . msg ( log . formatter . dropped ( item , ex , spider ) , level = log . WARNING , spider = spider ) <NEWLINE> return send_catch_log_deferred ( signal = signals . item_dropped , item = item , spider = spider , exception = output . value ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> log . err ( output , <STRING> % item , spider = spider ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> log . msg ( log . formatter . passed ( output , spider ) , log . INFO , spider = spider ) <NEWLINE> return send_catch_log_deferred ( signal = signals . item_passed , item = item , spider = spider , output = output ) <NEWLINE> <DEDENT> <DEDENT>
def download ( self , request , spider ) : <NEWLINE> <INDENT> slot = self . slots [ spider ] <NEWLINE> slot . add_request ( request ) <NEWLINE> if isinstance ( request , Response ) : <NEWLINE> <INDENT> return request <NEWLINE> <DEDENT> d = self . _download ( request , spider ) <NEWLINE> d . addCallback ( self . download , spider ) <NEWLINE> d . addBoth ( self . _remove_request , slot , request ) <NEWLINE> return d <NEWLINE> <DEDENT>
def _read_meta ( self , spider , request ) : <NEWLINE> <INDENT> rpath = self . _get_request_path ( spider , request ) <NEWLINE> metapath = os . path . join ( rpath , <STRING> ) <NEWLINE> if not os . path . exists ( metapath ) : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> mtime = os . stat ( metapath ) . st_mtime <NEWLINE> if 0 < self . expiration_secs < time ( ) - mtime : <NEWLINE> <INDENT> return <COMMENT> <NEWLINE> <DEDENT> with self . _open ( metapath , <STRING> ) as f : <NEWLINE> <INDENT> return pickle . load ( f ) <NEWLINE> <DEDENT> <DEDENT>
def test_damage_deal ( ) : <NEWLINE> <INDENT> logic . damage_system . deal_damage ( 0 , 2 ) <NEWLINE> enemy_start_health = world . get_health ( 2 ) <NEWLINE> player_damage = world . get_damage ( 0 ) <NEWLINE> if player_damage < enemy_start_health : <NEWLINE> <INDENT> assert len ( world . dead_entities ) == 1 <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> assert world . get_health ( 2 ) == enemy_start_health - player_damage <NEWLINE> <DEDENT> <DEDENT>
self . inst = Gpib . Gpib ( int ( board_index ) , int ( address ) ) <NEWLINE> <INDENT> Driver . __init__ ( self ) <NEWLINE> <DEDENT>
logging . info ( <STRING> ) <NEWLINE> <INDENT> return fo <NEWLINE> <DEDENT>
def positive_only_mse ( y_true , y_pred ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> diff = y_pred - y_true <NEWLINE> mask = y_pred >= 0 <NEWLINE> diff *= mask <NEWLINE> return K . mean ( K . square ( diff ) , axis = - 1 ) <NEWLINE> <DEDENT>
def initVars ( ) : <NEWLINE> <COMMENT> <NL> <INDENT> colHex1 = int ( ra . random ( ) * int ( <STRING> , 16 ) ) <NEWLINE> colHex2 = int ( ra . random ( ) * int ( <STRING> , 16 ) ) <NEWLINE> DEF = { <NEWLINE> <INDENT> <STRING> : QColor . fromRgba ( colHex1 ) . name ( ) , <COMMENT> <NEWLINE> <STRING> : QColor . fromRgba ( colHex2 ) . name ( ) , <NEWLINE> <STRING> : 0 , <COMMENT> <NEWLINE> <STRING> : 350 , <COMMENT> <NEWLINE> <STRING> : 2 , <COMMENT> <NEWLINE> <STRING> : 1 / 8 , <COMMENT> <NEWLINE> <STRING> : 100 , <COMMENT> <NEWLINE> <STRING> : 4 , <COMMENT> <NEWLINE> <STRING> : 60 , <COMMENT> <NEWLINE> <STRING> : 1000 , <COMMENT> <NEWLINE> <STRING> : 100000 , <NEWLINE> <STRING> : 5 , <NEWLINE> <STRING> : 1 , <NEWLINE> <STRING> : True <NEWLINE> <DEDENT> } <NEWLINE> TST = { <NEWLINE> <INDENT> <STRING> : QColor . fromRgba ( colHex1 ) . name ( ) , <COMMENT> <NEWLINE> <STRING> : QColor . fromRgba ( colHex2 ) . name ( ) , <NEWLINE> <STRING> : 0 , <COMMENT> <NEWLINE> <STRING> : 10 , <COMMENT> <NEWLINE> <STRING> : 16 , <COMMENT> <NEWLINE> <STRING> : 1 / 8 , <COMMENT> <NEWLINE> <STRING> : 10 , <COMMENT> <NEWLINE> <STRING> : 4 , <COMMENT> <NEWLINE> <STRING> : 60 , <COMMENT> <NEWLINE> <STRING> : 10 , <COMMENT> <NEWLINE> <STRING> : 100000 , <NEWLINE> <STRING> : 5 , <NEWLINE> <STRING> : 1 , <NEWLINE> <STRING> : True <NEWLINE> <DEDENT> } <NEWLINE> return TST <NEWLINE> <DEDENT>
def all_info ( color ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> divider , slurm_str = <STRING> , <STRING> <NEWLINE> if color : <NEWLINE> <INDENT> colors = sns . color_palette ( <STRING> , 8 ) . as_hex ( ) <NEWLINE> divider = colored . stylize ( divider , colored . fg ( colors [ 7 ] ) ) <NEWLINE> slurm_str = colored . stylize ( slurm_str , colored . fg ( colors [ 0 ] ) ) <NEWLINE> <DEDENT> print ( divider ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( divider ) <NEWLINE> resources = parse_all_gpus ( ) <NEWLINE> states = node_states ( ) <NEWLINE> for mode in ( <STRING> , <STRING> ) : <NEWLINE> <INDENT> summary ( mode = mode , resources = resources , states = states ) <NEWLINE> print ( divider ) <NEWLINE> <DEDENT> in_use ( resources ) <NEWLINE> print ( divider ) <NEWLINE> available ( resources = resources , states = states ) <NEWLINE> print ( divider ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> @ module . event ( <STRING> ) <NEWLINE> @ module . rule ( <STRING> ) <NEWLINE> def parse_event_005 ( bot , trigger ) : <NEWLINE> <INDENT> if trigger . args [ - 1 ] != <STRING> : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> parameters = trigger . args [ 1 : - 1 ] <NEWLINE> for param in parameters : <NEWLINE> <INDENT> if <STRING> in param : <NEWLINE> <INDENT> if param . startswith ( <STRING> ) : <NEWLINE> <INDENT> stderr ( param ) <NEWLINE> param = str ( param ) . split ( <STRING> ) [ 1 ] <NEWLINE> settings = str ( param ) . split ( <STRING> ) <NEWLINE> for setting in settings : <NEWLINE> <INDENT> if not setting . startswith ( tuple ( [ <STRING> , <STRING> ] ) ) : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> setting = str ( settings ) . split ( <STRING> ) [ 0 ] <NEWLINE> value = str ( settings ) . split ( <STRING> ) [ 1 ] or None <NEWLINE> if value : <NEWLINE> <INDENT> if setting == <STRING> : <NEWLINE> <INDENT> bot . config . MAXTARGCONFIG . notice = int ( value ) <NEWLINE> <DEDENT> elif setting == <STRING> : <NEWLINE> <INDENT> bot . config . MAXTARGCONFIG . privmsg = int ( value ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT> stderr ( <STRING> + str ( bot . config . MAXTARGCONFIG . privmsg ) ) <NEWLINE> stderr ( <STRING> + str ( bot . config . MAXTARGCONFIG . notice ) ) <NEWLINE> <DEDENT> <DEDENT>
grad = np . zeros ( data . shape ) <NEWLINE> <INDENT> for i in range ( batchsize ) : <NEWLINE> <INDENT> for j in range ( channel ) : <NEWLINE> <INDENT> for h in range ( 0 , height - kernel_size [ 0 ] + 1 , stride [ 0 ] ) : <NEWLINE> <INDENT> for w in range ( 0 , width - kernel_size [ 1 ] + 1 , stride [ 1 ] ) : <NEWLINE> <INDENT> mask = ( data [ i , j , h : h + kernel_size [ 0 ] , w : w + kernel_size [ 1 ] ] == np . max ( data [ i , j , h : h + kernel_size [ 0 ] , w : w + kernel_size [ 1 ] ] ) ) <NEWLINE> grad [ i , j , h : h + kernel_size [ 0 ] , w : w + kernel_size [ 1 ] ] += mask * grad_output [ i , j , h // stride [ 0 ] , w // stride [ 1 ] ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def backward ( ctx , grad_output ) : <NEWLINE> <INDENT> original_shape = ctx . saved_tensors ( ) [ 0 ] <NEWLINE> grad = grad_output . reshape ( original_shape ) <NEWLINE> return grad <NEWLINE> <DEDENT> <DEDENT>
if task_full_name . startswith ( <STRING> ) : <NEWLINE> <INDENT> task_full_name = task_full_name [ len ( carnival_tasks_module ) + 1 : ] <NEWLINE> <DEDENT>
paths = glob . glob ( os . path . join ( save_dir , <STRING> , <STRING> ) , recursive = True ) <NEWLINE> <INDENT> t0 = datetime . now ( ) <NEWLINE> for idx , path in enumerate ( paths ) : <NEWLINE> <INDENT> if os . path . basename ( os . path . dirname ( path ) ) != <STRING> : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> for lidx , line in enumerate ( utils . readlines ( path ) ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> data = json . loads ( line ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> logger . exception ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> continue <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if data . get ( <STRING> ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> if os . path . basename ( os . path . dirname ( path ) ) == <STRING> : <NEWLINE> <INDENT> data = utils . stream_to_search ( data ) <NEWLINE> <DEDENT> data = utils . timestamp_to_datetime ( data ) <NEWLINE> tweets . replace_one ( { <STRING> : data [ <STRING> ] } , data , upsert = True ) <NEWLINE> <DEDENT> <DEDENT> t_delta = datetime . now ( ) - t0 <NEWLINE> average = t_delta / ( idx + 1 ) <NEWLINE> remaining = str ( ( len ( paths ) - ( idx + 1 ) ) * average ) . split ( <STRING> ) [ 0 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> sql = <STRING> <NEWLINE> cursor . execute ( sql , [ table_name ] ) <NEWLINE> for constraint , column in list ( cursor . fetchall ( ) ) : <NEWLINE> <INDENT> if column not in constraints : <NEWLINE> <INDENT> constraints [ constraint ] = { <NEWLINE> <INDENT> <STRING> : [ ] , <NEWLINE> <STRING> : False , <NEWLINE> <STRING> : False , <NEWLINE> <STRING> : False , <NEWLINE> <STRING> : True , <NEWLINE> <STRING> : None , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> constraints [ constraint ] [ <STRING> ] . append ( column ) <NEWLINE> <DEDENT> <DEDENT>
if create_image ( absolute_path , resized_absolute_path , ** kwargs ) : <NEWLINE> <INDENT> return send_from_directory ( MEDIA_ROOT , resized_filename ) <NEWLINE> abort ( 500 ) <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def insertShocks ( flatData , i , nshk , nexo , shockList , shockPtr , shockVal ) : <NEWLINE> <INDENT> if nshk > 0 : <NEWLINE> <INDENT> start = shockPtr . array [ i , 0 ] - 1 <NEWLINE> for j in range ( 0 , nshk ) : <NEWLINE> <INDENT> shkInd = start + j <NEWLINE> if nshk == nexo : <NEWLINE> <INDENT> varInd = j <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> varInd = shockList . array [ shkInd , 0 ] - 1 <NEWLINE> <DEDENT> flatData [ varInd ] = shockVal . array [ shkInd , 0 ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def validate_categorical ( value , values , ** kwargs ) : <NEWLINE> <INDENT> test = True <NEWLINE> if value not in values : <NEWLINE> <INDENT> test = False <NEWLINE> <DEDENT> return test <NEWLINE> <DEDENT>
if <STRING> in search_space . keys ( ) and <STRING> in search_space . keys ( ) : <NEWLINE> <INDENT> if search_space [ <STRING> ] <= search_space [ <STRING> ] : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
def remoteSliderUpdate ( self , widget , value , sliderMoved = True ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if not sliderMoved : <NEWLINE> <INDENT> for wid in self . remoteWidgetLayout . list : <NEWLINE> <INDENT> if isinstance ( wid , MovableSlider ) : <NEWLINE> <INDENT> if wid . module == widget . module and wid . parameter == widget . parameter : <NEWLINE> <INDENT> wid . setValue ( float ( value ) ) <NEWLINE> wid . valueOn = value <NEWLINE> wid . label . setText ( wid . widgetName + <STRING> + <STRING> . format ( wid . value ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> widget . valueOn = value <NEWLINE> widget . label . setText ( widget . widgetName + <STRING> + <STRING> . format ( widget . value ) ) <NEWLINE> <DEDENT> <DEDENT>
print >> out , <STRING> % ( prefix , k , serialize ( v ) , prefix , k ) <NEWLINE>
for filename in [ <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> ] : <NEWLINE> file_path = os . path . join ( source_dir , filename ) <NEWLINE> if os . path . exists ( file_path ) : <NEWLINE> tarball . add ( file_path , <NEWLINE> <INDENT> arcname = os . path . join ( <STRING> , filename ) ) <NEWLINE> <COMMENT> <NL> open ( os . path . join ( temp_dir , <STRING> ) , <STRING> ) <NEWLINE> tarball . add ( os . path . join ( temp_dir , <STRING> ) , arcname = <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
prebaked = base_image in reduce ( lambda x , y : x + [ y [ 0 ] ] + y [ 1 ] , <NEWLINE> <INDENT> PREBAKED_DISTROS . items ( ) , [ ] ) <NEWLINE> if prebaked : <NEWLINE> base_image = [ k for k , v in PREBAKED_DISTROS . items ( ) <NEWLINE> if base_image in [ k ] + v ] [ 0 ] <NEWLINE> conductor_base = <STRING> % ( <NEWLINE> base_image . replace ( <STRING> , <STRING> ) , <NEWLINE> container . __version__ <NEWLINE> ) <NEWLINE> if not self . get_image_id_by_tag ( conductor_base ) : <NEWLINE> conductor_base = <STRING> % conductor_base <NEWLINE> else : <NEWLINE> conductor_base = <STRING> % ( <NEWLINE> base_image . replace ( <STRING> , <STRING> ) , <NEWLINE> container . __version__ <NEWLINE> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> target_layer . update ( update_params = filtered_payload , token = token ) <NEWLINE> <DEDENT>
start = time . time ( ) <NEWLINE> <INDENT> if self . _n_jobs > 1 : <NEWLINE> <INDENT> pool = multiprocess . Pool ( self . _n_jobs ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> pool = MockPool ( ) <NEWLINE> <DEDENT> <DEDENT>
heap = [ ] <NEWLINE> <INDENT> j = 0 <NEWLINE> for r , d in zip ( rand , data ) : <NEWLINE> <INDENT> if len ( r [ 0 ] ) == 0 : continue <NEWLINE> a = 1.0 * len ( r [ 0 ] ) / nbar <NEWLINE> j = j + 1 <NEWLINE> if len ( heap ) == 0 : <NEWLINE> <INDENT> heapq . heappush ( heap , ( a , j , r , d ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> a0 , j0 , r0 , d0 = heapq . heappop ( heap ) <NEWLINE> if a0 + a < Abar : <NEWLINE> <INDENT> a0 += a <NEWLINE> d0 = [ <NEWLINE> <INDENT> numpy . concatenate ( ( d0 [ i ] , d [ i ] ) , axis = - 1 ) <NEWLINE> for i in range ( len ( d ) ) <NEWLINE> ] <NEWLINE> <DEDENT> r0 = numpy . concatenate ( ( r0 , r ) , axis = - 1 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> heapq . heappush ( heap , ( a , j , r , d ) ) <NEWLINE> <DEDENT> heapq . heappush ( heap , ( a0 , j0 , r0 , d0 ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def login ( self , username , password , mode = <STRING> ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> url = <STRING> <NEWLINE> try : <NEWLINE> <INDENT> self . browser . visit ( url ) <NEWLINE> self . logger . debug ( <STRING> . format ( url = url ) ) <NEWLINE> <DEDENT> except selenium . common . exceptions . WebDriverException : <NEWLINE> <INDENT> self . logger . critical ( <STRING> ) <NEWLINE> return 0 <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> self . _name ( <STRING> ) . fill ( username ) <NEWLINE> self . _name ( <STRING> ) . fill ( password ) <NEWLINE> self . _css ( path [ <STRING> ] ) . click ( ) <NEWLINE> timeout = 30 <NEWLINE> while not self . _elCss ( path [ <STRING> ] ) : <NEWLINE> <INDENT> timeout -= 1 <NEWLINE> if timeout == 0 : <NEWLINE> <INDENT> self . logger . critical ( <STRING> ) <NEWLINE> return 0 <NEWLINE> <DEDENT> <DEDENT> sleep ( 1 ) <NEWLINE> self . logger . debug ( <STRING> ) <NEWLINE> if mode == <STRING> and self . _elCss ( path [ <STRING> ] ) : <NEWLINE> <INDENT> self . _css ( path [ <STRING> ] ) . click ( ) <NEWLINE> <DEDENT> return 1 <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> self . logger . critical ( <STRING> ) <NEWLINE> return 0 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> handler = self . _union_registry . get ( union ) <NEWLINE> if handler is not None : <NEWLINE> <INDENT> return handler ( obj , union ) <NEWLINE> <DEDENT> <DEDENT>
s3_source = <STRING> <NEWLINE> <INDENT> quote_option = <STRING> if delimiter == <STRING> else <STRING> <NEWLINE> region_option = <STRING> if region is not None else <STRING> <NEWLINE> escape_option = <STRING> if escape else <STRING> <NEWLINE> acceptinvchars_option = <STRING> if acceptinvchars else <STRING> <NEWLINE> null_option = <STRING> if null is not None else <STRING> <NEWLINE> aws_token = self . s3_config . get ( <STRING> ) <NEWLINE> aws_token_option = <STRING> if aws_token is not None else <STRING> <NEWLINE> copy_template = <STRING> <NEWLINE> self . run_query ( copy_template ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> ul = [ int ( pt [ 0 ] - 3 * sigma ) , int ( pt [ 1 ] - 3 * sigma ) ] <NEWLINE> br = [ int ( pt [ 0 ] + 3 * sigma + 1 ) , int ( pt [ 1 ] + 3 * sigma + 1 ) ] <NEWLINE> if ( ul [ 0 ] >= img . shape [ 1 ] or ul [ 1 ] >= img . shape [ 0 ] or <NEWLINE> <INDENT> br [ 0 ] < 0 or br [ 1 ] < 0 ) : <NEWLINE> <COMMENT> <NL> return to_torch ( img ) <NEWLINE> <DEDENT> <DEDENT>
def pretty_xml ( path , outpath = None , encoding = <STRING> ) : <NEWLINE> <INDENT> tree = etree . parse ( path ) <NEWLINE> if outpath is None : <NEWLINE> <INDENT> opener = compression . get ( path ) <NEWLINE> outpath = opener ( path , <STRING> ) <NEWLINE> <DEDENT> if hasattr ( outpath , <STRING> ) : <NEWLINE> <INDENT> outstream = outpath <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> opener = compression . get ( path ) <NEWLINE> outstream = opener ( outpath , <STRING> ) <NEWLINE> <DEDENT> with outstream : <NEWLINE> <INDENT> outstream . write ( <STRING> + encoding + <STRING> ) <NEWLINE> outstream . write ( <NEWLINE> <INDENT> etree . tostring ( tree , pretty_print = True ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def differ ( a , b ) : <NEWLINE> <INDENT> if not issubclass ( type ( a ) , type ( b ) ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> if isinstance ( a , dict ) : <NEWLINE> <INDENT> return dict_diff ( a , b ) <NEWLINE> <DEDENT> elif isinstance ( a , ( list , tuple ) ) : <NEWLINE> <INDENT> return all ( differ ( ai , bi ) for ai , bi in zip ( sorted ( a ) , sorted ( b ) ) ) <NEWLINE> <DEDENT> elif isinstance ( a , float ) : <NEWLINE> <INDENT> return abs ( a - b ) < 1e-3 <NEWLINE> <DEDENT> elif isinstance ( a , cvstr ) : <NEWLINE> <INDENT> return a . accession . lower ( ) == b . accession . lower ( ) <NEWLINE> <DEDENT> elif isinstance ( a , np . ndarray ) : <NEWLINE> <INDENT> return np . allclose ( a , b ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return a == b <NEWLINE> <DEDENT> <DEDENT>
options = self . prepare_request ( resource , url , * args ) <NEWLINE> <INDENT> raw = self . patchboard . session . request ( <NEWLINE> <INDENT> self . method , <NEWLINE> url , <NEWLINE> options <NEWLINE> <DEDENT> ) <NEWLINE> response = Response ( raw ) <NEWLINE> if response . status != self . success_status : <NEWLINE> <INDENT> err_msg = ( <STRING> + response . status + <NEWLINE> <INDENT> <STRING> + response . body ) <NEWLINE> <DEDENT> raise PatchboardError ( err_msg ) <NEWLINE> <DEDENT> <DEDENT>
return txt <NEWLINE>
if self . _loader_lock is None and len ( self . _loaders ) == 0 : <NEWLINE> <INDENT> self . _loader_lock = threading . Lock ( ) <NEWLINE> self . _loaders = [ ] <NEWLINE> <DEDENT>
if not os_lib . path . exists ( comp_root_folder ) : <NEWLINE> <INDENT> os_lib . mkdir ( comp_root_folder ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> data = { <STRING> : [ <STRING> ] } <NEWLINE> input_mapper , output_mapper = Automater ( ) . _create_mappers ( data ) <NEWLINE> self . assertEqual ( 1 , len ( input_mapper . features ) ) <NEWLINE> <DEDENT>
def __str__ ( self ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> return vstr ( self ) <NEWLINE> <DEDENT> except ImportError : <NEWLINE> <INDENT> return super ( Document , self ) . __str__ ( ) <NEWLINE> <DEDENT> <DEDENT>
def cleardata ( self , name , startdt , enddt ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> savedata = [ ] <NEWLINE> data = self . data [ name ] <NEWLINE> for val in data : <NEWLINE> <INDENT> if ( val [ 0 ] < startdt ) or ( val [ 0 ] >= enddt ) : <NEWLINE> <INDENT> savedata . append ( val ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if uuid in self . _graph . keys ( ) and self . _graph [ uuid ] . owner != self . _id : <NEWLINE> <INDENT> for node in self . _node_deps ( node = uuid ) : <NEWLINE> <INDENT> self . change_owner ( uuid = node , new_owner = new_owner ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , server , persistence_config = None , ember_config = None , <NEWLINE> <INDENT> node_id = None , storage_nw_ip = None , ** kwargs ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if persistence_config : <NEWLINE> cinderlib_extra_config = ember_config . copy ( ) <NEWLINE> cinderlib_extra_config . pop ( <STRING> ) <NEWLINE> cinderlib_extra_config [ <STRING> ] = False <NEWLINE> cinderlib . setup ( persistence_config = persistence_config , <NEWLINE> <INDENT> ** cinderlib_extra_config ) <NEWLINE> IdentityBase . __init__ ( self , server , ember_config ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , server , persistence_config , backend_config , <NEWLINE> <INDENT> ember_config = None , ** kwargs ) : <NEWLINE> cinderlib_extra_config = ember_config . copy ( ) <NEWLINE> cinderlib_extra_config . pop ( <STRING> ) <NEWLINE> cinderlib . setup ( persistence_config = persistence_config , <NEWLINE> <INDENT> ** cinderlib_extra_config ) <NEWLINE> self . backend = cinderlib . Backend ( ** backend_config ) <NEWLINE> IdentityBase . __init__ ( self , server , ember_config ) <NEWLINE> self . CSI . add_ControllerServicer_to_server ( self , server ) <NEWLINE> <DEDENT> <DEDENT>
if result [ <STRING> ] [ <STRING> ] : <NEWLINE> <INDENT> if result [ <STRING> ] [ <STRING> ] != <STRING> and not silent : <NEWLINE> <INDENT> self . send_update ( <STRING> , { <NEWLINE> <INDENT> <STRING> : self . execution_count , <NEWLINE> <STRING> : { <NEWLINE> <INDENT> <STRING> : result [ <STRING> ] [ <STRING> ] <NEWLINE> <DEDENT> } , <NEWLINE> <STRING> : { } <NEWLINE> <DEDENT> } ) <NEWLINE> <DEDENT> <DEDENT>
def writeSequence ( self , data ) : <NEWLINE> <INDENT> for chunk in data : <NEWLINE> <INDENT> self . write ( chunk ) <NEWLINE> <DEDENT> <DEDENT>
def appendVertex ( vertices , theta , z , dx = dxabs , dy = dyabs , norm = [ ] ) : <NEWLINE> <INDENT> c = _Vector ( [ 0 , 0 , 0 ] ) <NEWLINE> x = dx * ( ( ( self . zMax - z ) / self . zMax ) * _np . cos ( theta ) ) <COMMENT> <NEWLINE> y = dy * ( ( ( self . zMax - z ) / self . zMax ) * _np . sin ( theta ) ) <NEWLINE> d = _Vector ( <NEWLINE> <INDENT> x , <NEWLINE> y , <NEWLINE> z ) <NEWLINE> <DEDENT> if not norm : <NEWLINE> <INDENT> n = d <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> n = _Vector ( norm ) <NEWLINE> <DEDENT> vertices . append ( _Vertex ( c . plus ( d ) , n ) ) <NEWLINE> <DEDENT>
if addRegistry : <NEWLINE> <INDENT> registry . addSolid ( self ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> ws = _g4 . solid . Box ( <STRING> , wx , wy , wz , reg , <STRING> ) <NEWLINE> ps = _g4 . solid . GenericPolyhedra ( <STRING> , psphi , pdphi , pnsid , pr , pz , reg , <STRING> , <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> cols = [ ] <NEWLINE> if <STRING> in df . columns : <NEWLINE> <INDENT> cols += [ <STRING> ] <NEWLINE> <DEDENT> cols += [ col for col in groupby if col in df . columns ] <NEWLINE> cols += [ col for col in metrics if col in df . columns ] <NEWLINE> cols += [ col for col in df . columns if col not in cols ] <NEWLINE> df = df [ cols ] <NEWLINE> return QueryResult ( <NEWLINE> <INDENT> df = df , <NEWLINE> query = query_str , <NEWLINE> duration = datetime . now ( ) - qry_start_dttm ) <NEWLINE> <DEDENT> <DEDENT>
def all_table_names ( self , schema = None , force = False ) : <NEWLINE> <INDENT> if not schema : <NEWLINE> <INDENT> tables_dict = self . db_engine_spec . fetch_result_sets ( <NEWLINE> <INDENT> self , <STRING> , force = force ) <NEWLINE> <DEDENT> return tables_dict . get ( <STRING> , [ ] ) <NEWLINE> <DEDENT> return sorted ( <NEWLINE> <INDENT> self . db_engine_spec . get_table_names ( schema , self . inspector ) ) <NEWLINE> <DEDENT> <DEDENT>
try : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> result = subprocess . run ( <STRING> , check = True , shell = True , <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> input = bytes ( source , sys . getdefaultencoding ( ) ) , <NEWLINE> stdout = subprocess . PIPE , <NEWLINE> stderr = subprocess . PIPE <COMMENT> <NEWLINE> ) <NEWLINE> except OSError as exception : <NEWLINE> <DEDENT> msg = <STRING> <NEWLINE> raise SnutreeError ( msg ) <NEWLINE> <DEDENT>
test_tot_atom += num_batch_atom <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> test_elem , tmp_nne , tmp_eloss = sess . run ( [ self . next_elem , self . E , self . e_loss ] , feed_dict = test_fdict ) <NEWLINE> num_batch_struc = valid_elem [ <STRING> ] - 1 <NEWLINE> eloss += tmp_eloss * num_batch_struc <NEWLINE> <DEDENT> <DEDENT>
if not self . inputs [ <STRING> ] : <NEWLINE> <INDENT> os . remove ( ptem ) <NEWLINE> <DEDENT>
def upgrade ( ) : <NEWLINE> <INDENT> conn = op . get_bind ( ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> op . add_column ( <STRING> , sa . Column ( <STRING> , sa . DateTime ( ) , nullable = True ) ) <NEWLINE> graph_table = sa . sql . table ( <STRING> , <NEWLINE> <INDENT> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> ) <NEWLINE> <DEDENT> users_table = sa . sql . table ( <STRING> , <NEWLINE> <INDENT> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> sa . sql . column ( <STRING> , sa . DateTime ) , <NEWLINE> ) <NEWLINE> <DEDENT> conn . execute ( graph_table . update ( ) . values ( <NEWLINE> <INDENT> creation = datetime . utcnow ( ) , last_access = datetime . utcnow ( ) ) ) <NEWLINE> <DEDENT> conn . execute ( users_table . update ( ) . values ( <NEWLINE> <INDENT> creation = datetime . utcnow ( ) , last_connection = datetime . utcnow ( ) ) ) <NEWLINE> <DEDENT> if not is_sqlite ( conn ) : <NEWLINE> <INDENT> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> op . alter_column ( <STRING> , <STRING> , nullable = False ) <NEWLINE> <DEDENT> <DEDENT>
def setUp ( self ) : <NEWLINE> <INDENT> self . directory = tempfile . mkdtemp ( ) <NEWLINE> self . source_path = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path = importlib . util . cache_from_source ( self . source_path ) <NEWLINE> with open ( self . source_path , <STRING> ) as file : <NEWLINE> <INDENT> file . write ( <STRING> ) <NEWLINE> <DEDENT> self . source_path2 = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path2 = importlib . util . cache_from_source ( self . source_path2 ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path2 ) <NEWLINE> self . subdirectory = os . path . join ( self . directory , <STRING> ) <NEWLINE> os . mkdir ( self . subdirectory ) <NEWLINE> self . source_path3 = os . path . join ( self . subdirectory , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path3 ) <NEWLINE> many_directories = [ str ( number ) for number in range ( 1 , 100 ) ] <NEWLINE> self . long_path = os . path . join ( self . directory , <NEWLINE> <INDENT> <STRING> , <NEWLINE> * many_directories ) <NEWLINE> <DEDENT> os . makedirs ( self . long_path ) <NEWLINE> self . source_path_long = os . path . join ( self . long_path , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path_long ) <NEWLINE> self . bc_path_long = importlib . util . cache_from_source ( <NEWLINE> <INDENT> self . source_path_long <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
def setUp ( self ) : <NEWLINE> <INDENT> self . directory = tempfile . mkdtemp ( ) <NEWLINE> self . source_path = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path = importlib . util . cache_from_source ( self . source_path ) <NEWLINE> with open ( self . source_path , <STRING> ) as file : <NEWLINE> <INDENT> file . write ( <STRING> ) <NEWLINE> <DEDENT> self . source_path2 = os . path . join ( self . directory , <STRING> ) <NEWLINE> self . bc_path2 = importlib . util . cache_from_source ( self . source_path2 ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path2 ) <NEWLINE> self . subdirectory = os . path . join ( self . directory , <STRING> ) <NEWLINE> os . mkdir ( self . subdirectory ) <NEWLINE> self . source_path3 = os . path . join ( self . subdirectory , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path3 ) <NEWLINE> many_directories = [ str ( number ) for number in range ( 1 , 100 ) ] <NEWLINE> self . long_path = os . path . join ( self . directory , <NEWLINE> <INDENT> <STRING> , <NEWLINE> * many_directories ) <NEWLINE> <DEDENT> os . makedirs ( self . long_path ) <NEWLINE> self . source_path_long = os . path . join ( self . long_path , <STRING> ) <NEWLINE> shutil . copyfile ( self . source_path , self . source_path_long ) <NEWLINE> self . bc_path_long = importlib . util . cache_from_source ( <NEWLINE> <INDENT> self . source_path_long <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
def _complement ( self , metadata : Dict [ str , Any ] ) -> Dict [ str , Any ] : <NEWLINE> <INDENT> <STRING> <NEWLINE> metadata = super ( ) . _complement ( metadata ) <NEWLINE> <COMMENT> <NL> if not metadata . get ( <STRING> ) or not self . config . store_original : <NEWLINE> <INDENT> del metadata [ <STRING> ] <NEWLINE> <DEDENT> return metadata <NEWLINE> <DEDENT>
def createSwirlFile ( self , fileName ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> links = [ ] <NEWLINE> while os . path . islink ( fileName ) : <NEWLINE> <INDENT> p = os . readlink ( fileName ) <NEWLINE> if not os . path . isabs ( p ) : <NEWLINE> <INDENT> p = os . path . join ( os . path . dirname ( fileName ) , p ) <NEWLINE> <DEDENT> links . append ( fileName ) <NEWLINE> fileName = p <NEWLINE> <DEDENT> for swirlFile in self . swirlFiles : <NEWLINE> <INDENT> if swirlFile . path == fileName : <NEWLINE> <COMMENT> <NL> <INDENT> swirlFile . setLinks ( links ) <NEWLINE> return swirlFile <NEWLINE> <DEDENT> <DEDENT> swirlFile = SwirlFile ( fileName , links ) <NEWLINE> self . swirlFiles . append ( swirlFile ) <NEWLINE> return swirlFile <NEWLINE> <DEDENT>
def crop_resample ( bands , intensities , crops ) : <NEWLINE> <INDENT> intensities = np . atleast_2d ( intensities ) <NEWLINE> crops = sorted ( crops ) <NEWLINE> <COMMENT> <NL> prev_ub = float ( <STRING> ) <NEWLINE> for lb , ub , step in crops : <NEWLINE> <INDENT> if ub <= lb : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if lb < prev_ub : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> prev_ub = ub <NEWLINE> <COMMENT> <NL> <DEDENT> locs = sorted ( set ( c [ 0 ] for c in crops ) . union ( set ( c [ 1 ] for c in crops ) ) ) <NEWLINE> idxs = np . searchsorted ( bands , locs ) <NEWLINE> loc_idxs = dict ( zip ( locs , idxs ) ) <NEWLINE> <COMMENT> <NL> xs , ys = [ ] , [ ] <NEWLINE> for lb , ub , step in crops : <NEWLINE> <INDENT> s = slice ( loc_idxs [ lb ] , loc_idxs [ ub ] ) <NEWLINE> x = bands [ s ] <NEWLINE> if step > 0 : <NEWLINE> <INDENT> x_new = np . arange ( x [ 0 ] , x [ - 1 ] + step , step ) <NEWLINE> y_new = np . row_stack ( [ np . interp ( x_new , x , y ) for y in intensities [ : , s ] ] ) <NEWLINE> xs . append ( x_new ) <NEWLINE> ys . append ( y_new ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> xs . append ( x ) <NEWLINE> ys . append ( intensities [ : , s ] ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> return np . concatenate ( xs ) , np . hstack ( ys ) <NEWLINE> <DEDENT>
def post_prob_scheme ( t_table , words , stanza , scheme ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> myprob = 1 <NEWLINE> n = len ( words ) <NEWLINE> rhymelists = get_rhymelists ( stanza , scheme ) <NEWLINE> for rhymelist in rhymelists : <NEWLINE> <INDENT> for i , w in enumerate ( rhymelist ) : <NEWLINE> <INDENT> r = words . index ( w ) <NEWLINE> if i == 0 : <COMMENT> <NEWLINE> <INDENT> myprob *= t_table [ r , n ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> for v in rhymelist [ : i ] : <COMMENT> <NEWLINE> <INDENT> c = words . index ( v ) <NEWLINE> myprob *= t_table [ r , c ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> if myprob == 0 and len ( stanza ) > 30 : <COMMENT> <NEWLINE> <INDENT> myprob = 1e-300 <NEWLINE> <DEDENT> return myprob <NEWLINE> <DEDENT>
def __init__ ( <NEWLINE> <INDENT> self , <NEWLINE> signals : typing . Iterable [ <NEWLINE> <INDENT> typing . Union [ signal . Signals , int , str ] ] = _DEFAULT_SIGS , <NEWLINE> <DEDENT> callback : typing . Optional [ <NEWLINE> <INDENT> typing . Callable [ [ signal . Signals , typing . Optional [ FrameType ] ] , None ] ] = None , <NEWLINE> ) : <NEWLINE> <DEDENT> signals = list ( signals ) <NEWLINE> signals_tmp = [ ] <COMMENT> <NEWLINE> for sig in signals : <NEWLINE> <INDENT> if isinstance ( sig , int ) : <NEWLINE> <INDENT> sig = signal . Signals ( sig ) <NEWLINE> <DEDENT> elif isinstance ( sig , str ) : <NEWLINE> <INDENT> sig = signal . Signals [ sig ] <NEWLINE> <DEDENT> if isinstance ( sig , signal . Signals ) : <COMMENT> <NEWLINE> <INDENT> signals_tmp . append ( sig ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> % ( sig , ) ) <NEWLINE> <DEDENT> <DEDENT> if not signals : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if callback is None : <NEWLINE> <INDENT> callback = self . _default_callback <NEWLINE> <DEDENT> if not _two_pos_args ( callback ) : <NEWLINE> <INDENT> raise TypeError ( <NEWLINE> <INDENT> <STRING> % <NEWLINE> ( callback , ) ) <NEWLINE> <DEDENT> <DEDENT> if os . name == <STRING> : <NEWLINE> <INDENT> if not ( set ( signals_tmp ) <= set ( self . _DEFAULT_SIGS ) ) : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> % ( signals , ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . _signals = tuple ( signals_tmp ) <COMMENT> <NEWLINE> self . _callback = callback <NEWLINE> <COMMENT> <NL> self . _old_handlers = [ ] <COMMENT> <NEWLINE> self . _depth = 0 <NEWLINE> <DEDENT>
def creates_default_site ( sender , instance , created , * args , ** kwargs ) : <NEWLINE> <INDENT> if not created : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> site = Site . objects . get ( domain__icontains = DEFAULT_SITE_DOMAIN , <NEWLINE> <INDENT> tenant_site__tenant = instance ) <NEWLINE> <DEDENT> if site . domain != ( <STRING> % ( instance . slug , DEFAULT_SITE_DOMAIN ) ) : <NEWLINE> <INDENT> site . delete ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT> except Site . DoesNotExist : <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def __call__ ( self , h , c , x ) : <NEWLINE> <INDENT> if not self . test and self . dropout > 0 : <NEWLINE> <INDENT> gates = dy . vanilla_lstm_gates_dropout ( x , h , self . Whx , self . Whh , self . bh , self . dropout_mask_x , self . dropout_mask_h ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> gates = dy . vanilla_lstm_gates ( x , h , self . Whx , self . Whh , self . bh ) <NEWLINE> <DEDENT> new_c = dy . vanilla_lstm_c ( c , gates ) <NEWLINE> new_h = dy . vanilla_lstm_h ( new_c , gates ) <NEWLINE> return new_h , new_c <NEWLINE> <DEDENT>
def _test_recurrent_layer_bidirectional_transduction ( <NEWLINE> <INDENT> fwd_layer , <NEWLINE> bwd_layer , <NEWLINE> dummy_input , <NEWLINE> lengths , <NEWLINE> left_padded , <NEWLINE> ) : <NEWLINE> <COMMENT> <NL> tranductor = transduction_layers . BidirectionalLayer ( fwd_layer , bwd_layer ) <NEWLINE> <COMMENT> <NL> dy . renew_cg ( ) <NEWLINE> <COMMENT> <NL> seq = [ <NEWLINE> <INDENT> dy . inputTensor ( dummy_input , batched = True ) + i for i in range ( 10 ) <NEWLINE> <DEDENT> ] <NEWLINE> <COMMENT> <NL> tranductor . init ( test = False , update = True ) <NEWLINE> <COMMENT> <NL> fwd_states , bwd_states = tranductor ( <NEWLINE> <INDENT> seq , lengths = lengths , left_padded = left_padded <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> fwd_z = dy . mean_batches ( <NEWLINE> <INDENT> dy . esum ( [ dy . sum_elems ( state [ 0 ] ) for state in fwd_states ] ) <NEWLINE> <DEDENT> ) <NEWLINE> bwd_z = dy . mean_batches ( <NEWLINE> <INDENT> dy . esum ( [ dy . sum_elems ( state [ 0 ] ) for state in bwd_states ] ) <NEWLINE> <DEDENT> ) <NEWLINE> z = fwd_z + bwd_z <NEWLINE> z . forward ( ) <NEWLINE> z . backward ( ) <NEWLINE> <DEDENT>
def select_bounding_box ( self , names , column ) : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for name in names : <NEWLINE> <INDENT> target = self . store [ name ] <NEWLINE> if target . has_column ( ) and target . column < column : continue <NEWLINE> result . append ( target . row ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
def _parse ( string ) : <NEWLINE> <INDENT> result = [ ] <NEWLINE> for match in PYTHON_FORMAT . finditer ( string ) : <NEWLINE> <INDENT> name , format , typechar = match . groups ( ) <NEWLINE> if typechar == <STRING> and name is None : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> result . append ( ( name , str ( typechar ) ) ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
return tmin < tmax <NEWLINE>
@ property <NEWLINE> <INDENT> def rooms ( self ) -> list : <NEWLINE> <INDENT> return [ InTouchRoom ( r , self ) for r in [ <STRING> , <STRING> ] <NEWLINE> <INDENT> if True or _convert ( <NEWLINE> <INDENT> self . _data [ <STRING> . format ( r ) ] , <NEWLINE> self . _data [ <STRING> . format ( r ) ] ) is not None ] <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
def consume ( self , msg ) : <NEWLINE> <INDENT> body , topic = msg . get ( <STRING> ) , msg . get ( <STRING> ) <NEWLINE> pretty_text = fedmsg . text . msg2repr ( body ) <NEWLINE> log . debug ( pretty_text ) <NEWLINE> icon = fedmsg . text . _msg2icon ( msg ) or <STRING> <NEWLINE> log . debug ( <STRING> % icon ) <NEWLINE> if icon : <NEWLINE> <INDENT> icon_file = self . _icon_cache . get ( icon ) <NEWLINE> if not icon_file : <NEWLINE> <INDENT> icon_file , headers = urllib . urlretrieve ( icon ) <NEWLINE> log . debug ( <STRING> % ( icon . split ( <STRING> ) [ - 1 ] , <NEWLINE> <INDENT> icon_file ) ) <NEWLINE> <DEDENT> self . _icon_cache [ icon ] = icon_file <NEWLINE> <DEDENT> icon = icon_file <NEWLINE> <DEDENT> note = Notify . Notification . new ( <STRING> , pretty_text , icon ) <NEWLINE> note . show ( ) <NEWLINE> <DEDENT>
if len ( file_list ) == 2 : <NEWLINE> <INDENT> return find_probes ( first_probe_data , second_probe_data ) <NEWLINE> elif len ( file_list ) > 2 : <NEWLINE> return find_probes_recursively ( file_list [ 2 : ] , tail = find_probes ( first_probe_data , second_probe_data ) ) <NEWLINE> else : <NEWLINE> probe_data = read_probe_records_from_file ( file_list [ 0 ] ) <NEWLINE> <DEDENT>
def get_recipients ( args ) : <NEWLINE> <INDENT> if args [ <STRING> ] : <NEWLINE> <INDENT> recipients = [ ] <NEWLINE> for recipient in args [ <STRING> ] : <NEWLINE> <INDENT> key = binascii . unhexlify ( recipient ) <NEWLINE> assert len ( key ) == 32 <NEWLINE> recipients . append ( key ) <NEWLINE> <DEDENT> return recipients <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> private = get_private ( args ) <NEWLINE> public = libnacl . crypto_scalarmult_base ( private ) <NEWLINE> return [ public ] <NEWLINE> <DEDENT> <DEDENT>
date_edit_widgets = ( self . _ui . afterDateFilterDateEdit , <NEWLINE> <INDENT> self . _ui . beforeDateFilterDateEdit ) <NEWLINE> for widget in date_edit_widgets : <NEWLINE> self . connect ( widget , SIGNAL ( <STRING> ) , <NEWLINE> self . apply_filters ) <NEWLINE> <DEDENT>
def date ( s ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> y , m , d = s . strip ( ) . split ( <STRING> ) <NEWLINE> y = year ( y ) <NEWLINE> m = month ( m ) <NEWLINE> d = day ( d ) <NEWLINE> return datetime . date ( y , m , d ) <NEWLINE> <DEDENT>
ax2 . add_patch ( patches . Rectangle ( <NEWLINE> <INDENT> ( 1.5 , 1.5 ) , <NEWLINE> 6 , 6 , <NEWLINE> fill = False , <NEWLINE> lw = 2 , <NEWLINE> ls = <STRING> , <NEWLINE> edgecolor = <STRING> , <NEWLINE> ) ) <NEWLINE> ax2 . add_patch ( patches . Rectangle ( <NEWLINE> ( 0 , 0 ) , <NEWLINE> 9 , 9 , <NEWLINE> fill = False , <NEWLINE> lw = 1 , <NEWLINE> ls = <STRING> , <NEWLINE> edgecolor = <STRING> , <NEWLINE> ) ) <NEWLINE> <DEDENT>
for chunk in chunker_list ( list ( barcode_dict . keys ( ) ) , args . writer_threads - 1 ) : <NEWLINE> <INDENT> logger . debug ( <STRING> . format ( <STRING> . join ( chunk ) ) ) <NEWLINE> q = manager . Queue ( ) <NEWLINE> q_bc_dict = dict ( ( k , barcode_dict [ k ] ) for k in chunk ) <NEWLINE> writer_pool . apply_async ( _writer , ( q , q_bc_dict ) , callback = lambda x : print ( x ) ) <NEWLINE> queue_list . append ( q ) <NEWLINE> for bc in q_bc_dict . values ( ) : <NEWLINE> <INDENT> queues [ bc ] = q <NEWLINE> <DEDENT> <DEDENT>
def _do_write ( self , offset , data ) : <NEWLINE> <INDENT> n = offset / CHUNKDATASIZE <NEWLINE> while n >= len ( self . chunks ) : <NEWLINE> <INDENT> self . _add_new_chunk ( ) <NEWLINE> <DEDENT> <DEDENT>
def removeBelowValue ( requestContext , seriesList , n ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> for s in seriesList : <NEWLINE> <INDENT> s . name = <STRING> % ( s . name , n ) <NEWLINE> for ( index , val ) in enumerate ( s ) : <NEWLINE> <INDENT> if val < n : <NEWLINE> <INDENT> s [ index ] = None <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
_kde = kde ( x = _f_x , df = _df_i , x_steps = kde_steps ) [ 0 ] <NEWLINE> <INDENT> __x = _kde [ _f_x ] <NEWLINE> _y = _kde [ <STRING> ] <NEWLINE> _ax . plot ( __x , _y , linestyle = _f_distfit_line , color = _f_distfit_color , alpha = alpha , linewidth = 2 , <NEWLINE> <INDENT> label = _label_2 , ** kwargs ) <NEWLINE> <DEDENT> if not show_hist and _f_fill : <NEWLINE> <INDENT> _ax . fill_between ( __x , _y , color = _f_facecolor , alpha = alpha ) <NEWLINE> <DEDENT> <DEDENT>
_index_name = _df . index . name <NEWLINE> <INDENT> _df [ <STRING> ] = _df . index <NEWLINE> _k_split = int ( np . ceil ( _df . shape [ 0 ] / k ) ) <NEWLINE> <DEDENT>
async def edit ( self , name_ , * , name = None , description = utils . sentinel ) : <NEWLINE> <INDENT> return self . _new_emote ( await self . _http . edit ( name_ , name = name , description = description ) ) <NEWLINE> <DEDENT>
def get_entropy_for_each_line ( trained_model : TrainedModel , <NEWLINE> <INDENT> file : str , <NEWLINE> entropy_aggregator : Callable [ [ List [ float ] , List [ int ] ] , Union [ float , List [ float ] ] ] , <NEWLINE> verbose : bool = False ) -> Union [ List [ float ] , List [ List [ float ] ] ] : <NEWLINE> prep_lines_and_entropies : List [ Dict [ str , Union [ str , List [ str ] , List [ float ] , float ] ] ] = [ ] <NEWLINE> with open ( file , <STRING> ) as f : <NEWLINE> _ , extension = os . path . splitext ( file ) [ 1 : ] <NEWLINE> for line in f : <NEWLINE> time_measurer . tick ( <STRING> ) <NEWLINE> prep_line , entropies , word_boundaries = trained_model . get_entropies_for_text ( line , extension [ 1 : ] ) <NEWLINE> time_measurer . tock ( <STRING> ) <NEWLINE> line_entropy = entropy_aggregator ( entropies , word_boundaries ) <NEWLINE> prep_lines_and_entropies . append ( { <NEWLINE> <STRING> : line , <NEWLINE> <STRING> : prep_line , <NEWLINE> <STRING> : entropies , <NEWLINE> <STRING> : line_entropy <NEWLINE> } ) <NEWLINE> if verbose : <NEWLINE> for line in prep_lines_and_entropies : <NEWLINE> print ( line [ <STRING> ] ) <NEWLINE> print ( line [ <STRING> ] ) <NEWLINE> print ( <STRING> ) <NEWLINE> print ( <STRING> ) <NEWLINE> return list ( map ( lambda e : e [ <STRING> ] , prep_lines_and_entropies ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> with tqdm ( total = len ( self ) ) as pbar : <NEWLINE> <INDENT> on_error = False <NEWLINE> while True : <NEWLINE> <INDENT> case = self . _dump ( ) <NEWLINE> pbar . set_description ( <STRING> . join ( <NEWLINE> <INDENT> <STRING> . format ( c [ 1 ] [ - 10 : ] ) if isinstance ( c [ 1 ] , str ) <NEWLINE> else <STRING> . format ( c [ 1 ] ) <NEWLINE> for c in case <NEWLINE> <DEDENT> ) ) <NEWLINE> <COMMENT> <NL> for _fun , _args in self . _for : <NEWLINE> <INDENT> await _fun ( case ) ( * _args ) <NEWLINE> <COMMENT> <NL> <DEDENT> for _cmd in self . _exes : <NEWLINE> <INDENT> if await self . _execute ( _cmd ) : <NEWLINE> <INDENT> on_error = True <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> if on_error : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> yield case <NEWLINE> <COMMENT> <NL> await asyncio . sleep ( 0.1 ) <NEWLINE> pbar . update ( ) <NEWLINE> if self . _step ( ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> <DEDENT> <DEDENT> self . _lock = False <NEWLINE> <DEDENT>
test_cycle_rewards ) <NEWLINE>
def parse_node ( self , node , model , alias ) : <NEWLINE> <INDENT> query = [ ] <NEWLINE> query_data = [ ] <NEWLINE> nodes = [ ] <NEWLINE> for child in node . children : <NEWLINE> <INDENT> if isinstance ( child , Q ) : <NEWLINE> <INDENT> parsed , data = self . parse_q ( child , model , alias ) <NEWLINE> query . append ( parsed ) <NEWLINE> query_data . extend ( data ) <NEWLINE> <DEDENT> elif isinstance ( child , Node ) : <NEWLINE> <INDENT> parsed , data = self . parse_node ( child , model , alias ) <NEWLINE> query . append ( <STRING> % parsed ) <NEWLINE> query_data . extend ( data ) <NEWLINE> <DEDENT> <DEDENT> query . extend ( nodes ) <NEWLINE> connector = <STRING> % node . connector <NEWLINE> return connector . join ( query ) , query_data <NEWLINE> <DEDENT>
user_indexes = self . get_sorted_indexes ( User ) <NEWLINE> <INDENT> if BACKEND == <STRING> : <NEWLINE> <INDENT> user_indexes . pop ( 0 ) <NEWLINE> <DEDENT> <DEDENT>
class Meta : <NEWLINE> <INDENT> primary_key = CompositeKey ( <STRING> , <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if dest in accum and table not in accum : <NEWLINE> <INDENT> print_ ( <STRING> % dest ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> mpk = metadata . primary_key is not None <NEWLINE> can_populate_joined_pk = ( <NEWLINE> <INDENT> mpk and <NEWLINE> ( metadata . attr in inst . _data ) and <NEWLINE> ( getattr ( joined_inst , metadata . primary_key ) is None ) ) <NEWLINE> <DEDENT> if can_populate_joined_pk : <NEWLINE> <INDENT> setattr ( <NEWLINE> <INDENT> joined_inst , <NEWLINE> metadata . primary_key , <NEWLINE> inst . _data [ metadata . attr ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def get_engine ( ) : <NEWLINE> <INDENT> global __ENGINE__ <NEWLINE> if __ENGINE__ is None : <NEWLINE> <INDENT> cred = DB_CONNECTION . get ( <STRING> , <STRING> ) <NEWLINE> if cred : <NEWLINE> <INDENT> if <STRING> in DB_CONNECTION : <NEWLINE> <INDENT> cred = <STRING> . format ( ** DB_CONNECTION ) <NEWLINE> <DEDENT> cred += <STRING> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
urlpath = None <NEWLINE> <INDENT> path = path_from_link <NEWLINE> try : <NEWLINE> <INDENT> urlpath = models . URLPath . get_by_path ( article_title ) <NEWLINE> path = urlpath . get_absolute_url ( ) <NEWLINE> <DEDENT> except models . URLPath . DoesNotExist : <NEWLINE> <INDENT> pass <NEWLINE> else : <NEWLINE> <DEDENT> urlpath = models . URLPath . objects . get ( article = self . markdown . article ) <NEWLINE> source_components = urlpath . path . strip ( <STRING> ) . split ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> starting_level = max ( 0 , self . config [ <STRING> ] [ 0 ] - 1 ) <NEWLINE> starting_path = <STRING> . join ( source_components [ : starting_level ] ) <NEWLINE> <DEDENT>
if x < 0 or x > 8 or y < 0 or y > 8 : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT>
def get_alignment_error ( self , ename ) : <NEWLINE> <INDENT> align_error_conf = [ ] <NEWLINE> dx = self . _get_config ( ename , CONFIG_ALIGNMENT_DX , None ) <NEWLINE> if dx is not None : <NEWLINE> <INDENT> _LOGGER . info ( <STRING> . format ( ename , dx ) ) <NEWLINE> align_error_conf . append ( ( <STRING> , float ( dx ) ) ) <NEWLINE> <DEDENT> dy = self . _get_config ( ename , CONFIG_ALIGNMENT_DY , None ) <NEWLINE> if dy is not None : <NEWLINE> <INDENT> _LOGGER . info ( <STRING> . format ( ename , dx ) ) <NEWLINE> align_error_conf . append ( ( <STRING> , float ( dy ) ) ) <NEWLINE> <DEDENT> return align_error_conf <NEWLINE> <DEDENT>
yield from includeme ( self . config ) <NEWLINE> <INDENT> except Exception : <NEWLINE> <INDENT> log . exception ( <STRING> . format ( callable ) ) <NEWLINE> <DEDENT> <DEDENT>
def execute ( self , args ) : <NEWLINE> <INDENT> if not self . cfg . globals . enable_experimental : <NEWLINE> <INDENT> raise exception . ExperimentalFeature ( <STRING> ) <NEWLINE> <DEDENT> if len ( args ) < 3 : <NEWLINE> <INDENT> self . parser . error ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> ctag = args [ 0 ] <NEWLINE> lpath = args [ - 1 ] <NEWLINE> rpaths = args [ 1 : - 1 ] <NEWLINE> cl = self . cm . get_cluster ( ctag ) <NEWLINE> node = cl . get_node_by_alias ( self . opts . node ) <NEWLINE> if self . opts . user : <NEWLINE> <INDENT> node . ssh . switch_user ( self . opts . user ) <NEWLINE> <DEDENT> for rpath in rpaths : <NEWLINE> <INDENT> if not glob . has_magic ( rpath ) and not node . ssh . path_exists ( rpath ) : <NEWLINE> <INDENT> raise exception . BaseException ( <NEWLINE> <INDENT> <STRING> % rpath ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> node . ssh . get ( rpaths , lpath ) <NEWLINE> <DEDENT>
def execute_task ( task_name , args ) : <NEWLINE> <INDENT> task = retrieve_task ( task_name ) <NEWLINE> subprocess . run ( [ <STRING> , task ] + args ) <NEWLINE> <DEDENT>
@ raise_for_error <NEWLINE> <INDENT> def _request ( self , asins , marketplaceid ) : <NEWLINE> <INDENT> response = self . api . get_competitive_pricing_for_asin ( marketplaceid , asins ) <NEWLINE> write_response ( response , <STRING> ) <NEWLINE> response . raise_for_status ( ) <NEWLINE> return response . content <NEWLINE> <DEDENT> <DEDENT>
if opts . get ( <STRING> ) : <NEWLINE> <INDENT> out . info ( pprint . pformat ( context ) ) <NEWLINE> return <NEWLINE> <DEDENT>
def __ne__ ( self , other ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if isinstance ( other , Release ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> <DEDENT>
class Meteor : <NEWLINE> <INDENT> def __init__ ( self ) : <NEWLINE> <INDENT> base_path = os . path . dirname ( os . path . abspath ( __file__ ) ) <NEWLINE> jar_path = os . path . join ( base_path , METEOR_JAR ) <NEWLINE> gz_path = os . path . join ( base_path , os . path . basename ( METEOR_GZ_URL ) ) <NEWLINE> if not os . path . isfile ( jar_path ) : <NEWLINE> <INDENT> if not os . path . isfile ( gz_path ) : <NEWLINE> <INDENT> download_from_url ( METEOR_GZ_URL , gz_path ) <NEWLINE> <DEDENT> tar = tarfile . open ( gz_path , <STRING> ) <NEWLINE> tar . extractall ( path = os . path . dirname ( os . path . abspath ( __file__ ) ) ) <NEWLINE> tar . close ( ) <NEWLINE> os . remove ( gz_path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
next_states_set . extend ( curr_states ( <NEWLINE> <INDENT> state = nfa . state , <NEWLINE> captured = None , <NEWLINE> chars = ( char , next_char ) ) ) <NEWLINE> <DEDENT>
if Wallet . is_seed ( text3 ) : <NEWLINE> <INDENT> wallet . add_cosigner_seed ( text3 , <STRING> , password ) <NEWLINE> elif Wallet . is_xpub ( text3 ) : <NEWLINE> wallet . add_master_public_key ( <STRING> , text3 ) <NEWLINE> <DEDENT>
def set_parameters ( self , host , port , protocol , proxy , auto_connect ) : <NEWLINE> <INDENT> proxy_str = interface . serialize_proxy ( proxy ) <NEWLINE> server_str = <STRING> . join ( [ host , port , protocol ] ) <NEWLINE> self . config . set_key ( <STRING> , auto_connect , True ) <NEWLINE> self . config . set_key ( <STRING> , proxy_str , True ) <NEWLINE> self . config . set_key ( <STRING> , server_str , True ) <NEWLINE> <COMMENT> <NL> if self . config . get ( <STRING> ) != server_str or self . config . get ( <STRING> ) != proxy_str : <NEWLINE> <INDENT> return <NEWLINE> <DEDENT> <DEDENT>
@ hook <NEWLINE> <INDENT> def transaction_dialog ( self , d ) : <NEWLINE> <INDENT> self . send_button = b = QPushButton ( _ ( <STRING> ) ) <NEWLINE> b . clicked . connect ( lambda : self . do_send ( d . tx ) ) <NEWLINE> d . buttons . insert ( 0 , b ) <NEWLINE> self . transaction_dialog_update ( d ) <NEWLINE> <DEDENT> <DEDENT>
if self . tx . is_complete ( ) : <NEWLINE> <INDENT> if tx_hash in self . wallet . transactions . keys ( ) : <NEWLINE> <INDENT> desc = self . wallet . get_label ( tx_hash ) <NEWLINE> height , conf , timestamp = self . wallet . get_tx_height ( tx_hash ) <NEWLINE> if height > 0 : <NEWLINE> <INDENT> if conf : <NEWLINE> <INDENT> status = _ ( <STRING> ) % conf <NEWLINE> time_str = datetime . datetime . fromtimestamp ( timestamp ) . isoformat ( <STRING> ) [ : - 3 ] <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> status = _ ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> status = _ ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> status = _ ( <STRING> ) <NEWLINE> self . broadcast_button . show ( ) <NEWLINE> <COMMENT> <NL> if self . main_window . network is None : <NEWLINE> <INDENT> self . broadcast_button . setEnabled ( False ) <NEWLINE> else : <NEWLINE> <DEDENT> <DEDENT> s , r = self . tx . signature_count ( ) <NEWLINE> status = _ ( <STRING> ) if s == 0 else _ ( <STRING> ) + <STRING> % ( s , r ) <NEWLINE> tx_hash = _ ( <STRING> ) ; <NEWLINE> <DEDENT>
if len ( addrs ) == 1 : <NEWLINE> <INDENT> def show_address ( ) : <NEWLINE> <INDENT> keystore . thread . add ( partial ( self . show_address , wallet , addrs [ 0 ] , keystore ) ) <NEWLINE> <DEDENT> <DEDENT>
lon_str_min = longitude . split ( ) [ 1 ] [ : - 1 ] <NEWLINE> <INDENT> lat_str_min = latitude . split ( ) [ 1 ] [ : - 1 ] <NEWLINE> <COMMENT> <NL> lon_str_min = lon_str_min . replace ( <STRING> , <STRING> ) <NEWLINE> lat_str_min = lat_str_min . replace ( <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> lon = SIGN_WEST * float ( longitude . split ( ) [ 0 ] ) + float ( lon_str_min ) / 60. <NEWLINE> lat = SIGN_NORTH * float ( latitude . split ( ) [ 0 ] ) + float ( lat_str_min ) / 60. <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> index_to_draw , _ = _helpers . get_max_index ( bit_gate_rank , <NEWLINE> <INDENT> operation = operation ) <NEWLINE> <DEDENT> x_coord = _helpers . get_x_from_index ( index_to_draw ) <NEWLINE> yq_coord = _helpers . get_y_from_quantum_register ( qubits [ 0 ] , bit_mapping ) <NEWLINE> yc_coord = _helpers . get_y_from_classical_register ( number_of_clbits - 1 , total_qubits_number , <NEWLINE> <INDENT> bit_mapping ) <NEWLINE> <COMMENT> <NL> <DEDENT> _draw_classical_double_line ( drawing , x_coord , yq_coord , x_coord , yc_coord ) <NEWLINE> <DEDENT>
ret = np . ones_like ( num_splits ) * P_0 <NEWLINE> <INDENT> for ret_i , n in enumerate ( num_splits ) : <NEWLINE> <INDENT> P = np . zeros ( ( n + 1 ) ) <NEWLINE> P [ 0 ] = P_0 <NEWLINE> for i in range ( n ) : <NEWLINE> <INDENT> h_p = h [ ret_i ] if i == ( n - 1 ) else H [ i + 1 ] <NEWLINE> if L [ i ] != 0 : <NEWLINE> <INDENT> P [ i + 1 ] = P [ i ] * ( T [ i ] / ( T [ i ] + L [ i ] * ( h_p - H [ i ] ) ) ) ** ( 34.163 / L [ i ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> P [ i + 1 ] = P [ i ] * np . exp ( - 34.162 * ( h_p - H [ i ] ) / T [ i ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
alignment = calculate_alignment ( orient_roi ) <NEWLINE>
