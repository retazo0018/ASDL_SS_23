if self . state != AuctionState . BID : <NEWLINE> <INDENT> raise InvalidActionError ( <STRING> ) <NEWLINE> elif self . bid >= bid : <NEWLINE> raise InvalidActionError ( <STRING> + str ( bid ) + <STRING> + str ( self . bid ) ) <NEWLINE> elif not self . owners [ owner_id ] . can_buy ( self . nominee , bid ) : <NEWLINE> raise InvalidActionError ( <STRING> + str ( owner_id ) + <NEWLINE> <INDENT> <STRING> + str ( bid ) + <STRING> + self . nominee . name + <NEWLINE> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
async def handle ( ** data ) : <NEWLINE> <INDENT> messages = data [ <STRING> ] if <STRING> in data and len ( data [ <STRING> ] ) > 0 else { } <NEWLINE> for inner_data in messages : <NEWLINE> <INDENT> hub = inner_data [ <STRING> ] if <STRING> in inner_data else <STRING> <NEWLINE> if hub . lower ( ) == self . name . lower ( ) : <NEWLINE> <INDENT> method = inner_data [ <STRING> ] <NEWLINE> message = inner_data [ <STRING> ] <NEWLINE> await self . __handlers [ method ] ( inner_data ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
async def handle ( ** data ) : <NEWLINE> <INDENT> messages = data [ <STRING> ] if <STRING> in data and len ( data [ <STRING> ] ) > 0 else { } <NEWLINE> for inner_data in messages : <NEWLINE> <INDENT> hub = inner_data [ <STRING> ] if <STRING> in inner_data else <STRING> <NEWLINE> if hub . lower ( ) == self . name . lower ( ) : <NEWLINE> <INDENT> method = inner_data [ <STRING> ] <NEWLINE> message = inner_data [ <STRING> ] <NEWLINE> await self . __handlers [ method ] ( message ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
tasklists = res_urls <NEWLINE> <INDENT> if isinstance ( tasklists , string_types ) : <NEWLINE> <INDENT> tasklists = [ tasklists ] <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , starting_board = None ) : <NEWLINE> <INDENT> self . board = np . empty ( ( 8 , 8 ) , dtype = Piece ) <NEWLINE> if starting_board is None : <NEWLINE> <INDENT> self . _start_pos ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . board = starting_board <NEWLINE> <DEDENT> self . move_counter = 0 <NEWLINE> <DEDENT>
def sanitize_redirect ( host , redirect_to ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> if not redirect_to or not isinstance ( redirect_to , six . string_types ) or getattr ( redirect_to , <STRING> , None ) and not isinstance ( redirect_to . decode ( ) , six . string_types ) : <NEWLINE> <INDENT> return None <NEWLINE> <DEDENT> <DEDENT>
def partial_pipeline_data ( strategy , user , * args , ** kwargs ) : <NEWLINE> <INDENT> partial = strategy . session_get ( <STRING> , None ) <NEWLINE> if partial : <NEWLINE> <INDENT> idx , backend , xargs , xkwargs = strategy . partial_from_session ( partial ) <NEWLINE> kwargs = kwargs . copy ( ) <NEWLINE> kwargs . setdefault ( <STRING> , user ) <NEWLINE> kwargs . setdefault ( <STRING> , strategy . request ) <NEWLINE> kwargs . update ( xkwargs ) <NEWLINE> return idx , backend , xargs , kwargs <NEWLINE> <DEDENT> <DEDENT>
if isinstance ( exception , SocialAuthBaseException ) : <NEWLINE> <INDENT> backend_name = request . backend . name <NEWLINE> message = self . get_message ( request , exception ) <NEWLINE> url = self . get_redirect_uri ( request , exception ) <NEWLINE> try : <NEWLINE> <INDENT> messages . error ( request , message , <NEWLINE> <INDENT> extra_tags = <STRING> + backend_name ) <NEWLINE> <DEDENT> <DEDENT> except MessageFailure : <NEWLINE> <INDENT> url += ( <STRING> in url and <STRING> or <STRING> ) + <STRING> . format ( urlquote ( message ) , <NEWLINE> <INDENT> backend_name ) <NEWLINE> <DEDENT> <DEDENT> return redirect ( url ) <NEWLINE> <DEDENT>
if type ( data_x [ 0 ] ) in ucvar : <NEWLINE> <INDENT> values_x = [ d . n for d in data_x ] <NEWLINE> sigma_x = [ d . s if d . s != 0 else 1e-5 for d in data_x ] <NEWLINE> elif type ( data_x [ 0 ] ) in [ float , int ] : <NEWLINE> values_x = data_x <NEWLINE> <DEDENT>
route . begin_time = route . drives [ 0 ] . begin_time <NEWLINE> <INDENT> route . end_time = route . drives [ - 1 ] . end_time <NEWLINE> routes . append ( route ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if not first_line . startswith ( <STRING> ) : <NEWLINE> <INDENT> first_line = <STRING> . format ( UserCommandParser . run_command , first_line ) <NEWLINE> <DEDENT> <DEDENT>
if key . label : <NEWLINE> <INDENT> label = QGraphicsTextItem ( key . label ) <NEWLINE> label . setFont ( font ) <NEWLINE> label . setDefaultTextColor ( QColor ( key . font_color ) ) <NEWLINE> <DEDENT>
last = 0 <NEWLINE> <INDENT> for i , etime in enumerate ( ends_arr ) : <NEWLINE> <INDENT> if time < etime : <NEWLINE> <INDENT> return i , time - last <NEWLINE> <DEDENT> last = etime <NEWLINE> <DEDENT> if time == last : <NEWLINE> <INDENT> return len ( ends_arr ) - 1 , 0 <NEWLINE> <DEDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> handler = self . _union_registry . get ( union ) <NEWLINE> if handler is not None : <NEWLINE> <INDENT> return handler ( obj , union ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> self . E_data = self . _isolate_energy ( E_code , data ) <NEWLINE> self . monthly_data , self . yearly_data = self . _sep_freqs ( self . E_data ) <NEWLINE> for data_df in self . monthly_data , self . yearly_data : <NEWLINE> <INDENT> data_df . set_index ( <STRING> , inplace = True ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if len ( rightLabels ) == 0 : <NEWLINE> <INDENT> rightLabels = pd . Series ( dataFrame . right . unique ( ) ) . unique ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> check_data_matches_labels ( rightLabels , dataFrame [ <STRING> ] , <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> if colorDict is None : <NEWLINE> <INDENT> colorDict = { } <NEWLINE> palette = <STRING> <NEWLINE> colorPalette = sns . color_palette ( palette , len ( allLabels ) ) <NEWLINE> for i , label in enumerate ( allLabels ) : <NEWLINE> <INDENT> colorDict [ label ] = colorPalette [ i ] <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> missing = [ label for label in allLabels if label not in colorDict . keys ( ) ] <NEWLINE> if missing : <NEWLINE> <INDENT> msg = ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> msg += <STRING> . format ( <STRING> . join ( missing ) ) <NEWLINE> raise ValueError ( msg ) <NEWLINE> <DEDENT> <DEDENT> LOGGER . debug ( <STRING> ) <NEWLINE> <COMMENT> <NL> ns_l = defaultdict ( ) <NEWLINE> ns_r = defaultdict ( ) <NEWLINE> for leftLabel in leftLabels : <NEWLINE> <INDENT> leftDict = { } <NEWLINE> rightDict = { } <NEWLINE> for rightLabel in rightLabels : <NEWLINE> <INDENT> leftDict [ rightLabel ] = dataFrame [ <NEWLINE> <INDENT> ( dataFrame . left == leftLabel ) & ( dataFrame . right == rightLabel ) <NEWLINE> <DEDENT> ] . leftWeight . sum ( ) <NEWLINE> rightDict [ rightLabel ] = dataFrame [ <NEWLINE> <INDENT> ( dataFrame . left == leftLabel ) & ( dataFrame . right == rightLabel ) <NEWLINE> <DEDENT> ] . rightWeight . sum ( ) <NEWLINE> <DEDENT> ns_l [ leftLabel ] = leftDict <NEWLINE> ns_r [ leftLabel ] = rightDict <NEWLINE> <DEDENT> <DEDENT>
def handle_word ( word ) : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> API_KEY = load_api_key ( ) <NEWLINE> <COMMENT> <NL> <DEDENT> except ConfigFileError : <NEWLINE> <INDENT> click . echo ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> word_object = fetch_word ( word ) <NEWLINE> click . echo_via_pager ( word_object . stringify ( ) ) <NEWLINE> word_save_status = save_word ( word_object ) <NEWLINE> if word_save_status : <NEWLINE> <INDENT> click . echo ( <STRING> . format ( word ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> click . echo ( <STRING> . format ( word ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if origin is None and args == ( ) : <NEWLINE> <INDENT> return PrimitiveTp ( tp ) <NEWLINE> if origin is list and len ( args ) == 1 : <NEWLINE> return ListTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> if origin is tuple : <NEWLINE> if len ( args ) == 2 and args [ 1 ] is ... : <NEWLINE> <INDENT> return HomoTupleTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return HeteroTupleTp ( origin , * [ cls [ a ] for a in args ] ) <NEWLINE> if origin is set : <NEWLINE> <DEDENT> return SetTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> if origin is frozenset : <NEWLINE> return FrozenSetTp ( origin , cls [ args [ 0 ] ] ) <NEWLINE> <DEDENT>
while sector != ENDOFCHAIN and ( position + 1 ) * sector_size <= current : <NEWLINE> <INDENT> sector = self . next_fat ( sector ) <NEWLINE> position += 1 <NEWLINE> <DEDENT>
try : <NEWLINE> <INDENT> link_file = LinkFile ( link_file_path ) <NEWLINE> config_file = ConfigFile ( config_file_path , True ) <NEWLINE> if not validate ( link_file_path , config_file_path ) : <NEWLINE> <INDENT> click . secho ( <STRING> , fg = <STRING> , err = True ) <NEWLINE> click . get_current_context ( ) . exit ( 1 ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> def verify_required ( self ) : <NEWLINE> <INDENT> if self . urlparse ( ) ( self . api_base_url ) . scheme == <STRING> : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT>
def to_stan ( self , acc , indent = 0 ) : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> if self . lower : <NEWLINE> <INDENT> self . to_stan_prec ( self . lower , acc , indent ) <NEWLINE> <DEDENT> if self . lower or self . upper : <NEWLINE> <INDENT> acc += self . mkString ( <STRING> ) <NEWLINE> <DEDENT> if self . upper : <NEWLINE> <INDENT> self . to_stan_prec ( self . upper , acc , indent ) <NEWLINE> <DEDENT> <DEDENT>
def process_request ( self , request ) : <NEWLINE> <COMMENT> <NL> <INDENT> urls = tuple ( [ re . compile ( url ) for url in getattr ( settings , <STRING> , [ ] ) ] ) <NEWLINE> secure = any ( [ url . search ( request . path ) for url in urls ] ) <NEWLINE> if request . is_secure ( ) : <NEWLINE> <INDENT> if secure and not getattr ( request , <STRING> , False ) : <NEWLINE> <INDENT> if getattr ( settings , <STRING> , False ) : <NEWLINE> <COMMENT> <NL> <INDENT> return _redirect ( request , False ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> if secure : <NEWLINE> <INDENT> return _redirect ( request , True ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def find_storage_directories ( ) : <NEWLINE> <INDENT> home_dir = pathlib . Path ( os . environ [ <STRING> ] ) <NEWLINE> candidates = [ ] <NEWLINE> firefox_dir = home_dir / <STRING> / <STRING> <NEWLINE> if firefox_dir . exists ( ) : <NEWLINE> <INDENT> candidates . append ( firefox_dir . iterdir ( ) ) <NEWLINE> <DEDENT> zotero_dir = home_dir / <STRING> <NEWLINE> if zotero_dir . exists ( ) : <NEWLINE> <INDENT> candidates . append ( zotero_dir . iterdir ( ) ) <NEWLINE> <DEDENT> zotero5_dir = home_dir / <STRING> <NEWLINE> if zotero5_dir . exists ( ) : <NEWLINE> <INDENT> yield ( <STRING> , zotero5_dir ) <NEWLINE> <DEDENT> candidate_iter = itertools . chain . from_iterable ( candidates ) <NEWLINE> for fpath in candidate_iter : <NEWLINE> <INDENT> if not fpath . is_dir ( ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> match = PROFILE_PAT . match ( fpath . name ) <NEWLINE> if match : <NEWLINE> <INDENT> storage_path = fpath / <STRING> / <STRING> <NEWLINE> if storage_path . exists ( ) : <NEWLINE> <INDENT> yield ( match . group ( 2 ) , storage_path ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
desc = SymmetryFunction ( cutvalue , cutfunc , desc_params ) <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> enabled_now = { } <NEWLINE> for plugin_path_i in plugin_paths : <NEWLINE> <INDENT> plugin_link_path_i = enabled_path . joinpath ( plugin_path_i . name ) <NEWLINE> if not plugin_link_path_i . exists ( ) : <NEWLINE> <INDENT> if platform . system ( ) == <STRING> : <NEWLINE> <INDENT> plugin_path_i . junction ( plugin_link_path_i ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> plugin_path_i . symlink ( plugin_link_path_i ) <NEWLINE> <DEDENT> logger . debug ( <STRING> , <NEWLINE> <INDENT> plugin_path_i , plugin_link_path_i ) <NEWLINE> <DEDENT> enabled_now [ plugin_path_i . name ] = True <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> logger . debug ( <STRING> , plugin_path_i , <NEWLINE> <INDENT> plugin_link_path_i ) <NEWLINE> <DEDENT> enabled_now [ plugin_path_i . name ] = False <NEWLINE> <DEDENT> <DEDENT> return enabled_now if not singleton else enabled_now . values ( ) [ 0 ] <NEWLINE> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <INDENT> clahe_crop = clahe_image [ y1 : y2 , x1 : x2 ] <NEWLINE> <COMMENT> <NL> shape = predictor ( clahe_crop , detections ) <NEWLINE> return shape , clahe_crop <NEWLINE> <DEDENT>
def get ( self , path ) : <NEWLINE> <INDENT> if ( self . name == path ) : return [ self ] <NEWLINE> if ( not path . startswith ( self . name + <STRING> ) ) : return [ ] <NEWLINE> path = path [ len ( self . name ) + 1 : ] <NEWLINE> result = [ ] <NEWLINE> for n_row , row_name , row_objects in zip ( count ( 1 ) , <NEWLINE> <INDENT> self . row_names , <NEWLINE> self . row_objects ) : <NEWLINE> for alt_row_name in [ row_name , str ( n_row ) ] : <NEWLINE> if ( alt_row_name is None ) : continue <NEWLINE> if ( alt_row_name == path ) : <NEWLINE> result . extend ( row_objects ) <NEWLINE> elif ( path . startswith ( alt_row_name + <STRING> ) ) : <NEWLINE> for row_object in row_objects : <NEWLINE> result . extend ( row_object . get ( path = path [ len ( alt_row_name ) + 1 : ] ) ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT>
mon_lib_dna_rna_cif = [ <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> , <STRING> ] <NEWLINE> <INDENT> if ( <STRING> in __builtins__ ) : <NEWLINE> <INDENT> mon_lib_dna_rna_cif = set ( mon_lib_dna_rna_cif ) <NEWLINE> <DEDENT> <DEDENT>
def input ( <NEWLINE> <INDENT> file_name = None , <NEWLINE> source_info = Please_pass_string_or_None , <NEWLINE> lines = None , <NEWLINE> pdb_id = None ) : <NEWLINE> if ( pdb_id is not None ) : <NEWLINE> assert file_name is None <NEWLINE> file_name = ent_path_local_mirror ( pdb_id = pdb_id ) <NEWLINE> if ( file_name is not None ) : <NEWLINE> return ext . input ( <NEWLINE> <INDENT> source_info = <STRING> + file_name , <NEWLINE> lines = flex . split_lines ( smart_open . for_reading ( file_name ) . read ( ) ) ) <NEWLINE> assert source_info is not Please_pass_string_or_None <NEWLINE> if ( isinstance ( lines , str ) ) : <NEWLINE> <DEDENT> lines = flex . split_lines ( lines ) <NEWLINE> elif ( isinstance ( lines , ( list , tuple ) ) ) : <NEWLINE> lines = flex . std_string ( lines ) <NEWLINE> return ext . input ( source_info = source_info , lines = lines ) <NEWLINE> <DEDENT>
return pair_counts <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> distance = cspad_tbx . env_distance ( self . address , env , self . _detz_offset ) <NEWLINE> if distance is None : <NEWLINE> <INDENT> distance = float ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> detz = cspad_tbx . env_detz ( <STRING> , env ) <NEWLINE> if ( detz is None ) : <NEWLINE> <INDENT> self . m_no_detz += 1 <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> if self . detector == <STRING> or self . detector == <STRING> : <NEWLINE> <INDENT> distance = cspad_tbx . env_distance ( self . address , env , self . _detz_offset ) <NEWLINE> if distance is None : <NEWLINE> <INDENT> self . nfail += 1 <NEWLINE> self . logger . warning ( <STRING> ) <NEWLINE> evt . put ( True , <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> distance = float ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
return _wrapped <NEWLINE>
for docname in env . found_docs : <NEWLINE> <INDENT> abspath = env . doc2path ( docname ) <NEWLINE> mtime = os . path . getmtime ( abspath ) <NEWLINE> res . append ( ( docname , mtime ) ) <NEWLINE> <DEDENT>
if isinstance ( self . receiver , Group ) : <NEWLINE> <INDENT> return self . receiver . self <NEWLINE> <DEDENT>
card_name = friend_or_mp . nickname if isinstance ( friend_or_mp , Chat ) else friend_or_mp <NEWLINE> <INDENT> logger . info ( <STRING> . format ( CARD , self , card_name ) ) <NEWLINE> <DEDENT>
if ( <NEWLINE> <COMMENT> <NL> <INDENT> type ( value ) == type <NEWLINE> <COMMENT> <NL> or is_dunder ( attr ) <NEWLINE> <COMMENT> <NL> or inspect . isfunction ( value ) <NEWLINE> <COMMENT> <NL> or attr in ( <STRING> , <STRING> , <STRING> ) <NEWLINE> <COMMENT> <NL> or attr in fields <NEWLINE> <COMMENT> <NL> or isinstance ( value , Field ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , property ) <NEWLINE> <COMMENT> <NL> or isinstance ( value , RELATED_DESCRIPTORS ) <NEWLINE> ) : <NEWLINE> return False <NEWLINE> else : <NEWLINE> return True <NEWLINE> <DEDENT>
def check_type_backward ( self , in_types , out_types ) : <NEWLINE> <INDENT> type_check . expect ( out_types . size ( ) == 1 ) <NEWLINE> x_type , = in_types <NEWLINE> y_type , = out_types <NEWLINE> <DEDENT>
with cuda . get_device ( g_dst ) : <NEWLINE> <INDENT> if ( isinstance ( g_src , cuda . ndarray ) and <NEWLINE> <INDENT> g_dst . gpudata . device != g_src . gpudata . device ) : <NEWLINE> g_dst += cuda . copy ( g_src , out_device = g_dst . gpudata . device ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> g_dst += cuda . to_gpu ( g_src ) <NEWLINE> <DEDENT> <DEDENT>
def __init__ ( self , margin ) : <NEWLINE> <INDENT> if margin <= 0 : <NEWLINE> <INDENT> raise Exception ( <STRING> ) <NEWLINE> <DEDENT> self . margin = margin <NEWLINE> <DEDENT>
skip = ( slice ( None ) , ) * axis <NEWLINE> <INDENT> ret = [ ] <NEWLINE> i = 0 <NEWLINE> for index in indices : <NEWLINE> <INDENT> ret . append ( ary [ skip + ( slice ( i , index ) , ) ] ) <NEWLINE> i = index <NEWLINE> <DEDENT> ret . append ( ary [ skip + ( slice ( i , size ) , ) ] ) <NEWLINE> <DEDENT>
def _get_property ( self , key , default = False ) : <NEWLINE> <INDENT> attr = getattr ( self , <STRING> % key ) <NEWLINE> if attr : <NEWLINE> <INDENT> return attr <NEWLINE> <DEDENT> if default is not False and not self . app_key : <NEWLINE> <INDENT> return default <NEWLINE> <DEDENT> app = self . oauth . app or current_app <NEWLINE> config = app . config [ self . app_key ] <NEWLINE> if default is not False : <NEWLINE> <INDENT> return config . get ( key , default ) <NEWLINE> <DEDENT> return config [ key ] <NEWLINE> <DEDENT>
def parser_dnb ( data ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> data = re . split ( <STRING> , data ) <COMMENT> <NEWLINE> recs = { } <NEWLINE> recs [ <STRING> ] = [ ] <NEWLINE> try : <NEWLINE> <INDENT> for line in data : <NEWLINE> <INDENT> line = line . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) <NEWLINE> if len ( recs ) == 4 : <COMMENT> <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> authors = re . findall ( <STRING> , line ) [ 0 ] <NEWLINE> authors = authors . replace ( <STRING> , <STRING> ) <NEWLINE> authors = re . split ( <STRING> , authors ) <COMMENT> <NEWLINE> for auth in authors : <NEWLINE> <INDENT> if <STRING> in auth : <COMMENT> <NEWLINE> <INDENT> auth = re . findall ( <STRING> , auth ) [ 0 ] <NEWLINE> <COMMENT> <NL> <DEDENT> auth = u ( re . sub ( <STRING> , <STRING> , auth ) ) <NEWLINE> recs [ <STRING> ] . append ( auth ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> publisher = re . findall ( <STRING> , line ) [ 0 ] <NEWLINE> recs [ <STRING> ] = u ( publisher ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> title = re . findall ( <STRING> , line ) [ 0 ] <NEWLINE> title = u ( title . replace ( <STRING> , <STRING> ) . replace ( <STRING> , <STRING> ) ) <NEWLINE> recs [ <STRING> ] = u ( title ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> elif re . search ( <STRING> , line ) : <NEWLINE> <INDENT> recs [ <STRING> ] = u ( re . findall ( <STRING> , line ) [ 0 ] ) <NEWLINE> <DEDENT> elif line == <STRING> : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
while not self . stopping : <NEWLINE> <INDENT> actor_id , message = self . acomm . recv ( ) <NEWLINE> if actor_id not in self . local_actors : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> % actor_id ) <NEWLINE> <DEDENT> <DEDENT>
if device is None : <NEWLINE> <INDENT> result = service . SyncApplySettingToIPNetworkConnection ( SettingData = setting , Mode = mode ) <NEWLINE> else : <NEWLINE> result = service . SyncApplySettingToIPNetworkConnection ( SettingData = setting , IPNetworkConnection = device , Mode = mode ) <NEWLINE> if result . errorstr : <NEWLINE> raise LmiFailed ( <STRING> % result . errorstr ) <NEWLINE> return result . rval <NEWLINE> <DEDENT>
@ fixture <NEWLINE> <INDENT> def controller ( self , app , request , root_tree , data , matchdict ) : <NEWLINE> <INDENT> request . registry [ <STRING> ] = app . controller_plugins <NEWLINE> controller = self . _get_controller_class ( ) ( root_tree , request ) <NEWLINE> controller . data = data <NEWLINE> request . matchdict = matchdict <NEWLINE> return controller <NEWLINE> <DEDENT> <DEDENT>
if _are_there_selectors ( shape_map_file , shape_map_raw ) : <NEWLINE> <INDENT> sgraph = _get_adequate_sgraph ( endpoint_url = url_endpoint , <NEWLINE> <INDENT> raw_graph = raw_graph , <NEWLINE> graph_file_input = graph_file_input , <NEWLINE> url_input = url_input , <NEWLINE> graph_format = input_format , <NEWLINE> built_remote_graph = built_remote_graph ) <NEWLINE> <DEDENT> valid_shape_map = built_shape_map <NEWLINE> if built_shape_map is None : <NEWLINE> <INDENT> shape_map_parser = get_shape_map_parser ( format = shape_map_format , <NEWLINE> <INDENT> sgraph = sgraph , <NEWLINE> namespaces_prefix_dict = namespaces_dict ) <NEWLINE> <DEDENT> valid_shape_map = shape_map_parser . parse_shape_map ( source_file = shape_map_file , <NEWLINE> <INDENT> raw_content = shape_map_raw ) <NEWLINE> <DEDENT> <DEDENT> selectors_tracker = ShapeMapInstanceTracker ( shape_map = valid_shape_map ) <NEWLINE> if _are_there_some_target_classes ( target_classes , file_target_classes , all_classes_mode , shape_qualifiers_mode ) : <NEWLINE> model_classes = None <NEWLINE> if file_target_classes or target_classes is not None : <NEWLINE> <INDENT> list_of_str_target_classes = tune_target_classes_if_needed ( <NEWLINE> <INDENT> target_classes ) if target_classes is not None else read_target_classes_from_file ( file_target_classes ) <NEWLINE> <DEDENT> model_classes = get_list_of_model_classes ( list_of_str_target_classes ) <NEWLINE> <DEDENT> <DEDENT>
if categorical_features is not None : <NEWLINE> <INDENT> if categorical_features is <STRING> : <NEWLINE> <INDENT> categorical_features = np . arange ( 0 , X . shape [ 1 ] ) <NEWLINE> <DEDENT> for feature_no in categorical_features : <NEWLINE> <INDENT> if not np . array_equal ( X [ : , feature_no ] , X [ : , feature_no ] . astype ( int ) ) : <NEWLINE> <INDENT> warnings . warn ( <STRING> + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if max_categories is None : <NEWLINE> <INDENT> uniques = np . unique ( X [ : , feature_no ] ) . astype ( int ) <NEWLINE> if not np . array_equal ( uniques , np . arange ( 0 , np . max ( uniques ) + 1 ) ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> + <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if self . priors is None : <NEWLINE> <INDENT> self . priors = np . bincount ( y ) / num_samples <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> self . priors = np . asarray ( self . priors ) <NEWLINE> if len ( self . priors ) != num_classes : <NEWLINE> <INDENT> raise ValueError ( <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> if not np . isclose ( self . priors . sum ( ) , 1.0 ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if ( self . priors < 0 ) . any ( ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def match ( self , url , netloc , domain , origin = None ) : <NEWLINE> <INDENT> if self . options and not self . options . can_apply_rule ( domain , origin ) : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return True <NEWLINE> <DEDENT>
if cut_by == <STRING> : <NEWLINE> <INDENT> filename = OES_FILENAMES . get ( area_focus ) <NEWLINE> if filename == None : <NEWLINE> <INDENT> raise ValueError ( <STRING> <STRING> . format ( area_focus , [ <STRING> , <STRING> , <STRING> ] ) ) <NEWLINE> else : <NEWLINE> <DEDENT> filename = OES_FILENAMES . get ( cut_by ) <NEWLINE> <DEDENT>
def _on_pushToTunTaskDone ( task ) : <NEWLINE> <COMMENT> <NL> <INDENT> try : <NEWLINE> <INDENT> if isinstance ( task . exception ( ) , CancelledError ) : <NEWLINE> <INDENT> logging . error ( <STRING> % type ( task . exception ( ) ) ) <NEWLINE> <DEDENT> <DEDENT> except CancelledError : <COMMENT> <NEWLINE> <INDENT> pass <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> affinities = [ ] <NEWLINE> for ( tr , te ) in zip ( train , test ) : <NEWLINE> <INDENT> if len ( tr . T ) != len ( te . T ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> <NEWLINE> <INDENT> <STRING> <NEWLINE> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> affinities += [ make_affinity ( np . row_stack ( [ tr , te ] ) , K = K , mu = mu ) ] <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> log_data = info . copy ( ) <NEWLINE> length = np . array ( log_data . get ( <STRING> , 0 ) ) <NEWLINE> reward = np . array ( log_data . get ( <STRING> , 0.0 ) ) <NEWLINE> completed = np . array ( log_data . get ( <STRING> , False ) ) <NEWLINE> reward_possible = game . initial_available_points ( ) <NEWLINE> required_points = game . required_points ( ) <NEWLINE> if reward . shape : <NEWLINE> <COMMENT> <NL> <INDENT> log_data [ <STRING> ] = game . agent_names . tolist ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <COMMENT> <NL> <INDENT> reward_possible = np . sum ( reward_possible [ : 1 ] ) <NEWLINE> required_points = np . sum ( required_points [ : 1 ] ) <NEWLINE> <DEDENT> log_data [ <STRING> ] = game . title <NEWLINE> log_data [ <STRING> ] = length . tolist ( ) <NEWLINE> log_data [ <STRING> ] = reward . tolist ( ) <NEWLINE> log_data [ <STRING> ] = completed . tolist ( ) <NEWLINE> log_data [ <STRING> ] = reward_possible . tolist ( ) <NEWLINE> log_data [ <STRING> ] = required_points . tolist ( ) <NEWLINE> log_data [ <STRING> ] = datetime . utcnow ( ) . isoformat ( ) <NEWLINE> logger . info ( self . episode_msg . format ( ** log_data , ** self . cumulative_stats ) ) <NEWLINE> <DEDENT>
if wandb_run is not None and file_name == <STRING> : <NEWLINE> <INDENT> wandb_run . summary [ <STRING> ] = np . average ( success ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( length ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( side_effects ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( reward_frac ) <NEWLINE> wandb_run . summary [ <STRING> ] = np . average ( score ) <NEWLINE> <DEDENT>
local_config = sdk_config . load_config ( ) <NEWLINE> <INDENT> add_kubos_command = functools . partial ( kubos_options . command . add_command , local_config , subparser , <STRING> ) <COMMENT> <NEWLINE> add_yotta_command = functools . partial ( kubos_options . command . add_command , local_config , subparser , <STRING> ) <COMMENT> <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> + <NEWLINE> <STRING> + <NEWLINE> <STRING> , <NEWLINE> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_yotta_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> add_kubos_command ( <STRING> , <STRING> , <STRING> ) <NEWLINE> <DEDENT>
class vector ( data . Data ) : <NEWLINE> <INDENT> def __init__ ( self , asp , adr , cx ) : <NEWLINE> <INDENT> super ( ) . __init__ ( asp , adr , adr + 4 ) <NEWLINE> self . ws = asp . bu16 ( adr ) <NEWLINE> self . dstadr = asp . bu16 ( adr + 2 ) <NEWLINE> cx . disass ( self . dstadr , asp ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> end_amt_no_trades = ( float ( self . exchange . start_usd ) / float ( end_price ) ) + float ( self . exchange . start_btc ) <NEWLINE> end_amt = ( float ( self . exchange . usd_bal ) / float ( end_price ) ) + float ( self . exchange . btc_bal ) <NEWLINE> start_amt = ( float ( self . exchange . start_usd ) / float ( start_price ) ) + float ( self . exchange . start_btc ) <NEWLINE> strategy_performance = ( ( end_amt - start_amt ) / start_amt ) * 100 <NEWLINE> print ( <STRING> ) <NEWLINE> print ( <STRING> + str ( self . times_recalculated ) ) <NEWLINE> print ( <STRING> + str ( self . exchange . times_bought ) ) <NEWLINE> print ( <STRING> + str ( self . exchange . times_sold ) ) <NEWLINE> print ( <STRING> + str ( market_performance ) + <STRING> ) <NEWLINE> print ( <STRING> + str ( strategy_performance ) + <STRING> ) <NEWLINE> print ( <STRING> + str ( end_amt_no_trades ) + <STRING> ) <NEWLINE> print ( <STRING> + str ( end_amt ) + <STRING> ) <NEWLINE> strategy_performance_vs_market = strategy_performance - market_performance <NEWLINE> if strategy_performance > market_performance : <NEWLINE> <INDENT> print ( <STRING> + str ( strategy_performance_vs_market ) + <STRING> ) <NEWLINE> <DEDENT> elif strategy_performance < market_performance : <NEWLINE> <INDENT> print ( <STRING> + str ( strategy_performance_vs_market ) + <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
for key in params : <NEWLINE>
<COMMENT> <NL> <INDENT> for i in range ( 0 , len ( results [ <STRING> ] ) ) : <NEWLINE> <INDENT> iteration = { <NEWLINE> <INDENT> <STRING> : self . task_id , <NEWLINE> <STRING> : <STRING> . format ( method . name , score_regressor , time_regressor ) , <NEWLINE> <STRING> : i , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : type ( self . estimator ) . __name__ , <NEWLINE> <STRING> : results [ <STRING> ] [ i ] , <NEWLINE> <STRING> : seed , <NEWLINE> <STRING> : method . name , <NEWLINE> <STRING> : time_regressor , <NEWLINE> <STRING> : score_regressor <NEWLINE> <DEDENT> } <NEWLINE> self . db_table . insert ( iteration ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> dil_ccs = s . dilated_components ( psd_output , dil_param , cc_thresh ) <NEWLINE> <DEDENT>
model = imp . load_source ( <STRING> , local_model ) . InstantiatedModel <NEWLINE> <INDENT> model . load_state_dict ( torch . load ( local_chkpt ) ) <NEWLINE> <DEDENT>
def load_options ( self , chunk ) : <NEWLINE> <INDENT> for i , name in enumerate ( self . options . keys ( ) ) : <NEWLINE> <INDENT> value = chunk . chdt [ i ] <NEWLINE> setattr ( self , name , value ) <NEWLINE> <DEDENT> <DEDENT>
def contents_changed ( self , start , removed , added ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if self . _tree . lexicon : <NEWLINE> <INDENT> start , end = self . _builder ( ) . rebuild ( self . _tree , self . text ( ) , start , removed , added ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> end = start + added <NEWLINE> <DEDENT> self . set_modified_range ( start , end ) <NEWLINE> <DEDENT>
for field in self . COMPLEX_FIELDS : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> v = kwargs [ field ] <NEWLINE> <DEDENT> except KeyError : <NEWLINE> <INDENT> if partial : <NEWLINE> <INDENT> self . _incomplete . add ( field ) <NEWLINE> continue <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise AttributeError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> is_compound = isinstance ( v , list ) <NEWLINE> cls = get_class ( field ) <NEWLINE> if is_compound : <NEWLINE> <INDENT> result = list ( ) <NEWLINE> for data in v : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> data [ <STRING> ] = data . get ( <STRING> , singularize ( field ) ) <NEWLINE> <DEDENT> except AttributeError : <COMMENT> <NEWLINE> <INDENT> result . append ( data ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> result . append ( cls ( partial = True , ** data ) ) <NEWLINE> <DEDENT> <DEDENT> self . __setattr__ ( field , result ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> t = v . get ( <STRING> , field ) <NEWLINE> v [ <STRING> ] = CIVIC_TO_PYCLASS . get ( t , t ) <NEWLINE> self . __setattr__ ( field , cls ( partial = True , ** v ) ) <NEWLINE> <DEDENT> <DEDENT>
return new_row <NEWLINE>
def __init__ ( self , keras_model , worker_optimizer , loss , num_workers = 2 , batch_size = 32 , <NEWLINE> <INDENT> features_col = <STRING> , label_col = <STRING> , num_epoch = 1 ) : <NEWLINE> super ( AsynchronousDistributedTrainer , self ) . __init__ ( keras_model , worker_optimizer , loss , <NEWLINE> <INDENT> num_workers , batch_size , features_col , <NEWLINE> label_col , num_epoch ) <NEWLINE> <COMMENT> <NL> self . parallelism = 3 * num_workers <NEWLINE> <DEDENT> <DEDENT>
def handle_commit ( self , conn , addr ) : <NEWLINE> <COMMENT> <NL> <INDENT> data = recv_data ( conn ) <NEWLINE> <COMMENT> <NL> r = data [ <STRING> ] <NEWLINE> worker_id = data [ <STRING> ] <NEWLINE> with self . mutex : <NEWLINE> <INDENT> self . add_staleness ( worker_id ) <NEWLINE> <COMMENT> <NL> self . center_variable = self . center_variable + r <NEWLINE> <COMMENT> <NL> <DEDENT> self . next_update ( ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> def visit_Name ( self , n , shallow = False ) : <NEWLINE> <COMMENT> <NL> <INDENT> if any ( [ operators [ op ] in n . id for op in operators ] ) : <NEWLINE> <INDENT> name = n . id <NEWLINE> for op in operators : <NEWLINE> <INDENT> name = name . replace ( operators [ op ] , op ) <NEWLINE> <DEDENT> tree = ast . parse ( name ) . body [ 0 ] . value <NEWLINE> if shallow : <NEWLINE> <INDENT> return tree <NEWLINE> <DEDENT> return to_math ( tree , mul = self . mul , div = self . div , <NEWLINE> <INDENT> mat_size = self . mat_size , decimal = self . decimal , <NEWLINE> syntax = self . s , ital = self . ital ) <NEWLINE> <DEDENT> <DEDENT> if not self . subs or not shallow : <NEWLINE> <INDENT> return self . format_name ( n . id ) <NEWLINE> <COMMENT> <NL> <DEDENT> try : <NEWLINE> <COMMENT> <NL> <INDENT> if shallow : <NEWLINE> <INDENT> return _prep4lx ( self . dict [ n . id ] , self . s , self . mat_size ) . value <NEWLINE> <COMMENT> <NL> <DEDENT> if str ( self . dict [ n . id ] ) == n . id : <NEWLINE> <INDENT> return self . format_name ( str ( self . dict [ n . id ] ) ) <NEWLINE> <DEDENT> <DEDENT> except KeyError : <NEWLINE> <INDENT> log . warning ( <STRING> , n . id ) <NEWLINE> return <NEWLINE> <DEDENT> qty = self . visit ( _prep4lx ( self . dict [ n . id ] , self . s , self . mat_size ) ) <NEWLINE> unit = to_math ( self . dict [ n . id + UNIT_PF ] , div = <STRING> , syntax = self . s , ital = False ) if n . id + UNIT_PF in self . dict . keys ( ) else self . s . txt ( <STRING> ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> if hasattr ( n , <STRING> ) and n . is_in_power and unit and unit != <STRING> : <NEWLINE> <INDENT> return self . s . delmtd ( qty + unit ) <NEWLINE> <DEDENT> return qty + unit <NEWLINE> <DEDENT> <DEDENT>
def get_url ( url ) : <NEWLINE> <INDENT> fp = url_fp ( url ) <NEWLINE> if os . path . exists ( fp ) : <NEWLINE> <INDENT> os . remove ( fp ) <NEWLINE> <DEDENT> subprocess . check_call ( [ <STRING> , url ] ) <NEWLINE> return fp <NEWLINE> <DEDENT>
def add_subwindow ( self , aclass , flist ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> sub = PyJibeQMdiSubWindow ( ) <NEWLINE> inst = aclass ( sub ) <NEWLINE> sub . setWidget ( inst ) <NEWLINE> inst . add_files ( flist ) <NEWLINE> self . mdiArea . addSubWindow ( sub ) <NEWLINE> sub . show ( ) <NEWLINE> self . subwindows . append ( sub ) <NEWLINE> <COMMENT> <NL> if hasattr ( inst , <STRING> ) : <NEWLINE> <INDENT> choices = inst . get_export_choices ( ) <NEWLINE> menobj = self . menuExport . addMenu ( sub . windowTitle ( ) ) <NEWLINE> for choice in choices : <NEWLINE> <INDENT> action = menobj . addAction ( choice [ 0 ] ) <NEWLINE> action . triggered . connect ( getattr ( inst , choice [ 1 ] ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
with geo . Geo ( mesh_geo ) as g : <NEWLINE> <INDENT> g . include ( <STRING> ) <NEWLINE> g . include ( <STRING> ) <NEWLINE> g . attractor ( 1 , geo . range ( idx0 , idx ) ) <NEWLINE> g . threshold ( 2 , <NEWLINE> <INDENT> field = 1 , <NEWLINE> dist = ( 0 , 2 * min_size ) , <NEWLINE> lc = ( 1.1 * delta , min_size ) ) <NEWLINE> <DEDENT> g . min ( 3 , 2 ) <NEWLINE> g . background ( 3 ) <NEWLINE> <DEDENT>
ceryle . configure_logging ( <NEWLINE> <INDENT> level = { <NEWLINE> <INDENT> <STRING> : logging . DEBUG , <NEWLINE> <STRING> : logging . INFO , <NEWLINE> <STRING> : logging . WARN , <NEWLINE> <STRING> : logging . ERROR , <NEWLINE> <DEDENT> } [ args . pop ( <STRING> ) ] , <NEWLINE> console = args . pop ( <STRING> ) , <NEWLINE> filename = args . pop ( <STRING> ) ) <NEWLINE> logger . debug ( <STRING> ) <NEWLINE> <DEDENT>
def check_tor ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> return ( self . tor is not None ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> return filename , file_size <NEWLINE> <DEDENT>
def update_scrubber ( self , current , duration ) : <NEWLINE> <INDENT> if current is None or duration is None : <NEWLINE> <INDENT> self . song_duration_label . set_text ( <STRING> ) <NEWLINE> self . song_progress_label . set_text ( <STRING> ) <NEWLINE> self . song_scrubber . set_value ( 0 ) <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>
fname = os . path . join ( jobscriptdir , job + <STRING> ) <NEWLINE> <INDENT> f = open ( fname , <STRING> ) <NEWLINE> f . write ( content . format ( job , stime , oe , oe , memPerCPU , ntasks , mpiexec , sim ) ) <NEWLINE> f . close ( ) <NEWLINE> <DEDENT>
class BusSchedulerTest ( unittest . TestCase ) : <NEWLINE> <INDENT> def setUp ( self ) : <NEWLINE> <INDENT> self . stop1 = microbus . BusStop ( <STRING> ) <NEWLINE> self . stop2 = microbus . BusStop ( <STRING> ) <NEWLINE> self . stop3 = microbus . BusStop ( <STRING> ) <NEWLINE> self . stops = [ self . stop1 , self . stop2 , self . stop3 ] <NEWLINE> self . busRoute1 = microbus . BusRoute ( self . stops , <STRING> ) <NEWLINE> self . busRoute2 = self . busRoute1 [ : : - 1 ] <NEWLINE> self . bus = Bus ( keep_prev = 2 ) <NEWLINE> self . scheduler = BusScheduler ( self . bus ) <NEWLINE> <DEDENT> <DEDENT>
if event == <STRING> : <NEWLINE> <INDENT> ref = json . loads ( self . request . body . decode ( <STRING> ) ) [ <STRING> ] <NEWLINE> if ref == <STRING> . format ( branch = options . GITHUB_BRANCH ) : <NEWLINE> <INDENT> result = yield github_pull ( ) <NEWLINE> logger . warning ( result ) <NEWLINE> <DEDENT> <DEDENT>
self . minimum_barcode_fragments = minimum_barcode_fragments <NEWLINE> <INDENT> self . minimum_cell_fragments = minimum_cell_fragments <NEWLINE> self . minimum_jaccard_fragments = minimum_jaccard_fragments <NEWLINE> self . extract_mito = extract_mito <NEWLINE> self . drop_tag = drop_tag <NEWLINE> self . barcode_tag = barcode_tag <NEWLINE> <DEDENT>
if out is not None : <NEWLINE> <INDENT> hypernyms . extend ( out ) <NEWLINE> for h in out : <NEWLINE> <INDENT> local_graph . append ( ( str ( token ) , h ) ) <NEWLINE> <DEDENT> <DEDENT>
def run_casa ( cmd , raise_on_severe = False , timeout = 1800 ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> casa = drivecasa . Casapy ( ) <NEWLINE> try : <NEWLINE> <INDENT> casa_output , casa_error = casa . run_script ( cmd , raise_on_severe = True , timeout = timeout ) <NEWLINE> logger . debug ( <STRING> . join ( casa_error ) ) <NEWLINE> <DEDENT> except RuntimeError : <NEWLINE> <INDENT> logger . error ( <STRING> ) <NEWLINE> if raise_on_severe : <NEWLINE> <INDENT> raise <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if self . transfer_convert_selfcaluv2uvfits : <NEWLINE> <INDENT> subs_setinit . setinitdirs ( self ) <NEWLINE> subs_setinit . setdatasetnamestomiriad ( self ) <NEWLINE> subs_managefiles . director ( <NEWLINE> <INDENT> self , <STRING> , self . transferdir , verbose = True ) <NEWLINE> <DEDENT> if not transfertargetbeamsselfcaluv2uvfitsstatus : <NEWLINE> <COMMENT> <NL> <INDENT> selfcaltargetbeamsphasestatus = get_param_def ( <NEWLINE> <INDENT> self , sbeam + <STRING> , False ) <NEWLINE> <DEDENT> selfcaltargetbeamsampstatus = get_param_def ( <NEWLINE> <INDENT> self , sbeam + <STRING> , False ) <NEWLINE> <DEDENT> datasetname_amp = os . path . join ( <NEWLINE> <INDENT> self . selfcaldir , self . target ) . rstrip ( <STRING> ) + <STRING> <NEWLINE> <DEDENT> datasetname_phase = os . path . join ( <NEWLINE> <INDENT> self . selfcaldir , self . target ) <NEWLINE> <DEDENT> logger . debug ( <NEWLINE> <INDENT> <STRING> . format ( datasetname_amp ) ) <NEWLINE> <DEDENT> logger . debug ( <NEWLINE> <INDENT> <STRING> . format ( datasetname_phase ) ) <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <DEDENT> if os . path . isdir ( datasetname_amp ) and selfcaltargetbeamsampstatus : <NEWLINE> <INDENT> logger . info ( <STRING> + self . beam + <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> dataset = datasetname_amp <NEWLINE> <DEDENT> elif os . path . isdir ( datasetname_phase ) and selfcaltargetbeamsphasestatus : <NEWLINE> <INDENT> logger . info ( <NEWLINE> <INDENT> <STRING> + self . beam + <STRING> ) <NEWLINE> <DEDENT> dataset = datasetname_phase <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> dataset = None <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if not os . path . isdir ( beamoutname ) : <NEWLINE> <COMMENT> <NL> <INDENT> if corrtype == <STRING> : <NEWLINE> <INDENT> make_gaussian_beam ( beam_map_dir , beamoutname , bm_size , cell , fwhm , cutoff ) <NEWLINE> <DEDENT> elif corrtype == <STRING> : <NEWLINE> <INDENT> error = <STRING> <NEWLINE> logger . error ( error ) <NEWLINE> raise ApercalException ( error ) <NEWLINE> get_measured_beam_maps ( beam , beam_map_dir , primary_beam_path ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> error = <STRING> <NEWLINE> logger . error ( error ) <NEWLINE> raise ApercalException ( error ) <NEWLINE> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> logger . warning ( <STRING> . format ( beam ) ) <NEWLINE> <DEDENT> <DEDENT>
def _assert_wwn_translation ( self , expected , actual ) : <NEWLINE> <INDENT> self . assertEquals ( sysfs . translate_wwn ( actual ) , expected ) <NEWLINE> <DEDENT>
return val . lower ( ) in [ <STRING> , <STRING> , <STRING> ] <NEWLINE>
<COMMENT> <NL> <INDENT> arcproxy_setup = <STRING> % get_file_system_root_path ( ) <NEWLINE> envsetup = <STRING> % ( arcproxy_setup ) <NEWLINE> <DEDENT>
return x2 <NEWLINE>
def copy_output ( job , job_scratch_dir , work_dir ) : <NEWLINE> <INDENT> cp_start = time . time ( ) <NEWLINE> try : <NEWLINE> <INDENT> for outfile in job . output_files . keys ( ) : <NEWLINE> <INDENT> if os . path . exists ( outfile ) : <NEWLINE> <INDENT> copy ( os . path . join ( job_scratch_dir , outfile ) , os . path . join ( work_dir , outfile ) ) <NEWLINE> <DEDENT> <DEDENT> os . chdir ( work_dir ) <NEWLINE> <DEDENT> except IOError : <NEWLINE> <INDENT> raise FileHandlingFailure ( <STRING> ) <NEWLINE> <DEDENT> finally : <NEWLINE> <INDENT> cp_time = time . time ( ) - cp_start <NEWLINE> logger . info ( <STRING> . format ( cp_time ) ) <NEWLINE> <DEDENT> return 0 <NEWLINE> <DEDENT>
_cmd = get_trf_command ( command , transformation = transformation ) <NEWLINE> <INDENT> i = 0 <NEWLINE> imax = 120 <NEWLINE> while i < imax : <NEWLINE> <COMMENT> <NL> <INDENT> if not is_process_running ( pid ) : <NEWLINE> <INDENT> return - 1 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if not copytools : <NEWLINE> <INDENT> return <STRING> <NEWLINE> <DEDENT>
if config . Pilot . pandajob == <STRING> : <NEWLINE> <INDENT> time_before = int ( time . time ( ) ) <NEWLINE> res = https . request ( <STRING> . format ( pandaserver = pandaserver ) , data = data ) <NEWLINE> time_after = int ( time . time ( ) ) <NEWLINE> log . info ( <STRING> % ( time_after - time_before , job . jobid ) ) <NEWLINE> log . info ( <STRING> % str ( res ) ) <NEWLINE> if res is not None : <NEWLINE> <COMMENT> <NL> <INDENT> handle_backchannel_command ( res , job , args ) <NEWLINE> <DEDENT> <DEDENT>
def get_object ( self , ** kwargs ) : <NEWLINE> <INDENT> return get_thing_object ( self . model , self . kwargs [ <STRING> ] , self . request . user ) <NEWLINE> <DEDENT>
def element_conforms ( element , etype ) -> bool : <NEWLINE> <INDENT> if element is None and etype == object : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> elif isinstance ( etype , type ( type ) ) and ( issubclass ( etype , type ( None ) ) ) : <NEWLINE> <INDENT> return element is None <NEWLINE> <DEDENT> elif element is None : <NEWLINE> <INDENT> return False <NEWLINE> <DEDENT> return isinstance ( element , etype ) <NEWLINE> <DEDENT>
participants = list ( ) <NEWLINE> <INDENT> participant_nodes = xpath ( participants_node , <STRING> ) <NEWLINE> for node in participant_nodes : <NEWLINE> <INDENT> pid = node . get ( <STRING> ) <NEWLINE> initial_state_node = xpath ( node , <STRING> ) [ 0 ] <NEWLINE> common_state_vals = _extract_common_state_vals ( initial_state_node ) <NEWLINE> initial_state = InitialState ( <NEWLINE> <INDENT> ( float ( initial_state_node . get ( <STRING> ) ) , float ( initial_state_node . get ( <STRING> ) ) ) , <NEWLINE> float ( initial_state_node . get ( <STRING> ) ) , <NEWLINE> common_state_vals [ 0 ] , <NEWLINE> common_state_vals [ 1 ] , <NEWLINE> common_state_vals [ 2 ] <NEWLINE> <DEDENT> ) <NEWLINE> <COMMENT> <NL> ai_requests = list ( ) <NEWLINE> request_nodes = xpath ( node , <STRING> ) <NEWLINE> for req_node in request_nodes : <NEWLINE> <INDENT> tag = get_tag_name ( req_node ) <NEWLINE> rid = req_node . get ( <STRING> ) <NEWLINE> if tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( PositionRequest ( rid ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( SpeedRequest ( rid ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( SteeringAngleRequest ( rid ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> width = int ( req_node . get ( <STRING> ) ) <NEWLINE> height = int ( req_node . get ( <STRING> ) ) <NEWLINE> fov = int ( req_node . get ( <STRING> ) ) <NEWLINE> direction = CameraDirection [ req_node . get ( <STRING> ) ] <NEWLINE> ai_requests . append ( CameraRequest ( rid , width , height , fov , direction ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> radius = int ( req_node . get ( <STRING> ) ) <NEWLINE> ai_requests . append ( LidarRequest ( rid , radius ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( RoadCenterDistanceRequest ( rid , roads ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( CarToLaneAngleRequest ( rid , roads ) ) <NEWLINE> <DEDENT> elif tag == <STRING> : <NEWLINE> <INDENT> ai_requests . append ( BoundingBoxRequest ( rid ) ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> _logger . warning ( <STRING> + tag + <STRING> ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> ai_requests . extend ( [ <NEWLINE> <INDENT> BoundingBoxRequest ( <STRING> + pid + <STRING> ) <NEWLINE> <DEDENT> ] ) <NEWLINE> <COMMENT> <NL> movements = list ( ) <NEWLINE> waypoint_nodes = xpath ( node , <STRING> ) <NEWLINE> for wp_node in waypoint_nodes : <NEWLINE> <INDENT> common_state_vals = _extract_common_state_vals ( wp_node ) <NEWLINE> movements . append ( WayPoint ( <NEWLINE> <INDENT> ( float ( wp_node . get ( <STRING> ) ) , float ( wp_node . get ( <STRING> ) ) ) , <NEWLINE> float ( wp_node . get ( <STRING> ) ) , <NEWLINE> wp_node . get ( <STRING> ) , <NEWLINE> common_state_vals [ 0 ] , <NEWLINE> common_state_vals [ 1 ] , <NEWLINE> common_state_vals [ 2 ] <NEWLINE> <DEDENT> ) ) <NEWLINE> <DEDENT> participants . append ( Participant ( pid , initial_state , CarModel [ node . get ( <STRING> ) ] . value , movements , ai_requests ) ) <NEWLINE> <DEDENT> <DEDENT>
if <STRING> not in env : <NEWLINE> <INDENT> env . wsgi_file = env . django_appname + <STRING> <NEWLINE> <DEDENT>
def on_add_device ( self , ip ) : <NEWLINE> <INDENT> for d in self . device_list : <NEWLINE> <INDENT> if d . device . ip_addr == ip : <NEWLINE> <INDENT> last_volume = d . disconnect_volume <NEWLINE> self . devices . remove ( d . device ) <NEWLINE> self . device_list . remove ( d ) <NEWLINE> break <NEWLINE> <DEDENT> <DEDENT> d = CattDevice ( ip_addr = ip ) <NEWLINE> d . _cast . wait ( ) <NEWLINE> device = Device ( self , d , d . _cast , self . combo_box . count ( ) ) <NEWLINE> d . _cast . media_controller . register_status_listener ( device . media_listener ) <NEWLINE> d . _cast . register_status_listener ( device . status_listener ) <NEWLINE> self . devices . append ( d ) <NEWLINE> self . device_list . append ( device ) <NEWLINE> self . combo_box . addItem ( d . name ) <NEWLINE> if self . combo_box . currentIndex ( ) == device . index : <NEWLINE> <INDENT> self . play_button . setEnabled ( True ) <NEWLINE> self . stop_button . setEnabled ( True ) <NEWLINE> <DEDENT> device . disconnect_volume = round ( device . cast . status . volume_level * 100 ) <NEWLINE> if self . reconnect_volume == - 1 : <NEWLINE> <INDENT> if last_volume != round ( device . cast . status . volume_level * 100 ) : <NEWLINE> <INDENT> d . volume ( last_volume / 100 ) <NEWLINE> if device . index == self . combo_box . currentIndex ( ) : <NEWLINE> <INDENT> self . set_volume_label ( last_volume ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> d . volume ( self . reconnect_volume / 100 ) <NEWLINE> if device . index == self . combo_box . currentIndex ( ) : <NEWLINE> <INDENT> self . set_volume_label ( self . reconnect_volume ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def make_summary_abund_df ( df , cags , singletons ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> summary_df = pd . concat ( [ <NEWLINE> <INDENT> pd . DataFrame ( { <NEWLINE> <INDENT> cag_ix : df . loc [ cag ] . mean ( ) <NEWLINE> for cag_ix , cag in cags . items ( ) <NEWLINE> <DEDENT> } ) . T , <NEWLINE> df . loc [ singletons ] <NEWLINE> <DEDENT> ] ) <NEWLINE> <DEDENT>
elif container . set ( self . matcher . matchLinkText ( block ) ) : <NEWLINE> <INDENT> match = container . get ( ) <NEWLINE> em = LinkTextMatch ( match ) <NEWLINE> subelement = self . parseText ( em . text ( ) ) <NEWLINE> element = LinkElement ( subelement , em . url ( ) ) <NEWLINE> <DEDENT>
scanner = NetgearDeviceScanner ( host , username , password ) <NEWLINE>
def trigger ( hass , config , action ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if CONF_AFTER in config : <NEWLINE> <INDENT> after = dt_util . parse_time_str ( config [ CONF_AFTER ] ) <NEWLINE> if after is None : <NEWLINE> <INDENT> _error_time ( config [ CONF_AFTER ] , CONF_AFTER ) <NEWLINE> return False <NEWLINE> <DEDENT> hours , minutes , seconds = after . hour , after . minute , after . second <NEWLINE> <DEDENT> elif ( CONF_HOURS in config or CONF_MINUTES in config or <NEWLINE> <INDENT> CONF_SECONDS in config ) : <NEWLINE> hours = config . get ( CONF_HOURS ) <NEWLINE> minutes = config . get ( CONF_MINUTES ) <NEWLINE> seconds = config . get ( CONF_SECONDS ) <NEWLINE> if isinstance ( minutes , str ) and minutes . startswith ( <STRING> ) and not convert ( minutes . lstrip ( <STRING> ) , int ) % 60 == 0 : <NEWLINE> <INDENT> _LOGGER . warning ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if isinstance ( seconds , str ) and seconds . startswith ( <STRING> ) and not convert ( seconds . lstrip ( <STRING> ) , int ) % 60 == 0 : <NEWLINE> <DEDENT> _LOGGER . warning ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> if isinstance ( hours , str ) and hours . startswith ( <STRING> ) and not convert ( hours . lstrip ( <STRING> ) , int ) % 24 == 0 : <NEWLINE> <DEDENT> _LOGGER . warning ( <STRING> <NEWLINE> <INDENT> <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> else : <NEWLINE> <INDENT> _LOGGER . error ( <STRING> , <NEWLINE> <INDENT> CONF_HOURS , CONF_MINUTES , CONF_SECONDS , CONF_AFTER ) <NEWLINE> <DEDENT> return False <NEWLINE> <DEDENT> <DEDENT>
for comp_name , discovery in ( ( ( BINARY_SENSOR , DISCOVER_BINARY_SENSORS ) , <NEWLINE> <INDENT> ( SENSOR , DISCOVER_SENSORS ) , <NEWLINE> ( LIGHT , DISCOVER_LIGHTS ) , <NEWLINE> ( SWITCH , DISCOVER_SWITCHES ) ) ) : <NEWLINE> component = get_component ( comp_name ) <NEWLINE> bootstrap . setup_component ( hass , component . DOMAIN , base_config ) <NEWLINE> hass . bus . fire ( EVENT_PLATFORM_DISCOVERED , <NEWLINE> { ATTR_SERVICE : discovery , <NEWLINE> ATTR_DISCOVERED : { } } ) <NEWLINE> return True <NEWLINE> <DEDENT>
_SINGLE_GROUP_CONFIG = vol . Schema ( vol . All ( _conf_preprocess , { <NEWLINE> <INDENT> vol . Optional ( CONF_ENTITIES ) : vol . Any ( cv . entity_ids , None ) , <NEWLINE> CONF_VIEW : bool , <NEWLINE> CONF_NAME : str , <NEWLINE> CONF_ICON : cv . icon , <NEWLINE> } ) ) <NEWLINE> <DEDENT>
@ property <NEWLINE> <INDENT> def device_state_attributes ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> state_attr = { } <NEWLINE> if self . _ipcam . status_data is None : <NEWLINE> <INDENT> return state_attr <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if <STRING> in overlay_data : <NEWLINE> <INDENT> setting_data = overlay_data [ <STRING> ] <NEWLINE> setting = setting_data is not None <NEWLINE> <DEDENT>
if not targets : <NEWLINE> <COMMENT> <NL> <INDENT> self . _push_data ( filepath , message , title , url , self . pushbullet ) <NEWLINE> _LOGGER . info ( <STRING> ) <NEWLINE> return <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> for device in data . abode . get_devices ( generic_type = CONST . TYPE_SWITCH ) : <NEWLINE> <INDENT> if data . is_excluded ( device ) or data . is_light ( device ) : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
@ property <NEWLINE> <INDENT> def is_on ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> <COMMENT> <NL> return bool ( self . _zone [ <STRING> ] != <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
for pmname in coll . supported_values ( ) : <NEWLINE> <INDENT> if config . get ( CONF_NAME ) is not None : <NEWLINE> <INDENT> name = <STRING> . format ( config . get ( CONF_NAME ) , pmname ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> name = <STRING> . format ( pmname ) <NEWLINE> <DEDENT> dev . append ( ParticulateMatterSensor ( coll , name , pmname ) ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> player_devices = self . _player . devices ( ) <NEWLINE> if player_devices is not None : <NEWLINE> <INDENT> devices = player_devices . get ( <STRING> ) <NEWLINE> if devices is not None : <NEWLINE> <INDENT> old_devices = self . _devices <NEWLINE> self . _devices = { self . _aliases . get ( device . get ( <STRING> ) , <NEWLINE> <INDENT> device . get ( <STRING> ) ) : <NEWLINE> device . get ( <STRING> ) <NEWLINE> for device in devices } <NEWLINE> <DEDENT> device_diff = { name : id for name , id in self . _devices . items ( ) <NEWLINE> <INDENT> if old_devices . get ( name , None ) is None } <NEWLINE> <DEDENT> if device_diff : <NEWLINE> <INDENT> _LOGGER . info ( <STRING> , str ( device_diff ) ) <NEWLINE> <COMMENT> <NL> <DEDENT> <DEDENT> <DEDENT> current = self . _player . current_playback ( ) <NEWLINE> if current is None : <NEWLINE> <INDENT> self . _state = STATE_IDLE <NEWLINE> return <NEWLINE> <COMMENT> <NL> <DEDENT> item = current . get ( <STRING> ) <NEWLINE> if item : <NEWLINE> <INDENT> self . _album = item . get ( <STRING> ) . get ( <STRING> ) <NEWLINE> self . _title = item . get ( <STRING> ) <NEWLINE> self . _artist = <STRING> . join ( [ artist . get ( <STRING> ) <NEWLINE> <INDENT> for artist in item . get ( <STRING> ) ] ) <NEWLINE> <DEDENT> self . _uri = item . get ( <STRING> ) <NEWLINE> images = item . get ( <STRING> ) . get ( <STRING> ) <NEWLINE> self . _image_url = images [ 0 ] . get ( <STRING> ) if images else None <NEWLINE> <COMMENT> <NL> <DEDENT> self . _state = STATE_PAUSED <NEWLINE> if current . get ( <STRING> ) : <NEWLINE> <INDENT> self . _state = STATE_PLAYING <NEWLINE> <DEDENT> self . _shuffle = current . get ( <STRING> ) <NEWLINE> device = current . get ( <STRING> ) <NEWLINE> if device is None : <NEWLINE> <INDENT> self . _state = STATE_IDLE <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> if device . get ( <STRING> ) : <NEWLINE> <INDENT> self . _volume = device . get ( <STRING> ) / 100 <NEWLINE> <DEDENT> if device . get ( <STRING> ) : <NEWLINE> <INDENT> self . _current_device = device . get ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
self . assertEqual ( mock_add_bridge_acc . mock_calls , [ call ( state ) ] ) <NEWLINE> <INDENT> self . assertEqual ( mock_show_setup_msg . mock_calls , [ <NEWLINE> <INDENT> call ( self . hass , homekit . bridge ) ] ) <NEWLINE> <DEDENT> self . assertEqual ( homekit . driver . mock_calls , [ call . start ( ) ] ) <NEWLINE> self . assertTrue ( homekit . started ) <NEWLINE> <DEDENT>
to_remove = [ ] <NEWLINE> <INDENT> for listener_ref in new . update_listeners : <NEWLINE> <INDENT> listener = listener_ref ( ) <NEWLINE> if listener is None : <NEWLINE> <INDENT> to_remove . append ( listener_ref ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> listener . async_registry_updated ( old , new ) <NEWLINE> <DEDENT> except Exception : <COMMENT> <NEWLINE> <INDENT> _LOGGER . exception ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
domain_exposed_by_default = expose_by_default or entity . domain in exposed_domains <NEWLINE>
domain_exposed_by_default = expose_by_default and entity . domain in exposed_domains <NEWLINE>
@ HANDLERS . register ( <STRING> ) <NEWLINE> <INDENT> async def async_handle_state_update ( hass , context , msg ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> _LOGGER . debug ( <STRING> , context , msg ) <NEWLINE> entity_id = context . get ( ATTR_ENTITY_ID ) <NEWLINE> state = bool ( int ( msg . get ( ATTR_STATE ) ) ) <NEWLINE> if context . get ( CONF_INVERSE ) : <NEWLINE> <INDENT> state = not state <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if stage_1_domains : <NEWLINE> <INDENT> await asyncio . gather ( * [ <NEWLINE> <INDENT> async_setup_component ( hass , domain , config ) <NEWLINE> for domain in stage_1_domains <NEWLINE> <DEDENT> ] ) <NEWLINE> <DEDENT>
if _token_info : <NEWLINE> <INDENT> await store . async_save ( _token_info ) <NEWLINE> token_info = _token_info <NEWLINE> <DEDENT>
if self . exclude is not None : <NEWLINE> <INDENT> routes = { k : v for k , v in routes . items ( ) if <NEWLINE> <INDENT> self . exclude . lower ( ) not in k . lower ( ) } <NEWLINE> <DEDENT> <DEDENT>
async def _async_registry_updated ( self , event ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> data = event . data <NEWLINE> if data [ <STRING> ] != <STRING> or data . get ( <NEWLINE> <INDENT> <STRING> , data [ <STRING> ] ) != self . entity_id : <NEWLINE> return <NEWLINE> <DEDENT> <DEDENT>
vane_vertical = self . _device . vane_vertical <NEWLINE> <INDENT> if vane_vertical : <NEWLINE> <INDENT> attr . update ( <NEWLINE> <INDENT> { <NEWLINE> <INDENT> ATTR_VANE_VERTICAL : vane_vertical , <NEWLINE> ATTR_VANE_VERTICAL_POSITIONS : self . _device . vane_vertical_positions , <NEWLINE> <DEDENT> } <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT> return attr <NEWLINE> <DEDENT>
for conf in config [ DOMAIN ] : <NEWLINE> <INDENT> protocol = <STRING> if conf [ CONF_SSL ] else <STRING> <NEWLINE> <DEDENT>
conf = config [ DOMAIN ] <NEWLINE> <INDENT> name = conf [ CONF_NAME ] <NEWLINE> port = conf [ CONF_PORT ] <NEWLINE> ip_address = conf . get ( CONF_IP_ADDRESS ) <NEWLINE> advertise_ip = conf . get ( CONF_ADVERTISE_IP ) <NEWLINE> auto_start = conf [ CONF_AUTO_START ] <NEWLINE> safe_mode = conf [ CONF_SAFE_MODE ] <NEWLINE> entity_filter = conf [ CONF_FILTER ] <NEWLINE> entity_config = conf [ CONF_ENTITY_CONFIG ] <NEWLINE> interface_choice = ( <NEWLINE> <INDENT> InterfaceChoice . Default if conf . get ( CONF_ZEROCONF_DEFAULT_INTERFACE ) else None <NEWLINE> <DEDENT> ) <NEWLINE> <DEDENT>
async def async_update ( self ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> await super ( ) . async_update ( ) <NEWLINE> if self . sensor is not None : <NEWLINE> <INDENT> await self . hass . async_add_executor_job ( self . sensor . update ) <NEWLINE> <DEDENT> <DEDENT>
shark_vacs = await ayla_api . async_get_devices ( False ) <NEWLINE> <INDENT> device_names = <STRING> . join ( [ d . name for d in shark_vacs ] ) <NEWLINE> LOGGER . debug ( <STRING> , len ( shark_vacs ) , device_names ) <NEWLINE> coordinator = SharkIqUpdateCoordinator ( hass , config_entry , ayla_api , shark_vacs ) <NEWLINE> <DEDENT>
def run_validator_for_test_file ( filename : str ) -> List : <NEWLINE> <INDENT> test_file_path = os . path . join ( <NEWLINE> <INDENT> os . path . dirname ( os . path . abspath ( __file__ ) ) , <NEWLINE> <STRING> , <NEWLINE> filename , <NEWLINE> <DEDENT> ) <NEWLINE> with open ( test_file_path , <STRING> ) as file_handler : <NEWLINE> <INDENT> raw_content = file_handler . read ( ) <NEWLINE> <DEDENT> tree = ast . parse ( raw_content ) <NEWLINE> checker = SuperMarionChecker ( tree = tree , filename = test_file_path ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if self . _is_stop ( ) : <NEWLINE> <INDENT> break <NEWLINE> return xmin <NEWLINE> <DEDENT> <DEDENT>
profile , created = Profile . objects . update_or_create ( <NEWLINE> <INDENT> user = user , <NEWLINE> defaults = { <NEWLINE> <INDENT> <STRING> : slack_user [ <STRING> ] , <NEWLINE> <STRING> : slack_profile . get ( <STRING> , <STRING> ) , <NEWLINE> <DEDENT> } , <NEWLINE> ) <NEWLINE> <DEDENT>
with patch ( <STRING> ) as oauth2_mock : <NEWLINE> <INDENT> r = self . oauth_client . add ( add_data , params = { <STRING> : self . TEST_ACCESS } ) <NEWLINE> oauth2_mock . assert_called_once_with ( <STRING> , <STRING> , add_data , params = { <STRING> : self . TEST_ACCESS } ) <NEWLINE> self . assertTrue ( r ) <NEWLINE> <DEDENT>
if step < self . dihstep : <NEWLINE> <INDENT> self . set_dihedrals ( change , step , cut = 1 ) <NEWLINE> <DEDENT>
for ins in korcek_chain_filt : <NEWLINE> <INDENT> if bond [ ins [ 0 ] ] [ ins [ - 1 ] ] == 1 : <COMMENT> <NEWLINE> <INDENT> rxns += [ ins ] <NEWLINE> <DEDENT> <DEDENT>
kwargs . pop ( <STRING> , None ) <NEWLINE> <INDENT> pop = self . _population <NEWLINE> coords = pop . projection ( 2 , which , ** coords_kwargs ) <NEWLINE> return self . scatter_coords ( coords , ** scatter_kwargs ) <NEWLINE> <DEDENT>
@ functools . wraps ( func ) <NEWLINE> <INDENT> def decorated ( file , * args , ** kwargs ) : <NEWLINE> <INDENT> if isinstance ( file , str ) : <NEWLINE> <INDENT> with open ( file ) as F : <NEWLINE> <INDENT> result = func ( F , * args , ** kwargs ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return func ( file , * args , ** kwargs ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
if _centre_list [ j ] . covariance is not None : <NEWLINE> <INDENT> sensor_list [ i ] . covariance = interpolate_covariance ( <NEWLINE> <INDENT> sensor_list [ i ] . epoch_timestamp , <NEWLINE> _centre_list [ j - 1 ] . epoch_timestamp , <NEWLINE> _centre_list [ j ] . epoch_timestamp , <NEWLINE> _centre_list [ j - 1 ] . covariance , <NEWLINE> _centre_list [ j ] . covariance ) <NEWLINE> <DEDENT> <DEDENT>
for j in plotly_list : <NEWLINE> <INDENT> make_frame ( frame , <NEWLINE> <INDENT> [ j [ 0 ] , <NEWLINE> <INDENT> [ float ( k . epoch_timestamp ) for k in j [ 1 ] ] , <NEWLINE> [ float ( k . eastings ) for k in j [ 1 ] ] , <NEWLINE> [ float ( k . northings ) for k in j [ 1 ] ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( camera1_pf_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in camera1_pf_list ] , <NEWLINE> [ float ( i . eastings ) for i in camera1_pf_list ] , <NEWLINE> [ float ( i . northings ) for i in camera1_pf_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( pf_fusion_centre_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in pf_fusion_centre_list ] , <NEWLINE> [ float ( i . eastings ) for i in pf_fusion_centre_list ] , <NEWLINE> [ float ( i . northings ) for i in pf_fusion_centre_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( ekf_centre_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in ekf_centre_list ] , <NEWLINE> [ float ( i . eastings ) for i in ekf_centre_list ] , <NEWLINE> [ float ( i . northings ) for i in ekf_centre_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( pf_fusion_dvl_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> [ float ( i . epoch_timestamp ) for i in pf_fusion_dvl_list ] , <NEWLINE> [ float ( i . eastings ) for i in pf_fusion_dvl_list ] , <NEWLINE> [ float ( i . northings ) for i in pf_fusion_dvl_list ] ] , <NEWLINE> <DEDENT> i ) <NEWLINE> if len ( pf_fusion_centre_list ) > 1 : <NEWLINE> <DEDENT> make_frame ( frame , <NEWLINE> <INDENT> [ <STRING> , <NEWLINE> <INDENT> pf_timestamps_interval , <NEWLINE> pf_eastings_interval , <NEWLINE> pf_northings_interval ] , <NEWLINE> <DEDENT> i , mode = <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
if ic > best_ic : <NEWLINE> <INDENT> best_ic = ic <NEWLINE> best_model = m <NEWLINE> if ic > goal_inliers and stop_at_goal : <NEWLINE> <INDENT> break <NEWLINE> <COMMENT> <NL> best_model = fit_plane ( inliers ) <NEWLINE> return best_model , inliers , i <NEWLINE> <DEDENT> <DEDENT>
planes = [ ] <NEWLINE> <INDENT> for i in range ( 0 , self . num_iterations ) : <NEWLINE> <INDENT> point_cloud_local = random . sample ( inliers_cloud_list , cloud_sample_size ) <NEWLINE> total_no_points = len ( point_cloud_local ) <NEWLINE> p = Plane ( [ 1 , 0 , 0 , 1.5 ] ) <NEWLINE> m = p . fit_non_robust ( point_cloud_local ) <NEWLINE> <STRING> <NEWLINE> angle , pitch , yaw = get_angles ( m [ 0 : 3 ] ) <NEWLINE> planes . append ( [ angle , pitch , yaw ] ) <NEWLINE> Console . progress ( i , self . num_iterations , prefix = <STRING> ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if isinstance ( epsilon , numbers . Number ) : <COMMENT> <NEWLINE> <INDENT> self . epsilon_fitted = epsilon <NEWLINE> return self <NEWLINE> <DEDENT> elif epsilon == <STRING> : <COMMENT> <NEWLINE> <INDENT> if ( self . metric != <STRING> ) : <COMMENT> <NEWLINE> <INDENT> warnings . warn ( <STRING> % self . metric ) <NEWLINE> <DEDENT> if self . scaled_dists is None : <NEWLINE> <INDENT> self . scaled_dists = self . _get_scaled_distance_mat ( self . data , self . bandwidths ) <NEWLINE> <DEDENT> self . epsilon_fitted , self . d = choose_optimal_epsilon_BGH ( self . scaled_dists . data ** 2 ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> % epsilon ) <NEWLINE> <DEDENT> return self <NEWLINE> <DEDENT>
def get_devices ( self , location ) : <NEWLINE> <INDENT> response_data = self . __call_smart_system_get ( <NEWLINE> <INDENT> <STRING> <NEWLINE> <DEDENT> ) <NEWLINE> if len ( response_data [ <STRING> ] [ <STRING> ] [ <STRING> ] [ <STRING> ] ) < 1 : <NEWLINE> <INDENT> self . logger . error ( <STRING> ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> devices_smart_system = { } <NEWLINE> for device in response_data [ <STRING> ] : <NEWLINE> <INDENT> real_id = device [ <STRING> ] . split ( <STRING> ) [ 0 ] <NEWLINE> if real_id not in devices_smart_system : <NEWLINE> <INDENT> devices_smart_system [ real_id ] = { } <NEWLINE> <DEDENT> if ( <NEWLINE> <INDENT> device [ <STRING> ] in self . supported_services <NEWLINE> and device [ <STRING> ] not in devices_smart_system [ real_id ] <NEWLINE> <DEDENT> ) : <NEWLINE> <INDENT> devices_smart_system [ real_id ] [ device [ <STRING> ] ] = [ ] <NEWLINE> <DEDENT> devices_smart_system [ real_id ] [ device [ <STRING> ] ] . append ( device ) <NEWLINE> <DEDENT> for parsed_device in devices_smart_system . values ( ) : <NEWLINE> <INDENT> location . add_device ( DeviceFactory . build ( self , parsed_device ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def try_to_create ( self ) : <NEWLINE> <INDENT> limit = datetime . now ( ) - timedelta ( seconds = self . settings [ <STRING> ] ) <NEWLINE> if <STRING> not in self . last_snapshot or self . last_snapshot [ <STRING> ] <= limit : <NEWLINE> <INDENT> self . create ( ) <NEWLINE> <DEDENT> <DEDENT>
class PubsubQueue ( object ) : <NEWLINE> <INDENT> def __init__ ( self , queue_name , sub_name = None , verbose = 10 ) : <NEWLINE> <INDENT> assert <STRING> in os . environ . keys ( ) <NEWLINE> self . client = pubsub . Client ( ) <NEWLINE> self . topic = self . client . topic ( queue_name ) <NEWLINE> self . logger = logging . getLogger ( self . __class__ . __name__ ) <NEWLINE> self . logger . setLevel ( verbose ) <NEWLINE> sub_name = sub_name if sub_name else queue_name + <STRING> <NEWLINE> self . logger . info ( <STRING> . format ( queue_name ) ) <NEWLINE> self . logger . info ( <STRING> . format ( sub_name ) ) <NEWLINE> if queue_name not in [ t . name for t in self . client . list_topics ( ) ] : <NEWLINE> <INDENT> self . topic . create ( ) <NEWLINE> self . logger . info ( <STRING> . format ( queue_name ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if not n_records or ( not average and <NEWLINE> <INDENT> n_records < records_per_capture ) : <NEWLINE> time . sleep ( 1e-6 ) <NEWLINE> continue <NEWLINE> <DEDENT> if not get_data ( cu , id_ , buffers_ptr , <NEWLINE> <INDENT> n_records * samples_per_record , <NEWLINE> bytes_per_sample , <NEWLINE> retrieved_records , <NEWLINE> n_records , <NEWLINE> mask , <NEWLINE> 0 , <NEWLINE> samples_per_record , <NEWLINE> 0x00 ) : <NEWLINE> del avg , buffers <NEWLINE> self . _dll . DisarmTrigger ( self . _cu_id , self . _id ) <NEWLINE> self . _dll . MultiRecordClose ( self . _cu_id , self . _id ) <NEWLINE> self . close_connection ( ) <NEWLINE> self . _setup_library ( ) <NEWLINE> self . open_connection ( ) <NEWLINE> self . configure_board ( ) <NEWLINE> if retry : <NEWLINE> return self . get_traces ( channels , duration , delay , <NEWLINE> <INDENT> records_per_capture , False ) <NEWLINE> else : <NEWLINE> msg = <STRING> <NEWLINE> raise RuntimeError ( msg ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
alignment = calculate_alignment ( orient_roi ) <NEWLINE>
if downsample : <NEWLINE> <INDENT> fixed_shrunk = trans . resize_image ( fixed_image , fixed_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> rotated_shrunk = trans . resize_image ( rotated_image , fixed_image . GetSpacing ( ) [ 0 ] , downsample_target ) <NEWLINE> spacing = fixed_shrunk . GetSpacing ( ) <NEWLINE> <DEDENT>
if roi_size is None : <NEWLINE> <INDENT> with open ( output_path , <STRING> , newline = <STRING> ) as csvfile : <NEWLINE> <INDENT> print ( <STRING> . format ( <NEWLINE> <INDENT> ret_image_path . name , tile_size [ 0 ] ) ) <NEWLINE> <DEDENT> writer = csv . writer ( csvfile ) <NEWLINE> writer . writerow ( [ <STRING> , <STRING> , <STRING> , <STRING> , <NEWLINE> <INDENT> <STRING> , <STRING> , <STRING> ] ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
def fieldStrAndPer ( d ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> l1 = [ ] <NEWLINE> lper = [ ] <NEWLINE> l2 = [ ] <NEWLINE> for k , v in d . items ( ) : <NEWLINE> <INDENT> if v != None : <NEWLINE> <INDENT> l1 . append ( k ) <NEWLINE> noAppend = True <COMMENT> <NEWLINE> if isinstance ( v , str ) : <NEWLINE> <INDENT> if v . startswith ( <STRING> ) or v . startswith ( <STRING> ) or v . startswith ( <STRING> ) or v . startswith ( <STRING> ) : <NEWLINE> <INDENT> vv = dataToFloat ( v [ 1 : ] ) <NEWLINE> if vv : <NEWLINE> <INDENT> noAppend = False <NEWLINE> lper . append ( <STRING> . format ( k , v [ : 1 ] ) ) <NEWLINE> l2 . append ( vv ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> if noAppend : <NEWLINE> <INDENT> lper . append ( <STRING> ) <NEWLINE> l2 . append ( dataToStr ( v ) ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
while True : <NEWLINE> <COMMENT> <NL> <COMMENT> <NL> <INDENT> next_dt = pd . Timestamp ( next_dt . replace ( tzinfo = None ) ) <NEWLINE> next_dt = next_dt + interval <NEWLINE> next_dt = pd . Timestamp ( next_dt , tz = trading . environment . exchange_tz ) <NEWLINE> next_dt_utc = next_dt . tz_convert ( <STRING> ) <NEWLINE> if trading . environment . is_market_hours ( next_dt_utc ) : <NEWLINE> <INDENT> break <NEWLINE> <DEDENT> next_dt = next_dt_utc . tz_convert ( trading . environment . exchange_tz ) <NEWLINE> <DEDENT>
def create_test_df_source ( sim_params = None , bars = <STRING> ) : <NEWLINE> <INDENT> if bars == <STRING> : <NEWLINE> <INDENT> freq = pd . datetools . BDay ( ) <NEWLINE> <DEDENT> elif bars == <STRING> : <NEWLINE> <INDENT> freq = pd . datetools . Minute ( ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> raise ValueError ( <STRING> % bars ) <NEWLINE> <DEDENT> <DEDENT>
out_x_res_y = out_file_h5 . createCArray ( out_file_h5 . root , <NEWLINE> <INDENT> name = <STRING> % ( actual_dut ) , <NEWLINE> title = <STRING> % ( actual_dut ) , <NEWLINE> atom = tb . Atom . from_dtype ( hist_x_residual_y_hist . dtype ) , <NEWLINE> shape = hist_x_residual_y_hist . shape , <NEWLINE> filters = tb . Filters ( complib = <STRING> , complevel = 5 , fletcher32 = False ) ) <NEWLINE> out_x_res_y . attrs . xedges = hist_x_residual_y_xedges <NEWLINE> out_x_res_y . attrs . yedges = hist_x_residual_y_yedges <NEWLINE> out_x_res_y . attrs . fit_coeff = fit_x_residual_y [ 0 ] <NEWLINE> out_x_res_y . attrs . fit_cov = fit_x_residual_y [ 1 ] <NEWLINE> out_x_res_y [ : ] = hist_x_residual_y_hist <NEWLINE> <DEDENT>
out_col_res_row = out_file_h5 . createCArray ( out_file_h5 . root , <NEWLINE> <INDENT> name = <STRING> % ( actual_dut ) , <NEWLINE> title = <STRING> % ( actual_dut ) , <NEWLINE> atom = tb . Atom . from_dtype ( hist_col_residual_row_hist . dtype ) , <NEWLINE> shape = hist_col_residual_row_hist . shape , <NEWLINE> filters = tb . Filters ( complib = <STRING> , complevel = 5 , fletcher32 = False ) ) <NEWLINE> out_col_res_row . attrs . xedges = hist_col_residual_row_xedges <NEWLINE> out_col_res_row . attrs . yedges = hist_col_residual_row_yedges <NEWLINE> out_col_res_row . attrs . fit_coeff = fit_col_residual_row [ 0 ] <NEWLINE> out_col_res_row . attrs . fit_cov = fit_col_residual_row [ 1 ] <NEWLINE> out_col_res_row [ : ] = hist_col_residual_row_hist <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> n_slices = cpu_count ( ) <NEWLINE> slices = np . array_split ( track_hits , n_slices ) <NEWLINE> results = pool . map ( _fit_tracks_loop , slices ) <NEWLINE> del track_hits <NEWLINE> <DEDENT>
if not reference_hit_set and not np . isnan ( tr_column [ track_index ] [ dut_index ] ) : <COMMENT> <NEWLINE>
<COMMENT> <NL> <COMMENT> <NL> <INDENT> def loop_single_entry ( cfg ) : <NEWLINE> <INDENT> for v , _ in cfg . iter_nodes ( ) : <NEWLINE> <INDENT> if cfg . degree_in ( v ) >= 2 : <NEWLINE> <INDENT> preds = cfg . pred ( v ) <NEWLINE> back_preds = list ( filter ( lambda x : v <= x , preds ) ) <NEWLINE> if len ( back_preds ) < 2 : <NEWLINE> <INDENT> continue <NEWLINE> <DEDENT> print ( <STRING> , v ) <NEWLINE> print ( <STRING> , back_preds ) <NEWLINE> back_jumps = list ( filter ( lambda x : cfg . degree_out ( x ) == 1 , back_preds ) ) <NEWLINE> print ( <STRING> , back_jumps ) <NEWLINE> <COMMENT> <NL> landing_site = None <NEWLINE> for p in back_jumps : <NEWLINE> <INDENT> b = cfg . node ( p ) <NEWLINE> if not b . items : <NEWLINE> <INDENT> landing_site = p <NEWLINE> <DEDENT> <DEDENT> if not landing_site : <NEWLINE> <INDENT> farthest = max ( back_preds ) <NEWLINE> print ( <STRING> , farthest ) <NEWLINE> newb = BBlock ( farthest + <STRING> ) <NEWLINE> cfg . add_node ( newb . addr , newb ) <NEWLINE> cfg . add_edge ( newb . addr , v ) <NEWLINE> landing_site = newb . addr <NEWLINE> <DEDENT> print ( <STRING> , landing_site ) <NEWLINE> for p in back_preds : <NEWLINE> <INDENT> if p != landing_site : <NEWLINE> <INDENT> e = cfg . edge ( p , v ) <NEWLINE> cfg . remove_edge ( p , v ) <NEWLINE> cfg . add_edge ( p , landing_site , e ) <NEWLINE> <DEDENT> <DEDENT> return True <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
if self . train_with_fake_labels : <NEWLINE> <INDENT> x_train = np . reshape ( train_data . X , newshape = ( - 1 , * self . x_dim ) ) <NEWLINE> x = [ x_train , train_labels , pseudo_labels ] <NEWLINE> y = [ x_train , train_labels ] <NEWLINE> else : <NEWLINE> x_train = np . reshape ( train_data . X , newshape = ( - 1 , * self . x_dim ) ) <NEWLINE> x = [ x_train , train_labels , train_labels ] <NEWLINE> y = [ x_train , train_labels ] <NEWLINE> <DEDENT>
print ( train_data . shape , valid_data . shape ) <NEWLINE>
cell_type_data = train [ train . obs [ cell_type_key ] == cell_type ] <NEWLINE> <INDENT> cell_type_ctrl_data = train [ ( ( train . obs [ cell_type_key ] == cell_type ) & ( train . obs [ <STRING> ] == ctrl_key ) ) ] <NEWLINE> pred = network . predict ( cell_type_ctrl_data , <NEWLINE> <INDENT> encoder_labels = np . zeros ( ( cell_type_ctrl_data . shape [ 0 ] , 1 ) ) , <NEWLINE> decoder_labels = np . ones ( ( cell_type_ctrl_data . shape [ 0 ] , 1 ) ) ) <NEWLINE> <DEDENT> <DEDENT>
source_images_train = train_data [ train_data . obs [ <STRING> ] == source_key ] . X <NEWLINE> <INDENT> source_images_valid = valid_data [ valid_data . obs [ <STRING> ] == source_key ] . X <NEWLINE> <DEDENT>
for gene in top_100_genes [ : 3 ] : <NEWLINE> <INDENT> sc . pl . violin ( pred_adatas , keys = gene , groupby = condition_key , <NEWLINE> <INDENT> save = <STRING> , <NEWLINE> show = False , <NEWLINE> wspace = 0.2 , <NEWLINE> rotation = 90 , <NEWLINE> frameon = False ) <NEWLINE> <DEDENT> <DEDENT>
updates = kwargs <NEWLINE> <INDENT> persistence_converter = self . get_persistence_converter ( self . engine_name ) <NEWLINE> if persistence_converter is not None : <NEWLINE> <INDENT> updates = persistence_converter ( updates ) <NEWLINE> <DEDENT> self . engine . update ( primary_index , patch , context , updates ) <NEWLINE> self . get_data ( ) <NEWLINE> <DEDENT>
for subdir in subdirs : <NEWLINE> <INDENT> self . _fetch_files ( subdir , file_list ) <NEWLINE> <DEDENT>
outputFastaPath = os . path . join ( options . outputPath , outputFastaName ) <NEWLINE> <INDENT> if os . path . isfile ( outputFastaPath ) : <NEWLINE> <INDENT> print ( <STRING> % locus_name ) <NEWLINE> continue <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if issubclass ( bt . Strategy , strategy ) : <NEWLINE> <INDENT> strat_name = str ( strategy ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> strat_name = strategy <NEWLINE> strategy = STRATEGY_MAPPING [ strategy ] <NEWLINE> <DEDENT> <DEDENT>
for env_var in env_vars : <NEWLINE> <INDENT> def_value = config . instance [ section ] . get ( env_var , None ) <NEWLINE> value = click . prompt ( <STRING> , default = def_value ) <NEWLINE> config . instance . set ( section , env_var , value ) <NEWLINE> <DEDENT>
def write_report ( df_map , outFH , pcr_volume , mm_volume , <NEWLINE> <INDENT> fp_tube , fp_volume , rp_tube , rp_volume , <NEWLINE> n_rxn_reps , error_perc = 10.0 ) : <NEWLINE> <STRING> <NEWLINE> <COMMENT> <NL> n_rxn = df_map . shape [ 0 ] <NEWLINE> <COMMENT> <NL> total_pcr_volume = pcr_volume * n_rxn <NEWLINE> <COMMENT> <NL> total_mm_volume = mm_volume * n_rxn <NEWLINE> <COMMENT> <NL> if fp_tube > 0 and fp_volume > 0 : <NEWLINE> total_fp_volume = fp_volume * n_rxn <NEWLINE> else : <NEWLINE> total_fp_volume = None <NEWLINE> if rp_tube > 0 and rp_volume > 0 : <NEWLINE> total_rp_volume = rp_volume * n_rxn <NEWLINE> else : <NEWLINE> total_rp_volume = None <NEWLINE> <COMMENT> <NL> total_water_volume = sum ( df_map [ <STRING> ] ) <NEWLINE> <DEDENT>
except Exception as e : <NEWLINE> <COMMENT> <NL> <INDENT> result [ settings . UNKNOWN_LANGUAGE ] += paragraph <NEWLINE> logger . info ( <STRING> . format ( paragraph ) ) <NEWLINE> <DEDENT>
def list_run ( <NEWLINE> <INDENT> limit , activity_types , output , unique , include_glob , exclude_glob , <NEWLINE> under , relative , title , null , ** _ ) : <NEWLINE> <STRING> <NEWLINE> separator = <STRING> if null else <STRING> <NEWLINE> absunder = [ os . path . join ( os . path . abspath ( p ) , <STRING> ) for p in under ] <NEWLINE> include_glob += [ os . path . join ( p , <STRING> ) for p in absunder ] <NEWLINE> db = get_db ( ) <NEWLINE> paths = showpaths = list ( db . list_file_path ( <NEWLINE> limit , activity_types , unique , include_glob , exclude_glob ) ) <NEWLINE> if relative : <NEWLINE> showpaths = [ remove_prefix ( absunder , p ) for p in paths ] <NEWLINE> if title : <NEWLINE> from . filetitle import write_paths_and_titles <NEWLINE> write_paths_and_titles ( output , paths , showpaths , separator ) <NEWLINE> else : <NEWLINE> output . writelines ( interleave ( showpaths , itertools . repeat ( separator ) ) ) <NEWLINE> if output is not sys . stdout : <NEWLINE> output . close ( ) <NEWLINE> <DEDENT>
dehydrator = self . dehydrator_cls ( fields = self . fields ) <NEWLINE> <INDENT> if self . is_iterable : <NEWLINE> <INDENT> return map ( dehydrator . dehydrate , target ) <NEWLINE> <DEDENT> else : <NEWLINE> <INDENT> return dehydrator . dehydrate ( target ) <NEWLINE> <DEDENT> <DEDENT>
def rule ( self , <NEWLINE> <INDENT> ruleid , ForEach = None , ResultMod = None , Exists = None , Min = 0 , Max = None , <NEWLINE> Aggregates = None , IsLive = None , <NEWLINE> Command = None , Response = None , <NEWLINE> Show = None , Pass = <STRING> , Fail = <STRING> , NoMatch = <STRING> ) : <NEWLINE> rule = rdflib . URIRef ( ruleid , base = self . _base ) <NEWLINE> if ForEach : <NEWLINE> ruletype = MINIM . QueryTestRule <NEWLINE> querynode = rdflib . BNode ( ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . query , querynode ) ) <NEWLINE> self . _minimgr . add ( ( querynode , MINIM . sparql_query , rdflib . Literal ( ForEach ) ) ) <NEWLINE> if ResultMod : <NEWLINE> <INDENT> self . _minimgr . add ( ( querynode , MINIM . result_mod , rdflib . Literal ( ResultMod ) ) ) <NEWLINE> if Exists : <NEWLINE> existsnode = rdflib . BNode ( ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . exists , existsnode ) ) <NEWLINE> self . _minimgr . add ( ( existsnode , MINIM . sparql_query , rdflib . Literal ( Exists ) ) ) <NEWLINE> if Min : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . min , rdflib . Literal ( Min ) ) ) <NEWLINE> if Max : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . max , rdflib . Literal ( Max ) ) ) <NEWLINE> if Aggregates : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . aggregatesTemplate , rdflib . Literal ( Aggregates ) ) ) <NEWLINE> if IsLive : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . isLiveTemplate , rdflib . Literal ( IsLive ) ) ) <NEWLINE> elif Exists : <NEWLINE> ruletype = MINIM . QueryTestRule <NEWLINE> existsnode = rdflib . BNode ( ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . exists , existsnode ) ) <NEWLINE> self . _minimgr . add ( ( existsnode , MINIM . sparql_query , rdflib . Literal ( Exists ) ) ) <NEWLINE> elif Command : <NEWLINE> ruletype = MINIM . SoftwareEnvironmentRule <NEWLINE> self . _minimgr . add ( ( rule , MINIM . command , rdflib . Literal ( Command ) ) ) <NEWLINE> self . _minimgr . add ( ( rule , MINIM . response , rdflib . Literal ( Response ) ) ) <NEWLINE> else : <NEWLINE> raise ValueError ( <STRING> ) <NEWLINE> self . _minimgr . add ( ( rule , RDF . type , ruletype ) ) <NEWLINE> if Show : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . show , rdflib . Literal ( Show ) ) ) <NEWLINE> if Pass : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . showpass , rdflib . Literal ( Pass ) ) ) <NEWLINE> if Fail : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . showfail , rdflib . Literal ( Fail ) ) ) <NEWLINE> if NoMatch : <NEWLINE> self . _minimgr . add ( ( rule , MINIM . showmiss , rdflib . Literal ( NoMatch ) ) ) <NEWLINE> return rule <NEWLINE> <DEDENT> <DEDENT>
def dumpPkl ( obj , path ) : <NEWLINE> <INDENT> with open ( path , <STRING> ) as pf : <NEWLINE> <INDENT> pickle . dump ( obj , pf ) <NEWLINE> <DEDENT> <DEDENT>
if additional_params is not None : <NEWLINE> <INDENT> for param_field , param_collapsed in zip ( additional_params , additional_collapsed ) : <NEWLINE> <INDENT> param_collapsed . shape = ( Ncubes * collapse_channels , ) + param_collapsed . shape [ 2 : ] <NEWLINE> setattr ( self , param_field , param_collapsed ) <NEWLINE> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> for index , filepath in enumerate ( filepaths ) : <NEWLINE> <INDENT> with fits . open ( filepath , lazy_load_hdus = False ) as hdulist : <NEWLINE> <INDENT> cube = hdulist [ 1 ] . data <NEWLINE> prihdr = hdulist [ 0 ] . header <NEWLINE> exthdr = hdulist [ 1 ] . header <NEWLINE> w = wcs . WCS ( header = prihdr , naxis = [ 1 , 2 ] ) <NEWLINE> astr_hdrs = [ w . deepcopy ( ) for _ in range ( cube . shape [ 0 ] ) ] <COMMENT> <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if <STRING> in mode : <NEWLINE> <INDENT> if psf_library is None : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if psf_library . dataset is dataset : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> if aligned_center is not None : <NEWLINE> <INDENT> if not np . array_equal ( aligned_center , psf_library . aligned_center ) : <NEWLINE> <INDENT> raise ValueError ( <STRING> ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT> <DEDENT>
<COMMENT> <NL> <INDENT> if maskout_edge is not None : <NEWLINE> <INDENT> IWA , OWA , inner_mask , outer_mask = get_occ ( image_cpy , centroid = ( center [ 0 ] [ 0 ] + stamp_size // 2 , center [ 0 ] [ 1 ] + stamp_size // 2 ) ) <NEWLINE> conv_kernel = np . ones ( ( maskout_edge , maskout_edge ) ) <NEWLINE> flat_cube_wider_mask = convolve2d ( outer_mask , conv_kernel , mode = <STRING> ) <NEWLINE> image_cpy [ np . where ( np . isnan ( flat_cube_wider_mask ) ) ] = np . nan <NEWLINE> <DEDENT> <DEDENT>
def amount_as_string ( self , min_decimals : Optional [ int ] = None , max_decimals : Optional [ int ] = None ) -> str : <NEWLINE> <INDENT> if min_decimals is None and max_decimals is None : <NEWLINE> <INDENT> if self . _currency and isinstance ( self . _currency , BaseCurrency ) : <NEWLINE> <INDENT> min_decimals = self . _currency . decimal_digits <NEWLINE> <DEDENT> min_decimals = DEFAULT_MIN_DECIMALS if min_decimals is None else min_decimals <NEWLINE> max_decimals = max ( cast ( int , min_decimals ) , DEFAULT_MAX_DECIMALS ) <NEWLINE> <DEDENT> elif min_decimals is None : <NEWLINE> <INDENT> if self . _currency and isinstance ( self . _currency , BaseCurrency ) : <NEWLINE> <INDENT> min_decimals = self . _currency . decimal_digits <NEWLINE> <DEDENT> min_decimals = DEFAULT_MIN_DECIMALS if min_decimals is None else min_decimals <NEWLINE> min_decimals = min ( cast ( int , min_decimals ) , max_decimals ) <NEWLINE> <DEDENT> elif max_decimals is None : <NEWLINE> <INDENT> max_decimals = max ( min_decimals , DEFAULT_MAX_DECIMALS ) <NEWLINE> <DEDENT> <DEDENT>
if <STRING> in content : <NEWLINE> <INDENT> err = ErrorSchema ( ) . load ( content [ <STRING> ] ) <NEWLINE> text = <STRING> if err [ <STRING> ] else err [ <STRING> ] <NEWLINE> raise ErrorResponse ( text ) <NEWLINE> <DEDENT>
self . assertEqual ( len ( feed . entries ) , len ( datasets ) ) <NEWLINE> <INDENT> for i in range ( 1 , len ( feed . entries ) ) : <NEWLINE> <INDENT> published_date = feed . entries [ i ] . published_parsed <NEWLINE> prev_published_date = feed . entries [ i - 1 ] . published_parsed <NEWLINE> self . assertGreaterEqual ( prev_published_date , published_date ) <NEWLINE> <DEDENT> <DEDENT>
@ ns . route ( <STRING> , endpoint = <STRING> ) <NEWLINE> <INDENT> class DatasetBadgesAPI ( API ) : <NEWLINE> <INDENT> @ api . doc ( <STRING> , ** common_doc ) <NEWLINE> @ api . expect ( badge_fields ) <NEWLINE> @ api . marshal_with ( badge_fields ) <NEWLINE> @ api . secure ( admin_permission ) <NEWLINE> def post ( self , dataset ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> form = api . validate ( BadgeForm ) <NEWLINE> badge = DatasetBadge ( created = datetime . now ( ) , <NEWLINE> <INDENT> created_by = current_user . id ) <NEWLINE> <DEDENT> form . populate_obj ( badge ) <NEWLINE> for existing_badge in dataset . badges : <NEWLINE> <INDENT> if existing_badge . kind == badge . kind : <NEWLINE> <INDENT> return existing_badge <NEWLINE> <DEDENT> <DEDENT> dataset . add_badge ( badge ) <NEWLINE> return badge , 201 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
@ ns . route ( <STRING> , endpoint = <STRING> ) <NEWLINE> <INDENT> class ReuseDatasetsAPI ( API ) : <NEWLINE> <INDENT> @ api . secure <NEWLINE> @ api . doc ( <STRING> , ** common_doc ) <NEWLINE> @ api . expect ( dataset_ref_fields ) <NEWLINE> @ api . response ( 200 , <STRING> , reuse_fields ) <NEWLINE> @ api . marshal_with ( reuse_fields , code = 201 ) <NEWLINE> def post ( self , reuse ) : <NEWLINE> <INDENT> <STRING> <NEWLINE> if <STRING> not in request . json : <NEWLINE> <INDENT> api . abort ( 400 , <STRING> ) <NEWLINE> <DEDENT> try : <NEWLINE> <INDENT> dataset = Dataset . objects . get_or_404 ( id = request . json [ <STRING> ] ) <NEWLINE> <DEDENT> except Dataset . DoesNotExist : <NEWLINE> <INDENT> api . abort ( 404 , <STRING> . format ( request . json [ <STRING> ] ) ) <NEWLINE> <DEDENT> if dataset in reuse . datasets : <NEWLINE> <INDENT> return reuse <NEWLINE> <DEDENT> reuse . datasets . append ( dataset ) <NEWLINE> reuse . save ( ) <NEWLINE> return reuse , 201 <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
assert type ( cleaned_options [ <STRING> ] . ttl ) is int <NEWLINE> <INDENT> cleaned_options [ <STRING> ] = cleaned_options [ <STRING> ] . ttl <NEWLINE> <DEDENT>
def crawl ( url ) : <NEWLINE> <INDENT> items = requests . get ( url , headers = lt . DEFAULT_HEADERS ) . json ( ) <NEWLINE> for item in items : <NEWLINE> <INDENT> data = { } <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> data [ <STRING> ] = item [ <STRING> ] <NEWLINE> pprint ( data ) <NEWLINE> total . append ( data ) <NEWLINE> <DEDENT> <DEDENT>
@ click . command ( help = help_text ) <NEWLINE> <INDENT> @ click . option ( <STRING> , <STRING> , type = int , default = 200 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> , ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , default = 3.0 , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , type = float , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . option ( <STRING> , <STRING> , is_flag = True ) <NEWLINE> @ click . option ( <STRING> , is_flag = True , <NEWLINE> <INDENT> help = <STRING> ) <NEWLINE> <DEDENT> @ click . argument ( <STRING> , type = click . Path ( exists = True , file_okay = False ) ) <NEWLINE> def cli ( filter_level , max_unknowns , c_deviations , s_deviations , m_deviations , <NEWLINE> <INDENT> dry_run , species , path ) : <NEWLINE> if species : <NEWLINE> from genbankqc import Species <NEWLINE> try : <NEWLINE> <INDENT> s = Species ( path , max_unknowns , c_deviations , s_deviations , <NEWLINE> <INDENT> m_deviations ) <NEWLINE> <DEDENT> s . qc ( ) <NEWLINE> print ( <STRING> , s . species ) <NEWLINE> print ( s ) <NEWLINE> <DEDENT> except Exception : <NEWLINE> <INDENT> print ( <STRING> , s . species ) <NEWLINE> traceback . print_exc ( ) <NEWLINE> else : <NEWLINE> <DEDENT> from genbankqc import Genbank <NEWLINE> genbank = Genbank ( path ) <NEWLINE> genbank . qc ( ) <NEWLINE> <DEDENT> <DEDENT>
def eq_contents ( path , text ) : <NEWLINE> <INDENT> with open ( path ) as fd : <NEWLINE> <INDENT> eq_ ( text , fd . read ( ) ) <NEWLINE> <DEDENT> <DEDENT>
global MASK_PREDICTOR_HANDLER <NEWLINE> <INDENT> with LOCK : <NEWLINE> <INDENT> if MASK_PREDICTOR_HANDLER is None : <NEWLINE> <INDENT> MASK_PREDICTOR_HANDLER = MaskPredictor ( boxSize , deepLearningModel , gpus ) <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
class KademliaProtocol ( RPCProtocol ) : <NEWLINE> <INDENT> def __init__ ( self , sourceNode , storage , ksize ) : <NEWLINE> <INDENT> RPCProtocol . __init__ ( self ) <NEWLINE> self . router = RoutingTable ( self , ksize , sourceNode ) <NEWLINE> self . storage = storage <NEWLINE> self . sourceID = sourceNode . id <NEWLINE> self . log = Logger ( system = self ) <NEWLINE> <DEDENT> <DEDENT>
def verified_email_required ( function = None , <NEWLINE> <INDENT> login_url = None , <NEWLINE> redirect_field_name = REDIRECT_FIELD_NAME ) : <NEWLINE> <STRING> <NEWLINE> def decorator ( view_func ) : <NEWLINE> @ login_required ( redirect_field_name = redirect_field_name , <NEWLINE> login_url = login_url ) <NEWLINE> def _wrapped_view ( request , * args , ** kwargs ) : <NEWLINE> if not EmailAddress . objects . filter ( user = request . user , <NEWLINE> <INDENT> verified = True ) . exists ( ) : <NEWLINE> send_email_confirmation ( request , request . user ) <NEWLINE> return render ( request , <NEWLINE> <STRING> ) <NEWLINE> return view_func ( request , * args , ** kwargs ) <NEWLINE> return _wrapped_view <NEWLINE> <DEDENT> <DEDENT>
def render_to_string ( template_name , dictionary , context_instance = None ) : <NEWLINE> <INDENT> context_instance = context_instance or Context ( dictionary ) <NEWLINE> <COMMENT> <NL> context_instance . update ( dictionary or { } ) <NEWLINE> <COMMENT> <NL> context_dictionary = { } <NEWLINE> for d in context_instance : <NEWLINE> <INDENT> context_dictionary . update ( d ) <NEWLINE> <COMMENT> <NL> <DEDENT> template = middleware . lookup . get_template ( template_name ) <NEWLINE> return template . render ( ** context_dictionary ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> if result > - 1 : <NEWLINE> <INDENT> tmp = result <NEWLINE> result = - 1 <NEWLINE> return str ( tmp ) <NEWLINE> <DEDENT> return <STRING> <NEWLINE> <DEDENT>
@ app . task ( bind = True , queue = <STRING> ) <NEWLINE> <INDENT> def exec_operator ( self , model , job_name ) : <NEWLINE> <INDENT> result = None <NEWLINE> wf = json2model ( model ) <NEWLINE> op_node = wf . spec . get ( <STRING> , { } ) . get ( job_name , { } ) <NEWLINE> if op_node : <NEWLINE> <INDENT> router = Router ( wf ) <NEWLINE> result = router . route ( wf , job_name , op_node , op_node [ <STRING> ] , op_node [ <STRING> ] ) <NEWLINE> wf . set_result ( job_name , result ) <NEWLINE> <DEDENT> return result <NEWLINE> <DEDENT> <DEDENT>
if <STRING> in translated_key : <NEWLINE> <INDENT> translated_value = reduce ( lambda i , acc : ( acc , i ) , reversed ( translated_key . split ( <STRING> ) [ 1 : ] + [ translated_value ] ) ) <NEWLINE> translated_key = translated_key . split ( <STRING> ) [ 0 ] <NEWLINE> elif translated_key == <STRING> : <NEWLINE> return key , value <NEWLINE> return translated_key , translated_value <NEWLINE> <DEDENT>
message ( <STRING> + output_file_geojson ) <NEWLINE> <INDENT> output = open ( output_file_geojson , <STRING> ) <NEWLINE> output . write ( geojson_str_pretty ) <NEWLINE> output . close ( ) <NEWLINE> <DEDENT>
@ staticmethod <NEWLINE> <INDENT> def _check_and_parse_output_plugin ( string ) : <NEWLINE> <INDENT> try : <NEWLINE> <INDENT> raw_ouputs = string . split ( <STRING> ) <NEWLINE> outputs = [ ] <NEWLINE> for output in raw_ouputs : <NEWLINE> <INDENT> name , arguments = output . split ( <STRING> ) <NEWLINE> outputs . append ( ( name , arguments ) ) <NEWLINE> <DEDENT> if len ( outputs ) == 0 : <NEWLINE> <INDENT> raise ValueError ( ) <NEWLINE> <DEDENT> <DEDENT> except ValueError : <NEWLINE> <INDENT> raise RuntimeError ( <STRING> ) <NEWLINE> <DEDENT> return outputs <NEWLINE> <DEDENT> <DEDENT>
if biggest_centroid is None : <NEWLINE> <INDENT> biggest_centroid = compute_centroid ( points ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> temp_path = datasetDoc_uri . split ( <STRING> ) <NEWLINE> problemDoc_uri = <STRING> . join ( temp_path [ : - 2 ] ) + <STRING> + <STRING> . join ( temp_path [ - 2 : ] ) . replace ( <STRING> , <STRING> ) <NEWLINE> <DEDENT>
<COMMENT> <NL> <INDENT> G_largest = [ 0 ] <NEWLINE> components = np . zeros ( len ( G ) , dtype = int ) <NEWLINE> for i , connected_component in enumerate ( nx . connected_components ( G ) ) : <NEWLINE> <COMMENT> <NL> <INDENT> if len ( connected_component ) > len ( G_largest ) : <NEWLINE> <COMMENT> <NL> <INDENT> G_largest = connected_component <NEWLINE> <COMMENT> <NL> <DEDENT> temp_indices = [ i for i , x in enumerate ( nodeIDs ) <NEWLINE> <INDENT> if x in list ( connected_component ) ] <NEWLINE> <DEDENT> components [ temp_indices ] = i + 1 <NEWLINE> <DEDENT> <DEDENT>
return DictUtils . get_required_value ( context , name , ** kwargs ) <NEWLINE> <INDENT> else : <NEWLINE> <INDENT> if default_value is not None : <NEWLINE> <INDENT> kwargs [ <STRING> ] = default_value <NEWLINE> <DEDENT> <DEDENT> <DEDENT>
type_definition = ( type_name + skip ( equals ) + type_ + skip ( finished ) ) >> _make_type_definition <NEWLINE>
structural_type_attr = ( attr_name + skip ( colon ) + explicit_type ) >> tuple <NEWLINE> <INDENT> structural_type_attrs = many ( structural_type_attr ) <NEWLINE> <DEDENT>
if not siz == <STRING> or not siz . isdecimal ( ) : <NEWLINE> <INDENT> print ( <STRING> ) <NEWLINE> return <STRING> <NEWLINE> <DEDENT>
for item in value : <NEWLINE> <INDENT> self . validate ( item ) <NEWLINE> old_val = copy . deepcopy ( self . value ) <NEWLINE> sync_val = { } <NEWLINE> self . value . clear ( ) <NEWLINE> self . value . extend ( value ) <NEWLINE> self . document . update_sync ( name , old_val ) <NEWLINE> <DEDENT>
verify_group . add_argument ( <STRING> , <STRING> , type = str , <NEWLINE> <INDENT> dest = <STRING> , <NEWLINE> help = <STRING> ) <NEWLINE> <DEDENT>
